{
    "Which of these versions of the Old Testament typically contains the most books?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "catholic"
            ],
            "lines": [
                [
                    0.20233100233100232,
                    0.19471153846153846,
                    0.26425352567701627,
                    0.2728832663744709,
                    0.6075910621365167,
                    0.5632883862548934,
                    0.5517057569296375,
                    0.35092567007460623,
                    0.28238038277511956,
                    0.562358276643991,
                    0.5258620689655172,
                    0.3354817058408883,
                    0.7532467532467533,
                    0.6276595744680851,
                    -1.0
                ],
                [
                    0.37505827505827505,
                    0.4326923076923077,
                    0.5909460673959568,
                    0.14621734367712147,
                    0.35506580961126416,
                    0.274032187907786,
                    0.4131130063965885,
                    0.5664548217739707,
                    0.2269554568238779,
                    0.3798185941043084,
                    0.4482758620689655,
                    0.33665468795280556,
                    0.24675324675324675,
                    0.3723404255319149,
                    -1.0
                ],
                [
                    0.42261072261072263,
                    0.37259615384615385,
                    0.1448004069270269,
                    0.5808993899484076,
                    0.03734312825221916,
                    0.16267942583732056,
                    0.035181236673773986,
                    0.08261950815142305,
                    0.4906641604010026,
                    0.05782312925170068,
                    0.02586206896551724,
                    0.3278636062063061,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "catholic": 0.4353342121557168,
                "protestant": 0.3688841494820278,
                "eastern orthodox": 0.1957816383622553
            },
            "question": "which of these versions of the old testament typically contains the most books?",
            "rate_limited": false,
            "answers": [
                "catholic",
                "protestant",
                "eastern orthodox"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "catholic": 0.6512410678882583,
                "protestant": 0.43352434910603954,
                "eastern orthodox": 0.19762704271275722
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.01289023504533,
                    2.0199281277168333,
                    1.9671816372378368
                ],
                "result_count_important_words": [
                    2070000.0,
                    1550000.0,
                    132000.0
                ],
                "wikipedia_search": [
                    1.411901913875598,
                    1.1347772841193895,
                    2.453320802005013
                ],
                "word_count_appended_bing": [
                    61.0,
                    52.0,
                    3.0
                ],
                "answer_relation_to_question_bing": [
                    0.7788461538461539,
                    1.7307692307692308,
                    1.4903846153846154
                ],
                "cosine_similarity_raw": [
                    0.07316697388887405,
                    0.1636221706867218,
                    0.0400925874710083
                ],
                "result_count_noun_chunks": [
                    12700000.0,
                    20500000.0,
                    2990000.0
                ],
                "question_answer_similarity": [
                    2.474046828225255,
                    1.3256531269289553,
                    5.266619358211756
                ],
                "word_count_noun_chunks": [
                    116.0,
                    38.0,
                    0.0
                ],
                "word_count_raw": [
                    59.0,
                    35.0,
                    0.0
                ],
                "result_count_bing": [
                    2590000.0,
                    1260000.0,
                    748000.0
                ],
                "word_count_appended": [
                    496.0,
                    335.0,
                    51.0
                ],
                "answer_relation_to_question": [
                    1.0116550116550116,
                    1.8752913752913751,
                    2.113053613053613
                ],
                "result_count": [
                    3970000.0,
                    2320000.0,
                    244000.0
                ]
            },
            "integer_answers": {
                "catholic": 7,
                "protestant": 4,
                "eastern orthodox": 3
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "The material that forms images in an Etch A Sketch is also the main component in which item?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "soda cans"
            ],
            "lines": [
                [
                    0.3732890499194847,
                    0.35155145929339476,
                    0.3547598550790928,
                    0.4638491537378296,
                    0.021739130434782608,
                    0.07805402820941908,
                    0.015503875968992248,
                    0.024691358024691357,
                    0.5069444444444444,
                    0.19230769230769232,
                    0.3333333333333333,
                    0.18723346258808535,
                    0,
                    0.3333333333333333,
                    -1.0
                ],
                [
                    0.4759863123993559,
                    0.32754224270353305,
                    0.20819305408269634,
                    0.28927240464062043,
                    0.30434782608695654,
                    0.04578054028209419,
                    0.3875968992248062,
                    0.43209876543209874,
                    0.23333333333333334,
                    0.15384615384615385,
                    0.3333333333333333,
                    0.2638081805215039,
                    0,
                    0.3333333333333333,
                    -1.0
                ],
                [
                    0.1507246376811594,
                    0.3209062980030722,
                    0.43704709083821086,
                    0.24687844162154998,
                    0.6739130434782609,
                    0.8761654315084867,
                    0.5968992248062015,
                    0.5432098765432098,
                    0.25972222222222224,
                    0.6538461538461539,
                    0.3333333333333333,
                    0.5489583568904108,
                    0,
                    0.3333333333333333,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "zinc supplement tablets": 0.24896847512881354,
                "soda cans": 0.45961057262350813,
                "u.s. nickels": 0.2914209522476784
            },
            "question": "the material that forms images in an etch a sketch is also the main component in which item?",
            "rate_limited": false,
            "answers": [
                "zinc supplement tablets",
                "u.s. nickels",
                "soda cans"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "zinc supplement tablets": 0.2759109726539752,
                "soda cans": 0.4908365697962334,
                "u.s. nickels": 0.22918877229780046
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.4978677007046826,
                    2.1104654441720307,
                    4.391666855123286
                ],
                "result_count_important_words": [
                    2.0,
                    50.0,
                    77.0
                ],
                "wikipedia_search": [
                    3.548611111111111,
                    1.6333333333333333,
                    1.8180555555555555
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.7577572964669739,
                    1.6377112135176652,
                    1.6045314900153609
                ],
                "cosine_similarity_raw": [
                    0.10283080488443375,
                    0.06034690514206886,
                    0.1266826093196869
                ],
                "result_count_noun_chunks": [
                    2.0,
                    35.0,
                    44.0
                ],
                "question_answer_similarity": [
                    10.300920786336064,
                    6.42401112918742,
                    5.4825480449944735
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    1.0
                ],
                "result_count_bing": [
                    6530.0,
                    3830.0,
                    73300.0
                ],
                "word_count_appended": [
                    5.0,
                    4.0,
                    17.0
                ],
                "answer_relation_to_question": [
                    2.239734299516908,
                    2.8559178743961353,
                    0.9043478260869564
                ],
                "result_count": [
                    2.0,
                    28.0,
                    62.0
                ]
            },
            "integer_answers": {
                "zinc supplement tablets": 5,
                "soda cans": 7,
                "u.s. nickels": 1
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these actresses is NOT mentioned in Madonna\u2019s song \u201cVogue\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "audrey hepburn"
            ],
            "lines": [
                [
                    0.3792673992673993,
                    0.5,
                    0.1927205580097775,
                    3.5234309945219278,
                    0.035348986909685465,
                    0.4660347551342812,
                    0.39348591549295775,
                    0.4082244008714597,
                    0.5,
                    0.3064516129032258,
                    0.35714285714285715,
                    0.30207569780386595,
                    0.5,
                    0.0,
                    -1.0
                ],
                [
                    0.27010989010989017,
                    0.16666666666666669,
                    0.39846338613765797,
                    1.9884020602008625,
                    0.4999720453862043,
                    0.2367035281727225,
                    0.3362676056338028,
                    0.13235294117647056,
                    0.06666666666666665,
                    0.4153225806451613,
                    0.38095238095238093,
                    0.3852049572818859,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.35062271062271066,
                    0.33333333333333337,
                    0.40881605585256453,
                    -4.51183305472279,
                    0.4646789677041102,
                    0.2972617166929963,
                    0.27024647887323944,
                    0.4594226579520697,
                    0.43333333333333335,
                    0.2782258064516129,
                    0.2619047619047619,
                    0.31271934491424813,
                    0.0,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "audrey hepburn": 0.10327361299566107,
                "jean harlow": -0.1234547397224911,
                "rita hayworth": 1.0201811267268301
            },
            "question": "which of these actresses is not mentioned in madonna\u2019s song \u201cvogue\u201d?",
            "rate_limited": false,
            "answers": [
                "jean harlow",
                "audrey hepburn",
                "rita hayworth"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "audrey hepburn": 0.46013602695950284,
                "jean harlow": 0.19317945683304036,
                "rita hayworth": 0.11364186956046968
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9792430219613404,
                    1.147950427181141,
                    1.872806550857519
                ],
                "result_count_important_words": [
                    121000.0,
                    186000.0,
                    261000.0
                ],
                "wikipedia_search": [
                    0.0,
                    2.6,
                    0.4
                ],
                "word_count_appended_bing": [
                    6.0,
                    5.0,
                    10.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    2.0,
                    1.0
                ],
                "cosine_similarity_raw": [
                    0.07597820460796356,
                    0.0251060388982296,
                    0.022546228021383286
                ],
                "result_count_noun_chunks": [
                    337000.0,
                    1350000.0,
                    149000.0
                ],
                "question_answer_similarity": [
                    0.8370858241105452,
                    0.41208820953033864,
                    -1.3876071292907
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    12900.0,
                    100000.0,
                    77000.0
                ],
                "result_count": [
                    1230000.0,
                    74.0,
                    93500.0
                ],
                "answer_relation_to_question": [
                    0.7243956043956045,
                    1.3793406593406594,
                    0.8962637362637363
                ],
                "word_count_appended": [
                    48.0,
                    21.0,
                    55.0
                ]
            },
            "integer_answers": {
                "audrey hepburn": 5,
                "jean harlow": 5,
                "rita hayworth": 4
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "How many U.S. state names are only four letters long?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "50 states"
            ],
            "lines": [
                [
                    0.40708525578759913,
                    0.18672014260249556,
                    0.23181936492583202,
                    0.3242400805892452,
                    0.10464135021097046,
                    0.27068832173240526,
                    0.1233362910381544,
                    0.20202020202020202,
                    0.3599468488990129,
                    0.2109375,
                    0.2222222222222222,
                    0.3105236460902757,
                    0.0,
                    0,
                    5.0
                ],
                [
                    0.4245628946090335,
                    0.6115453174276704,
                    0.386373473493258,
                    0.3526356132102328,
                    0.6860759493670886,
                    0.2008249548852797,
                    0.6699201419698314,
                    0.5367965367965368,
                    0.5162743609212858,
                    0.6015625,
                    0.5555555555555556,
                    0.3646824987772076,
                    1.0,
                    0,
                    5.0
                ],
                [
                    0.16835184960336733,
                    0.20173453996983412,
                    0.38180716158091,
                    0.323124306200522,
                    0.20928270042194091,
                    0.5284867233823151,
                    0.2067435669920142,
                    0.2611832611832612,
                    0.12377879017970134,
                    0.1875,
                    0.2222222222222222,
                    0.32479385513251663,
                    0.0,
                    0,
                    5.0
                ]
            ],
            "fraction_answers": {
                "12 states": 0.22724470970141655,
                "50 states": 0.5312930613086908,
                "3 states": 0.24146222898989264
            },
            "question": "how many u.s. state names are only four letters long?",
            "rate_limited": false,
            "answers": [
                "12 states",
                "50 states",
                "3 states"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "12 states": 0.1100019832461876,
                "50 states": 0.4797004894037442,
                "3 states": 0.09746506547939993
            },
            "integer_answers": {
                "12 states": 0,
                "50 states": 12,
                "3 states": 1
            },
            "categorical_data": {
                "question_type": 5
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.5526182304513787,
                    1.823412493886038,
                    1.6239692756625832
                ],
                "result_count_important_words": [
                    1390000.0,
                    7550000.0,
                    2330000.0
                ],
                "wikipedia_search": [
                    1.0798405466970387,
                    1.5488230827638572,
                    0.371336370539104
                ],
                "word_count_appended_bing": [
                    2.0,
                    5.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.5601604278074866,
                    1.8346359522830111,
                    0.6052036199095023
                ],
                "cosine_similarity_raw": [
                    0.08051872253417969,
                    0.13420060276985168,
                    0.13261456787586212
                ],
                "result_count_noun_chunks": [
                    1400000.0,
                    3720000.0,
                    1810000.0
                ],
                "question_answer_similarity": [
                    6.924035631120205,
                    7.53041248396039,
                    6.900208652019501
                ],
                "word_count_noun_chunks": [
                    0.0,
                    5.0,
                    0.0
                ],
                "result_count_bing": [
                    105000000.0,
                    77900000.0,
                    205000000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    1240000.0,
                    8130000.0,
                    2480000.0
                ],
                "answer_relation_to_question": [
                    1.6283410231503965,
                    1.698251578436134,
                    0.6734073984134693
                ],
                "word_count_appended": [
                    27.0,
                    77.0,
                    24.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What play has the stage direction, \u201cEnter a Messenger, with two heads and a hand\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "titus andronicus"
            ],
            "lines": [
                [
                    0.27561885041723755,
                    0.37450578885262714,
                    0.05003639455789304,
                    0.11433539847364375,
                    0.02702702702702703,
                    0.0963572267920094,
                    0.14213197969543148,
                    0.1618320610687023,
                    0.22264552781439853,
                    0.034482758620689655,
                    0.08771929824561403,
                    0.11281885813827591,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.5460914008149031,
                    0.2544561443576968,
                    0.8856872225180772,
                    0.5014528563136913,
                    0.7972972972972973,
                    0.010575793184488837,
                    0.19923857868020303,
                    0.2916030534351145,
                    0.7545288609212119,
                    0.6206896551724138,
                    0.24561403508771928,
                    0.6828696198863226,
                    0.8888888888888888,
                    1.0,
                    1.0
                ],
                [
                    0.17828974876785939,
                    0.371038066789676,
                    0.0642763829240298,
                    0.38421174521266493,
                    0.17567567567567569,
                    0.8930669800235017,
                    0.6586294416243654,
                    0.5465648854961832,
                    0.022825611264389683,
                    0.3448275862068966,
                    0.6666666666666666,
                    0.20431152197540148,
                    0.1111111111111111,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "oedipus rex": 0.12139365497882497,
                "agamemnon": 0.3301068159813158,
                "titus andronicus": 0.5484995290398592
            },
            "question": "what play has the stage direction, \u201center a messenger, with two heads and a hand\u201d?",
            "rate_limited": false,
            "answers": [
                "oedipus rex",
                "titus andronicus",
                "agamemnon"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "oedipus rex": 0.16653385161899864,
                "agamemnon": 0.1305101182285861,
                "titus andronicus": 0.7906221929744528
            },
            "integer_answers": {
                "oedipus rex": 2,
                "agamemnon": 4,
                "titus andronicus": 8
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.7897320069679314,
                    4.780087339204258,
                    1.4301806538278106
                ],
                "result_count_important_words": [
                    11200.0,
                    15700.0,
                    51900.0
                ],
                "wikipedia_search": [
                    1.5585186947007896,
                    5.281702026448483,
                    0.1597792788507278
                ],
                "word_count_appended_bing": [
                    5.0,
                    14.0,
                    38.0
                ],
                "answer_relation_to_question_bing": [
                    2.247034733115763,
                    1.5267368661461809,
                    2.226228400738056
                ],
                "cosine_similarity_raw": [
                    0.02357955276966095,
                    0.41737836599349976,
                    0.030290119349956512
                ],
                "result_count_noun_chunks": [
                    10600.0,
                    19100.0,
                    35800.0
                ],
                "question_answer_similarity": [
                    -1.0870871188817546,
                    -4.767753015272319,
                    -3.6530387327075005
                ],
                "word_count_noun_chunks": [
                    0.0,
                    16.0,
                    2.0
                ],
                "result_count_bing": [
                    164000.0,
                    18000.0,
                    1520000.0
                ],
                "word_count_raw": [
                    0.0,
                    10.0,
                    0.0
                ],
                "result_count": [
                    2.0,
                    59.0,
                    13.0
                ],
                "answer_relation_to_question": [
                    1.9293319529206627,
                    3.822639805704322,
                    1.2480282413750157
                ],
                "word_count_appended": [
                    3.0,
                    54.0,
                    30.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Wrestling legend Ric Flair entered the ring to the same music used in what classic film?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "2001: a space odyssey"
            ],
            "lines": [
                [
                    0.16209301593752226,
                    0.19445294893056086,
                    0.46911630422421624,
                    0.2685614919917738,
                    0.6457964166174444,
                    0.06811182184316512,
                    0.6468708596368171,
                    0.3382076947137942,
                    0.732760827637417,
                    0.2222222222222222,
                    0.2857142857142857,
                    0.27603922855535024,
                    0,
                    0,
                    1.0
                ],
                [
                    0.4784342043176911,
                    0.45434047710167114,
                    0.1376273995809698,
                    0.2846153253030925,
                    0.3516440244142548,
                    0.8884150675195451,
                    0.34993375418907335,
                    0.6607757272442916,
                    0.0445646010268299,
                    0.5925925925925926,
                    0.42857142857142855,
                    0.42312651639710924,
                    0,
                    0,
                    1.0
                ],
                [
                    0.35947277974478664,
                    0.351206573967768,
                    0.39325629619481395,
                    0.44682318270513366,
                    0.0025595589683008466,
                    0.043473110637289744,
                    0.0031953861741095785,
                    0.0010165780419142947,
                    0.22267457133575316,
                    0.18518518518518517,
                    0.2857142857142857,
                    0.3008342550475405,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "star wars: episode iv": 0.35916225983538075,
                "back to the future": 0.21628431364307343,
                "2001: a space odyssey": 0.42455342652154576
            },
            "question": "wrestling legend ric flair entered the ring to the same music used in what classic film?",
            "rate_limited": false,
            "answers": [
                "star wars: episode iv",
                "2001: a space odyssey",
                "back to the future"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "star wars: episode iv": 0.0757528235662117,
                "back to the future": 0.10564275655914847,
                "2001: a space odyssey": 0.5968649866727558
            },
            "integer_answers": {
                "star wars: episode iv": 4,
                "back to the future": 1,
                "2001: a space odyssey": 7
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.484353056998152,
                    3.8081386475739833,
                    2.7075082954278646
                ],
                "result_count_important_words": [
                    16600.0,
                    8980.0,
                    82.0
                ],
                "wikipedia_search": [
                    5.129325793461918,
                    0.31195220718780925,
                    1.5587219993502717
                ],
                "word_count_appended_bing": [
                    2.0,
                    3.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.9722647446528043,
                    2.271702385508356,
                    1.75603286983884
                ],
                "cosine_similarity_raw": [
                    0.09000672399997711,
                    0.026405800133943558,
                    0.07545188814401627
                ],
                "result_count_noun_chunks": [
                    17300.0,
                    33800.0,
                    52.0
                ],
                "question_answer_similarity": [
                    15.269633424468338,
                    16.18240817822516,
                    25.405080061405897
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    1150.0,
                    15000.0,
                    734.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    16400.0,
                    8930.0,
                    65.0
                ],
                "answer_relation_to_question": [
                    1.1346511115626559,
                    3.3490394302238378,
                    2.5163094582135064
                ],
                "word_count_appended": [
                    12.0,
                    32.0,
                    10.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Yellow and blue mix to form what color?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "green"
            ],
            "question": "yellow and blue mix to form what color?",
            "lines": [
                [
                    0.34855612927902085,
                    0.38417021271815516,
                    0.570252563256666,
                    0.3504476153953488,
                    0.21354933726067746,
                    0.4289940828402367,
                    0.6892523364485982,
                    0.291497975708502,
                    0.31567261146800824,
                    0.3607112616426757,
                    0.40217391304347827,
                    0.3460102995641351,
                    0.7242990654205608,
                    0.8604651162790697,
                    1.0
                ],
                [
                    0.4282370752852681,
                    0.3256745922976916,
                    0.222001152552386,
                    0.31872289327726966,
                    0.6126656848306333,
                    0.24334319526627218,
                    0.10319314641744548,
                    0.5700404858299595,
                    0.40445411424634736,
                    0.3310753598645216,
                    0.266304347826087,
                    0.3261373519027365,
                    0.1308411214953271,
                    0.046511627906976744,
                    1.0
                ],
                [
                    0.2232067954357111,
                    0.2901551949841532,
                    0.20774628419094798,
                    0.3308294913273816,
                    0.17378497790868924,
                    0.3276627218934911,
                    0.2075545171339564,
                    0.13846153846153847,
                    0.2798732742856444,
                    0.3082133784928027,
                    0.33152173913043476,
                    0.32785234853312845,
                    0.14485981308411214,
                    0.09302325581395349,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "green",
                "purple",
                "orange"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "purple": 0.10111410098766807,
                "orange": 0.155223699259861,
                "green": 0.6542426232908667
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7300514978206756,
                    1.6306867595136825,
                    1.6392617426656422
                ],
                "result_count_important_words": [
                    177000000.0,
                    26500000.0,
                    53300000.0
                ],
                "wikipedia_search": [
                    1.5783630573400411,
                    2.022270571231737,
                    1.399366371428222
                ],
                "word_count_appended_bing": [
                    74.0,
                    49.0,
                    61.0
                ],
                "answer_relation_to_question_bing": [
                    1.5366808508726206,
                    1.3026983691907663,
                    1.1606207799366128
                ],
                "cosine_similarity_raw": [
                    0.15311259031295776,
                    0.05960722267627716,
                    0.055779796093702316
                ],
                "result_count_noun_chunks": [
                    36000000.0,
                    70400000.0,
                    17100000.0
                ],
                "question_answer_similarity": [
                    3.91780623793602,
                    3.563141778111458,
                    3.69848670065403
                ],
                "word_count_noun_chunks": [
                    155.0,
                    28.0,
                    31.0
                ],
                "result_count_bing": [
                    116000000.0,
                    65800000.0,
                    88600000.0
                ],
                "word_count_raw": [
                    37.0,
                    2.0,
                    4.0
                ],
                "result_count": [
                    14500000.0,
                    41600000.0,
                    11800000.0
                ],
                "answer_relation_to_question": [
                    1.7427806463951043,
                    2.1411853764263404,
                    1.1160339771785555
                ],
                "word_count_appended": [
                    426.0,
                    391.0,
                    364.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "The man famously known as the Science Guy holds a patent for which of these items?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ballet shoe"
            ],
            "lines": [
                [
                    0.30780494015788135,
                    0.18225108225108225,
                    0.12068199972408458,
                    0.42282793452805056,
                    0.15178571428571427,
                    0.35185185185185186,
                    0.09302325581395349,
                    0.007476635514018692,
                    0.23888888888888887,
                    0.20833333333333334,
                    0.36363636363636365,
                    0.2248363705141521,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.27324929971988793,
                    0.5251082251082251,
                    0.14554702170233577,
                    0.2974400935604148,
                    0.5357142857142857,
                    0.31296296296296294,
                    0.5813953488372093,
                    0.9688473520249221,
                    0.711111111111111,
                    0.625,
                    0.36363636363636365,
                    0.49670524654195886,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.41894576012223067,
                    0.29264069264069265,
                    0.7337709785735796,
                    0.27973197191153465,
                    0.3125,
                    0.3351851851851852,
                    0.32558139534883723,
                    0.02367601246105919,
                    0.05,
                    0.16666666666666666,
                    0.2727272727272727,
                    0.2784583829438891,
                    0,
                    1.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "pulse rate monitor": 0.20564602849995192,
                "mechanical pencil": 0.448978254686129,
                "ballet shoe": 0.34537571681391904
            },
            "question": "the man famously known as the science guy holds a patent for which of these items?",
            "rate_limited": false,
            "answers": [
                "pulse rate monitor",
                "mechanical pencil",
                "ballet shoe"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "pulse rate monitor": 0.12523368309219907,
                "mechanical pencil": 0.4289524847469227,
                "ballet shoe": 0.6532297972916821
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7986909641132167,
                    3.973641972335671,
                    2.2276670635511127
                ],
                "result_count_important_words": [
                    8.0,
                    50.0,
                    28.0
                ],
                "wikipedia_search": [
                    1.1944444444444444,
                    3.5555555555555554,
                    0.25
                ],
                "word_count_appended_bing": [
                    4.0,
                    4.0,
                    3.0
                ],
                "answer_relation_to_question_bing": [
                    0.5467532467532468,
                    1.5753246753246752,
                    0.8779220779220779
                ],
                "cosine_similarity_raw": [
                    0.017867956310510635,
                    0.021549426019191742,
                    0.10864078998565674
                ],
                "result_count_noun_chunks": [
                    24.0,
                    3110.0,
                    76.0
                ],
                "question_answer_similarity": [
                    9.788729492109269,
                    6.885923039168119,
                    6.475969016551971
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    190000.0,
                    169000.0,
                    181000.0
                ],
                "word_count_appended": [
                    10.0,
                    30.0,
                    8.0
                ],
                "answer_relation_to_question": [
                    1.846829640947288,
                    1.6394957983193277,
                    2.513674560733384
                ],
                "result_count": [
                    17.0,
                    60.0,
                    35.0
                ]
            },
            "integer_answers": {
                "pulse rate monitor": 3,
                "mechanical pencil": 7,
                "ballet shoe": 3
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "\u201cGinevra de\u2019 Benci\u201d is the only work by what artist on permanent display in the U.S.?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "michelangelo"
            ],
            "lines": [
                [
                    0.27995730226797116,
                    0.3783421944829712,
                    0.17950328414083574,
                    -0.6423444691094917,
                    0.44790313838979845,
                    0.019997515836542044,
                    0.3613933236574746,
                    0.43271139341008336,
                    0.25134760035092263,
                    0.38461538461538464,
                    0.5342465753424658,
                    0.33698566295063165,
                    0.02702702702702703,
                    0.0,
                    1.0
                ],
                [
                    0.20524972080747284,
                    0.32531495607951916,
                    0.6318225618560575,
                    6.2929301845616035,
                    0.551186738794752,
                    0.1565022978511986,
                    0.4288824383164006,
                    0.5289797538705836,
                    0.49388165207426815,
                    0.2913752913752914,
                    0.0958904109589041,
                    0.38248987921989536,
                    0.9594594594594594,
                    1.0,
                    1.0
                ],
                [
                    0.514792976924556,
                    0.2963428494375096,
                    0.18867415400310678,
                    -4.650585715452111,
                    0.0009101228154495904,
                    0.8235001863122593,
                    0.20972423802612483,
                    0.03830885271933307,
                    0.2547707475748092,
                    0.32400932400932403,
                    0.3698630136986301,
                    0.2805244578294729,
                    0.013513513513513514,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "michelangelo": 0.21369185238304403,
                "botticelli": -0.09540366275628732,
                "leonardo da vinci": 0.8817118103732433
            },
            "question": "\u201cginevra de\u2019 benci\u201d is the only work by what artist on permanent display in the u.s.?",
            "rate_limited": false,
            "answers": [
                "michelangelo",
                "leonardo da vinci",
                "botticelli"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "michelangelo": 0.5377014037721425,
                "botticelli": 0.21714473912902957,
                "leonardo da vinci": 0.16078826456898168
            },
            "integer_answers": {
                "michelangelo": 3,
                "botticelli": 2,
                "leonardo da vinci": 9
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.3588996406544216,
                    2.6774291545392677,
                    1.9636712048063103
                ],
                "result_count_important_words": [
                    49800.0,
                    59100.0,
                    28900.0
                ],
                "wikipedia_search": [
                    1.0053904014036905,
                    1.9755266082970726,
                    1.0190829902992369
                ],
                "word_count_appended_bing": [
                    39.0,
                    7.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    1.5133687779318847,
                    1.3012598243180766,
                    1.1853713977500384
                ],
                "cosine_similarity_raw": [
                    0.048628970980644226,
                    0.17116612195968628,
                    0.05111343786120415
                ],
                "result_count_noun_chunks": [
                    43600.0,
                    53300.0,
                    3860.0
                ],
                "question_answer_similarity": [
                    -0.29872069600969553,
                    2.926511513796868,
                    -2.1627433076500893
                ],
                "word_count_noun_chunks": [
                    2.0,
                    71.0,
                    1.0
                ],
                "result_count_bing": [
                    16100.0,
                    126000.0,
                    663000.0
                ],
                "word_count_raw": [
                    0.0,
                    18.0,
                    0.0
                ],
                "result_count": [
                    43800.0,
                    53900.0,
                    89.0
                ],
                "answer_relation_to_question": [
                    1.1198292090718847,
                    0.8209988832298913,
                    2.059171907698224
                ],
                "word_count_appended": [
                    165.0,
                    125.0,
                    139.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which alum from \u201cThe Hills\u201d founded a wildly popular millennial skincare line?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "emily weiss"
            ],
            "lines": [
                [
                    0.5469387755102041,
                    0.44570745861068445,
                    0.6120818865067976,
                    -0.04123031383057541,
                    0.1348314606741573,
                    0.7626279300099043,
                    0.24475524475524477,
                    0.24516129032258063,
                    0.3932291666666667,
                    0.19047619047619047,
                    0.25,
                    0.30182110838955745,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.22315148281534838,
                    0.1764635603345281,
                    0.1907682835862498,
                    0.2483397479870632,
                    0.6067415730337079,
                    0.16705183228788378,
                    0.4755244755244755,
                    0.44516129032258067,
                    0.5166495901639344,
                    0.5476190476190477,
                    0.39285714285714285,
                    0.4048033064515427,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.22990974167444755,
                    0.3778289810547875,
                    0.1971498299069526,
                    0.7928905658435123,
                    0.25842696629213485,
                    0.07032023770221195,
                    0.27972027972027974,
                    0.3096774193548387,
                    0.0901212431693989,
                    0.2619047619047619,
                    0.35714285714285715,
                    0.29337558515889983,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "emily weiss": 0.4347428712922437,
                "whitney port": 0.2513191763517916,
                "lauren conrad": 0.3139379523559647
            },
            "question": "which alum from \u201cthe hills\u201d founded a wildly popular millennial skincare line?",
            "rate_limited": false,
            "answers": [
                "emily weiss",
                "lauren conrad",
                "whitney port"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "emily weiss": 0.693692889839821,
                "whitney port": 0.1382318303807124,
                "lauren conrad": 0.37612599762387366
            },
            "integer_answers": {
                "emily weiss": 6,
                "whitney port": 1,
                "lauren conrad": 7
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.4145688671164596,
                    3.2384264516123418,
                    2.3470046812711987
                ],
                "result_count_important_words": [
                    35.0,
                    68.0,
                    40.0
                ],
                "wikipedia_search": [
                    1.5729166666666667,
                    2.0665983606557377,
                    0.3604849726775956
                ],
                "word_count_appended_bing": [
                    7.0,
                    11.0,
                    10.0
                ],
                "answer_relation_to_question_bing": [
                    2.228537293053422,
                    0.8823178016726404,
                    1.8891449052739375
                ],
                "cosine_similarity_raw": [
                    0.25154754519462585,
                    0.07840012013912201,
                    0.08102274686098099
                ],
                "result_count_noun_chunks": [
                    38.0,
                    69.0,
                    48.0
                ],
                "question_answer_similarity": [
                    -0.11563500203192234,
                    0.6964964511571452,
                    2.2237498014001176
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    23100.0,
                    5060.0,
                    2130.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    12.0,
                    54.0,
                    23.0
                ],
                "answer_relation_to_question": [
                    3.8285714285714287,
                    1.5620603797074386,
                    1.6093681917211329
                ],
                "word_count_appended": [
                    8.0,
                    23.0,
                    11.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which color is NOT represented in the original electronic Simon game?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "orange"
            ],
            "lines": [
                [
                    0.29876957494407164,
                    0.16168146987112503,
                    0.36215639074047185,
                    0.32260869715949814,
                    0.4503454231433506,
                    0.3492537313432836,
                    0.46136977058029693,
                    0.3652207591014717,
                    0.3572197484054904,
                    0.31991525423728817,
                    0.27941176470588236,
                    0.32402292489224727,
                    0.23076923076923078,
                    0.5,
                    -1.0
                ],
                [
                    0.41181208053691276,
                    0.4283133054684779,
                    0.3559571241684453,
                    0.3519548682225229,
                    0.0984455958549223,
                    0.30149253731343284,
                    0.07658569500674763,
                    0.2618125484120837,
                    0.4315209564118391,
                    0.336864406779661,
                    0.36764705882352944,
                    0.34765352688125983,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.28941834451901566,
                    0.41000522466039707,
                    0.28188648509108283,
                    0.32543643461797894,
                    0.4512089810017271,
                    0.3492537313432836,
                    0.4620445344129555,
                    0.37296669248644465,
                    0.21125929518267056,
                    0.3432203389830508,
                    0.3529411764705882,
                    0.3283235482264929,
                    0.2692307692307692,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "blue": 0.3167507514437559,
                "orange": 0.31856289944573785,
                "green": 0.3646863491105061
            },
            "question": "which color is not represented in the original electronic simon game?",
            "rate_limited": false,
            "answers": [
                "blue",
                "orange",
                "green"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "blue": 0.18968499210616951,
                "orange": 0.460608549387696,
                "green": 0.20600199105557998
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.1117249012930324,
                    1.8281576774248824,
                    2.0601174212820847
                ],
                "result_count_important_words": [
                    2290000.0,
                    25100000.0,
                    2250000.0
                ],
                "wikipedia_search": [
                    1.1422420127560768,
                    0.5478323487052876,
                    2.3099256385386355
                ],
                "word_count_appended_bing": [
                    45.0,
                    27.0,
                    30.0
                ],
                "answer_relation_to_question_bing": [
                    2.706548241031,
                    0.5734935562521769,
                    0.7199582027168234
                ],
                "cosine_similarity_raw": [
                    0.03717388957738876,
                    0.038845717906951904,
                    0.05882120877504349
                ],
                "result_count_noun_chunks": [
                    3480000.0,
                    6150000.0,
                    3280000.0
                ],
                "question_answer_similarity": [
                    3.33958138525486,
                    2.787108264863491,
                    3.286346197128296
                ],
                "word_count_noun_chunks": [
                    7.0,
                    0.0,
                    6.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    1010000.0,
                    1330000.0,
                    1010000.0
                ],
                "word_count_appended": [
                    255.0,
                    231.0,
                    222.0
                ],
                "answer_relation_to_question": [
                    2.012304250559284,
                    0.8818791946308724,
                    2.1058165548098433
                ],
                "result_count": [
                    2300000.0,
                    18600000.0,
                    2260000.0
                ]
            },
            "integer_answers": {
                "blue": 6,
                "orange": 4,
                "green": 4
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What car brand is sung about by Will Smith, Charli XCX and Janis Joplin?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "porsche"
            ],
            "lines": [
                [
                    0.47250276701715554,
                    0.28678878428162097,
                    0.4530487405919576,
                    0.3745326544279725,
                    0.39824224114254325,
                    0.3333333333333333,
                    0.32585895117540686,
                    0.3958218801539307,
                    0.004477611940298508,
                    0.2644628099173554,
                    0.23684210526315788,
                    0.2774532792728293,
                    0.0,
                    0,
                    1.0
                ],
                [
                    0.38554925290536807,
                    0.3729192249965889,
                    0.33006118148499336,
                    0.46095148978453654,
                    0.32957978577313923,
                    0.3333333333333333,
                    0.3515370705244123,
                    0.3326003298515668,
                    0.6001105583195135,
                    0.28512396694214875,
                    0.2631578947368421,
                    0.3845163220303863,
                    0.08333333333333333,
                    0,
                    1.0
                ],
                [
                    0.1419479800774765,
                    0.3402919907217901,
                    0.21689007792304904,
                    0.16451585578749095,
                    0.2721779730843175,
                    0.3333333333333333,
                    0.32260397830018084,
                    0.2715777899945025,
                    0.39541182974018796,
                    0.45041322314049587,
                    0.5,
                    0.3380303986967843,
                    0.9166666666666666,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "porsche": 0.3587585459589443,
                "rolls-royce": 0.29410501219365864,
                "mercedes-benz": 0.34713644184739717
            },
            "question": "what car brand is sung about by will smith, charli xcx and janis joplin?",
            "rate_limited": false,
            "answers": [
                "rolls-royce",
                "mercedes-benz",
                "porsche"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "porsche": 0.671306739815384,
                "rolls-royce": 0.07900863413967375,
                "mercedes-benz": 0.1093476985273776
            },
            "integer_answers": {
                "porsche": 3,
                "rolls-royce": 5,
                "mercedes-benz": 5
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.2196262341826345,
                    3.0761305762430906,
                    2.7042431895742745
                ],
                "result_count_important_words": [
                    9010.0,
                    9720.0,
                    8920.0
                ],
                "wikipedia_search": [
                    0.01791044776119403,
                    2.400442233278054,
                    1.5816473189607518
                ],
                "word_count_appended_bing": [
                    27.0,
                    30.0,
                    57.0
                ],
                "answer_relation_to_question_bing": [
                    0.8603663528448628,
                    1.1187576749897667,
                    1.0208759721653704
                ],
                "cosine_similarity_raw": [
                    0.04062693566083908,
                    0.029598083347082138,
                    0.019449517130851746
                ],
                "result_count_noun_chunks": [
                    14400.0,
                    12100.0,
                    9880.0
                ],
                "question_answer_similarity": [
                    4.242357361596078,
                    5.221229505375959,
                    1.8634825125336647
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    11.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    156000.0,
                    156000.0,
                    156000.0
                ],
                "result_count": [
                    14500.0,
                    12000.0,
                    9910.0
                ],
                "answer_relation_to_question": [
                    1.890011068068622,
                    1.542197011621472,
                    0.5677919203099059
                ],
                "word_count_appended": [
                    64.0,
                    69.0,
                    109.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which person is most likely to use a Reuleaux triangle at work?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "banksy"
            ],
            "lines": [
                [
                    0.4965033010987736,
                    0.3389250058045043,
                    0.16945508310603025,
                    0.46353960348186013,
                    0.06962025316455696,
                    0.35054347826086957,
                    0.06790123456790123,
                    0.08379888268156424,
                    0.02611754966887417,
                    0.09090909090909091,
                    0.09375,
                    0.21357557264180946,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.20866310192366436,
                    0.3335074684621933,
                    0.12688118408278357,
                    -0.09629011091222073,
                    0.34810126582278483,
                    0.34782608695652173,
                    0.36419753086419754,
                    0.36312849162011174,
                    0.15378565970453387,
                    0.07575757575757576,
                    0.0625,
                    0.33178567335061326,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.2948335969775621,
                    0.32756752573330233,
                    0.7036637328111862,
                    0.6327505074303607,
                    0.5822784810126582,
                    0.3016304347826087,
                    0.5679012345679012,
                    0.553072625698324,
                    0.820096790626592,
                    0.8333333333333334,
                    0.84375,
                    0.4546387540075773,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "banksy": 0.5762930847484505,
                "adam levine": 0.2183203273027299,
                "greta gerwig": 0.2053865879488195
            },
            "question": "which person is most likely to use a reuleaux triangle at work?",
            "rate_limited": false,
            "answers": [
                "greta gerwig",
                "adam levine",
                "banksy"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "banksy": 0.5827066102259297,
                "adam levine": 0.13171778124794792,
                "greta gerwig": 0.23471487233925584
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2814534358508567,
                    1.9907140401036796,
                    2.7278325240454637
                ],
                "result_count_important_words": [
                    11.0,
                    59.0,
                    92.0
                ],
                "wikipedia_search": [
                    0.07835264900662252,
                    0.46135697911360163,
                    2.460290371879776
                ],
                "word_count_appended_bing": [
                    3.0,
                    2.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    1.0167750174135128,
                    1.00052240538658,
                    0.9827025771999071
                ],
                "cosine_similarity_raw": [
                    0.03144431486725807,
                    0.023544244468212128,
                    0.13057279586791992
                ],
                "result_count_noun_chunks": [
                    15.0,
                    65.0,
                    99.0
                ],
                "question_answer_similarity": [
                    -0.7148150510620326,
                    0.1484870333224535,
                    -0.9757517650723457
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    129000.0,
                    128000.0,
                    111000.0
                ],
                "word_count_appended": [
                    6.0,
                    5.0,
                    55.0
                ],
                "answer_relation_to_question": [
                    1.9860132043950942,
                    0.8346524076946573,
                    1.1793343879102482
                ],
                "result_count": [
                    11.0,
                    55.0,
                    92.0
                ]
            },
            "integer_answers": {
                "banksy": 8,
                "adam levine": 1,
                "greta gerwig": 3
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What spice comes from the crocus flower?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "saffron"
            ],
            "question": "what spice comes from the crocus flower?",
            "lines": [
                [
                    0.2866986736667588,
                    0.21632996632996634,
                    0.036096921707444156,
                    0.3524370956702531,
                    0.13568904593639575,
                    0.6873156342182891,
                    0.6490469779635797,
                    0.057910547396528705,
                    0.12972103004291846,
                    0.1794095382286147,
                    0.1921182266009852,
                    0.30971356479917145,
                    0.0019455252918287938,
                    0.0,
                    1.0
                ],
                [
                    0.3549236667587731,
                    0.27946127946127947,
                    0.018041737790959585,
                    0.18123040536564214,
                    0.06395759717314488,
                    0.16755162241887905,
                    0.02359526677531354,
                    0.060914552736982645,
                    0.14019313304721032,
                    0.17713853141559424,
                    0.1477832512315271,
                    0.3279009916114701,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.3583776595744681,
                    0.5042087542087542,
                    0.9458613405015963,
                    0.4663324989641047,
                    0.8003533568904594,
                    0.14513274336283186,
                    0.3273577552611068,
                    0.8811748998664887,
                    0.7300858369098713,
                    0.6434519303557911,
                    0.6600985221674877,
                    0.36238544358935837,
                    0.9980544747081712,
                    1.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "cloves",
                "allspice",
                "saffron"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "cloves": 0.060317000258034235,
                "allspice": 0.1957251093736472,
                "saffron": 0.897113359330006
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2388542591966858,
                    1.3116039664458805,
                    1.4495417743574335
                ],
                "result_count_important_words": [
                    916000.0,
                    33300.0,
                    462000.0
                ],
                "wikipedia_search": [
                    0.5188841201716738,
                    0.5607725321888413,
                    2.920343347639485
                ],
                "word_count_appended_bing": [
                    39.0,
                    30.0,
                    134.0
                ],
                "answer_relation_to_question_bing": [
                    0.648989898989899,
                    0.8383838383838383,
                    1.5126262626262628
                ],
                "cosine_similarity_raw": [
                    0.0270970668643713,
                    0.013543486595153809,
                    0.7100347280502319
                ],
                "result_count_noun_chunks": [
                    34700.0,
                    36500.0,
                    528000.0
                ],
                "question_answer_similarity": [
                    1.2277398607693613,
                    0.6313290949910879,
                    1.6245026541873813
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    513.0
                ],
                "result_count_bing": [
                    2330000.0,
                    568000.0,
                    492000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    152.0
                ],
                "result_count": [
                    76800.0,
                    36200.0,
                    453000.0
                ],
                "answer_relation_to_question": [
                    1.146794694667035,
                    1.4196946670350925,
                    1.4335106382978724
                ],
                "word_count_appended": [
                    237.0,
                    234.0,
                    850.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these songs was written by the man nicknamed \u201cSlowhand\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "lay down sally"
            ],
            "lines": [
                [
                    0.3583851809954751,
                    0.5307528409090909,
                    0.8170045398442776,
                    0.2857582590948446,
                    0.9992678631497715,
                    0.9965955011750183,
                    0.9993088114054446,
                    0.6146953405017921,
                    0.2256383712905452,
                    0.717948717948718,
                    0.4782608695652174,
                    0.6421594351305486,
                    0,
                    1.0,
                    -1.0
                ],
                [
                    0.35859728506787336,
                    0.3730113636363636,
                    0.06971840804309642,
                    0.35340078968954997,
                    0.00042543087243010073,
                    0.0019321093426005998,
                    0.0003997235245621778,
                    0.2007168458781362,
                    0.6922015182884748,
                    0.16666666666666666,
                    0.43478260869565216,
                    0.20992457072288348,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.2830175339366516,
                    0.09623579545454546,
                    0.11327705211262598,
                    0.36084095121560544,
                    0.0003067059777984447,
                    0.001472389482381156,
                    0.00029146506999325466,
                    0.18458781362007168,
                    0.08216011042097998,
                    0.11538461538461539,
                    0.08695652173913043,
                    0.14791599414656798,
                    0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "lay down sally": 0.6665981331546726,
                "lover lay down": 0.11326514988930515,
                "lay lady lay": 0.22013671695602227
            },
            "question": "which of these songs was written by the man nicknamed \u201cslowhand\u201d?",
            "rate_limited": false,
            "answers": [
                "lay down sally",
                "lay lady lay",
                "lover lay down"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "lay down sally": 0.7610576120966963,
                "lover lay down": 0.09645008842074577,
                "lay lady lay": 0.14524095017430944
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.210797175652743,
                    1.0496228536144174,
                    0.7395799707328399
                ],
                "result_count_important_words": [
                    120000.0,
                    48.0,
                    35.0
                ],
                "wikipedia_search": [
                    0.6769151138716356,
                    2.0766045548654244,
                    0.24648033126293994
                ],
                "word_count_appended_bing": [
                    11.0,
                    10.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    2.1230113636363637,
                    1.4920454545454545,
                    0.3849431818181818
                ],
                "cosine_similarity_raw": [
                    0.36614540219306946,
                    0.031244715675711632,
                    0.05076577886939049
                ],
                "result_count_noun_chunks": [
                    343000.0,
                    112000.0,
                    103000.0
                ],
                "question_answer_similarity": [
                    8.875952580478042,
                    10.97700084373355,
                    11.208100099116564
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    31000000.0,
                    60100.0,
                    45800.0
                ],
                "word_count_appended": [
                    56.0,
                    13.0,
                    9.0
                ],
                "answer_relation_to_question": [
                    1.4335407239819005,
                    1.4343891402714934,
                    1.1320701357466063
                ],
                "result_count": [
                    101000.0,
                    43.0,
                    31.0
                ]
            },
            "integer_answers": {
                "lay down sally": 10,
                "lover lay down": 1,
                "lay lady lay": 2
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these quantities is the largest?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "two half-dozens"
            ],
            "lines": [
                [
                    0.6666666666666667,
                    0.0,
                    0.2150256011156894,
                    0.13127215168613302,
                    0.6051722612407617,
                    0.48461747890008167,
                    0.7308921535405081,
                    0.656397326543452,
                    0,
                    0.7505197505197505,
                    0.6888888888888889,
                    0.5245962802093774,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.16666666666666669,
                    0.0,
                    0.6155967652629101,
                    0.5353932794064021,
                    5.008322161992511e-06,
                    0.25619384699156006,
                    6.6444741230955274e-06,
                    1.1848327193925126e-06,
                    0,
                    0.008316008316008316,
                    0.044444444444444446,
                    0.0743345584140773,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.16666666666666669,
                    1.0,
                    0.16937763362140051,
                    0.3333345689074648,
                    0.3948227304370763,
                    0.2591886741083583,
                    0.26910120198536885,
                    0.34360148862382867,
                    0,
                    0.24116424116424118,
                    0.26666666666666666,
                    0.40106916137654525,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "two half-dozens": 0.15463258246646125,
                "baker's dozen": 0.3495448212325106,
                "dozen": 0.49582259630102815
            },
            "question": "which of these quantities is the largest?",
            "rate_limited": false,
            "answers": [
                "dozen",
                "two half-dozens",
                "baker's dozen"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "two half-dozens": 0.39979374258720957,
                "baker's dozen": 0.2799566335016134,
                "dozen": 0.3781749382740332
            },
            "integer_answers": {
                "two half-dozens": 2,
                "baker's dozen": 1,
                "dozen": 8
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.0491925604187549,
                    0.1486691168281546,
                    0.8021383227530905
                ],
                "result_count_important_words": [
                    1540000.0,
                    14.0,
                    567000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    31.0,
                    2.0,
                    12.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "cosine_similarity_raw": [
                    0.04077501595020294,
                    0.11673478782176971,
                    0.03211885318160057
                ],
                "result_count_noun_chunks": [
                    8310000.0,
                    15.0,
                    4350000.0
                ],
                "question_answer_similarity": [
                    2.4537939727306366,
                    10.007795142941177,
                    6.230829201638699
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    17800000.0,
                    9410000.0,
                    9520000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    361.0,
                    4.0,
                    116.0
                ],
                "answer_relation_to_question": [
                    1.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "result_count": [
                    1450000.0,
                    12.0,
                    946000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "The actor who played Don Draper provides the voice for what car company\u2019s ads?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "jaguar"
            ],
            "lines": [
                [
                    0.37814709279938385,
                    0.18521078565562551,
                    0.5215220832730078,
                    0.5822662502763025,
                    0.18874773139745918,
                    0.419281232173417,
                    0.14620689655172414,
                    0.1596779604159678,
                    0.6269208965637537,
                    0.3378607809847199,
                    0.35789473684210527,
                    0.3402702042614314,
                    0.7777777777777778,
                    0.75,
                    1.0
                ],
                [
                    0.36049595687331537,
                    0.3863981658910484,
                    0.28301735247995113,
                    0.24786561416360686,
                    0.4192377495462795,
                    0.33371363377067886,
                    0.4772413793103448,
                    0.37739013753773903,
                    0.33484459488923773,
                    0.31918505942275044,
                    0.28421052631578947,
                    0.3332806442603006,
                    0.1111111111111111,
                    0.25,
                    1.0
                ],
                [
                    0.2613569503273007,
                    0.42839104845332604,
                    0.1954605642470411,
                    0.16986813556009067,
                    0.39201451905626133,
                    0.24700513405590416,
                    0.37655172413793103,
                    0.4629319020462932,
                    0.03823450854700854,
                    0.34295415959252973,
                    0.35789473684210527,
                    0.3264491514782681,
                    0.1111111111111111,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "jaguar": 0.26501597467536936,
                "bmw": 0.3227137089694395,
                "mercedes-benz": 0.4122703163551911
            },
            "question": "the actor who played don draper provides the voice for what car company\u2019s ads?",
            "rate_limited": false,
            "answers": [
                "mercedes-benz",
                "bmw",
                "jaguar"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "jaguar": 0.6107807549403994,
                "bmw": 0.21816757588868702,
                "mercedes-benz": 0.5465037702352603
            },
            "integer_answers": {
                "jaguar": 3,
                "bmw": 2,
                "mercedes-benz": 9
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.7221616340914507,
                    2.666245154082404,
                    2.6115932118261442
                ],
                "result_count_important_words": [
                    1060000.0,
                    3460000.0,
                    2730000.0
                ],
                "wikipedia_search": [
                    3.761525379382522,
                    2.0090675693354263,
                    0.22940705128205127
                ],
                "word_count_appended_bing": [
                    34.0,
                    27.0,
                    34.0
                ],
                "answer_relation_to_question_bing": [
                    0.9260539282781276,
                    1.9319908294552421,
                    2.14195524226663
                ],
                "cosine_similarity_raw": [
                    0.06537061929702759,
                    0.03547504544258118,
                    0.0245001669973135
                ],
                "result_count_noun_chunks": [
                    952000.0,
                    2250000.0,
                    2760000.0
                ],
                "question_answer_similarity": [
                    3.793542579282075,
                    1.6148776626214385,
                    1.106713646557182
                ],
                "word_count_noun_chunks": [
                    21.0,
                    3.0,
                    3.0
                ],
                "result_count_bing": [
                    147000.0,
                    117000.0,
                    86600.0
                ],
                "word_count_raw": [
                    6.0,
                    2.0,
                    0.0
                ],
                "result_count": [
                    1040000.0,
                    2310000.0,
                    2160000.0
                ],
                "answer_relation_to_question": [
                    1.8907354639969194,
                    1.8024797843665767,
                    1.3067847516365036
                ],
                "word_count_appended": [
                    199.0,
                    188.0,
                    202.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these substances expands when it freezes?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "carbon dioxide"
            ],
            "lines": [
                [
                    0.3333333333333333,
                    0.0,
                    0.36573226741823556,
                    0.36134323360892967,
                    0.22020665547667811,
                    0.26605504587155965,
                    0.4199177188253653,
                    0.40579304554677115,
                    0.0,
                    0.27472527472527475,
                    0.3157894736842105,
                    0.3279938615105052,
                    0.0,
                    0,
                    2.0
                ],
                [
                    0.0,
                    0.0,
                    0.2757820595324963,
                    0.4964247601308732,
                    0.7776528742358213,
                    0.5504587155963303,
                    0.5518513264292807,
                    0.5499195410340726,
                    1.0,
                    0.38095238095238093,
                    0.3684210526315789,
                    0.3609837153086255,
                    1.0,
                    0,
                    2.0
                ],
                [
                    0.6666666666666666,
                    1.0,
                    0.3584856730492682,
                    0.1422320062601971,
                    0.0021404702875005774,
                    0.1834862385321101,
                    0.02823095474535395,
                    0.04428741341915623,
                    0.0,
                    0.3443223443223443,
                    0.3157894736842105,
                    0.3110224231808693,
                    0.0,
                    0,
                    2.0
                ]
            ],
            "fraction_answers": {
                "sodium chloride": 0.25314537769237405,
                "carbon dioxide": 0.4855728019885739,
                "dihydrogen monoxide": 0.26128182031905206
            },
            "question": "which of these substances expands when it freezes?",
            "rate_limited": false,
            "answers": [
                "sodium chloride",
                "carbon dioxide",
                "dihydrogen monoxide"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sodium chloride": 0.0596709997427442,
                "carbon dioxide": 0.3906118941771382,
                "dihydrogen monoxide": 0.128900673471955
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 2
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.9839815845315155,
                    1.0829511459258765,
                    0.9330672695426079
                ],
                "result_count_important_words": [
                    296000.0,
                    389000.0,
                    19900.0
                ],
                "wikipedia_search": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    6.0,
                    7.0,
                    6.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "cosine_similarity_raw": [
                    0.028181370347738266,
                    0.02125028893351555,
                    0.02762298658490181
                ],
                "result_count_noun_chunks": [
                    5800000.0,
                    7860000.0,
                    633000.0
                ],
                "question_answer_similarity": [
                    2.973916858434677,
                    4.085661016404629,
                    1.170593834016472
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    2030000.0,
                    4200000.0,
                    1400000.0
                ],
                "word_count_appended": [
                    75.0,
                    104.0,
                    94.0
                ],
                "answer_relation_to_question": [
                    0.3333333333333333,
                    0.0,
                    0.6666666666666666
                ],
                "result_count": [
                    286000.0,
                    1010000.0,
                    2780.0
                ]
            },
            "integer_answers": {
                "sodium chloride": 1,
                "carbon dioxide": 10,
                "dihydrogen monoxide": 2
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which U.S. president's wife was NOT born in North America?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "martin van buren"
            ],
            "lines": [
                [
                    0.2530783241312107,
                    0.23306196503127447,
                    0.07525737448809561,
                    0.278352176574437,
                    0.17150395778364114,
                    0.4024822695035461,
                    0.17532467532467533,
                    0.17577197149643703,
                    0.34878386668759187,
                    0.31155778894472363,
                    0.4270833333333333,
                    0.33276247336560744,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.37045488951212613,
                    0.4144180414640773,
                    0.4842463262880563,
                    0.37061459986955764,
                    0.4270448548812665,
                    0.1453900709219858,
                    0.42514089683901,
                    0.42304038004750594,
                    0.39652199759360457,
                    0.3756281407035176,
                    0.3854166666666667,
                    0.337679348623596,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.37646678635666325,
                    0.35251999350464824,
                    0.4404962992238481,
                    0.35103322355600536,
                    0.40145118733509233,
                    0.4521276595744681,
                    0.39953442783631465,
                    0.40118764845605703,
                    0.25469413571880367,
                    0.3128140703517588,
                    0.1875,
                    0.3295581780107966,
                    0.5,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "martin van buren": 0.24865948429650628,
                "john quincy adams": 0.5449971176193465,
                "rutherford b. hayes": 0.20634339808414714
            },
            "question": "which u.s. president's wife was not born in north america?",
            "rate_limited": false,
            "answers": [
                "john quincy adams",
                "rutherford b. hayes",
                "martin van buren"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "martin van buren": 0.4027208663980872,
                "john quincy adams": 0.07687381715380737,
                "rutherford b. hayes": 0.3683872075766367
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.0068503196127114,
                    1.9478478165168482,
                    2.0453018638704408
                ],
                "result_count_important_words": [
                    265000.0,
                    61100.0,
                    82000.0
                ],
                "wikipedia_search": [
                    1.5121613331240817,
                    1.034780024063955,
                    2.4530586428119636
                ],
                "word_count_appended_bing": [
                    7.0,
                    11.0,
                    30.0
                ],
                "answer_relation_to_question_bing": [
                    3.203256419624706,
                    1.0269835024310727,
                    1.769760077944221
                ],
                "cosine_similarity_raw": [
                    0.4483479857444763,
                    0.01662919484078884,
                    0.06281065940856934
                ],
                "result_count_noun_chunks": [
                    273000.0,
                    64800.0,
                    83200.0
                ],
                "question_answer_similarity": [
                    5.790412285365164,
                    3.380113546270877,
                    3.8916648902813904
                ],
                "word_count_noun_chunks": [
                    5.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    6.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    3850000.0,
                    14000000.0,
                    1890000.0
                ],
                "result_count": [
                    249000.0,
                    55300.0,
                    74700.0
                ],
                "answer_relation_to_question": [
                    2.9630601104254723,
                    1.5545413258544867,
                    1.4823985637200412
                ],
                "word_count_appended": [
                    150.0,
                    99.0,
                    149.0
                ]
            },
            "integer_answers": {
                "martin van buren": 3,
                "john quincy adams": 10,
                "rutherford b. hayes": 1
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Who was the president of the Screen Actors Guild before its merger with AFTRA?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "gabrielle carteris"
            ],
            "lines": [
                [
                    0.42054615654423294,
                    0.5453968253968254,
                    0.6219188481405568,
                    -0.17112530697925354,
                    0.0028997100289971,
                    0.07977917298116476,
                    0.21305841924398625,
                    0.14945054945054945,
                    0.3790243902439025,
                    0.24782608695652175,
                    0.4050632911392405,
                    0.1805547670081146,
                    0.1016949152542373,
                    0.6923076923076923,
                    0.0
                ],
                [
                    0.07500520644942071,
                    0.15158730158730158,
                    0.037733299716115706,
                    0.29934179764972413,
                    0.9919008099190081,
                    0.03691275167785235,
                    0.20587316463605124,
                    0.7076923076923077,
                    0.10941734417344173,
                    0.2956521739130435,
                    0.08860759493670886,
                    0.5035735620189074,
                    0.05084745762711865,
                    0.0,
                    0.0
                ],
                [
                    0.5044486370063465,
                    0.303015873015873,
                    0.3403478521433276,
                    0.8717835093295294,
                    0.0051994800519948,
                    0.8833080753409829,
                    0.5810684161199625,
                    0.14285714285714285,
                    0.5115582655826558,
                    0.45652173913043476,
                    0.5063291139240507,
                    0.31587167097297814,
                    0.847457627118644,
                    0.3076923076923077,
                    0.0
                ]
            ],
            "fraction_answers": {
                "gabrielle carteris": 0.27631396555119775,
                "ken howard": 0.4698185507347307,
                "melissa gilbert": 0.25386748371407153
            },
            "question": "who was the president of the screen actors guild before its merger with aftra?",
            "rate_limited": false,
            "answers": [
                "gabrielle carteris",
                "melissa gilbert",
                "ken howard"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "gabrielle carteris": 0.614437924037881,
                "ken howard": 0.2968112698996444,
                "melissa gilbert": 0.22911291777945456
            },
            "integer_answers": {
                "gabrielle carteris": 3,
                "ken howard": 8,
                "melissa gilbert": 3
            },
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.0833286020486876,
                    3.021441372113444,
                    1.8952300258378687
                ],
                "result_count_important_words": [
                    6820.0,
                    6590.0,
                    18600.0
                ],
                "wikipedia_search": [
                    2.274146341463415,
                    0.6565040650406504,
                    3.069349593495935
                ],
                "word_count_appended_bing": [
                    32.0,
                    7.0,
                    40.0
                ],
                "answer_relation_to_question_bing": [
                    2.726984126984127,
                    0.7579365079365079,
                    1.515079365079365
                ],
                "cosine_similarity_raw": [
                    0.42212677001953125,
                    0.02561143785715103,
                    0.23101074993610382
                ],
                "result_count_noun_chunks": [
                    13600.0,
                    64400.0,
                    13000.0
                ],
                "question_answer_similarity": [
                    -0.4703610949218273,
                    0.8227814937708899,
                    2.3962151082232594
                ],
                "word_count_noun_chunks": [
                    6.0,
                    3.0,
                    50.0
                ],
                "result_count_bing": [
                    73700.0,
                    34100.0,
                    816000.0
                ],
                "word_count_raw": [
                    9.0,
                    0.0,
                    4.0
                ],
                "result_count": [
                    29.0,
                    9920.0,
                    52.0
                ],
                "answer_relation_to_question": [
                    2.5232769392653975,
                    0.4500312386965243,
                    3.0266918220380785
                ],
                "word_count_appended": [
                    57.0,
                    68.0,
                    105.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Jean Valjean, the protagonist of \u201cLes Mis\u00e9rables,\u201d is identified by what prisoner number?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "24601"
            ],
            "lines": [
                [
                    0.0027472527472527475,
                    0.025,
                    0.022622645358009074,
                    0.4163131932104924,
                    0.005112268930082507,
                    0.2989821882951654,
                    0.03311513284559107,
                    0.002796537054115967,
                    0.06878980891719745,
                    0.20551378446115287,
                    0.2621359223300971,
                    0.2586525636272882,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.804029304029304,
                    0.9083333333333334,
                    0.9602745559466378,
                    0.0845695215090429,
                    0.994624902727746,
                    0.40458015267175573,
                    0.9626492106276473,
                    0.9966382054562223,
                    0.8203821656050956,
                    0.6591478696741855,
                    0.46601941747572817,
                    0.579253800814999,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.1932234432234432,
                    0.06666666666666667,
                    0.0171027986953531,
                    0.4991172852804647,
                    0.0002628283421715805,
                    0.2964376590330789,
                    0.004235656526761648,
                    0.000565257489661738,
                    0.11082802547770701,
                    0.13533834586466165,
                    0.27184466019417475,
                    0.1620936355577128,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "y2k": 0.1144129498411746,
                "24601": 0.7600358885622641,
                "867-5309": 0.12555116159656124
            },
            "question": "jean valjean, the protagonist of \u201cles mis\u00e9rables,\u201d is identified by what prisoner number?",
            "rate_limited": false,
            "answers": [
                "y2k",
                "24601",
                "867-5309"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "y2k": 0.13928949556759776,
                "24601": 0.7906221929744528,
                "867-5309": 0.11671855008043819
            },
            "integer_answers": {
                "y2k": 0,
                "24601": 14,
                "867-5309": 0
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.5519153817637292,
                    3.475522804889994,
                    0.9725618133462768
                ],
                "result_count_important_words": [
                    86.0,
                    2500.0,
                    11.0
                ],
                "wikipedia_search": [
                    0.34394904458598724,
                    4.101910828025478,
                    0.554140127388535
                ],
                "word_count_appended_bing": [
                    27.0,
                    48.0,
                    28.0
                ],
                "answer_relation_to_question_bing": [
                    0.125,
                    4.541666666666667,
                    0.3333333333333333
                ],
                "cosine_similarity_raw": [
                    0.010702831670641899,
                    0.4543083608150482,
                    0.008091378025710583
                ],
                "result_count_noun_chunks": [
                    94.0,
                    33500.0,
                    19.0
                ],
                "question_answer_similarity": [
                    -1.5125535689294338,
                    -0.3072588946670294,
                    -1.8133982863801066
                ],
                "word_count_noun_chunks": [
                    0.0,
                    38.0,
                    0.0
                ],
                "result_count_bing": [
                    23500.0,
                    31800.0,
                    23300.0
                ],
                "word_count_raw": [
                    0.0,
                    12.0,
                    0.0
                ],
                "word_count_appended": [
                    82.0,
                    263.0,
                    54.0
                ],
                "answer_relation_to_question": [
                    0.016483516483516484,
                    4.824175824175824,
                    1.1593406593406592
                ],
                "result_count": [
                    992.0,
                    193000.0,
                    51.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these swimmers has NOT been featured on a Wheaties box?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "amy van dyken"
            ],
            "lines": [
                [
                    0.40705128205128205,
                    0.3675736961451247,
                    0.4288912686319266,
                    0.14035321572949488,
                    0.49918060262952063,
                    0.3247863247863248,
                    0.4989417805842759,
                    0.49287593160894344,
                    0.3380711318795431,
                    0.38031914893617025,
                    0.3,
                    0.39037211255622317,
                    0.2,
                    0.5,
                    -1.0
                ],
                [
                    0.24786324786324787,
                    0.4009070294784581,
                    0.42229759428307095,
                    0.33153579148140055,
                    0.49432008640917724,
                    0.300976800976801,
                    0.49200263687460966,
                    0.464160455940377,
                    0.4842289719626168,
                    0.29787234042553196,
                    0.44,
                    0.26776051054930744,
                    0.4,
                    0.5,
                    -1.0
                ],
                [
                    0.3450854700854701,
                    0.23151927437641723,
                    0.14881113708500243,
                    0.5281109927891046,
                    0.006499310961302074,
                    0.37423687423687424,
                    0.009055582541114438,
                    0.042963612450679534,
                    0.1776998961578401,
                    0.3218085106382979,
                    0.26,
                    0.34186737689446944,
                    0.4,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "amy van dyken": 0.24736907206588146,
                "norman ross": 0.20801064767934307,
                "ryan lochte": 0.5446202802547755
            },
            "question": "which of these swimmers has not been featured on a wheaties box?",
            "rate_limited": false,
            "answers": [
                "amy van dyken",
                "norman ross",
                "ryan lochte"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "amy van dyken": 0.4556145011180031,
                "norman ross": 0.21834245662727622,
                "ryan lochte": 0.4214740476190364
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.8770230995502146,
                    1.8579159156055405,
                    1.2650609848442447
                ],
                "result_count_important_words": [
                    61.0,
                    461.0,
                    28300.0
                ],
                "wikipedia_search": [
                    0.9715732087227413,
                    0.09462616822429906,
                    1.9338006230529594
                ],
                "word_count_appended_bing": [
                    10.0,
                    3.0,
                    12.0
                ],
                "answer_relation_to_question_bing": [
                    0.7945578231292517,
                    0.5945578231292517,
                    1.6108843537414965
                ],
                "cosine_similarity_raw": [
                    0.018851783126592636,
                    0.0205998457968235,
                    0.09310440719127655
                ],
                "result_count_noun_chunks": [
                    65.0,
                    327.0,
                    4170.0
                ],
                "question_answer_similarity": [
                    2.776972336694598,
                    1.3007775051519275,
                    -0.2170558802317828
                ],
                "word_count_noun_chunks": [
                    3.0,
                    1.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    28700.0,
                    32600.0,
                    20600.0
                ],
                "word_count_appended": [
                    45.0,
                    76.0,
                    67.0
                ],
                "answer_relation_to_question": [
                    0.5576923076923077,
                    1.5128205128205128,
                    0.9294871794871795
                ],
                "result_count": [
                    44.0,
                    305.0,
                    26500.0
                ]
            },
            "integer_answers": {
                "amy van dyken": 2,
                "norman ross": 4,
                "ryan lochte": 8
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What is the grammatically correct way to announce people have arrived?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "they're here"
            ],
            "lines": [
                [
                    0.38744343891402716,
                    0.44000000000000006,
                    0.21004873493846968,
                    0.4380747507787122,
                    0.04234156863528331,
                    0.6425470332850941,
                    0.44341779939407305,
                    0.8759124087591241,
                    0.0,
                    0.7115384615384616,
                    0.3333333333333333,
                    0.37786737443081436,
                    0,
                    0,
                    1.0
                ],
                [
                    0.4332579185520362,
                    0.20666666666666664,
                    0.33951411565257633,
                    0.2967852168339446,
                    0.9546825380974255,
                    0.19247467438494936,
                    0.5548526390323741,
                    0.08759124087591241,
                    0.5,
                    0.07692307692307693,
                    0.3333333333333333,
                    0.3849612004576095,
                    0,
                    0,
                    1.0
                ],
                [
                    0.17929864253393665,
                    0.35333333333333333,
                    0.450437149408954,
                    0.2651400323873432,
                    0.0029758932672911383,
                    0.1649782923299566,
                    0.001729561573552798,
                    0.0364963503649635,
                    0.5,
                    0.21153846153846154,
                    0.3333333333333333,
                    0.23717142511157616,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "their here": 0.22803603959855853,
                "there here": 0.36342021840082533,
                "they're here": 0.4085437420006161
            },
            "question": "what is the grammatically correct way to announce people have arrived?",
            "rate_limited": false,
            "answers": [
                "they're here",
                "there here",
                "their here"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "their here": 0.10746396581328271,
                "there here": 0.10483264576317793,
                "they're here": 0.4238356690071211
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.2672042465848863,
                    2.309767202745657,
                    1.4230285506694569
                ],
                "result_count_important_words": [
                    1910000.0,
                    2390000.0,
                    7450.0
                ],
                "wikipedia_search": [
                    0.0,
                    2.0,
                    2.0
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    2.2,
                    1.0333333333333332,
                    1.7666666666666666
                ],
                "cosine_similarity_raw": [
                    0.05089009553194046,
                    0.08225665241479874,
                    0.10913081467151642
                ],
                "result_count_noun_chunks": [
                    2280000.0,
                    228000.0,
                    95000.0
                ],
                "question_answer_similarity": [
                    17.52471286058426,
                    11.872575849294662,
                    10.60664401948452
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    444000.0,
                    133000.0,
                    114000.0
                ],
                "word_count_appended": [
                    37.0,
                    4.0,
                    11.0
                ],
                "answer_relation_to_question": [
                    1.5497737556561086,
                    1.7330316742081449,
                    0.7171945701357466
                ],
                "result_count": [
                    106000.0,
                    2390000.0,
                    7450.0
                ]
            },
            "integer_answers": {
                "their here": 1,
                "there here": 5,
                "they're here": 6
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "In a 2017 report, what country did the UN name as the world\u2019s happiest?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "norway"
            ],
            "question": "in a 2017 report, what country did the un name as the world\u2019s happiest?",
            "lines": [
                [
                    0.4142937812966977,
                    0.38378707333225937,
                    0.11710988916276709,
                    0.3510428671640295,
                    0.1504950495049505,
                    0.32577720207253885,
                    0.43805165086629616,
                    0.35181975736568455,
                    0.27379677250233875,
                    0.2857142857142857,
                    0.2631578947368421,
                    0.2899270898245983,
                    0.045454545454545456,
                    0.0,
                    1.0
                ],
                [
                    0.2504394317741376,
                    0.24494263366088526,
                    0.6525626891414307,
                    0.8168723728440077,
                    0.15742574257425743,
                    0.34196891191709844,
                    0.4968944099378882,
                    0.36048526863084923,
                    0.5302742719295983,
                    0.4773809523809524,
                    0.4824561403508772,
                    0.4318948503852316,
                    0.9545454545454546,
                    1.0,
                    1.0
                ],
                [
                    0.3352667869291647,
                    0.3712702930068553,
                    0.23032742169580228,
                    -0.16791524000803726,
                    0.692079207920792,
                    0.3322538860103627,
                    0.06505393919581563,
                    0.2876949740034662,
                    0.19592895556806292,
                    0.2369047619047619,
                    0.2543859649122807,
                    0.27817805979017013,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "nepal",
                "norway",
                "nicaragua"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "norway": 0.8474588156456981,
                "nepal": 0.050320676001852874,
                "nicaragua": 0.12746140109447912
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.73956253894759,
                    2.59136910231139,
                    1.6690683587410207
                ],
                "result_count_important_words": [
                    1340000.0,
                    1520000.0,
                    199000.0
                ],
                "wikipedia_search": [
                    1.6427806350140324,
                    3.18164563157759,
                    1.1755737334083776
                ],
                "word_count_appended_bing": [
                    30.0,
                    55.0,
                    29.0
                ],
                "answer_relation_to_question_bing": [
                    1.918935366661297,
                    1.2247131683044263,
                    1.8563514650342765
                ],
                "cosine_similarity_raw": [
                    0.01776442490518093,
                    0.09898737818002701,
                    0.03493841737508774
                ],
                "result_count_noun_chunks": [
                    6090000.0,
                    6240000.0,
                    4980000.0
                ],
                "question_answer_similarity": [
                    0.5069574299268425,
                    1.1796836154535413,
                    -0.24249425492598675
                ],
                "word_count_noun_chunks": [
                    1.0,
                    21.0,
                    0.0
                ],
                "result_count_bing": [
                    50300000.0,
                    52800000.0,
                    51300000.0
                ],
                "word_count_raw": [
                    0.0,
                    47.0,
                    0.0
                ],
                "word_count_appended": [
                    240.0,
                    401.0,
                    199.0
                ],
                "answer_relation_to_question": [
                    2.0714689064834886,
                    1.252197158870688,
                    1.6763339346458233
                ],
                "result_count": [
                    1520000.0,
                    1590000.0,
                    6990000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these is NOT a Slavic language?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "serbian"
            ],
            "lines": [
                [
                    0.2805944055944056,
                    0.2809224318658281,
                    0.34664560477454076,
                    0.3406021580808316,
                    0.4350254524886878,
                    0.327755905511811,
                    0.4743980738362761,
                    0.4315270935960591,
                    0.1857976653696498,
                    0.33437744714173845,
                    0.3218390804597701,
                    0.3236600370477425,
                    0.13157894736842107,
                    0.25,
                    -1.0
                ],
                [
                    0.30157342657342656,
                    0.31813417190775684,
                    0.3703479882766807,
                    0.3457162185637367,
                    0.13518099547511314,
                    0.375,
                    0.09069020866773675,
                    0.18103448275862066,
                    0.43093385214007784,
                    0.33555207517619423,
                    0.3247126436781609,
                    0.3249196472438972,
                    0.368421052631579,
                    0.25,
                    -1.0
                ],
                [
                    0.4178321678321678,
                    0.40094339622641506,
                    0.28300640694877854,
                    0.3136816233554317,
                    0.42979355203619907,
                    0.297244094488189,
                    0.43491171749598717,
                    0.3874384236453202,
                    0.38326848249027234,
                    0.3300704776820673,
                    0.35344827586206895,
                    0.3514203157083603,
                    0.5,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "bulgarian": 0.36218224240917685,
                "hungarian": 0.23099158088982039,
                "serbian": 0.4068261767010029
            },
            "question": "which of these is not a slavic language?",
            "rate_limited": false,
            "answers": [
                "bulgarian",
                "serbian",
                "hungarian"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "bulgarian": 0.10870923980965118,
                "hungarian": 0.3542047625882654,
                "serbian": 0.5054717125249937
            },
            "integer_answers": {
                "bulgarian": 7,
                "hungarian": 4,
                "serbian": 3
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.7053598518090299,
                    0.7003214110244111,
                    0.5943187371665588
                ],
                "result_count_important_words": [
                    319000.0,
                    5100000.0,
                    811000.0
                ],
                "wikipedia_search": [
                    1.2568093385214008,
                    0.27626459143968873,
                    0.4669260700389105
                ],
                "word_count_appended_bing": [
                    62.0,
                    61.0,
                    51.0
                ],
                "answer_relation_to_question_bing": [
                    0.8763102725366876,
                    0.7274633123689727,
                    0.39622641509433965
                ],
                "cosine_similarity_raw": [
                    0.051789019256830215,
                    0.04378453269600868,
                    0.07328049093484879
                ],
                "result_count_noun_chunks": [
                    556000.0,
                    2590000.0,
                    914000.0
                ],
                "question_answer_similarity": [
                    0.840626840479672,
                    0.81365648470819,
                    0.9825994279235601
                ],
                "word_count_noun_chunks": [
                    14.0,
                    5.0,
                    0.0
                ],
                "result_count_bing": [
                    17500000.0,
                    12700000.0,
                    20600000.0
                ],
                "word_count_raw": [
                    4.0,
                    4.0,
                    0.0
                ],
                "result_count": [
                    919000.0,
                    5160000.0,
                    993000.0
                ],
                "answer_relation_to_question": [
                    0.8776223776223776,
                    0.7937062937062938,
                    0.32867132867132864
                ],
                "word_count_appended": [
                    423.0,
                    420.0,
                    434.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "For professional reasons, a phlebotomist should NOT be afraid of what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "thunder"
            ],
            "question": "for professional reasons, a phlebotomist should not be afraid of what?",
            "lines": [
                [
                    0.08333333333333331,
                    0.5,
                    0.25029383670004896,
                    0.3335373401602195,
                    0.2093295827473043,
                    0.3085289066971952,
                    0.18504479669193657,
                    0.44733475479744134,
                    0.5,
                    0.24959612277867527,
                    0.3412698412698413,
                    0.28338770345298886,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.41666666666666663,
                    0.0,
                    0.3832754285217247,
                    0.3511920890316521,
                    0.38396624472573837,
                    0.32942186605609614,
                    0.44038594073053067,
                    0.45778251599147124,
                    0.5,
                    0.38206785137318255,
                    0.35317460317460314,
                    0.36422136192998794,
                    0.5,
                    0.5,
                    1.0
                ],
                [
                    0.5,
                    0.5,
                    0.3664307347782263,
                    0.31527057080812837,
                    0.40670417252695734,
                    0.3620492272467086,
                    0.37456926257753276,
                    0.09488272921108742,
                    0.0,
                    0.3683360258481422,
                    0.3055555555555556,
                    0.35239093461702325,
                    0.5,
                    0.5,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "needles",
                "thunder",
                "snakes"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "thunder": 0.7784925821989044,
                "needles": 0.054978268691088114,
                "snakes": 0.7426740745844145
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7328983723760893,
                    1.0862291045600967,
                    1.1808725230638142
                ],
                "result_count_important_words": [
                    4570000.0,
                    865000.0,
                    1820000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_count_appended_bing": [
                    40.0,
                    37.0,
                    49.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    1.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.061258211731910706,
                    0.02863501012325287,
                    0.03276737034320831
                ],
                "result_count_noun_chunks": [
                    247000.0,
                    198000.0,
                    1900000.0
                ],
                "question_answer_similarity": [
                    2.038867197930813,
                    1.8226283825933933,
                    2.262602159753442
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    669000.0,
                    596000.0,
                    482000.0
                ],
                "word_count_appended": [
                    310.0,
                    146.0,
                    163.0
                ],
                "answer_relation_to_question": [
                    1.6666666666666665,
                    0.3333333333333333,
                    0.0
                ],
                "result_count": [
                    124000.0,
                    49500.0,
                    39800.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Catching a catfish with your bare hands is called what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "noodling"
            ],
            "lines": [
                [
                    0.03434343434343434,
                    0.0,
                    0.014583527288588062,
                    0.8138307149619655,
                    0.02710086329362911,
                    0.3318777292576419,
                    0.019532939310941327,
                    0.017388555169143217,
                    0.06666666666666667,
                    0.17766497461928935,
                    0.22875816993464052,
                    0.30416475927441833,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.15833333333333333,
                    0.0,
                    0.008060569149646845,
                    -0.19897317461342343,
                    0.0003278330237132554,
                    0.33449781659388644,
                    0.0002162318004901254,
                    0.0002709904701684657,
                    0.06666666666666667,
                    0.04060913705583756,
                    0.20261437908496732,
                    0.05542314514760412,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.8073232323232323,
                    1.0,
                    0.977355903561765,
                    0.385142459651458,
                    0.9725713036826577,
                    0.33362445414847164,
                    0.9802508288885685,
                    0.9823404543606883,
                    0.8666666666666666,
                    0.7817258883248731,
                    0.5686274509803921,
                    0.6404120955779776,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "noodling": 0.8068600527261964,
                "whiskering": 0.04771763769377791,
                "strumming": 0.14542230958002555
            },
            "question": "catching a catfish with your bare hands is called what?",
            "rate_limited": false,
            "answers": [
                "strumming",
                "whiskering",
                "noodling"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "noodling": 0.8537698832596685,
                "whiskering": 0.08752269759025946,
                "strumming": 0.06161239096497993
            },
            "integer_answers": {
                "noodling": 12,
                "whiskering": 1,
                "strumming": 1
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.5208237963720916,
                    0.2771157257380206,
                    3.202060477889888
                ],
                "result_count_important_words": [
                    542.0,
                    6.0,
                    27200.0
                ],
                "wikipedia_search": [
                    0.3333333333333333,
                    0.3333333333333333,
                    4.333333333333333
                ],
                "answer_relation_to_question": [
                    0.1717171717171717,
                    0.7916666666666666,
                    4.036616161616162
                ],
                "word_count_appended_bing": [
                    35.0,
                    31.0,
                    87.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    3.0
                ],
                "cosine_similarity_raw": [
                    0.011091290041804314,
                    0.006130348891019821,
                    0.7433138489723206
                ],
                "result_count_noun_chunks": [
                    770.0,
                    12.0,
                    43500.0
                ],
                "question_answer_similarity": [
                    1.4455587603151798,
                    -0.35342413396574557,
                    0.6841054856777191
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    144.0
                ],
                "result_count_bing": [
                    3800000.0,
                    3830000.0,
                    3820000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    73.0
                ],
                "result_count": [
                    496.0,
                    6.0,
                    17800.0
                ],
                "word_count_appended": [
                    105.0,
                    24.0,
                    462.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Talking is discouraged on what Amtrak car?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "quiet car"
            ],
            "lines": [
                [
                    0.13563760146635245,
                    0.4236874236874237,
                    0.18355840100394022,
                    0.3747164863362907,
                    0.9894422843657126,
                    0.20611265471078555,
                    0.9909605065983468,
                    0.7996187248464308,
                    0.23684210526315788,
                    0.09401709401709402,
                    0.13333333333333333,
                    0.31346059063812154,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.02641180064589334,
                    0.047619047619047616,
                    0.0533048761957842,
                    0.36944963713959095,
                    0.010187590927913633,
                    0.7476635514018691,
                    0.00873132885386958,
                    0.1678669773353103,
                    0.15789473684210525,
                    0.0,
                    0.0,
                    0.3383513228520838,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.8379505978877542,
                    0.5286935286935287,
                    0.7631367228002756,
                    0.2558338765241184,
                    0.0003701247063738406,
                    0.04622379388734529,
                    0.00030816454778363224,
                    0.03251429781825884,
                    0.6052631578947368,
                    0.905982905982906,
                    0.8666666666666667,
                    0.3481880865097946,
                    0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "sports argument car": 0.3754913235589991,
                "quiet car": 0.47624091722458023,
                "meet & greet car": 0.1482677592164206
            },
            "question": "talking is discouraged on what amtrak car?",
            "rate_limited": false,
            "answers": [
                "sports argument car",
                "meet & greet car",
                "quiet car"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sports argument car": 0.09573648083777203,
                "quiet car": 0.7610576120966963,
                "meet & greet car": 0.2712017493599086
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2538423625524862,
                    1.3534052914083352,
                    1.3927523460391784
                ],
                "result_count_important_words": [
                    3280000.0,
                    28900.0,
                    1020.0
                ],
                "wikipedia_search": [
                    0.47368421052631576,
                    0.3157894736842105,
                    1.2105263157894737
                ],
                "word_count_appended_bing": [
                    2.0,
                    0.0,
                    13.0
                ],
                "answer_relation_to_question_bing": [
                    1.271062271062271,
                    0.14285714285714285,
                    1.5860805860805862
                ],
                "cosine_similarity_raw": [
                    0.11452482640743256,
                    0.033257707953453064,
                    0.4761323928833008
                ],
                "result_count_noun_chunks": [
                    1510000.0,
                    317000.0,
                    61400.0
                ],
                "question_answer_similarity": [
                    7.02377974241972,
                    6.925056600943208,
                    4.7954143062233925
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    2.0
                ],
                "result_count_bing": [
                    4080000.0,
                    14800000.0,
                    915000.0
                ],
                "word_count_appended": [
                    11.0,
                    0.0,
                    106.0
                ],
                "answer_relation_to_question": [
                    0.40691280439905736,
                    0.07923540193768003,
                    2.5138517936632625
                ],
                "result_count": [
                    2700000.0,
                    27800.0,
                    1010.0
                ]
            },
            "integer_answers": {
                "sports argument car": 4,
                "quiet car": 8,
                "meet & greet car": 1
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What generation of the iPod was the first to offer video?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "u2 special edition"
            ],
            "lines": [
                [
                    0.23642049487063682,
                    0.21116724597849812,
                    0.13295137908355617,
                    0.36328898009034727,
                    0.4896551724137931,
                    0.47173647824318826,
                    0.3778060144006777,
                    0.42745861733203505,
                    0.005108556832694763,
                    0.2556818181818182,
                    0.3,
                    0.3303793209022658,
                    0,
                    0,
                    1.0
                ],
                [
                    0.501546765802357,
                    0.39122104827704257,
                    0.6692121477259302,
                    0.3134969339667955,
                    0.2620689655172414,
                    0.058560390402602684,
                    0.17746717492587885,
                    0.32424537487828625,
                    0.572972972972973,
                    0.5340909090909091,
                    0.25,
                    0.3108609855428548,
                    0,
                    0,
                    1.0
                ],
                [
                    0.26203273932700616,
                    0.39761170574445925,
                    0.19783647319051367,
                    0.3232140859428573,
                    0.2482758620689655,
                    0.46970313135420905,
                    0.44472681067344344,
                    0.24829600778967867,
                    0.4219184701943323,
                    0.21022727272727273,
                    0.45,
                    0.3587596935548793,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "third generation": 0.3001378398607926,
                "u2 special edition": 0.36381197242523927,
                "fifth generation": 0.33605018771396816
            },
            "question": "what generation of the ipod was the first to offer video?",
            "rate_limited": false,
            "answers": [
                "third generation",
                "u2 special edition",
                "fifth generation"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "third generation": 0.10255437968158654,
                "u2 special edition": 0.5073212434532997,
                "fifth generation": 0.32498995956951027
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3215172836090632,
                    1.2434439421714192,
                    1.4350387742195172
                ],
                "result_count_important_words": [
                    892000.0,
                    419000.0,
                    1050000.0
                ],
                "wikipedia_search": [
                    0.01532567049808429,
                    1.7189189189189191,
                    1.265755410582997
                ],
                "word_count_appended_bing": [
                    6.0,
                    5.0,
                    9.0
                ],
                "answer_relation_to_question_bing": [
                    0.8446689839139925,
                    1.5648841931081703,
                    1.590446822977837
                ],
                "cosine_similarity_raw": [
                    0.055243492126464844,
                    0.2780686914920807,
                    0.08220431953668594
                ],
                "result_count_noun_chunks": [
                    439000.0,
                    333000.0,
                    255000.0
                ],
                "question_answer_similarity": [
                    8.978684231638908,
                    7.748074210714549,
                    7.988233543932438
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    23200000.0,
                    2880000.0,
                    23100000.0
                ],
                "word_count_appended": [
                    45.0,
                    94.0,
                    37.0
                ],
                "answer_relation_to_question": [
                    0.9456819794825473,
                    2.006187063209428,
                    1.0481309573080246
                ],
                "result_count": [
                    213000.0,
                    114000.0,
                    108000.0
                ]
            },
            "integer_answers": {
                "third generation": 4,
                "u2 special edition": 4,
                "fifth generation": 4
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these two U.S. cities are in the same time zone?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "el paso / pierre"
            ],
            "lines": [
                [
                    0.2738816738816739,
                    0.2527322404371585,
                    0.37000025464194986,
                    0.30697650259698,
                    0.9999526088810957,
                    0.33602150537634407,
                    0.8018018018018018,
                    0.9998577727208079,
                    0.014492753623188406,
                    0.5,
                    0.3333333333333333,
                    0.7640071286527677,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.4554988662131519,
                    0.5027322404371585,
                    0.35693725360378614,
                    0.11830706248428483,
                    4.739111890431733e-05,
                    0.3279569892473118,
                    0.1981981981981982,
                    0.00014222727919214906,
                    0.7671497584541062,
                    0.25,
                    0.3333333333333333,
                    0.14671445452397658,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.27061945990517416,
                    0.24453551912568305,
                    0.273062491754264,
                    0.5747164349187351,
                    0.0,
                    0.33602150537634407,
                    0.0,
                    0.0,
                    0.2183574879227053,
                    0.25,
                    0.3333333333333333,
                    0.08927841682325573,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "el paso / pierre": 0.4960881313289252,
                "pensacola / sioux falls": 0.2158270540966246,
                "bismarck / cheyenne": 0.2880848145744503
            },
            "question": "which of these two u.s. cities are in the same time zone?",
            "rate_limited": false,
            "answers": [
                "el paso / pierre",
                "bismarck / cheyenne",
                "pensacola / sioux falls"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "el paso / pierre": 0.3647841823829532,
                "pensacola / sioux falls": 0.09118398094194266,
                "bismarck / cheyenne": 0.117972943594293
            },
            "integer_answers": {
                "el paso / pierre": 8,
                "pensacola / sioux falls": 1,
                "bismarck / cheyenne": 3
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.0560285146110706,
                    0.5868578180959063,
                    0.3571136672930229
                ],
                "result_count_important_words": [
                    89.0,
                    22.0,
                    0
                ],
                "wikipedia_search": [
                    0.043478260869565216,
                    2.3014492753623186,
                    0.6550724637681159
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.010928961748634,
                    2.010928961748634,
                    0.9781420765027322
                ],
                "cosine_similarity_raw": [
                    0.05261365324258804,
                    0.05075610801577568,
                    0.03882920369505882
                ],
                "result_count_noun_chunks": [
                    70300.0,
                    10.0,
                    0
                ],
                "question_answer_similarity": [
                    2.8946413625963032,
                    1.115578924305737,
                    5.419300663750619
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    12500000.0,
                    12200000.0,
                    12500000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    6.0,
                    3.0,
                    3.0
                ],
                "answer_relation_to_question": [
                    1.0955266955266956,
                    1.8219954648526075,
                    1.0824778396206967
                ],
                "result_count": [
                    211000.0,
                    10.0,
                    0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Who was NOT a wife of Henry VIII?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "catherine of york"
            ],
            "lines": [
                [
                    0.30416878387533874,
                    0.273227969348659,
                    0.24627353963669912,
                    0.45652423034116846,
                    0.24540162716018565,
                    0.48688658351789255,
                    0.24504072246510822,
                    0.2510118876884342,
                    0.4230769230769231,
                    0.26884422110552764,
                    0.24285714285714288,
                    0.2775194125226105,
                    0.13636363636363635,
                    0.2647058823529412,
                    0.0
                ],
                [
                    0.3618342193037315,
                    0.4308634373289546,
                    0.41860003039952504,
                    0.18501086754974566,
                    0.49997730599462153,
                    0.026168551113183336,
                    0.4999758534388964,
                    0.4999681295216241,
                    0.3205128205128205,
                    0.4623115577889447,
                    0.4857142857142857,
                    0.45132491706825284,
                    0.5,
                    0.5,
                    0.0
                ],
                [
                    0.33399699682092976,
                    0.2959085933223864,
                    0.33512642996377584,
                    0.3584649021090859,
                    0.2546210668451928,
                    0.4869448653689241,
                    0.2549834240959954,
                    0.2490199827899417,
                    0.2564102564102564,
                    0.26884422110552764,
                    0.27142857142857146,
                    0.2711556704091367,
                    0.36363636363636365,
                    0.23529411764705882,
                    0.0
                ]
            ],
            "fraction_answers": {
                "catherine of york": 0.19396257489505914,
                "catherine parr": 0.4111567768125332,
                "catherine howard": 0.39488064829240765
            },
            "question": "who was not a wife of henry viii?",
            "rate_limited": false,
            "answers": [
                "catherine parr",
                "catherine of york",
                "catherine howard"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "catherine of york": 0.5570992134828493,
                "catherine parr": 0.10179986881572885,
                "catherine howard": 0.16192750036978845
            },
            "integer_answers": {
                "catherine of york": 2,
                "catherine parr": 8,
                "catherine howard": 4
            },
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3348835248643371,
                    0.2920504975904829,
                    1.37306597754518
                ],
                "result_count_important_words": [
                    359000.0,
                    34.0,
                    345000.0
                ],
                "wikipedia_search": [
                    0.46153846153846156,
                    1.0769230769230769,
                    1.4615384615384617
                ],
                "word_count_appended_bing": [
                    36.0,
                    2.0,
                    32.0
                ],
                "answer_relation_to_question_bing": [
                    1.360632183908046,
                    0.41481937602627256,
                    1.2245484400656814
                ],
                "cosine_similarity_raw": [
                    0.22217632830142975,
                    0.07127812504768372,
                    0.14437203109264374
                ],
                "result_count_noun_chunks": [
                    375000.0,
                    48.0,
                    378000.0
                ],
                "question_answer_similarity": [
                    0.8619754314422607,
                    6.24515438079834,
                    2.8061556592583656
                ],
                "word_count_noun_chunks": [
                    8.0,
                    0.0,
                    3.0
                ],
                "result_count_bing": [
                    2250000.0,
                    81300000.0,
                    2240000.0
                ],
                "word_count_raw": [
                    8.0,
                    0.0,
                    9.0
                ],
                "result_count": [
                    359000.0,
                    32.0,
                    346000.0
                ],
                "answer_relation_to_question": [
                    1.1749872967479675,
                    0.828994684177611,
                    0.9960180190744216
                ],
                "word_count_appended": [
                    184.0,
                    30.0,
                    184.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "The metal band that holds the eraser on the end of a pencil is called what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ferrule"
            ],
            "lines": [
                [
                    0.09639671840958607,
                    0.12777777777777777,
                    0.07271303740153966,
                    0.4560681975686811,
                    0.0038883306391716546,
                    0.38692390139335475,
                    0.0709186101856259,
                    0.0506631616680607,
                    0.04591836734693877,
                    0.17455138662316477,
                    0.25471698113207547,
                    0.24879361895225713,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.5333792892156862,
                    0.3333333333333333,
                    0.7818324590790396,
                    -0.836822073734617,
                    0.4805801913582944,
                    0.21864951768488747,
                    0.8215135649690624,
                    0.8603178396463138,
                    0.8649659863945578,
                    0.5269168026101142,
                    0.4716981132075472,
                    0.42890744482396587,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.3702239923747277,
                    0.5388888888888889,
                    0.14545450351942066,
                    1.380753876165936,
                    0.515531478002534,
                    0.39442658092175775,
                    0.10756782484531176,
                    0.08901899868562552,
                    0.0891156462585034,
                    0.29853181076672103,
                    0.27358490566037735,
                    0.32229893622377703,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "tragus": 0.14209500636415956,
                "aglet": 0.3232426744509701,
                "ferrule": 0.5346623191848705
            },
            "question": "the metal band that holds the eraser on the end of a pencil is called what?",
            "rate_limited": false,
            "answers": [
                "tragus",
                "ferrule",
                "aglet"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "tragus": 0.13412876539199733,
                "aglet": 0.10994925386666303,
                "ferrule": 0.776432132067992
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7415553326658,
                    3.002352113767761,
                    2.256092553566439
                ],
                "result_count_important_words": [
                    7450.0,
                    86300.0,
                    11300.0
                ],
                "wikipedia_search": [
                    0.3214285714285714,
                    6.054761904761905,
                    0.6238095238095238
                ],
                "word_count_appended_bing": [
                    27.0,
                    50.0,
                    29.0
                ],
                "answer_relation_to_question_bing": [
                    0.3833333333333333,
                    1.0,
                    1.6166666666666667
                ],
                "cosine_similarity_raw": [
                    0.02090507745742798,
                    0.2247776836156845,
                    0.04181832820177078
                ],
                "result_count_noun_chunks": [
                    8480.0,
                    144000.0,
                    14900.0
                ],
                "question_answer_similarity": [
                    -0.521819218993187,
                    0.9574661054648459,
                    -1.5798161615384743
                ],
                "word_count_noun_chunks": [
                    0.0,
                    70.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    43.0,
                    0.0
                ],
                "result_count_bing": [
                    361000.0,
                    204000.0,
                    368000.0
                ],
                "word_count_appended": [
                    107.0,
                    323.0,
                    183.0
                ],
                "answer_relation_to_question": [
                    0.5783803104575164,
                    3.2002757352941176,
                    2.221343954248366
                ],
                "result_count": [
                    89.0,
                    11000.0,
                    11800.0
                ]
            },
            "integer_answers": {
                "tragus": 0,
                "aglet": 3,
                "ferrule": 11
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these is NOT a step in the famous Korean 10-step skin care regime?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "spelunking"
            ],
            "lines": [
                [
                    0.31541468076655743,
                    0.43366666666666664,
                    0.29150090393827166,
                    0.15906619240928,
                    0.2187250672760957,
                    0.14471403812824957,
                    0.20857666323142104,
                    0.16343564889606893,
                    0.4184626436781609,
                    0.29101899827288424,
                    0.31521739130434784,
                    0.29717944872943014,
                    0.11363636363636365,
                    0.07142857142857145,
                    -1.0
                ],
                [
                    0.20074272664084997,
                    0.14966666666666667,
                    0.26285103966736934,
                    0.2844611204772735,
                    0.28498695046586164,
                    0.48994800693240903,
                    0.2984655902897057,
                    0.34203913121522167,
                    0.3315373563218391,
                    0.33506044905008636,
                    0.3315217391304348,
                    0.2961734585174761,
                    0.38636363636363635,
                    0.4285714285714286,
                    -1.0
                ],
                [
                    0.4838425925925926,
                    0.4166666666666667,
                    0.445648056394359,
                    0.5564726871134464,
                    0.4962879822580427,
                    0.3653379549393414,
                    0.49295774647887325,
                    0.4945252198887094,
                    0.25,
                    0.3739205526770294,
                    0.3532608695652174,
                    0.4066470927530937,
                    0.5,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "moisturizing": 0.36823009995567735,
                "spelunking": 0.12349036838180404,
                "cleansing": 0.5082795316625186
            },
            "question": "which of these is not a step in the famous korean 10-step skin care regime?",
            "rate_limited": false,
            "answers": [
                "cleansing",
                "moisturizing",
                "spelunking"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "moisturizing": 0.2063267973494002,
                "spelunking": 0.44465527131842314,
                "cleansing": 0.1615122192269276
            },
            "integer_answers": {
                "moisturizing": 4,
                "spelunking": 1,
                "cleansing": 9
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.839487717787977,
                    2.853571580755334,
                    1.3069407014566883
                ],
                "result_count_important_words": [
                    509000.0,
                    352000.0,
                    12300.0
                ],
                "wikipedia_search": [
                    0.6522988505747127,
                    1.3477011494252873,
                    2.0
                ],
                "answer_relation_to_question": [
                    1.4766825538675405,
                    2.3940581868732003,
                    0.12925925925925924
                ],
                "word_count_appended_bing": [
                    34.0,
                    31.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    0.6633333333333332,
                    3.5033333333333334,
                    0.8333333333333333
                ],
                "cosine_similarity_raw": [
                    0.16280262172222137,
                    0.18517333269119263,
                    0.04243969917297363
                ],
                "result_count_noun_chunks": [
                    750000.0,
                    352000.0,
                    12200.0
                ],
                "question_answer_similarity": [
                    3.367438416928053,
                    2.1288997661322355,
                    -0.5577865610830486
                ],
                "word_count_noun_chunks": [
                    17.0,
                    5.0,
                    0.0
                ],
                "result_count_bing": [
                    2050000.0,
                    58000.0,
                    777000.0
                ],
                "word_count_raw": [
                    6.0,
                    1.0,
                    0.0
                ],
                "result_count": [
                    416000.0,
                    318000.0,
                    5490.0
                ],
                "word_count_appended": [
                    242.0,
                    191.0,
                    146.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these artists does NOT have a namesake museum?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "\u00c9douard manet"
            ],
            "question": "which of these artists does not have a namesake museum?",
            "lines": [
                [
                    0.3441884790040308,
                    0.3055555555555556,
                    0.36480940447249355,
                    0.9614161315794076,
                    0.3567073170731707,
                    0.25512905360688287,
                    0.34878204543381075,
                    0.35108024691358025,
                    0.3980855855855856,
                    0.5,
                    0.5,
                    0.3439276632861057,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.3553880300015307,
                    0.3888888888888889,
                    0.3751144100632036,
                    0.5909437532678581,
                    0.45286116322701686,
                    0.4880873593646592,
                    0.4887783961317398,
                    0.48842592592592593,
                    0.3091216216216216,
                    0.2777777777777778,
                    0.36486486486486486,
                    0.3405998211456239,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.3004234909944385,
                    0.3055555555555556,
                    0.26007618546430283,
                    -0.5523598848472658,
                    0.19043151969981237,
                    0.25678358702845794,
                    0.1624395584344494,
                    0.16049382716049382,
                    0.2927927927927928,
                    0.2222222222222222,
                    0.13513513513513514,
                    0.3154725155682704,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "\u00c9douard manet",
                "james mcneill whistler",
                "john singer sargent"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "\u00c9douard manet": 0.4764541606576105,
                "john singer sargent": 0.07271779142883318,
                "james mcneill whistler": 0.21808986374749326
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.9364340202833659,
                    0.9564010731262567,
                    1.1071649065903775
                ],
                "result_count_important_words": [
                    66300.0,
                    4920.0,
                    148000.0
                ],
                "wikipedia_search": [
                    0.40765765765765766,
                    0.7635135135135135,
                    0.8288288288288288
                ],
                "word_count_appended_bing": [
                    0.0,
                    10.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    0.7777777777777778,
                    0.4444444444444444,
                    0.7777777777777777
                ],
                "cosine_similarity_raw": [
                    0.02975149266421795,
                    0.027483662590384483,
                    0.05280020833015442
                ],
                "result_count_noun_chunks": [
                    57900.0,
                    4500.0,
                    132000.0
                ],
                "question_answer_similarity": [
                    -1.6083155190572143,
                    -0.31699422653764486,
                    3.668113484978676
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    5920000.0,
                    288000.0,
                    5880000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    61100.0,
                    20100.0,
                    132000.0
                ],
                "answer_relation_to_question": [
                    0.6232460839838767,
                    0.5784478799938773,
                    0.798306036022246
                ],
                "word_count_appended": [
                    0.0,
                    36.0,
                    45.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Napa cabbage is named after what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "city in california"
            ],
            "lines": [
                [
                    0.3181818181818182,
                    0.0,
                    0.04559913817466954,
                    0.31364525355347306,
                    0.7333333333333333,
                    0.5239096163951655,
                    0.7567567567567568,
                    0.5111111111111111,
                    0.0,
                    0.8125,
                    0.5,
                    0.5214486638537271,
                    0,
                    1.0,
                    1.0
                ],
                [
                    0.3246753246753247,
                    1.0,
                    0.6300719691543386,
                    0.21419926224198277,
                    0.05,
                    0.28218602207041515,
                    0.04054054054054054,
                    0.06666666666666667,
                    0.0,
                    0.1875,
                    0.5,
                    0.16877637130801687,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.35714285714285715,
                    0.0,
                    0.32432889267099185,
                    0.4721554842045442,
                    0.21666666666666667,
                    0.19390436153441934,
                    0.20270270270270271,
                    0.4222222222222222,
                    1.0,
                    0.0,
                    0.0,
                    0.30977496483825595,
                    0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "japanese for \u201cvegetable\u201d": 0.26914601169097385,
                "scientist edward napa": 0.26650893512748347,
                "city in california": 0.4643450531815426
            },
            "question": "napa cabbage is named after what?",
            "rate_limited": false,
            "answers": [
                "city in california",
                "scientist edward napa",
                "japanese for \u201cvegetable\u201d"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "japanese for \u201cvegetable\u201d": 0.1226139585438765,
                "scientist edward napa": 0.36705948320355886,
                "city in california": 0.5757220238212634
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.0428973277074542,
                    0.33755274261603374,
                    0.6195499296765119
                ],
                "result_count_important_words": [
                    56.0,
                    3.0,
                    15.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    1.0
                ],
                "answer_relation_to_question": [
                    0.6363636363636364,
                    0.6493506493506493,
                    0.7142857142857143
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    0.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    1.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.026053015142679214,
                    0.3599908947944641,
                    0.1853049397468567
                ],
                "result_count_noun_chunks": [
                    23.0,
                    3.0,
                    19.0
                ],
                "question_answer_similarity": [
                    5.399515964090824,
                    3.6875174193410203,
                    8.128326654434204
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    997000.0,
                    537000.0,
                    369000.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    44.0,
                    3.0,
                    13.0
                ],
                "word_count_appended": [
                    26.0,
                    6.0,
                    0.0
                ]
            },
            "integer_answers": {
                "japanese for \u201cvegetable\u201d": 3,
                "scientist edward napa": 2,
                "city in california": 8
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Lonnie Lynn's only Academy Award win was in what category?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "best original song"
            ],
            "lines": [
                [
                    0.3242604855925482,
                    0.17758118306743909,
                    0.22759196403908546,
                    0.31892247574764543,
                    0.05231004200076365,
                    0.11740890688259109,
                    0.0671892497200448,
                    0.05420331356105543,
                    0.3192177423386492,
                    0.2582781456953642,
                    0.2857142857142857,
                    0.3223458832957832,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.3417405858219797,
                    0.4073828315686249,
                    0.2858051931779614,
                    0.2546514258871344,
                    0.046582665139366174,
                    0.23481781376518218,
                    0.05655095184770437,
                    0.0478625485784414,
                    0.26135276137150854,
                    0.17880794701986755,
                    0.14285714285714285,
                    0.3253283650798625,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.3339989285854721,
                    0.415035985363936,
                    0.4866028427829532,
                    0.4264260983652201,
                    0.9011072928598702,
                    0.6477732793522267,
                    0.8762597984322509,
                    0.8979341378605031,
                    0.4194294962898423,
                    0.5629139072847682,
                    0.5714285714285714,
                    0.3523257516243543,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "best adapted screenplay": 0.18035883411823248,
                "best original song": 0.6350882921592835,
                "best cinematography": 0.18455287372248397
            },
            "question": "lonnie lynn's only academy award win was in what category?",
            "rate_limited": false,
            "answers": [
                "best adapted screenplay",
                "best cinematography",
                "best original song"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "best adapted screenplay": 0.12583740508419558,
                "best original song": 0.508863522024177,
                "best cinematography": 0.04799501952180854
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.934075299774699,
                    1.951970190479175,
                    2.113954509746126
                ],
                "result_count_important_words": [
                    2400.0,
                    2020.0,
                    31300.0
                ],
                "wikipedia_search": [
                    1.2768709693545968,
                    1.0454110454860341,
                    1.677717985159369
                ],
                "word_count_appended_bing": [
                    4.0,
                    2.0,
                    8.0
                ],
                "answer_relation_to_question_bing": [
                    0.7103247322697563,
                    1.6295313262744997,
                    1.660143941455744
                ],
                "cosine_similarity_raw": [
                    0.05464781075716019,
                    0.0686255693435669,
                    0.11683971434831619
                ],
                "result_count_noun_chunks": [
                    2650.0,
                    2340.0,
                    43900.0
                ],
                "question_answer_similarity": [
                    6.861212283256464,
                    5.478502219542861,
                    9.174016278237104
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    16.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    4.0
                ],
                "result_count_bing": [
                    20300.0,
                    40600.0,
                    112000.0
                ],
                "word_count_appended": [
                    39.0,
                    27.0,
                    85.0
                ],
                "answer_relation_to_question": [
                    1.2970419423701929,
                    1.3669623432879188,
                    1.3359957143418884
                ],
                "result_count": [
                    2740.0,
                    2440.0,
                    47200.0
                ]
            },
            "integer_answers": {
                "best adapted screenplay": 0,
                "best original song": 13,
                "best cinematography": 1
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these products was featured on \u201cShark Tank\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "scrub daddy"
            ],
            "lines": [
                [
                    0.005555555555555556,
                    0.09821428571428573,
                    0.038109894076648484,
                    0.3683393131998138,
                    0.4501851228542578,
                    0.7504288164665524,
                    0.9774117370087895,
                    0.3508887425938117,
                    0.03125,
                    0.17829457364341086,
                    0.08333333333333333,
                    0.28948285021393894,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.8104166666666667,
                    0.7678571428571429,
                    0.9450204982722142,
                    0.2753979538597314,
                    0.5469538875799395,
                    0.12735849056603774,
                    0.022484437793165658,
                    0.27518104015799866,
                    0.76875,
                    0.6705426356589147,
                    0.8333333333333334,
                    0.5165360463329981,
                    1.0,
                    0,
                    -1.0
                ],
                [
                    0.1840277777777778,
                    0.13392857142857145,
                    0.016869607651137264,
                    0.35626273294045474,
                    0.00286098956580276,
                    0.12221269296740996,
                    0.000103825198044912,
                    0.3739302172481896,
                    0.2,
                    0.1511627906976744,
                    0.08333333333333333,
                    0.1939811034530629,
                    0.0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "scrub daddy": 0.5815255486983187,
                "sticky buddy": 0.1398979724816507,
                "instant pot": 0.2785764788200306
            },
            "question": "which of these products was featured on \u201cshark tank\u201d?",
            "rate_limited": false,
            "answers": [
                "instant pot",
                "scrub daddy",
                "sticky buddy"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "scrub daddy": 0.6733220496632761,
                "sticky buddy": 0.08670646316010823,
                "instant pot": 0.0704715938999442
            },
            "integer_answers": {
                "scrub daddy": 9,
                "sticky buddy": 1,
                "instant pot": 3
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1579314008557557,
                    2.0661441853319924,
                    0.7759244138122516
                ],
                "result_count_important_words": [
                    7390000.0,
                    170000.0,
                    785.0
                ],
                "wikipedia_search": [
                    0.125,
                    3.075,
                    0.8
                ],
                "word_count_appended_bing": [
                    2.0,
                    20.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.39285714285714285,
                    3.071428571428571,
                    0.5357142857142857
                ],
                "cosine_similarity_raw": [
                    0.00944620929658413,
                    0.23423999547958374,
                    0.004181429743766785
                ],
                "result_count_noun_chunks": [
                    53300.0,
                    41800.0,
                    56800.0
                ],
                "question_answer_similarity": [
                    4.791550762951374,
                    3.5825208676978946,
                    4.634452279889956
                ],
                "word_count_noun_chunks": [
                    0.0,
                    4.0,
                    0.0
                ],
                "result_count_bing": [
                    1750000.0,
                    297000.0,
                    285000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    10700.0,
                    13000.0,
                    68.0
                ],
                "answer_relation_to_question": [
                    0.022222222222222223,
                    3.2416666666666667,
                    0.7361111111111112
                ],
                "word_count_appended": [
                    46.0,
                    173.0,
                    39.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What are the first words spoken by God in the King James Bible?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "let there be light"
            ],
            "lines": [
                [
                    0.46868173258003765,
                    0.5514705882352942,
                    0.374939533513722,
                    0.3809094603511877,
                    0.9983455987221176,
                    0.40141378439787934,
                    0.9993512184572658,
                    0.5830703800246948,
                    0.5000000000000001,
                    0.8095238095238095,
                    0.75,
                    0.6579292088249895,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.38002690341673395,
                    0.2647058823529412,
                    0.5085845423171628,
                    0.27323773210923086,
                    0.00031376575959837983,
                    0.2375662711436506,
                    0.000125203455615375,
                    0.00020578954589106874,
                    0.33333333333333337,
                    0.047619047619047616,
                    0.125,
                    0.09527312440451795,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.1512913640032284,
                    0.18382352941176472,
                    0.11647592416911523,
                    0.3458528075395815,
                    0.0013406355182839864,
                    0.3610199444584701,
                    0.0005235780871188408,
                    0.4167238304294142,
                    0.16666666666666669,
                    0.14285714285714285,
                    0.125,
                    0.24679766677049245,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "hello, my children": 0.16185654253269452,
                "let there be light": 0.6768310939022141,
                "this is my gift": 0.16131236356509135
            },
            "question": "what are the first words spoken by god in the king james bible?",
            "rate_limited": false,
            "answers": [
                "let there be light",
                "hello, my children",
                "this is my gift"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "hello, my children": 0.2558424637303858,
                "let there be light": 0.6580514912745106,
                "this is my gift": 0.07256102755443461
            },
            "integer_answers": {
                "hello, my children": 1,
                "let there be light": 13,
                "this is my gift": 0
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.947575252949937,
                    0.5716387464271077,
                    1.4807860006229547
                ],
                "result_count_important_words": [
                    87800.0,
                    11.0,
                    46.0
                ],
                "wikipedia_search": [
                    1.0,
                    0.6666666666666666,
                    0.3333333333333333
                ],
                "word_count_appended_bing": [
                    12.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    2.2058823529411766,
                    1.0588235294117647,
                    0.7352941176470589
                ],
                "cosine_similarity_raw": [
                    0.09085695445537567,
                    0.12324238568544388,
                    0.028224945068359375
                ],
                "result_count_noun_chunks": [
                    102000.0,
                    36.0,
                    72900.0
                ],
                "question_answer_similarity": [
                    21.874170124530792,
                    15.690995521843433,
                    19.861000940203667
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    1590000.0,
                    941000.0,
                    1430000.0
                ],
                "word_count_raw": [
                    4.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    35000.0,
                    11.0,
                    47.0
                ],
                "answer_relation_to_question": [
                    2.3434086629001882,
                    1.9001345170836696,
                    0.756456820016142
                ],
                "word_count_appended": [
                    85.0,
                    5.0,
                    15.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these grape varieties is typically white?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "riesling"
            ],
            "lines": [
                [
                    0.5826687574671446,
                    0.6956521739130436,
                    0.3668847392777292,
                    0.2126182900314809,
                    0.43727598566308246,
                    0.527729130180969,
                    0.14986251145737856,
                    0.8035714285714286,
                    0.646306572358468,
                    0.3544165757906216,
                    0.4827586206896552,
                    0.3589324879107484,
                    0.3103448275862069,
                    0.3333333333333333,
                    -1.0
                ],
                [
                    0.37126642771804064,
                    0.24235104669887278,
                    0.5987790309443232,
                    0.41102817302529326,
                    0.28136200716845877,
                    0.021599532983070636,
                    0.10311640696608616,
                    0.1736111111111111,
                    0.3536934276415321,
                    0.3446019629225736,
                    0.1724137931034483,
                    0.32097361672778635,
                    0.3793103448275862,
                    0.5,
                    -1.0
                ],
                [
                    0.046064814814814815,
                    0.06199677938808374,
                    0.03433622977794762,
                    0.3763535369432258,
                    0.28136200716845877,
                    0.4506713368359603,
                    0.7470210815765352,
                    0.022817460317460316,
                    0.0,
                    0.3009814612868048,
                    0.3448275862068966,
                    0.32009389536146526,
                    0.3103448275862069,
                    0.16666666666666666,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "riesling": 0.4473111024450922,
                "merlot": 0.30529334870272734,
                "concord": 0.24739554885218046
            },
            "question": "which of these grape varieties is typically white?",
            "rate_limited": false,
            "answers": [
                "riesling",
                "merlot",
                "concord"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "riesling": 0.4833428494178032,
                "merlot": 0.3900449659487445,
                "concord": 0.210513268384477
            },
            "integer_answers": {
                "riesling": 9,
                "merlot": 4,
                "concord": 1
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.4357299516429936,
                    1.2838944669111454,
                    1.280375581445861
                ],
                "result_count_important_words": [
                    327000.0,
                    225000.0,
                    1630000.0
                ],
                "wikipedia_search": [
                    2.585226289433872,
                    1.4147737105661284,
                    0.0
                ],
                "word_count_appended_bing": [
                    84.0,
                    30.0,
                    60.0
                ],
                "answer_relation_to_question_bing": [
                    2.0869565217391304,
                    0.7270531400966183,
                    0.1859903381642512
                ],
                "cosine_similarity_raw": [
                    0.20458167791366577,
                    0.3338901996612549,
                    0.01914651319384575
                ],
                "result_count_noun_chunks": [
                    4860000.0,
                    1050000.0,
                    138000.0
                ],
                "question_answer_similarity": [
                    0.5918847410939634,
                    1.1442162559833378,
                    1.047689339146018
                ],
                "word_count_noun_chunks": [
                    9.0,
                    11.0,
                    9.0
                ],
                "result_count_bing": [
                    4520000.0,
                    185000.0,
                    3860000.0
                ],
                "word_count_raw": [
                    2.0,
                    3.0,
                    1.0
                ],
                "result_count": [
                    244000.0,
                    157000.0,
                    157000.0
                ],
                "answer_relation_to_question": [
                    1.7480062724014338,
                    1.113799283154122,
                    0.13819444444444445
                ],
                "word_count_appended": [
                    325.0,
                    316.0,
                    276.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of the following is NOT the name of a RuPaul song?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "jealous of my boogie"
            ],
            "lines": [
                [
                    0.13820097192915148,
                    0.16666666666666669,
                    0.1801377842951825,
                    0.030878462450571575,
                    0.49990688178124804,
                    0.49632507349853006,
                    0.49991052185211987,
                    0.412751677852349,
                    0.0,
                    0.4548254620123203,
                    0.3605769230769231,
                    0.37338812423450596,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.4,
                    0.33333333333333337,
                    0.4252936264164583,
                    0.4347467175699773,
                    0.14477123957593274,
                    0.4856152876942461,
                    0.08178691751670908,
                    0.21588366890380312,
                    0.5,
                    0.26899383983572894,
                    0.2980769230769231,
                    0.314840483265326,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.4617990280708485,
                    0.5,
                    0.3945685892883592,
                    0.5343748199794511,
                    0.35532187864281917,
                    0.018059638807223832,
                    0.4183025606311711,
                    0.3713646532438479,
                    0.5,
                    0.27618069815195073,
                    0.34134615384615385,
                    0.31177139250016817,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "jealous of my boogie": 0.39773857505840526,
                "snapshot": 0.34944299380192706,
                "werk": 0.25281843113966773
            },
            "question": "which of the following is not the name of a rupaul song?",
            "rate_limited": false,
            "answers": [
                "jealous of my boogie",
                "snapshot",
                "werk"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "jealous of my boogie": 0.35831548559175835,
                "snapshot": 0.12997825150715375,
                "werk": 0.14304549007289052
            },
            "integer_answers": {
                "jealous of my boogie": 5,
                "snapshot": 5,
                "werk": 2
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.7596712545929643,
                    1.1109571004080443,
                    1.1293716449989912
                ],
                "result_count_important_words": [
                    92.0,
                    430000.0,
                    84000.0
                ],
                "wikipedia_search": [
                    1.0,
                    0.0,
                    0.0
                ],
                "answer_relation_to_question": [
                    2.170794168425091,
                    0.6,
                    0.22920583157490887
                ],
                "word_count_appended_bing": [
                    29.0,
                    42.0,
                    33.0
                ],
                "answer_relation_to_question_bing": [
                    2.0,
                    1.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.1416553556919098,
                    0.03308473899960518,
                    0.04669174179434776
                ],
                "result_count_noun_chunks": [
                    39000.0,
                    127000.0,
                    57500.0
                ],
                "question_answer_similarity": [
                    14.626556504517794,
                    2.0345065109431744,
                    -1.0717590358108282
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    350000.0,
                    1370000.0,
                    45900000.0
                ],
                "word_count_appended": [
                    44.0,
                    225.0,
                    218.0
                ],
                "result_count": [
                    54.0,
                    206000.0,
                    83900.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "The U.S. government once paid what composer to write a pro-IRS song called \u201cI Paid My Income Tax Today\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "irving berlin"
            ],
            "lines": [
                [
                    0.28015066390343857,
                    0.2583618345225023,
                    0.026243189771084138,
                    -0.07760693555683054,
                    0.0,
                    0.18253968253968253,
                    0.3333333333333333,
                    0.9781021897810219,
                    0.25093425234063155,
                    0.17647058823529413,
                    0.3,
                    0.18479084227337608,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.45123654518563566,
                    0.41588497459650303,
                    0.951473554185036,
                    0.4801909397261631,
                    0.09615384615384616,
                    0.2619047619047619,
                    0.3825136612021858,
                    0.010381184103811841,
                    0.4741721937084036,
                    0.4117647058823529,
                    0.5,
                    0.2503211207778855,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.26861279091092577,
                    0.32575319088099475,
                    0.022283256043879866,
                    0.5974159958306674,
                    0.9038461538461539,
                    0.5555555555555556,
                    0.28415300546448086,
                    0.01151662611516626,
                    0.2748935539509649,
                    0.4117647058823529,
                    0.2,
                    0.5648880369487383,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "irving berlin": 0.47757124910189896,
                "woody guthrie": 0.20666568865310955,
                "carole king": 0.31576306224499145
            },
            "question": "the u.s. government once paid what composer to write a pro-irs song called \u201ci paid my income tax today\u201d?",
            "rate_limited": false,
            "answers": [
                "woody guthrie",
                "irving berlin",
                "carole king"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "irving berlin": 0.7573851840267674,
                "woody guthrie": 0.09974918843942385,
                "carole king": 0.37430793030533077
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.402280949553889,
                    3.2541745701125113,
                    7.343544480333599
                ],
                "result_count_important_words": [
                    61.0,
                    70.0,
                    52.0
                ],
                "wikipedia_search": [
                    3.011211028087579,
                    5.690066324500843,
                    3.2987226474115787
                ],
                "word_count_appended_bing": [
                    3.0,
                    5.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    2.3252565107025203,
                    3.7429647713685275,
                    2.9317787179289527
                ],
                "cosine_similarity_raw": [
                    0.016130901873111725,
                    0.5848422646522522,
                    0.013696849346160889
                ],
                "result_count_noun_chunks": [
                    6030.0,
                    64.0,
                    71.0
                ],
                "question_answer_similarity": [
                    -0.564964628778398,
                    3.4957042699679732,
                    4.349081739783287
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    4.0,
                    0.0
                ],
                "result_count_bing": [
                    39100.0,
                    56100.0,
                    119000.0
                ],
                "result_count": [
                    0,
                    5.0,
                    47.0
                ],
                "answer_relation_to_question": [
                    2.5213559751309473,
                    4.061128906670721,
                    2.417515118198332
                ],
                "word_count_appended": [
                    3.0,
                    7.0,
                    7.0
                ]
            },
            "integer_answers": {
                "irving berlin": 9,
                "woody guthrie": 1,
                "carole king": 4
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which Beatle shares a last name with two U.S. presidents?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "george"
            ],
            "question": "which beatle shares a last name with two u.s. presidents?",
            "lines": [
                [
                    0.16666666666666666,
                    0.14058355437665784,
                    0.44184060657572327,
                    0.46694267605248413,
                    0.5168031629483197,
                    0.3337722185648453,
                    0.40617283950617283,
                    0.5258964143426295,
                    0.0,
                    0.27432885906040266,
                    0.1350210970464135,
                    0.2759756648180922,
                    0.4,
                    0.0,
                    -1.0
                ],
                [
                    0.5,
                    0.1149425287356322,
                    0.3440789194609361,
                    0.5264097626936268,
                    0.43208133295679185,
                    0.3337722185648453,
                    0.4345679012345679,
                    0.3399734395750332,
                    0.0,
                    0.3162751677852349,
                    0.16033755274261605,
                    0.3959418821430792,
                    0.6,
                    1.0,
                    -1.0
                ],
                [
                    0.3333333333333333,
                    0.74447391688771,
                    0.21408047396334062,
                    0.0066475612538889955,
                    0.05111550409488845,
                    0.3324555628703094,
                    0.15925925925925927,
                    0.1341301460823373,
                    1.0,
                    0.40939597315436244,
                    0.7046413502109705,
                    0.32808245303882866,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "paul",
                "george",
                "ringo"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "ringo": 0.3686108201017437,
                "paul": 0.23896485658545896,
                "george": 0.8427219586611471
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1039026592723689,
                    1.5837675285723167,
                    1.3123298121553146
                ],
                "result_count_important_words": [
                    3290000.0,
                    3520000.0,
                    1290000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_appended_bing": [
                    32.0,
                    38.0,
                    167.0
                ],
                "answer_relation_to_question_bing": [
                    0.4217506631299735,
                    0.3448275862068966,
                    2.23342175066313
                ],
                "cosine_similarity_raw": [
                    0.08781041204929352,
                    0.06838147342205048,
                    0.042545873671770096
                ],
                "result_count_noun_chunks": [
                    39600000.0,
                    25600000.0,
                    10100000.0
                ],
                "question_answer_similarity": [
                    1.8400910291820765,
                    2.0744342543184757,
                    0.026196187362074852
                ],
                "word_count_noun_chunks": [
                    4.0,
                    6.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    3.0,
                    0.0
                ],
                "result_count_bing": [
                    5070000.0,
                    5070000.0,
                    5050000.0
                ],
                "word_count_appended": [
                    327.0,
                    377.0,
                    488.0
                ],
                "answer_relation_to_question": [
                    0.5,
                    1.5,
                    1.0
                ],
                "result_count": [
                    18300000.0,
                    15300000.0,
                    1810000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "The inventor of the Erector Set made another toy that contained what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "uranium ore"
            ],
            "lines": [
                [
                    0.3690378289473684,
                    0.3675889328063241,
                    0.25497711972187104,
                    0.23705153961247064,
                    0.6049382716049383,
                    0.1618682021753039,
                    0.34615384615384615,
                    0.39473684210526316,
                    0.2516666666666667,
                    0.6153846153846154,
                    0.3333333333333333,
                    0.5573524593451482,
                    0,
                    0,
                    1.0
                ],
                [
                    0.21504934210526316,
                    0.25691699604743085,
                    0.3517550321990939,
                    0.44099157939983646,
                    0.2345679012345679,
                    0.37747920665387075,
                    0.5384615384615384,
                    0.4824561403508772,
                    0.12666666666666668,
                    0.25,
                    0.3333333333333333,
                    0.2781703580839518,
                    0,
                    0,
                    1.0
                ],
                [
                    0.4159128289473684,
                    0.37549407114624506,
                    0.3932678480790351,
                    0.3219568809876929,
                    0.16049382716049382,
                    0.46065259117082535,
                    0.11538461538461539,
                    0.12280701754385964,
                    0.6216666666666667,
                    0.1346153846153846,
                    0.3333333333333333,
                    0.16447718257089994,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "uranium ore": 0.3745074714880958,
                "asbestos powder": 0.3016718539672017,
                "live ants": 0.32382067454470254
            },
            "question": "the inventor of the erector set made another toy that contained what?",
            "rate_limited": false,
            "answers": [
                "uranium ore",
                "live ants",
                "asbestos powder"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "uranium ore": 0.45842188051008687,
                "asbestos powder": 0.15964102633967603,
                "live ants": 0.08871006430833575
            },
            "integer_answers": {
                "uranium ore": 4,
                "asbestos powder": 5,
                "live ants": 3
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.7867622967257413,
                    1.390851790419759,
                    0.8223859128544997
                ],
                "result_count_important_words": [
                    36.0,
                    56.0,
                    12.0
                ],
                "wikipedia_search": [
                    0.5033333333333334,
                    0.25333333333333335,
                    1.2433333333333334
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.3675889328063241,
                    0.25691699604743085,
                    0.37549407114624506
                ],
                "cosine_similarity_raw": [
                    0.013845871202647686,
                    0.01910114474594593,
                    0.021355390548706055
                ],
                "result_count_noun_chunks": [
                    45.0,
                    55.0,
                    14.0
                ],
                "question_answer_similarity": [
                    2.8342179199680686,
                    5.272550597786903,
                    3.8493568236008286
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    7590.0,
                    17700.0,
                    21600.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    49.0,
                    19.0,
                    13.0
                ],
                "answer_relation_to_question": [
                    1.4761513157894737,
                    0.8601973684210527,
                    1.6636513157894737
                ],
                "word_count_appended": [
                    32.0,
                    13.0,
                    7.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "The inventor of the mood ring was also behind what \u201890s fad?": {
        "raw_data": {
            "data": {
                "word_count_appended_relation_to_question": [
                    2.045102172757767,
                    1.3207853271999213,
                    1.6341125000423111
                ],
                "result_count_important_words": [
                    29.0,
                    25.0,
                    27.0
                ],
                "wikipedia_search": [
                    1.823518118550516,
                    0.8722925023141006,
                    2.3041893791353836
                ],
                "word_count_appended_bing": [
                    33.0,
                    4.0,
                    10.0
                ],
                "answer_relation_to_question_bing": [
                    0.559375,
                    0.74375,
                    0.696875
                ],
                "cosine_similarity_raw": [
                    0.05774715542793274,
                    0.034202106297016144,
                    0.00940750166773796
                ],
                "result_count_noun_chunks": [
                    300.0,
                    80.0,
                    122000.0
                ],
                "question_answer_similarity": [
                    0.0,
                    5.286474524065852,
                    4.706119440495968
                ],
                "word_count_noun_chunks": [
                    8.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    96500.0,
                    37400.0,
                    269000.0
                ],
                "word_count_appended": [
                    109.0,
                    15.0,
                    26.0
                ],
                "answer_relation_to_question": [
                    1.9086178861788619,
                    1.4433062330623307,
                    1.6480758807588076
                ],
                "result_count": [
                    46.0,
                    22.0,
                    32.0
                ]
            },
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "z-best_answer_by_ml": [
                "thighmaster"
            ],
            "lines": [
                [
                    0.3817235772357724,
                    0.2796875,
                    0.5697415100381881,
                    0.0,
                    0.46,
                    0.23951352692975925,
                    0.35802469135802467,
                    0.0024513809445987906,
                    0.36470362371010323,
                    0.7266666666666667,
                    0.7021276595744681,
                    0.40902043455155346,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.28866124661246617,
                    0.371875,
                    0.3374427630892949,
                    0.5290392607579214,
                    0.22,
                    0.09282700421940929,
                    0.30864197530864196,
                    0.0006537015852263442,
                    0.17445850046282013,
                    0.1,
                    0.0851063829787234,
                    0.26415706543998424,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.3296151761517615,
                    0.3484375,
                    0.09281572687251709,
                    0.4709607392420786,
                    0.32,
                    0.6676594688508315,
                    0.3333333333333333,
                    0.9968949174701749,
                    0.4608378758270767,
                    0.17333333333333334,
                    0.2127659574468085,
                    0.32682250000846225,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "question": "the inventor of the mood ring was also behind what \u201890s fad?",
            "rate_limited": false,
            "answers": [
                "thighmaster",
                "slap bracelet",
                "troll dolls"
            ],
            "ml_answers": {
                "thighmaster": 0.8131681804349941,
                "slap bracelet": 0.11702191227204317,
                "troll dolls": 0.07869694159535148
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these outfits can be defined as a \u201cCanadian tuxedo\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "jean jacket / jeans"
            ],
            "lines": [
                [
                    0.0,
                    0.3333333333333333,
                    0.1577529001161312,
                    0.2512944300771902,
                    0.0,
                    0.25715746421267893,
                    0.9999867189263892,
                    0.3289902280130293,
                    0.0,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.25929686618587655,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.36904761904761907,
                    0.3333333333333333,
                    0.1467306923909904,
                    0.46840606948299485,
                    0.0,
                    0.27044989775051126,
                    0.0,
                    0.3322475570032573,
                    0.1111111111111111,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.29170897445911115,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.6309523809523809,
                    0.3333333333333333,
                    0.6955164074928785,
                    0.28029950043981494,
                    1.0,
                    0.4723926380368098,
                    1.3281073610741108e-05,
                    0.33876221498371334,
                    0.8888888888888888,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.44899415935501225,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "cotton t-shirt / shorts": 0.24914182677046628,
                "jean jacket / jeans": 0.4796516226019257,
                "wool sweater / slacks": 0.27120655062760796
            },
            "question": "which of these outfits can be defined as a \u201ccanadian tuxedo\u201d?",
            "rate_limited": false,
            "answers": [
                "wool sweater / slacks",
                "cotton t-shirt / shorts",
                "jean jacket / jeans"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "cotton t-shirt / shorts": 0.10103074982006632,
                "jean jacket / jeans": 0.47217137453320795,
                "wool sweater / slacks": 0.1017591768970551
            },
            "integer_answers": {
                "cotton t-shirt / shorts": 1,
                "jean jacket / jeans": 7,
                "wool sweater / slacks": 4
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.0371874647435062,
                    1.1668358978364446,
                    1.795976637420049
                ],
                "result_count_important_words": [
                    1280000.0,
                    0,
                    17.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.3333333333333333,
                    2.6666666666666665
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "cosine_similarity_raw": [
                    0.04592807963490486,
                    0.04271908104419708,
                    0.20249220728874207
                ],
                "result_count_noun_chunks": [
                    1010000.0,
                    1020000.0,
                    1040000.0
                ],
                "question_answer_similarity": [
                    8.091569856274873,
                    15.082468923646957,
                    9.025520334020257
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    50300.0,
                    52900.0,
                    92400.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    3.0,
                    3.0,
                    3.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    1.4761904761904763,
                    2.5238095238095237
                ],
                "result_count": [
                    0,
                    0,
                    13.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What was the first product to have its barcode scanned at purchase?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "wrigley's chewing gum"
            ],
            "question": "what was the first product to have its barcode scanned at purchase?",
            "lines": [
                [
                    0.2583333333333333,
                    0.14285714285714285,
                    0.03105797373548177,
                    0.35632521445243076,
                    0.993359247956392,
                    0.3382352941176471,
                    0.9798440488450787,
                    0.9904489105061984,
                    0.07142857142857142,
                    0.0,
                    0.0,
                    0.297575434476619,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.385,
                    0.7142857142857143,
                    0.929776413391596,
                    0.5152295189661107,
                    0.0008323020477124163,
                    0.33144796380090497,
                    0.0015006620567897602,
                    0.00041384954476550076,
                    0.6785714285714285,
                    0.625,
                    0.5,
                    0.4603738458350482,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.3566666666666667,
                    0.14285714285714285,
                    0.039165612872922195,
                    0.12844526658145855,
                    0.005808449995895639,
                    0.33031674208144796,
                    0.01865528909813153,
                    0.009137239949036056,
                    0.25,
                    0.375,
                    0.5,
                    0.24205071968833283,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "rubik\u2019s cube",
                "wrigley's chewing gum",
                "apple iie"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "apple iie": 0.11087482934314496,
                "wrigley's chewing gum": 0.931007854288402,
                "rubik\u2019s cube": 0.06650666252220014
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.190301737906476,
                    1.8414953833401928,
                    0.9682028787533313
                ],
                "result_count_important_words": [
                    333000.0,
                    510.0,
                    6340.0
                ],
                "wikipedia_search": [
                    0.21428571428571427,
                    2.0357142857142856,
                    0.75
                ],
                "answer_relation_to_question": [
                    0.5166666666666666,
                    0.77,
                    0.7133333333333334
                ],
                "answer_relation_to_question_bing": [
                    0.14285714285714285,
                    0.7142857142857143,
                    0.14285714285714285
                ],
                "word_count_appended": [
                    0.0,
                    60.0,
                    36.0
                ],
                "cosine_similarity_raw": [
                    0.015872150659561157,
                    0.47516143321990967,
                    0.020015552639961243
                ],
                "result_count_noun_chunks": [
                    4260000.0,
                    1780.0,
                    39300.0
                ],
                "question_answer_similarity": [
                    5.095247627934441,
                    7.3674886813387275,
                    1.8366941583808511
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count_bing": [
                    2990000.0,
                    2930000.0,
                    2920000.0
                ],
                "word_count_raw": [
                    0.0,
                    2.0,
                    0.0
                ],
                "result_count": [
                    956000.0,
                    801.0,
                    5590.0
                ],
                "word_count_appended_bing": [
                    0.0,
                    2.0,
                    2.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these creatures is most likely to bark?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "dog"
            ],
            "lines": [
                [
                    0.0,
                    0.0,
                    0.02378844171978863,
                    0.07340714686114677,
                    0.1388888888888889,
                    0.1868941450174486,
                    0.012319339298004129,
                    0.10802200984651028,
                    0.038461538461538464,
                    0.21470019342359767,
                    0.20930232558139536,
                    0.2893370122245085,
                    0.18181818181818182,
                    0.012658227848101266,
                    -1.0
                ],
                [
                    0.3888888888888889,
                    0.0,
                    0.04373486482716273,
                    0.5597380241609013,
                    0.36997635933806144,
                    0.10740597130670802,
                    0.037921541637990365,
                    0.15783376773819865,
                    0.2692307692307692,
                    0.14216634429400388,
                    0.12403100775193798,
                    0.30936546836892626,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.6111111111111112,
                    1.0,
                    0.9324766934530486,
                    0.36685482897795185,
                    0.49113475177304966,
                    0.7056998836758434,
                    0.9497591190640055,
                    0.7341442224152911,
                    0.6923076923076923,
                    0.6431334622823984,
                    0.6666666666666666,
                    0.40129751940656516,
                    0.8181818181818182,
                    0.9873417721518988,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "blue whale": 0.1793066433959678,
                "mime": 0.10639981792779361,
                "dog": 0.7142935386762386
            },
            "question": "which of these creatures is most likely to bark?",
            "rate_limited": false,
            "answers": [
                "mime",
                "blue whale",
                "dog"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "blue whale": 0.1377539060681903,
                "mime": 0.16332483679320706,
                "dog": 0.7906221929744528
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.8680110366735256,
                    0.9280964051067787,
                    1.2038925582196955
                ],
                "result_count_important_words": [
                    179000.0,
                    551000.0,
                    13800000.0
                ],
                "wikipedia_search": [
                    0.07692307692307693,
                    0.5384615384615384,
                    1.3846153846153846
                ],
                "word_count_appended_bing": [
                    27.0,
                    16.0,
                    86.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "cosine_similarity_raw": [
                    0.017171353101730347,
                    0.03156939893960953,
                    0.6730952262878418
                ],
                "result_count_noun_chunks": [
                    746000.0,
                    1090000.0,
                    5070000.0
                ],
                "question_answer_similarity": [
                    0.6018534117611125,
                    4.58920219540596,
                    3.0077838450670242
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    9.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    78.0
                ],
                "result_count_bing": [
                    4820000.0,
                    2770000.0,
                    18200000.0
                ],
                "word_count_appended": [
                    222.0,
                    147.0,
                    665.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    1.1666666666666667,
                    1.8333333333333335
                ],
                "result_count": [
                    235000.0,
                    626000.0,
                    831000.0
                ]
            },
            "integer_answers": {
                "blue whale": 1,
                "mime": 0,
                "dog": 13
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these phrases, written backwards, is a hip hop group?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "beat chefs"
            ],
            "lines": [
                [
                    0.3867880485527544,
                    0.5873015873015873,
                    0.43304426099506443,
                    0.3006900407745902,
                    0.007735204173412484,
                    0.1198005919925222,
                    0.001029738858225554,
                    2.4377030071802792e-05,
                    0.03571428571428571,
                    0.37209302325581395,
                    0.3333333333333333,
                    0.2358692240465259,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.5373093681917211,
                    0.18253968253968253,
                    0.3729286237440798,
                    0.30967054954573076,
                    0.0010793308148947653,
                    0.3053435114503817,
                    0.00012356866298706648,
                    1.9899616385145134e-05,
                    0.0,
                    0.2558139534883721,
                    0.3333333333333333,
                    0.1325731991006469,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.07590258325552442,
                    0.23015873015873015,
                    0.19402711526085573,
                    0.389639409679679,
                    0.9911854650116928,
                    0.5748558965570961,
                    0.9988466924787874,
                    0.9999557233535431,
                    0.9642857142857143,
                    0.37209302325581395,
                    0.3333333333333333,
                    0.6315575768528271,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "beat chefs": 0.2025612517073513,
                "drummers ear": 0.23445197633568227,
                "blues rhythm": 0.5629867719569664
            },
            "question": "which of these phrases, written backwards, is a hip hop group?",
            "rate_limited": false,
            "answers": [
                "drummers ear",
                "beat chefs",
                "blues rhythm"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "beat chefs": 0.31693190505617475,
                "drummers ear": 0.290991099710818,
                "blues rhythm": 0.23526724641060173
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.4152153442791555,
                    0.7954391946038815,
                    3.789345461116963
                ],
                "result_count_important_words": [
                    50.0,
                    6.0,
                    48500.0
                ],
                "wikipedia_search": [
                    0.14285714285714285,
                    0.0,
                    3.857142857142857
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.7619047619047619,
                    0.5476190476190476,
                    0.6904761904761905
                ],
                "cosine_similarity_raw": [
                    0.058199070394039154,
                    0.05011981725692749,
                    0.026076313108205795
                ],
                "result_count_noun_chunks": [
                    49.0,
                    40.0,
                    2010000.0
                ],
                "question_answer_similarity": [
                    4.85761278308928,
                    5.0026918621733785,
                    6.2945795357227325
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    76900.0,
                    196000.0,
                    369000.0
                ],
                "word_count_appended": [
                    16.0,
                    11.0,
                    16.0
                ],
                "answer_relation_to_question": [
                    2.3207282913165264,
                    3.223856209150327,
                    0.45541549953314653
                ],
                "result_count": [
                    43.0,
                    6.0,
                    5510.0
                ]
            },
            "integer_answers": {
                "beat chefs": 1,
                "drummers ear": 4,
                "blues rhythm": 7
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these is a French territory?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "french guiana"
            ],
            "lines": [
                [
                    0.7015376166941242,
                    0.71712158808933,
                    0.7572529712598172,
                    0.22107387686146088,
                    0.9998595287788639,
                    0.5757561278608629,
                    0.9981115148908668,
                    0.9982419187232177,
                    0.48484848484848486,
                    0.6218181818181818,
                    0.8235294117647058,
                    0.48912059909511674,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.18753432180120816,
                    0.184863523573201,
                    0.09038061833842549,
                    0.3876407651729475,
                    0.00013822347490870598,
                    0.008552725882746429,
                    0.0018839855903562545,
                    0.0017539203805604197,
                    0.18181818181818182,
                    0.3018181818181818,
                    0.11764705882352941,
                    0.32189921472775496,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.11092806150466777,
                    0.09801488833746898,
                    0.15236641040175739,
                    0.3912853579655916,
                    2.247746227487368e-06,
                    0.41569114625639064,
                    4.499518776967863e-06,
                    4.160896221967805e-06,
                    0.3333333333333333,
                    0.07636363636363637,
                    0.058823529411764705,
                    0.1889801861771283,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "french guiana": 0.7420194157632166,
                "french stewart": 0.12756648010014301,
                "french cyprus": 0.1304141041366404
            },
            "question": "which of these is a french territory?",
            "rate_limited": false,
            "answers": [
                "french guiana",
                "french stewart",
                "french cyprus"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "french guiana": 0.8130286131296881,
                "french stewart": 0.17421635870487695,
                "french cyprus": 0.07792305784352663
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.9782411981902335,
                    0.6437984294555099,
                    0.3779603723542566
                ],
                "result_count_important_words": [
                    99600000.0,
                    188000.0,
                    449.0
                ],
                "wikipedia_search": [
                    0.9696969696969697,
                    0.36363636363636365,
                    0.6666666666666666
                ],
                "answer_relation_to_question": [
                    1.4030752333882481,
                    0.37506864360241626,
                    0.22185612300933552
                ],
                "answer_relation_to_question_bing": [
                    1.43424317617866,
                    0.369727047146402,
                    0.19602977667493796
                ],
                "word_count_appended": [
                    171.0,
                    83.0,
                    21.0
                ],
                "cosine_similarity_raw": [
                    0.13738316297531128,
                    0.016397129744291306,
                    0.027642782777547836
                ],
                "result_count_noun_chunks": [
                    107000000.0,
                    188000.0,
                    446.0
                ],
                "question_answer_similarity": [
                    1.5891291573643684,
                    2.786449721083045,
                    2.8126478805206716
                ],
                "word_count_noun_chunks": [
                    4.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    24100000.0,
                    358000.0,
                    17400000.0
                ],
                "result_count": [
                    38700000.0,
                    5350.0,
                    87.0
                ],
                "word_count_appended_bing": [
                    28.0,
                    4.0,
                    2.0
                ]
            },
            "integer_answers": {
                "french guiana": 12,
                "french stewart": 0,
                "french cyprus": 1
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What makeup item often contains dried cochineal bugs as an ingredient?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "lipstick"
            ],
            "lines": [
                [
                    0.3391568092205035,
                    0.36101626016260163,
                    0.22502266078615404,
                    0.3281592940631894,
                    0.3304964539007092,
                    0.30303030303030304,
                    0.11180954928447238,
                    0.11073431589930872,
                    0.1989795918367347,
                    0.2612859097127223,
                    0.3157894736842105,
                    0.33898237930573166,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.32438580527752503,
                    0.23430894308943093,
                    0.10328677617987099,
                    0.2816521449245784,
                    0.20851063829787234,
                    0.25170068027210885,
                    0.07235522268289421,
                    0.07147515325420634,
                    0.07312925170068027,
                    0.2079343365253078,
                    0.23684210526315788,
                    0.2853409199353945,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.33645738550197146,
                    0.4046747967479675,
                    0.671690563033975,
                    0.39018856101223226,
                    0.46099290780141844,
                    0.4452690166975881,
                    0.8158352280326334,
                    0.8177905308464849,
                    0.7278911564625851,
                    0.53077975376197,
                    0.4473684210526316,
                    0.37567670075887394,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "lipstick": 0.6017582158364523,
                "eyeliner": 0.1679229983859305,
                "mascara": 0.23031878577761722
            },
            "question": "what makeup item often contains dried cochineal bugs as an ingredient?",
            "rate_limited": false,
            "answers": [
                "mascara",
                "eyeliner",
                "lipstick"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "lipstick": 0.7745553692078216,
                "eyeliner": 0.13918375967050825,
                "mascara": 0.3221183380254922
            },
            "integer_answers": {
                "lipstick": 13,
                "eyeliner": 0,
                "mascara": 1
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.3728766551401215,
                    1.9973864395477614,
                    2.6297369053121176
                ],
                "result_count_important_words": [
                    8360.0,
                    5410.0,
                    61000.0
                ],
                "wikipedia_search": [
                    1.3928571428571428,
                    0.5119047619047619,
                    5.095238095238096
                ],
                "answer_relation_to_question": [
                    1.356627236882014,
                    1.2975432211101001,
                    1.3458295420078858
                ],
                "word_count_appended_bing": [
                    36.0,
                    27.0,
                    51.0
                ],
                "answer_relation_to_question_bing": [
                    1.4440650406504063,
                    0.9372357723577236,
                    1.6186991869918699
                ],
                "cosine_similarity_raw": [
                    0.040849216282367706,
                    0.018750039860606194,
                    0.12193453311920166
                ],
                "result_count_noun_chunks": [
                    8490.0,
                    5480.0,
                    62700.0
                ],
                "question_answer_similarity": [
                    2.2088891826570034,
                    1.8958426211029291,
                    2.6264174357056618
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    63.0
                ],
                "result_count_bing": [
                    49000.0,
                    40700.0,
                    72000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    18.0
                ],
                "result_count": [
                    4660.0,
                    2940.0,
                    6500.0
                ],
                "word_count_appended": [
                    191.0,
                    152.0,
                    388.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What term describes a person from the state between New York and Rhode Island?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "hoosier"
            ],
            "lines": [
                [
                    0.31622791335180717,
                    0.3278957286432161,
                    0.2761783223035961,
                    0.8662525025803617,
                    0.008923431203223949,
                    0.3345367027677497,
                    0.02101915110694041,
                    0.5128636287203632,
                    0.2922962962962963,
                    0.2638580931263858,
                    0.32926829268292684,
                    0.28454492549048427,
                    0,
                    0,
                    1.0
                ],
                [
                    0.462582809411718,
                    0.2591520100502513,
                    0.41484585400109436,
                    -0.0,
                    0.005604659825933828,
                    0.33574007220216606,
                    0.002569656999026512,
                    0.005801244324869682,
                    0.0654074074074074,
                    0.29933481152993346,
                    0.32926829268292684,
                    0.3234356651036388,
                    0,
                    0,
                    1.0
                ],
                [
                    0.22118927723647486,
                    0.4129522613065327,
                    0.3089758236953095,
                    0.13374749741963826,
                    0.9854719089708422,
                    0.3297232250300842,
                    0.976411191894033,
                    0.4813351269547671,
                    0.6422962962962963,
                    0.43680709534368073,
                    0.34146341463414637,
                    0.39201940940587693,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "hoosier": 0.47186604401564014,
                "nutmegger": 0.2086452069615805,
                "cheesehead": 0.31948874902277924
            },
            "question": "what term describes a person from the state between new york and rhode island?",
            "rate_limited": false,
            "answers": [
                "cheesehead",
                "nutmegger",
                "hoosier"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "hoosier": 0.32731553267653396,
                "nutmegger": 0.16516141950389213,
                "cheesehead": 0.06305124328207959
            },
            "integer_answers": {
                "hoosier": 7,
                "nutmegger": 3,
                "cheesehead": 1
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.276359403923874,
                    2.5874853208291104,
                    3.1361552752470154
                ],
                "result_count_important_words": [
                    7190.0,
                    879.0,
                    334000.0
                ],
                "wikipedia_search": [
                    1.4614814814814814,
                    0.327037037037037,
                    3.2114814814814814
                ],
                "word_count_appended_bing": [
                    27.0,
                    27.0,
                    28.0
                ],
                "answer_relation_to_question_bing": [
                    1.3115829145728644,
                    1.0366080402010052,
                    1.6518090452261307
                ],
                "cosine_similarity_raw": [
                    0.03117109090089798,
                    0.04682191461324692,
                    0.03487280756235123
                ],
                "result_count_noun_chunks": [
                    244000.0,
                    2760.0,
                    229000.0
                ],
                "question_answer_similarity": [
                    -1.2163297208026052,
                    0.0,
                    -0.1877986565232277
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    27800000.0,
                    27900000.0,
                    27400000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    5270.0,
                    3310.0,
                    582000.0
                ],
                "answer_relation_to_question": [
                    1.8973674801108429,
                    2.775496856470308,
                    1.3271356634188491
                ],
                "word_count_appended": [
                    119.0,
                    135.0,
                    197.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Where in the home does the Maillard reaction typically occur?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "kitchen"
            ],
            "lines": [
                [
                    0.6632124352331606,
                    0.3428571428571429,
                    0.6096959892668528,
                    0.36107745697333116,
                    0.7786116322701688,
                    0.4020319303338171,
                    0.7777149321266968,
                    0.2511415525114155,
                    0.7032784793978824,
                    0.46153846153846156,
                    0.35398230088495575,
                    0.4265766208373214,
                    1.0,
                    0,
                    3.0
                ],
                [
                    0.17357512953367876,
                    0.4285714285714286,
                    0.21774296632798598,
                    0.29718541806908144,
                    0.04127579737335835,
                    0.22496371552975328,
                    0.041289592760180995,
                    0.319634703196347,
                    0.12590891695369308,
                    0.25040916530278234,
                    0.24778761061946902,
                    0.264138823729545,
                    0.0,
                    0,
                    3.0
                ],
                [
                    0.16321243523316062,
                    0.2285714285714286,
                    0.17256104440516126,
                    0.34173712495758735,
                    0.1801125703564728,
                    0.37300435413642963,
                    0.18099547511312217,
                    0.4292237442922374,
                    0.17081260364842454,
                    0.28805237315875615,
                    0.39823008849557523,
                    0.30928455543313366,
                    0.0,
                    0,
                    3.0
                ]
            ],
            "fraction_answers": {
                "bathroom": 0.24890752290780688,
                "kitchen": 0.5485937641716313,
                "bedroom": 0.20249871292056182
            },
            "question": "where in the home does the maillard reaction typically occur?",
            "rate_limited": false,
            "answers": [
                "kitchen",
                "bedroom",
                "bathroom"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "bathroom": 0.083052429444395,
                "kitchen": 0.6275505259603896,
                "bedroom": 0.13551494393744656
            },
            "integer_answers": {
                "bathroom": 2,
                "kitchen": 10,
                "bedroom": 1
            },
            "categorical_data": {
                "question_type": 3
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.132883104186607,
                    1.320694118647725,
                    1.5464227771656682
                ],
                "result_count_important_words": [
                    165000.0,
                    8760.0,
                    38400.0
                ],
                "wikipedia_search": [
                    2.109835438193647,
                    0.3777267508610792,
                    0.5124378109452736
                ],
                "word_count_appended_bing": [
                    40.0,
                    28.0,
                    45.0
                ],
                "answer_relation_to_question_bing": [
                    0.34285714285714286,
                    0.42857142857142855,
                    0.22857142857142856
                ],
                "cosine_similarity_raw": [
                    0.1090206429362297,
                    0.038934942334890366,
                    0.030855895951390266
                ],
                "result_count_noun_chunks": [
                    82500.0,
                    105000.0,
                    141000.0
                ],
                "question_answer_similarity": [
                    2.3942533154040575,
                    1.9705942831933498,
                    2.266010321676731
                ],
                "word_count_noun_chunks": [
                    5.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    277000.0,
                    155000.0,
                    257000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    166000.0,
                    8800.0,
                    38400.0
                ],
                "answer_relation_to_question": [
                    1.3264248704663213,
                    0.3471502590673575,
                    0.32642487046632124
                ],
                "word_count_appended": [
                    282.0,
                    153.0,
                    176.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What is the correct spelling of the Pennsylvania town famous for its groundhog?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "punxsutawney"
            ],
            "question": "what is the correct spelling of the pennsylvania town famous for its groundhog?",
            "lines": [
                [
                    0.49230769230769234,
                    0.42894078987481504,
                    0.5000000325663608,
                    -0.0,
                    0.0,
                    0.17015209125475286,
                    0.0,
                    0.0,
                    0.0,
                    0.031683168316831684,
                    0.18045112781954886,
                    0.07469762368616616,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.015384615384615385,
                    0.17927565392354122,
                    0.0,
                    -0.0,
                    0.0,
                    0.6615969581749049,
                    0.0,
                    0.0,
                    0.0,
                    0.031683168316831684,
                    0.21052631578947367,
                    0.06649798782113667,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.49230769230769234,
                    0.3917835562016436,
                    0.4999999674336392,
                    1.0,
                    1.0,
                    0.1682509505703422,
                    1.0,
                    1.0,
                    1.0,
                    0.9366336633663367,
                    0.6090225563909775,
                    0.8588043884926972,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "punksatawany",
                "puncksehtany",
                "punxsutawney"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "punksatawany": 0.33366647021590257,
                "puncksehtany": 0.14270831565118966,
                "punxsutawney": 0.7285768453459914
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.4481857421169969,
                    0.39898792692682,
                    5.1528263309561835
                ],
                "result_count_important_words": [
                    0,
                    0,
                    177000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    3.0
                ],
                "word_count_appended_bing": [
                    24.0,
                    28.0,
                    81.0
                ],
                "answer_relation_to_question_bing": [
                    2.144703949374075,
                    0.8963782696177062,
                    1.9589177810082181
                ],
                "cosine_similarity_raw": [
                    0.9151260256767273,
                    0.0,
                    0.9151259064674377
                ],
                "result_count_noun_chunks": [
                    0,
                    0,
                    25600.0
                ],
                "question_answer_similarity": [
                    0.0,
                    0.0,
                    -0.9890388045459986
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    252.0
                ],
                "result_count_bing": [
                    17900.0,
                    69600.0,
                    17700.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    77.0
                ],
                "word_count_appended": [
                    16.0,
                    16.0,
                    473.0
                ],
                "answer_relation_to_question": [
                    2.4615384615384617,
                    0.07692307692307693,
                    2.4615384615384617
                ],
                "result_count": [
                    0,
                    0,
                    56200.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What hometown city of Charlie Parker is one of the birthplaces of bebop jazz?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "kansas city"
            ],
            "question": "what hometown city of charlie parker is one of the birthplaces of bebop jazz?",
            "lines": [
                [
                    0.4339672048253693,
                    0.8954545454545455,
                    0.8451846749910547,
                    0.43385302626831684,
                    0.206401766004415,
                    0.2032967032967033,
                    0.07038012796386903,
                    0.00575496763471856,
                    0.1103154305200341,
                    0.2515923566878981,
                    0.13333333333333333,
                    0.3489433549716021,
                    0.0,
                    1.0,
                    1.0
                ],
                [
                    0.023837902264600714,
                    0.03532467532467533,
                    0.06539288952921621,
                    0.18164803002731356,
                    0.44885945548197204,
                    0.43223443223443225,
                    0.15305482373604315,
                    0.012515259383879936,
                    0.3438192668371697,
                    0.5700636942675159,
                    0.6444444444444445,
                    0.33512818515512777,
                    0.4375,
                    0.0,
                    1.0
                ],
                [
                    0.54219489291003,
                    0.06922077922077922,
                    0.08942243547972908,
                    0.3844989437043696,
                    0.34473877851361295,
                    0.36446886446886445,
                    0.7765650483000878,
                    0.9817297729814015,
                    0.5458653026427963,
                    0.17834394904458598,
                    0.2222222222222222,
                    0.31592845987327,
                    0.5625,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "kansas city",
                "chicago",
                "new orleans"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "new orleans": -0.014127702469694869,
                "kansas city": 0.9033343746189233,
                "chicago": 0.06411613802007338
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.4426034848012153,
                    2.345897296085895,
                    2.2114992191128904
                ],
                "result_count_important_words": [
                    5610.0,
                    12200.0,
                    61900.0
                ],
                "wikipedia_search": [
                    0.22063086104006818,
                    0.6876385336743392,
                    1.0917306052855924
                ],
                "word_count_appended_bing": [
                    6.0,
                    29.0,
                    10.0
                ],
                "answer_relation_to_question_bing": [
                    4.4772727272727275,
                    0.17662337662337663,
                    0.34610389610389614
                ],
                "cosine_similarity_raw": [
                    0.4267655909061432,
                    0.03301933407783508,
                    0.045152757316827774
                ],
                "result_count_noun_chunks": [
                    5610.0,
                    12200.0,
                    957000.0
                ],
                "question_answer_similarity": [
                    8.147160571068525,
                    3.4110991014167666,
                    7.220359071157873
                ],
                "word_count_noun_chunks": [
                    0.0,
                    7.0,
                    9.0
                ],
                "result_count_bing": [
                    11100.0,
                    23600.0,
                    19900.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    5610.0,
                    12200.0,
                    9370.0
                ],
                "answer_relation_to_question": [
                    1.301901614476108,
                    0.07151370679380215,
                    1.62658467873009
                ],
                "word_count_appended": [
                    79.0,
                    179.0,
                    56.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Who is the director of \u201cTyler Perry\u2019s Madea\u2019s Family Reunion\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "tyler perry"
            ],
            "lines": [
                [
                    0.8729718354029918,
                    0.7275362318840579,
                    0.9789312606346264,
                    0.4348694138685192,
                    0.9997500624843789,
                    0.15185264510601842,
                    0.2843866171003718,
                    0.5833333333333334,
                    0.6392543859649122,
                    0.8474025974025974,
                    0.8103448275862069,
                    0.7318589772296638,
                    0.9950980392156863,
                    1.0,
                    0.0
                ],
                [
                    0.09968588322246857,
                    0.18550724637681157,
                    0.011959257828212074,
                    0.3171647754190037,
                    0.00014996250937265683,
                    0.3769543799528807,
                    0.30111524163568776,
                    0.26282051282051283,
                    0.15021929824561403,
                    0.07467532467532467,
                    0.05172413793103448,
                    0.13724151622085706,
                    0.0,
                    0.0,
                    0.0
                ],
                [
                    0.027342281374539434,
                    0.08695652173913043,
                    0.009109481537161467,
                    0.24796581071247714,
                    9.997500624843789e-05,
                    0.47119297494110085,
                    0.4144981412639405,
                    0.15384615384615385,
                    0.21052631578947367,
                    0.07792207792207792,
                    0.13793103448275862,
                    0.13089950654947913,
                    0.004901960784313725,
                    0.0,
                    0.0
                ]
            ],
            "fraction_answers": {
                "george lucas": 0.14065839548841289,
                "abraham lincoln": 0.14094230256777537,
                "tyler perry": 0.7183993019438117
            },
            "question": "who is the director of \u201ctyler perry\u2019s madea\u2019s family reunion\u201d?",
            "rate_limited": false,
            "answers": [
                "tyler perry",
                "george lucas",
                "abraham lincoln"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "george lucas": 0.05963498183813205,
                "abraham lincoln": 0.09569297740300124,
                "tyler perry": 0.8648556707336332
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    4.3911538633779825,
                    0.8234490973251424,
                    0.7853970392968748
                ],
                "result_count_important_words": [
                    153000.0,
                    162000.0,
                    223000.0
                ],
                "wikipedia_search": [
                    3.8355263157894735,
                    0.9013157894736842,
                    1.263157894736842
                ],
                "word_count_appended_bing": [
                    47.0,
                    3.0,
                    8.0
                ],
                "answer_relation_to_question_bing": [
                    3.6376811594202896,
                    0.9275362318840579,
                    0.43478260869565216
                ],
                "cosine_similarity_raw": [
                    0.7948856949806213,
                    0.009710838086903095,
                    0.007396838627755642
                ],
                "result_count_noun_chunks": [
                    91.0,
                    41.0,
                    24.0
                ],
                "question_answer_similarity": [
                    5.762457792647183,
                    4.202752765268087,
                    3.2857967764139175
                ],
                "word_count_noun_chunks": [
                    203.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    53.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    709000.0,
                    1760000.0,
                    2200000.0
                ],
                "result_count": [
                    260000.0,
                    39.0,
                    26.0
                ],
                "answer_relation_to_question": [
                    5.237831012417952,
                    0.5981152993348116,
                    0.16405368824723662
                ],
                "word_count_appended": [
                    261.0,
                    23.0,
                    24.0
                ]
            },
            "integer_answers": {
                "george lucas": 0,
                "abraham lincoln": 2,
                "tyler perry": 12
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "The first person to lead an expedition to the South Pole came from what country?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "norway"
            ],
            "lines": [
                [
                    0.18134920634920634,
                    0.1564102564102564,
                    0.47738855806418595,
                    0.24177599908722341,
                    0.2912552891396333,
                    0.19661563255439163,
                    0.2733812949640288,
                    0.2917369308600337,
                    0.3841568718276065,
                    0.3048327137546468,
                    0.3333333333333333,
                    0.3443073112576064,
                    1.0,
                    0.25,
                    1.0
                ],
                [
                    0.25396825396825395,
                    0.5056410256410258,
                    0.15151608208606487,
                    0.12069222672759346,
                    0.15867418899858957,
                    0.1385979049153908,
                    0.21007194244604316,
                    0.18549747048903878,
                    0.3842689900612428,
                    0.3828996282527881,
                    0.3125,
                    0.31897436866404494,
                    0.0,
                    0.25,
                    1.0
                ],
                [
                    0.5646825396825397,
                    0.33794871794871795,
                    0.37109535984974923,
                    0.6375317741851831,
                    0.5500705218617772,
                    0.6647864625302176,
                    0.516546762589928,
                    0.5227655986509275,
                    0.23157413811115066,
                    0.31226765799256506,
                    0.3541666666666667,
                    0.33671832007834873,
                    0.0,
                    0.5,
                    1.0
                ]
            ],
            "fraction_answers": {
                "canada": 0.4214396085819837,
                "iceland": 0.24095014873214832,
                "norway": 0.33761024268586803
            },
            "question": "the first person to lead an expedition to the south pole came from what country?",
            "rate_limited": false,
            "answers": [
                "norway",
                "iceland",
                "canada"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "canada": 0.22638971214216727,
                "iceland": 0.1979473130238837,
                "norway": 0.4537116745438933
            },
            "integer_answers": {
                "canada": 8,
                "iceland": 3,
                "norway": 3
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.4101511788032446,
                    2.2328205806483146,
                    2.3570282405484413
                ],
                "result_count_important_words": [
                    190000.0,
                    146000.0,
                    359000.0
                ],
                "wikipedia_search": [
                    2.6890981027932455,
                    2.6898829304286997,
                    1.6210189667780546
                ],
                "word_count_appended_bing": [
                    32.0,
                    30.0,
                    34.0
                ],
                "answer_relation_to_question_bing": [
                    0.782051282051282,
                    2.5282051282051285,
                    1.6897435897435897
                ],
                "cosine_similarity_raw": [
                    0.06393487006425858,
                    0.02029198408126831,
                    0.04969941824674606
                ],
                "result_count_noun_chunks": [
                    173000.0,
                    110000.0,
                    310000.0
                ],
                "question_answer_similarity": [
                    1.361483957618475,
                    0.6796395470155403,
                    3.590055614709854
                ],
                "word_count_noun_chunks": [
                    6.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    488000.0,
                    344000.0,
                    1650000.0
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    2.0
                ],
                "result_count": [
                    82600.0,
                    45000.0,
                    156000.0
                ],
                "answer_relation_to_question": [
                    1.088095238095238,
                    1.5238095238095237,
                    3.388095238095238
                ],
                "word_count_appended": [
                    246.0,
                    309.0,
                    252.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "The \u201cAmerican Craftsman\u201d style of house was an architectural reaction to what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "industrial revolution"
            ],
            "lines": [
                [
                    0.17899091914015036,
                    0.24887935145445872,
                    0.20550140654288626,
                    0.37899132670912994,
                    0.0009731290808638875,
                    0.42342342342342343,
                    0.4986217457886677,
                    0.017676767676767676,
                    0.45637403125397025,
                    0.22807017543859648,
                    0.2,
                    0.28966956211352496,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.3212077817670221,
                    0.510824988078207,
                    0.6480408076388888,
                    0.20377879766972998,
                    0.9982420894023104,
                    0.36486486486486486,
                    0.01010719754977029,
                    0.9659090909090909,
                    0.37042391902765426,
                    0.6228070175438597,
                    0.6666666666666666,
                    0.5008923684449349,
                    0.9,
                    1.0,
                    1.0
                ],
                [
                    0.49980129909282756,
                    0.2402956604673343,
                    0.14645778581822486,
                    0.4172298756211401,
                    0.0007847815168257157,
                    0.21171171171171171,
                    0.491271056661562,
                    0.016414141414141416,
                    0.17320204971837544,
                    0.14912280701754385,
                    0.13333333333333333,
                    0.20943806944154017,
                    0.1,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "world war i": 0.22336941704445998,
                "industrial revolution": 0.5774118278259286,
                "the great depression": 0.19921875512961149
            },
            "question": "the \u201camerican craftsman\u201d style of house was an architectural reaction to what?",
            "rate_limited": false,
            "answers": [
                "world war i",
                "industrial revolution",
                "the great depression"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "world war i": 0.05937964371759157,
                "industrial revolution": 0.841973849738393,
                "the great depression": 0.06636761137326008
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7380173726811496,
                    3.0053542106696094,
                    1.256628416649241
                ],
                "result_count_important_words": [
                    4070000.0,
                    82500.0,
                    4010000.0
                ],
                "wikipedia_search": [
                    2.2818701562698513,
                    1.8521195951382712,
                    0.8660102485918773
                ],
                "word_count_appended_bing": [
                    3.0,
                    10.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.7466380543633762,
                    1.532474964234621,
                    0.7208869814020029
                ],
                "cosine_similarity_raw": [
                    0.02848064713180065,
                    0.08981262892484665,
                    0.020297732204198837
                ],
                "result_count_noun_chunks": [
                    56.0,
                    3060.0,
                    52.0
                ],
                "question_answer_similarity": [
                    13.826534636318684,
                    7.434351146221161,
                    15.22157083824277
                ],
                "word_count_noun_chunks": [
                    0.0,
                    9.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    2.0,
                    0.0
                ],
                "result_count_bing": [
                    188000.0,
                    162000.0,
                    94000.0
                ],
                "result_count": [
                    62.0,
                    63600.0,
                    50.0
                ],
                "answer_relation_to_question": [
                    0.8949545957007519,
                    1.6060389088351104,
                    2.499006495464138
                ],
                "word_count_appended": [
                    26.0,
                    71.0,
                    17.0
                ]
            },
            "integer_answers": {
                "world war i": 3,
                "industrial revolution": 9,
                "the great depression": 2
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Catherine de\u2019 Medici was present at the performance of the first-ever formal what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ballet"
            ],
            "question": "catherine de\u2019 medici was present at the performance of the first-ever formal what?",
            "lines": [
                [
                    0.08222811671087533,
                    0.13507625272331156,
                    0.07816444551380787,
                    0.6003400338647699,
                    0.9831492272042116,
                    0.4196505299341163,
                    0.5323741007194245,
                    0.5549738219895288,
                    0.31058582222529885,
                    0.30802292263610315,
                    0.3225806451612903,
                    0.34251158999112846,
                    0.0,
                    0.05714285714285714,
                    1.0
                ],
                [
                    0.11352785145888596,
                    0.15468409586056645,
                    0.1153983734532698,
                    -0.10404605610236911,
                    0.002654727890391235,
                    0.12489258092237181,
                    0.06806862202545656,
                    0.07657068062827226,
                    0.504052631444006,
                    0.0,
                    0.0,
                    0.31350112422706666,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.8042440318302386,
                    0.710239651416122,
                    0.8064371810329224,
                    0.5037060222375992,
                    0.014196044905397196,
                    0.4554568891435119,
                    0.39955727725511897,
                    0.36845549738219896,
                    0.1853615463306951,
                    0.6919770773638968,
                    0.6774193548387096,
                    0.34398728578180493,
                    1.0,
                    0.9428571428571428,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "opera",
                "commedia dell\u2019arte",
                "ballet"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "opera": 0.07722765325456858,
                "ballet": 0.8906167103308177,
                "commedia dell\u2019arte": 0.1645709992650553
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7125579499556423,
                    1.5675056211353333,
                    1.7199364289090247
                ],
                "result_count_important_words": [
                    96200.0,
                    12300.0,
                    72200.0
                ],
                "wikipedia_search": [
                    1.5529291111264942,
                    2.5202631572200302,
                    0.9268077316534755
                ],
                "word_count_appended_bing": [
                    30.0,
                    0.0,
                    63.0
                ],
                "answer_relation_to_question_bing": [
                    0.40522875816993464,
                    0.46405228758169936,
                    2.130718954248366
                ],
                "cosine_similarity_raw": [
                    0.03004775382578373,
                    0.044361114501953125,
                    0.31000828742980957
                ],
                "result_count_noun_chunks": [
                    84800.0,
                    11700.0,
                    56300.0
                ],
                "question_answer_similarity": [
                    2.802265089121647,
                    -0.48566581308841705,
                    2.3511971910484135
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    38.0
                ],
                "result_count_bing": [
                    29300.0,
                    8720.0,
                    31800.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    33.0
                ],
                "word_count_appended": [
                    215.0,
                    0.0,
                    483.0
                ],
                "answer_relation_to_question": [
                    0.41114058355437666,
                    0.5676392572944298,
                    4.021220159151193
                ],
                "result_count": [
                    4370000.0,
                    11800.0,
                    63100.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "For which presidential candidate did Olympic hero Jesse Owens campaign in 1936?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "alf landon"
            ],
            "question": "for which presidential candidate did olympic hero jesse owens campaign in 1936?",
            "lines": [
                [
                    0.16344852236165958,
                    0.03488372093023256,
                    0.30385434980621534,
                    0.9256643978248802,
                    0.8524438267315265,
                    0.675739089629282,
                    0.8938459104572062,
                    0.7158218125960062,
                    0.5481935356238762,
                    0.224,
                    0.2857142857142857,
                    0.32680782438381567,
                    0.0,
                    0.3333333333333333,
                    -1.0
                ],
                [
                    0.6746241543566615,
                    0.5642764857881137,
                    0.6013344113690662,
                    0.08222221361286315,
                    0.05304609682649988,
                    0.22477709995307368,
                    0.03591203923429837,
                    0.08064516129032258,
                    0.1379371253674659,
                    0.584,
                    0.5714285714285714,
                    0.40375698163191415,
                    1.0,
                    0.6666666666666666,
                    -1.0
                ],
                [
                    0.161927323281679,
                    0.40083979328165376,
                    0.09481123882471844,
                    -0.007886611437743317,
                    0.0945100764419736,
                    0.0994838104176443,
                    0.0702420503084955,
                    0.20353302611367127,
                    0.3138693390086579,
                    0.192,
                    0.14285714285714285,
                    0.2694351939842701,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "franklin roosevelt",
                "alf landon",
                "wendell willkie"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "wendell willkie": 0.11043211933365504,
                "franklin roosevelt": 0.2689482827157812,
                "alf landon": 0.7097847526785325
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.6144625950705254,
                    3.230055853055313,
                    2.155481551874161
                ],
                "result_count_important_words": [
                    56500.0,
                    2270.0,
                    4440.0
                ],
                "wikipedia_search": [
                    2.740967678119381,
                    0.6896856268373296,
                    1.5693466950432895
                ],
                "word_count_appended_bing": [
                    4.0,
                    8.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.13953488372093023,
                    2.257105943152455,
                    1.603359173126615
                ],
                "cosine_similarity_raw": [
                    0.04888065159320831,
                    0.096735879778862,
                    0.015252159908413887
                ],
                "result_count_noun_chunks": [
                    93200.0,
                    10500.0,
                    26500.0
                ],
                "question_answer_similarity": [
                    3.9653243941720575,
                    0.3522202540661965,
                    -0.033784352941438556
                ],
                "word_count_noun_chunks": [
                    0.0,
                    2.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    2.0,
                    0.0
                ],
                "result_count_bing": [
                    28800.0,
                    9580.0,
                    4240.0
                ],
                "result_count": [
                    36800.0,
                    2290.0,
                    4080.0
                ],
                "answer_relation_to_question": [
                    1.3075881788932766,
                    5.396993234853292,
                    1.295418586253432
                ],
                "word_count_appended": [
                    28.0,
                    73.0,
                    24.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What is the scarlet pimpernel in the classic novel of the same name?": {
        "raw_data": {
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2732006687696662,
                    1.0505501776346164,
                    1.6762491535957174
                ],
                "result_count_important_words": [
                    148000.0,
                    2860000.0,
                    116000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_appended_bing": [
                    27.0,
                    28.0,
                    36.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    207.0,
                    152.0,
                    196.0
                ],
                "cosine_similarity_raw": [
                    0.016340222209692,
                    0.05248356610536575,
                    0.03994456306099892
                ],
                "result_count_noun_chunks": [
                    123000.0,
                    1840000.0,
                    96300.0
                ],
                "question_answer_similarity": [
                    3.3082328606396914,
                    2.031268745660782,
                    3.0717219412326813
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    8.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    2.0
                ],
                "result_count_bing": [
                    806000.0,
                    199000.0,
                    736000.0
                ],
                "result_count": [
                    123000.0,
                    1840000.0,
                    5410000.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    0.0,
                    1.0
                ]
            },
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "z-best_answer_by_ml": [
                "flower"
            ],
            "lines": [
                [
                    0.0,
                    0,
                    0.15022956588904404,
                    0.3933117271160782,
                    0.016682490166824902,
                    0.462952326249282,
                    0.04737516005121639,
                    0.05972903413781382,
                    0.0,
                    0.372972972972973,
                    0.2967032967032967,
                    0.31830016719241655,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.0,
                    0,
                    0.48252607896858346,
                    0.24149503745581852,
                    0.24955920249559202,
                    0.11430212521539346,
                    0.9154929577464789,
                    0.89350750254941,
                    0.0,
                    0.27387387387387385,
                    0.3076923076923077,
                    0.2626375444086541,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    1.0,
                    0,
                    0.3672443551423725,
                    0.3651932354281033,
                    0.7337583073375831,
                    0.42274554853532453,
                    0.03713188220230474,
                    0.046763463312776185,
                    1.0,
                    0.35315315315315315,
                    0.3956043956043956,
                    0.41906228839892934,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "question": "what is the scarlet pimpernel in the classic novel of the same name?",
            "rate_limited": false,
            "answers": [
                "dress",
                "dagger",
                "flower"
            ],
            "ml_answers": {
                "dagger": 0.19573346400715566,
                "flower": 0.8229886475598878,
                "dress": 0.09379171163613181
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "In computing, what unit is half a byte?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "nibble"
            ],
            "lines": [
                [
                    0.34293945074614224,
                    0.2616279069767442,
                    0.6170152798188766,
                    0.5522246340505271,
                    0.46401225114854516,
                    0.34991119005328597,
                    0.4774774774774775,
                    0.9098712446351931,
                    1.0,
                    0.6594533029612756,
                    0.4722222222222222,
                    0.5017473363094983,
                    0.8571428571428571,
                    0.9012345679012346,
                    1.0
                ],
                [
                    0.23159136402630828,
                    0.11046511627906977,
                    0.014212299352627699,
                    0.0,
                    0.0,
                    0.3019538188277087,
                    0.0,
                    0.0,
                    0.0,
                    0.018223234624145785,
                    0.17222222222222222,
                    0.029609307693500425,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.42546918522754956,
                    0.627906976744186,
                    0.3687724208284957,
                    0.44777536594947287,
                    0.5359877488514548,
                    0.3481349911190053,
                    0.5225225225225225,
                    0.09012875536480687,
                    0.0,
                    0.3223234624145786,
                    0.35555555555555557,
                    0.46864335599700124,
                    0.14285714285714285,
                    0.09876543209876543,
                    1.0
                ]
            ],
            "fraction_answers": {
                "demibyte": 0.0627340973589702,
                "octet": 0.3396316368236098,
                "nibble": 0.59763426581742
            },
            "question": "in computing, what unit is half a byte?",
            "rate_limited": false,
            "answers": [
                "nibble",
                "demibyte",
                "octet"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "demibyte": 0.15104784383726919,
                "octet": 0.44646454127428176,
                "nibble": 0.7906221929744528
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.006989345237993,
                    0.1184372307740017,
                    1.874573423988005
                ],
                "result_count_important_words": [
                    1060000.0,
                    0,
                    1160000.0
                ],
                "wikipedia_search": [
                    4.0,
                    0.0,
                    0.0
                ],
                "answer_relation_to_question": [
                    1.3717578029845687,
                    0.926365456105233,
                    1.701876740910198
                ],
                "result_count": [
                    90900.0,
                    0,
                    105000.0
                ],
                "answer_relation_to_question_bing": [
                    1.0465116279069768,
                    0.4418604651162791,
                    2.511627906976744
                ],
                "cosine_similarity_raw": [
                    0.327290803194046,
                    0.007538800127804279,
                    0.195612370967865
                ],
                "result_count_noun_chunks": [
                    1060000.0,
                    0,
                    105000.0
                ],
                "question_answer_similarity": [
                    0.9009848218411207,
                    0.0,
                    0.7305701039731503
                ],
                "word_count_noun_chunks": [
                    162.0,
                    0.0,
                    27.0
                ],
                "word_count_raw": [
                    73.0,
                    0.0,
                    8.0
                ],
                "result_count_bing": [
                    1970000.0,
                    1700000.0,
                    1960000.0
                ],
                "word_count_appended": [
                    579.0,
                    16.0,
                    283.0
                ],
                "word_count_appended_bing": [
                    85.0,
                    31.0,
                    64.0
                ]
            },
            "integer_answers": {
                "demibyte": 0,
                "octet": 4,
                "nibble": 10
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What Japanese word means \u201cempty orchestra\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "karaoke"
            ],
            "lines": [
                [
                    0.27706766917293235,
                    0.46266166040884443,
                    0.019554620348090957,
                    0.44658484516116204,
                    0.04940970703979012,
                    0.23235613463626492,
                    0.3717884130982368,
                    0.15374677002583978,
                    0.15625,
                    0.19955898566703417,
                    0.3076923076923077,
                    0.29997514686541915,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.3902255639097744,
                    0.1118064246975386,
                    0.11970479896181072,
                    0.29825212549289426,
                    0.05421950153038916,
                    0.6514657980456026,
                    0.29974811083123426,
                    0.3695090439276486,
                    0.4270833333333333,
                    0.22601984564498345,
                    0.24786324786324787,
                    0.31240709193252547,
                    0.017605633802816902,
                    0.025,
                    1.0
                ],
                [
                    0.33270676691729323,
                    0.425531914893617,
                    0.8607405806900983,
                    0.2551630293459437,
                    0.8963707914298207,
                    0.11617806731813246,
                    0.32846347607052895,
                    0.47674418604651164,
                    0.4166666666666667,
                    0.5744211686879823,
                    0.4444444444444444,
                    0.3876177612020554,
                    0.9823943661971831,
                    0.975,
                    1.0
                ]
            ],
            "fraction_answers": {
                "sake": 0.2126175900082802,
                "anime": 0.2536364657124142,
                "karaoke": 0.5337459442793056
            },
            "question": "what japanese word means \u201cempty orchestra\u201d?",
            "rate_limited": false,
            "answers": [
                "sake",
                "anime",
                "karaoke"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sake": 0.04758609181201683,
                "anime": 0.06971040142761664,
                "karaoke": 0.767716294146466
            },
            "integer_answers": {
                "sake": 3,
                "anime": 3,
                "karaoke": 8
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1999005874616766,
                    1.2496283677301019,
                    1.5504710448082215
                ],
                "result_count_important_words": [
                    738000.0,
                    595000.0,
                    652000.0
                ],
                "wikipedia_search": [
                    0.625,
                    1.7083333333333333,
                    1.6666666666666667
                ],
                "word_count_appended_bing": [
                    36.0,
                    29.0,
                    52.0
                ],
                "answer_relation_to_question_bing": [
                    1.3879849812265332,
                    0.3354192740926158,
                    1.2765957446808511
                ],
                "cosine_similarity_raw": [
                    0.010764405131340027,
                    0.0658949613571167,
                    0.4738194942474365
                ],
                "result_count_noun_chunks": [
                    11900.0,
                    28600.0,
                    36900.0
                ],
                "question_answer_similarity": [
                    2.953303724527359,
                    1.9723667800426483,
                    1.6874149069190025
                ],
                "word_count_noun_chunks": [
                    0.0,
                    5.0,
                    279.0
                ],
                "result_count_bing": [
                    4280000.0,
                    12000000.0,
                    2140000.0
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    39.0
                ],
                "result_count": [
                    1130.0,
                    1240.0,
                    20500.0
                ],
                "answer_relation_to_question": [
                    1.1082706766917294,
                    1.5609022556390977,
                    1.330827067669173
                ],
                "word_count_appended": [
                    181.0,
                    205.0,
                    521.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Before it was discontinued, how large was a McDonald's Supersized drink?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "42oz"
            ],
            "lines": [
                [
                    0.1528801528801529,
                    0.6666666666666667,
                    0.24071388651818026,
                    0.24697548392689953,
                    0.06521739130434782,
                    0.328125,
                    0.06382978723404255,
                    0.06036745406824147,
                    0.125,
                    0.22826086956521738,
                    0.3176470588235294,
                    0.21246133046780882,
                    0.0,
                    0,
                    5.0
                ],
                [
                    0.6120666120666121,
                    0.16666666666666669,
                    0.5190978507310615,
                    0.3211479586075576,
                    0.6086956521739131,
                    0.35,
                    0.6382978723404256,
                    0.7952755905511811,
                    0.125,
                    0.43478260869565216,
                    0.36470588235294116,
                    0.4621024755821681,
                    1.0,
                    0,
                    5.0
                ],
                [
                    0.23505323505323505,
                    0.16666666666666669,
                    0.24018826275075825,
                    0.4318765574655429,
                    0.32608695652173914,
                    0.321875,
                    0.2978723404255319,
                    0.14435695538057744,
                    0.75,
                    0.33695652173913043,
                    0.3176470588235294,
                    0.3254361939500231,
                    0.0,
                    0,
                    5.0
                ]
            ],
            "fraction_answers": {
                "42oz": 0.49214147459755225,
                "76oz": 0.20831885241962209,
                "60oz": 0.29953967298282574
            },
            "question": "before it was discontinued, how large was a mcdonald's supersized drink?",
            "rate_limited": false,
            "answers": [
                "76oz",
                "42oz",
                "60oz"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "42oz": 0.4944274791058199,
                "76oz": 0.17465879337275242,
                "60oz": 0.13031897174928292
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 5
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.062306652339044,
                    2.3105123779108405,
                    1.6271809697501154
                ],
                "result_count_important_words": [
                    3.0,
                    30.0,
                    14.0
                ],
                "wikipedia_search": [
                    0.25,
                    0.25,
                    1.5
                ],
                "word_count_appended_bing": [
                    27.0,
                    31.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    1.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "cosine_similarity_raw": [
                    0.024352703243494034,
                    0.05251643806695938,
                    0.02429952658712864
                ],
                "result_count_noun_chunks": [
                    23.0,
                    303.0,
                    55.0
                ],
                "question_answer_similarity": [
                    1.8175161466933787,
                    2.363358463626355,
                    3.178220785688609
                ],
                "word_count_noun_chunks": [
                    0.0,
                    4.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    105000.0,
                    112000.0,
                    103000.0
                ],
                "result_count": [
                    3.0,
                    28.0,
                    15.0
                ],
                "answer_relation_to_question": [
                    0.45864045864045866,
                    1.8361998361998362,
                    0.7051597051597052
                ],
                "word_count_appended": [
                    21.0,
                    40.0,
                    31.0
                ]
            },
            "integer_answers": {
                "42oz": 10,
                "76oz": 1,
                "60oz": 2
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What advertising mascot wears epaulettes?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "cap'n crunch"
            ],
            "lines": [
                [
                    0.2593201754385965,
                    0.1875,
                    0.31883078843428436,
                    0.5168333507362561,
                    0.06930693069306931,
                    0.03712770297837617,
                    0.07766990291262135,
                    0.06666666666666667,
                    0.08333333333333333,
                    0.15,
                    0.11224489795918367,
                    0.18644387195267403,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.34813596491228066,
                    0.6875,
                    0.29941114708592725,
                    0.3766530791713206,
                    0.5742574257425742,
                    0.7071943424452605,
                    0.5825242718446602,
                    0.6285714285714286,
                    0.21794871794871795,
                    0.3375,
                    0.2653061224489796,
                    0.4048589315641491,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.3925438596491228,
                    0.125,
                    0.3817580644797884,
                    0.10651357009242324,
                    0.3564356435643564,
                    0.2556779545763634,
                    0.33980582524271846,
                    0.3047619047619048,
                    0.6987179487179488,
                    0.5125,
                    0.6224489795918368,
                    0.4086971964831769,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "sun-maid raisin girl": 0.14751983007893293,
                "mr. peanut": 0.38784724512394997,
                "cap'n crunch": 0.46463292479711704
            },
            "question": "what advertising mascot wears epaulettes?",
            "rate_limited": false,
            "answers": [
                "sun-maid raisin girl",
                "mr. peanut",
                "cap'n crunch"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sun-maid raisin girl": 0.23951736155597145,
                "mr. peanut": 0.2429223168910891,
                "cap'n crunch": 0.6419846675078794
            },
            "integer_answers": {
                "sun-maid raisin girl": 1,
                "mr. peanut": 5,
                "cap'n crunch": 8
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.7457754878106961,
                    1.6194357262565964,
                    1.6347887859327077
                ],
                "result_count_important_words": [
                    8.0,
                    60.0,
                    35.0
                ],
                "wikipedia_search": [
                    0.25,
                    0.6538461538461539,
                    2.0961538461538463
                ],
                "word_count_appended_bing": [
                    11.0,
                    26.0,
                    61.0
                ],
                "answer_relation_to_question_bing": [
                    0.375,
                    1.375,
                    0.25
                ],
                "cosine_similarity_raw": [
                    0.06325826793909073,
                    0.05940527468919754,
                    0.07574348151683807
                ],
                "result_count_noun_chunks": [
                    7.0,
                    66.0,
                    32.0
                ],
                "question_answer_similarity": [
                    3.976561378221959,
                    2.898002006812021,
                    0.8195248013362288
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    10.0
                ],
                "result_count_bing": [
                    27300.0,
                    520000.0,
                    188000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count": [
                    7.0,
                    58.0,
                    36.0
                ],
                "answer_relation_to_question": [
                    0.7779605263157895,
                    1.044407894736842,
                    1.1776315789473684
                ],
                "word_count_appended": [
                    12.0,
                    27.0,
                    41.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Made famous in a documentary, where is the \u201cGrey Gardens\u201d home located?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "east hampton, ny"
            ],
            "lines": [
                [
                    0.1996732026143791,
                    0.425,
                    0.11839124318058028,
                    0.3674592436367138,
                    0.22972972972972974,
                    0.07704972011853803,
                    0.000474861305752771,
                    0.7539742619227857,
                    0.3482142857142857,
                    0.12121212121212122,
                    0.2857142857142857,
                    0.1761709863152494,
                    0.0,
                    0,
                    3.0
                ],
                [
                    0.6416258169934641,
                    0.2625,
                    0.7604082370831856,
                    0.39010097015012135,
                    0.5945945945945946,
                    0.06848864010536714,
                    0.3497758886276508,
                    0.22649507948523845,
                    0.38125,
                    0.696969696969697,
                    0.2857142857142857,
                    0.6665708212135255,
                    1.0,
                    0,
                    3.0
                ],
                [
                    0.15870098039215685,
                    0.3125,
                    0.12120051973623412,
                    0.24243978621316487,
                    0.17567567567567569,
                    0.8544616397760948,
                    0.6497492500665965,
                    0.019530658591975777,
                    0.27053571428571427,
                    0.18181818181818182,
                    0.42857142857142855,
                    0.15725819247122505,
                    0.0,
                    0,
                    3.0
                ]
            ],
            "fraction_answers": {
                "east hampton, ny": 0.48649954084131775,
                "savannah, ga": 0.2748032328921883,
                "cape cod, ma": 0.238697226266494
            },
            "question": "made famous in a documentary, where is the \u201cgrey gardens\u201d home located?",
            "rate_limited": false,
            "answers": [
                "cape cod, ma",
                "east hampton, ny",
                "savannah, ga"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "east hampton, ny": 0.6087735524260762,
                "savannah, ga": 0.12853168771410994,
                "cape cod, ma": 0.12023459203783665
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 3
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.0570259178914965,
                    3.999424927281153,
                    0.9435491548273502
                ],
                "result_count_important_words": [
                    41.0,
                    30200.0,
                    56100.0
                ],
                "wikipedia_search": [
                    1.3928571428571428,
                    1.525,
                    1.082142857142857
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    3.0
                ],
                "answer_relation_to_question_bing": [
                    1.7,
                    1.05,
                    1.25
                ],
                "cosine_similarity_raw": [
                    0.020820097997784615,
                    0.13372419774532318,
                    0.02131413295865059
                ],
                "result_count_noun_chunks": [
                    249000.0,
                    74800.0,
                    6450.0
                ],
                "question_answer_similarity": [
                    10.66745613887906,
                    11.324752502143383,
                    7.038102403283119
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    468000.0,
                    416000.0,
                    5190000.0
                ],
                "word_count_appended": [
                    4.0,
                    23.0,
                    6.0
                ],
                "answer_relation_to_question": [
                    0.7986928104575164,
                    2.5665032679738564,
                    0.6348039215686274
                ],
                "result_count": [
                    34.0,
                    88.0,
                    26.0
                ]
            },
            "integer_answers": {
                "east hampton, ny": 8,
                "savannah, ga": 3,
                "cape cod, ma": 2
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these is NOT a machine used for printing?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "hydraulophone"
            ],
            "lines": [
                [
                    0.4982014388489209,
                    0.5,
                    0.45385086093595184,
                    0.5,
                    0.4894996110967073,
                    0.33333333333333337,
                    0.4937595795927305,
                    0.46811195228263364,
                    0.5,
                    0.3304794520547945,
                    0.364406779661017,
                    0.41345403780827406,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.35322991606714627,
                    0.3235632183908046,
                    0.29731279112378983,
                    0.5,
                    0.041958344136202586,
                    0.33333333333333337,
                    0.09710970002189623,
                    0.10082587749483829,
                    0.30978260869565216,
                    0.261986301369863,
                    0.2542372881355932,
                    0.27521275249180194,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.14856864508393286,
                    0.1764367816091954,
                    0.24883634794025833,
                    0.0,
                    0.46854204476709016,
                    0.33333333333333337,
                    0.40913072038537335,
                    0.43106217022252813,
                    0.19021739130434784,
                    0.4075342465753425,
                    0.3813559322033898,
                    0.31133320969992406,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "spirit duplicator": 0.4156081961458807,
                "hectograph": 0.47524131145651305,
                "hydraulophone": 0.10915049239760616
            },
            "question": "which of these is not a machine used for printing?",
            "rate_limited": false,
            "answers": [
                "hydraulophone",
                "hectograph",
                "spirit duplicator"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "spirit duplicator": 0.035669042525297276,
                "hectograph": 0.04424620419912512,
                "hydraulophone": 0.2017875957928535
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.3461838487669039,
                    0.8991489900327923,
                    0.7546671612003037
                ],
                "result_count_important_words": [
                    2850.0,
                    184000.0,
                    41500.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.7608695652173914,
                    1.2391304347826086
                ],
                "word_count_appended_bing": [
                    32.0,
                    58.0,
                    28.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.7057471264367816,
                    1.2942528735632184
                ],
                "cosine_similarity_raw": [
                    0.02854781597852707,
                    0.12538212537765503,
                    0.1553696095943451
                ],
                "result_count_noun_chunks": [
                    27800.0,
                    348000.0,
                    60100.0
                ],
                "question_answer_similarity": [
                    0.0,
                    0.0,
                    3.38682275544852
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    78900000.0,
                    78900000.0,
                    78900000.0
                ],
                "word_count_appended": [
                    198.0,
                    278.0,
                    108.0
                ],
                "answer_relation_to_question": [
                    0.007194244604316547,
                    0.5870803357314149,
                    1.4057254196642686
                ],
                "result_count": [
                    2430.0,
                    106000.0,
                    7280.0
                ]
            },
            "integer_answers": {
                "spirit duplicator": 5,
                "hectograph": 6,
                "hydraulophone": 1
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Dominique Ansel is credited with creating what food craze?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "cronut"
            ],
            "lines": [
                [
                    0.22114624505928854,
                    0.23025641025641025,
                    0.021502256258065993,
                    0.47362092526786315,
                    0.18125,
                    0.2005475701574264,
                    0.0006185975157123769,
                    0.07399523592316659,
                    0.0,
                    0.05088495575221239,
                    0.06422018348623854,
                    0.19062803679023496,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.23197314762532156,
                    0.16102564102564104,
                    0.05111069014371639,
                    0.5263790747321369,
                    0.4625,
                    0.33744010951403147,
                    0.0022022071559360617,
                    0.04921190005574984,
                    0.24393939393939396,
                    0.15486725663716813,
                    0.09174311926605505,
                    0.38480744614668616,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.5468806073153899,
                    0.6087179487179488,
                    0.9273870535982176,
                    0.0,
                    0.35625,
                    0.4620123203285421,
                    0.9971791953283515,
                    0.8767928640210836,
                    0.7560606060606061,
                    0.7942477876106194,
                    0.8440366972477065,
                    0.4245645170630789,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "ramen burger": 0.19265714187441693,
                "cronut": 0.6852949712351103,
                "rainbow bagel": 0.12204788689047279
            },
            "question": "dominique ansel is credited with creating what food craze?",
            "rate_limited": false,
            "answers": [
                "rainbow bagel",
                "ramen burger",
                "cronut"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "ramen burger": 0.23871471702857464,
                "cronut": 0.7906221929744528,
                "rainbow bagel": 0.15104784383726919
            },
            "integer_answers": {
                "ramen burger": 2,
                "cronut": 12,
                "rainbow bagel": 0
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1437682207414097,
                    2.308844676880117,
                    2.5473871023784733
                ],
                "result_count_important_words": [
                    25.0,
                    89.0,
                    40300.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.9757575757575758,
                    3.0242424242424244
                ],
                "word_count_appended_bing": [
                    7.0,
                    10.0,
                    92.0
                ],
                "answer_relation_to_question_bing": [
                    1.1512820512820512,
                    0.8051282051282052,
                    3.043589743589744
                ],
                "cosine_similarity_raw": [
                    0.016849393025040627,
                    0.040050871670246124,
                    0.7267102003097534
                ],
                "result_count_noun_chunks": [
                    14600.0,
                    9710.0,
                    173000.0
                ],
                "question_answer_similarity": [
                    2.7380473613739014,
                    3.043047212995589,
                    0.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    145.0
                ],
                "result_count_bing": [
                    5860.0,
                    9860.0,
                    13500.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    77.0
                ],
                "result_count": [
                    29.0,
                    74.0,
                    57.0
                ],
                "answer_relation_to_question": [
                    1.1057312252964426,
                    1.1598657381266078,
                    2.7344030365769494
                ],
                "word_count_appended": [
                    23.0,
                    70.0,
                    359.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "The mouse that came with the original 1998 iMac was criticized for being what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "round"
            ],
            "lines": [
                [
                    0.375,
                    0.36231884057971014,
                    0.15638847396493522,
                    0.24347519038833706,
                    0.30027932960893855,
                    0.168,
                    0.028759630653414102,
                    0.2982456140350877,
                    0.6666666666666666,
                    0.30267558528428096,
                    0.18490566037735848,
                    0.3357687486003062,
                    0.017241379310344827,
                    0.0,
                    1.0
                ],
                [
                    0.25,
                    0.2971014492753623,
                    0.25496208731250436,
                    0.40539016868714883,
                    0.3861731843575419,
                    0.5186666666666667,
                    0.9410104099276598,
                    0.43859649122807015,
                    0.0,
                    0.3193979933110368,
                    0.24150943396226415,
                    0.3462470733461473,
                    0.05172413793103448,
                    0.020833333333333332,
                    1.0
                ],
                [
                    0.375,
                    0.3405797101449275,
                    0.5886494387225604,
                    0.3511346409245141,
                    0.31354748603351956,
                    0.31333333333333335,
                    0.03022995941892607,
                    0.2631578947368421,
                    0.3333333333333333,
                    0.3779264214046823,
                    0.5735849056603773,
                    0.3179841780535465,
                    0.9310344827586207,
                    0.9791666666666666,
                    1.0
                ]
            ],
            "fraction_answers": {
                "wireless": 0.24569465139066995,
                "large": 0.3194008878099122,
                "round": 0.4349044607994178
            },
            "question": "the mouse that came with the original 1998 imac was criticized for being what?",
            "rate_limited": false,
            "answers": [
                "wireless",
                "large",
                "round"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "wireless": 0.12691597782648173,
                "large": 0.1371372219105358,
                "round": 0.7962475017889267
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.014612491601837,
                    2.0774824400768837,
                    1.907905068321279
                ],
                "result_count_important_words": [
                    48900.0,
                    1600000.0,
                    51400.0
                ],
                "wikipedia_search": [
                    2.6666666666666665,
                    0.0,
                    1.3333333333333333
                ],
                "word_count_appended_bing": [
                    49.0,
                    64.0,
                    152.0
                ],
                "answer_relation_to_question_bing": [
                    0.7246376811594203,
                    0.5942028985507246,
                    0.681159420289855
                ],
                "cosine_similarity_raw": [
                    0.02047910913825035,
                    0.03338734805583954,
                    0.07708378881216049
                ],
                "result_count_noun_chunks": [
                    170000.0,
                    250000.0,
                    150000.0
                ],
                "question_answer_similarity": [
                    2.6738502476364374,
                    4.452004334423691,
                    3.856168856844306
                ],
                "word_count_noun_chunks": [
                    1.0,
                    3.0,
                    54.0
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    47.0
                ],
                "result_count_bing": [
                    1260000.0,
                    3890000.0,
                    2350000.0
                ],
                "word_count_appended": [
                    181.0,
                    191.0,
                    226.0
                ],
                "answer_relation_to_question": [
                    1.5,
                    1.0,
                    1.5
                ],
                "result_count": [
                    43000.0,
                    55300.0,
                    44900.0
                ]
            },
            "integer_answers": {
                "wireless": 3,
                "large": 6,
                "round": 5
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "How does the second verse of \u201cThe Star-Spangled Banner\u201d begin?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "on the shore"
            ],
            "lines": [
                [
                    0.1439958592132505,
                    0.22273425499231952,
                    0.3637799558175296,
                    0.22182574101272395,
                    0.003394014193150262,
                    0.21284403669724772,
                    0.01005620770083738,
                    0.002071576541181513,
                    0.5833333333333334,
                    0.03529411764705882,
                    0.1111111111111111,
                    0.0453411203301235,
                    0.0,
                    0,
                    5.0
                ],
                [
                    0.3432712215320911,
                    0.5545314900153611,
                    0.2482138644181911,
                    0.38012396174727164,
                    0.6572045664918235,
                    0.5743119266055046,
                    0.9482659733109012,
                    0.8643474533895278,
                    0.06250000000000001,
                    0.5176470588235295,
                    0.4444444444444444,
                    0.48295626749763615,
                    0.7142857142857143,
                    0,
                    5.0
                ],
                [
                    0.5127329192546585,
                    0.22273425499231952,
                    0.3880061797642793,
                    0.3980502972400044,
                    0.3394014193150262,
                    0.21284403669724772,
                    0.04167781898826139,
                    0.13358097006929065,
                    0.3541666666666667,
                    0.4470588235294118,
                    0.4444444444444444,
                    0.47170261217224035,
                    0.2857142857142857,
                    0,
                    5.0
                ]
            ],
            "fraction_answers": {
                "on the shore": 0.5224695340432305,
                "travels far": 0.15044471758383596,
                "where the foe": 0.3270857483729336
            },
            "question": "how does the second verse of \u201cthe star-spangled banner\u201d begin?",
            "rate_limited": false,
            "answers": [
                "travels far",
                "on the shore",
                "where the foe"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "on the shore": 0.46363366563711295,
                "travels far": 0.26716352298764634,
                "where the foe": 0.4620127898196733
            },
            "integer_answers": {
                "on the shore": 9,
                "travels far": 1,
                "where the foe": 3
            },
            "categorical_data": {
                "question_type": 5
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.272046721980741,
                    2.897737604985817,
                    2.8302156730334422
                ],
                "result_count_important_words": [
                    2630.0,
                    248000.0,
                    10900.0
                ],
                "wikipedia_search": [
                    2.333333333333333,
                    0.25,
                    1.4166666666666665
                ],
                "word_count_appended_bing": [
                    2.0,
                    8.0,
                    8.0
                ],
                "answer_relation_to_question_bing": [
                    0.6682027649769584,
                    1.6635944700460827,
                    0.6682027649769584
                ],
                "cosine_similarity_raw": [
                    0.05813048407435417,
                    0.039663515985012054,
                    0.062001731246709824
                ],
                "result_count_noun_chunks": [
                    29.0,
                    12100.0,
                    1870.0
                ],
                "question_answer_similarity": [
                    7.946084663271904,
                    13.616531466512242,
                    14.258675966411829
                ],
                "word_count_noun_chunks": [
                    0.0,
                    5.0,
                    2.0
                ],
                "result_count_bing": [
                    116000.0,
                    313000.0,
                    116000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    11.0,
                    2130.0,
                    1100.0
                ],
                "answer_relation_to_question": [
                    0.7199792960662525,
                    1.7163561076604554,
                    2.5636645962732922
                ],
                "word_count_appended": [
                    3.0,
                    44.0,
                    38.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these elements would earn you the highest score in Scrabble?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "oxygen"
            ],
            "question": "which of these elements would earn you the highest score in scrabble?",
            "lines": [
                [
                    0.29251355013550134,
                    0.18115942028985507,
                    0.546338901334228,
                    0.5474390729476273,
                    0.4488200376429709,
                    0.38769230769230767,
                    0.006641062570011202,
                    0.4570990806945863,
                    0.5664794007490637,
                    0.37450980392156863,
                    0.37755102040816324,
                    0.385547954734964,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.48204607046070463,
                    0.13768115942028986,
                    0.18145074566814454,
                    0.05241256827694494,
                    0.002219970078664157,
                    0.4153846153846154,
                    0.0052008321331413025,
                    0.03983656792645557,
                    0.12890137328339576,
                    0.29215686274509806,
                    0.29591836734693877,
                    0.2930454376671805,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.22544037940379402,
                    0.6811594202898551,
                    0.2722103529976275,
                    0.40014835877542776,
                    0.5489599922783649,
                    0.19692307692307692,
                    0.9881581052968474,
                    0.5030643513789581,
                    0.3046192259675406,
                    0.3333333333333333,
                    0.32653061224489793,
                    0.32140660759785555,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "oxygen",
                "xenon",
                "zinc"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "oxygen": 0.5504480696088643,
                "zinc": 0.20833639973478155,
                "xenon": 0.10575629902989722
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.92773977367482,
                    1.4652271883359025,
                    1.6070330379892777
                ],
                "result_count_important_words": [
                    166000.0,
                    130000.0,
                    24700000.0
                ],
                "wikipedia_search": [
                    1.6994382022471908,
                    0.38670411985018727,
                    0.9138576779026217
                ],
                "word_count_appended_bing": [
                    37.0,
                    29.0,
                    32.0
                ],
                "answer_relation_to_question_bing": [
                    0.36231884057971014,
                    0.2753623188405797,
                    1.3623188405797102
                ],
                "cosine_similarity_raw": [
                    0.053580109030008316,
                    0.01779509149491787,
                    0.026695994660258293
                ],
                "result_count_noun_chunks": [
                    1790000.0,
                    156000.0,
                    1970000.0
                ],
                "question_answer_similarity": [
                    2.3608030527830124,
                    0.22602652478963137,
                    1.725619367789477
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    252000.0,
                    270000.0,
                    128000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    3720000.0,
                    18400.0,
                    4550000.0
                ],
                "answer_relation_to_question": [
                    0.8775406504065041,
                    1.446138211382114,
                    0.6763211382113821
                ],
                "word_count_appended": [
                    191.0,
                    149.0,
                    170.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these substances is both artificially made and found in nature?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "nylon"
            ],
            "lines": [
                [
                    0.4347826086956522,
                    0.16666666666666666,
                    0.40998680945592125,
                    0.08729518727437834,
                    0.26376146788990823,
                    0.3333333333333333,
                    0.34657039711191334,
                    0.42771804062126645,
                    0.2833333333333333,
                    0.29746192893401013,
                    0.26851851851851855,
                    0.32527956678686865,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.3478260869565218,
                    0.08333333333333333,
                    0.3732966375498108,
                    0.43601795054528153,
                    0.3532110091743119,
                    0.3333333333333333,
                    0.31227436823104693,
                    0.26523297491039427,
                    0.09999999999999999,
                    0.3116751269035533,
                    0.35185185185185186,
                    0.34434278259483403,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.2173913043478261,
                    0.75,
                    0.21671655299426795,
                    0.4766868621803401,
                    0.3830275229357798,
                    0.3333333333333333,
                    0.34115523465703973,
                    0.3070489844683393,
                    0.6166666666666667,
                    0.39086294416243655,
                    0.37962962962962965,
                    0.3303776506182973,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "latex": 0.3952413904994964,
                "teflon": 0.3037256548851475,
                "nylon": 0.3010329546153561
            },
            "question": "which of these substances is both artificially made and found in nature?",
            "rate_limited": false,
            "answers": [
                "teflon",
                "nylon",
                "latex"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "latex": 0.2333228432113928,
                "teflon": 0.13035051088164756,
                "nylon": 0.29888308289832444
            },
            "integer_answers": {
                "latex": 6,
                "teflon": 5,
                "nylon": 1
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3011182671474746,
                    1.3773711303793361,
                    1.3215106024731893
                ],
                "result_count_important_words": [
                    1920000.0,
                    1730000.0,
                    1890000.0
                ],
                "wikipedia_search": [
                    0.85,
                    0.3,
                    1.85
                ],
                "word_count_appended_bing": [
                    29.0,
                    38.0,
                    41.0
                ],
                "answer_relation_to_question_bing": [
                    0.3333333333333333,
                    0.16666666666666666,
                    1.5
                ],
                "cosine_similarity_raw": [
                    0.048627905547618866,
                    0.04427614063024521,
                    0.025704417377710342
                ],
                "result_count_noun_chunks": [
                    3580000.0,
                    2220000.0,
                    2570000.0
                ],
                "question_answer_similarity": [
                    0.3373199393681716,
                    1.684829980134964,
                    1.8419799357652664
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    2850000.0,
                    2850000.0,
                    2850000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    2300000.0,
                    3080000.0,
                    3340000.0
                ],
                "answer_relation_to_question": [
                    0.43478260869565216,
                    0.34782608695652173,
                    0.21739130434782608
                ],
                "word_count_appended": [
                    293.0,
                    307.0,
                    385.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Every U.S. state that starts with which of these letters has a Democratic governor?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "v"
            ],
            "lines": [
                [
                    0.16948529411764707,
                    0.2348694316436252,
                    0.1166343046951322,
                    0.29650389086959006,
                    0.3261538461538461,
                    0.33346550356859633,
                    0.3241296518607443,
                    0.34154589371980676,
                    0.18421052631578946,
                    0.2512703439133957,
                    0.22309552599758162,
                    0.3297837029038114,
                    0.20944962766412736,
                    0.2496119217634275,
                    -1.0
                ],
                [
                    0.1860294117647059,
                    0.30284178187403993,
                    0.34168646231759614,
                    0.48435498304797675,
                    0.3630769230769231,
                    0.3326724821570182,
                    0.35414165666266506,
                    0.3502415458937198,
                    0.5,
                    0.45216879004344945,
                    0.577589681580008,
                    0.32377968743986396,
                    0.4698279551485064,
                    0.4889785780813412,
                    -1.0
                ],
                [
                    0.6444852941176471,
                    0.46228878648233485,
                    0.5416792329872716,
                    0.2191411260824332,
                    0.31076923076923074,
                    0.3338620142743854,
                    0.3217286914765906,
                    0.30821256038647343,
                    0.3157894736842105,
                    0.29656086604315485,
                    0.19931479242241032,
                    0.34643660965632456,
                    0.32072241718736627,
                    0.2614095001552313,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "c": 0.394813567077701,
                "w": 0.2564435332276515,
                "v": 0.34874289969464745
            },
            "question": "every u.s. state that starts with which of these letters has a democratic governor?",
            "rate_limited": false,
            "answers": [
                "w",
                "c",
                "v"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "c": 0.2777643218584197,
                "w": 0.16896246226856335,
                "v": 0.5073009419815131
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9787022174228686,
                    1.9426781246391838,
                    2.0786196579379475
                ],
                "result_count_important_words": [
                    2700000.0,
                    2950000.0,
                    2680000.0
                ],
                "wikipedia_search": [
                    0.3684210526315789,
                    1.0,
                    0.631578947368421
                ],
                "word_count_appended_bing": [
                    1107.0,
                    2866.0,
                    989.0
                ],
                "answer_relation_to_question_bing": [
                    0.7046082949308756,
                    0.9085253456221198,
                    1.3868663594470045
                ],
                "cosine_similarity_raw": [
                    0.01763390377163887,
                    0.051659468561410904,
                    0.08189631253480911
                ],
                "result_count_noun_chunks": [
                    7070000.0,
                    7250000.0,
                    6380000.0
                ],
                "question_answer_similarity": [
                    1.6639529606327415,
                    2.718156263232231,
                    1.2298001367598772
                ],
                "word_count_noun_chunks": [
                    2447.0,
                    5489.0,
                    3747.0
                ],
                "word_count_raw": [
                    804.0,
                    1575.0,
                    842.0
                ],
                "result_count_bing": [
                    8410000.0,
                    8390000.0,
                    8420000.0
                ],
                "result_count": [
                    10600000.0,
                    11800000.0,
                    10100000.0
                ],
                "answer_relation_to_question": [
                    0.6779411764705883,
                    0.7441176470588236,
                    2.5779411764705884
                ],
                "word_count_appended": [
                    3412.0,
                    6140.0,
                    4027.0
                ]
            },
            "integer_answers": {
                "c": 9,
                "w": 0,
                "v": 5
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What does a rattlesnake typically do when it feels threatened?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "rattles its tail"
            ],
            "lines": [
                [
                    0.4841269841269842,
                    1.0,
                    0.8441031028169786,
                    0.25338692452378664,
                    7.417032253102105e-05,
                    0.04995044598612488,
                    0.00029905583799718033,
                    0.002845269543608324,
                    0.3333333333333333,
                    0.6666666666666666,
                    0.42857142857142855,
                    0.22200920379288755,
                    0,
                    0,
                    1.0
                ],
                [
                    0.29166666666666663,
                    0.0,
                    0.08290045471694302,
                    0.3386160289864902,
                    0.999925829677469,
                    0.08780971258671952,
                    0.9997009441620028,
                    0.9971495097783301,
                    0.3333333333333333,
                    0.2222222222222222,
                    0.2857142857142857,
                    0.7024397889388905,
                    0,
                    0,
                    1.0
                ],
                [
                    0.22420634920634921,
                    0.0,
                    0.07299644246607835,
                    0.40799704648972324,
                    0.0,
                    0.8622398414271556,
                    0.0,
                    5.2206780616666494e-06,
                    0.3333333333333333,
                    0.1111111111111111,
                    0.2857142857142857,
                    0.07555100726822203,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "sends an angry email": 0.19776288647452667,
                "rattles its tail": 0.3571138821268606,
                "eats its feelings": 0.4451232313986127
            },
            "question": "what does a rattlesnake typically do when it feels threatened?",
            "rate_limited": false,
            "answers": [
                "rattles its tail",
                "eats its feelings",
                "sends an angry email"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sends an angry email": 0.1340598123239047,
                "rattles its tail": 0.5727286961733392,
                "eats its feelings": 0.22003344740209582
            },
            "integer_answers": {
                "sends an angry email": 2,
                "rattles its tail": 6,
                "eats its feelings": 4
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.8880368151715502,
                    2.809759155755562,
                    0.3022040290728881
                ],
                "result_count_important_words": [
                    28.0,
                    93600.0,
                    0
                ],
                "wikipedia_search": [
                    1.0,
                    1.0,
                    1.0
                ],
                "answer_relation_to_question": [
                    1.9365079365079367,
                    1.1666666666666665,
                    0.8968253968253969
                ],
                "word_count_appended_bing": [
                    3.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    3.0,
                    0.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.5034751892089844,
                    0.0494469478726387,
                    0.043539583683013916
                ],
                "result_count_noun_chunks": [
                    1090.0,
                    382000.0,
                    2.0
                ],
                "question_answer_similarity": [
                    8.451388319954276,
                    11.294093243777752,
                    13.608206026256084
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    25200.0,
                    44300.0,
                    435000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    27.0,
                    364000.0,
                    0
                ],
                "word_count_appended": [
                    18.0,
                    6.0,
                    3.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these was a name the ancient Greeks gave the planet Venus?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "phosphorus"
            ],
            "lines": [
                [
                    0.16666666666666666,
                    0.0,
                    0.1814252335545699,
                    2.7138827596323836,
                    0.005840565203447974,
                    0.33228346456692914,
                    0.018299656881433472,
                    0.0179405718557279,
                    0.14285714285714288,
                    0.1495176848874598,
                    0.27,
                    0.2106727386126816,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.16666666666666666,
                    0.0,
                    0.10822112267953117,
                    -1.7518389244612156,
                    0.5160299705542221,
                    0.33070866141732286,
                    0.6052230270682425,
                    0.6139039431881891,
                    0.634920634920635,
                    0.5868167202572347,
                    0.46,
                    0.4386959378330034,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.6666666666666666,
                    1.0,
                    0.7103536437658989,
                    0.037956164828831886,
                    0.47812946424233,
                    0.33700787401574805,
                    0.37647731605032403,
                    0.36815548495608297,
                    0.22222222222222224,
                    0.26366559485530544,
                    0.27,
                    0.350631323554315,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "antimony": 0.36294755393983746,
                "flourine": 0.3006704631941745,
                "phosphorus": 0.33638198286598797
            },
            "question": "which of these was a name the ancient greeks gave the planet venus?",
            "rate_limited": false,
            "answers": [
                "flourine",
                "phosphorus",
                "antimony"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "antimony": 0.5455694793689033,
                "flourine": 0.1828073842602098,
                "phosphorus": 0.6580514912745106
            },
            "integer_answers": {
                "antimony": 4,
                "flourine": 0,
                "phosphorus": 10
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.053363693063408,
                    2.193479689165017,
                    1.753156617771575
                ],
                "result_count_important_words": [
                    1920.0,
                    63500.0,
                    39500.0
                ],
                "wikipedia_search": [
                    0.42857142857142855,
                    1.9047619047619047,
                    0.6666666666666666
                ],
                "answer_relation_to_question": [
                    0.5,
                    0.5,
                    2.0
                ],
                "word_count_appended_bing": [
                    27.0,
                    46.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "cosine_similarity_raw": [
                    0.014865988865494728,
                    0.008867641910910606,
                    0.05820639804005623
                ],
                "result_count_noun_chunks": [
                    1920.0,
                    65700.0,
                    39400.0
                ],
                "question_answer_similarity": [
                    -1.692743442952633,
                    1.0926831094548106,
                    -0.023674585390836
                ],
                "word_count_noun_chunks": [
                    0.0,
                    4.0,
                    0.0
                ],
                "result_count_bing": [
                    211000.0,
                    210000.0,
                    214000.0
                ],
                "word_count_raw": [
                    0.0,
                    3.0,
                    0.0
                ],
                "word_count_appended": [
                    93.0,
                    365.0,
                    164.0
                ],
                "result_count": [
                    601.0,
                    53100.0,
                    49200.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these film composers most recently won an Oscar?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "hans zimmer"
            ],
            "lines": [
                [
                    0.2654353148275531,
                    0.29545454545454547,
                    0.31486712641627623,
                    1.1155468231376293,
                    0.33712574850299404,
                    0.014243690973834959,
                    0.2991869918699187,
                    0.041682313180623355,
                    0.633674007152268,
                    0.3394919168591224,
                    0.3838383838383838,
                    0.34514277117682796,
                    0.2,
                    0.25,
                    -1.0
                ],
                [
                    0.25150990007470353,
                    0.6818181818181819,
                    0.4084554500868702,
                    0.07337292286749557,
                    0.09880239520958084,
                    0.01553388037363885,
                    0.5089430894308943,
                    0.06458880961321818,
                    0.23200451722190857,
                    0.36027713625866054,
                    0.30303030303030304,
                    0.3196959462518601,
                    0.7,
                    0.75,
                    -1.0
                ],
                [
                    0.48305478509774336,
                    0.022727272727272728,
                    0.2766774234968536,
                    -0.18891974600512493,
                    0.5640718562874252,
                    0.9702224286525262,
                    0.191869918699187,
                    0.8937288772061585,
                    0.13432147562582347,
                    0.3002309468822171,
                    0.31313131313131315,
                    0.33516128257131195,
                    0.1,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "ennio morricone": 0.34540640238499837,
                "danny elfman": 0.31401984531233623,
                "hans zimmer": 0.3405737523026654
            },
            "question": "which of these film composers most recently won an oscar?",
            "rate_limited": false,
            "answers": [
                "ennio morricone",
                "hans zimmer",
                "danny elfman"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "ennio morricone": 0.0998322557733811,
                "danny elfman": 0.10022805209863705,
                "hans zimmer": 0.622159261266539
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7257138558841398,
                    1.5984797312593004,
                    1.6758064128565597
                ],
                "result_count_important_words": [
                    184000.0,
                    313000.0,
                    118000.0
                ],
                "wikipedia_search": [
                    3.16837003576134,
                    1.1600225861095428,
                    0.6716073781291173
                ],
                "word_count_appended_bing": [
                    38.0,
                    30.0,
                    31.0
                ],
                "answer_relation_to_question_bing": [
                    1.1818181818181819,
                    2.7272727272727275,
                    0.09090909090909091
                ],
                "cosine_similarity_raw": [
                    0.08479215204715729,
                    0.10999502241611481,
                    0.07450785487890244
                ],
                "result_count_noun_chunks": [
                    111000.0,
                    172000.0,
                    2380000.0
                ],
                "question_answer_similarity": [
                    -1.6189286317676306,
                    -0.10648188239429146,
                    0.27416830882430077
                ],
                "word_count_noun_chunks": [
                    2.0,
                    7.0,
                    1.0
                ],
                "word_count_raw": [
                    1.0,
                    3.0,
                    0.0
                ],
                "result_count_bing": [
                    27600.0,
                    30100.0,
                    1880000.0
                ],
                "result_count": [
                    563000.0,
                    165000.0,
                    942000.0
                ],
                "answer_relation_to_question": [
                    1.3271765741377655,
                    1.2575495003735178,
                    2.4152739254887168
                ],
                "word_count_appended": [
                    147.0,
                    156.0,
                    130.0
                ]
            },
            "integer_answers": {
                "ennio morricone": 3,
                "danny elfman": 5,
                "hans zimmer": 6
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these figure skating jumps was invented the most recently?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "salchow"
            ],
            "lines": [
                [
                    0.06541405962458594,
                    0.05555555555555556,
                    0.3071242117278146,
                    0.9895528445480137,
                    0.26724357342835325,
                    0.3878453038674033,
                    0.0306439674315322,
                    0.06243386243386243,
                    0.013447286636385369,
                    0.32857142857142857,
                    0.24,
                    0.329252106535553,
                    0.22857142857142856,
                    0.2857142857142857,
                    -1.0
                ],
                [
                    0.8212771439087229,
                    0.8333333333333334,
                    0.5475391184346996,
                    0.8631500662168717,
                    0.1244591499109188,
                    0.20994475138121546,
                    0.05151739452257587,
                    0.7671957671957672,
                    0.9759144154912742,
                    0.2985714285714286,
                    0.35428571428571426,
                    0.33779559356545236,
                    0.24285714285714285,
                    0.17142857142857143,
                    -1.0
                ],
                [
                    0.1133087964666912,
                    0.11111111111111112,
                    0.14533666983748578,
                    -0.8527029107648855,
                    0.608297276660728,
                    0.4022099447513812,
                    0.9178386380458919,
                    0.17037037037037037,
                    0.010638297872340425,
                    0.37285714285714283,
                    0.4057142857142857,
                    0.3329522998989947,
                    0.5285714285714286,
                    0.5428571428571428,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "salchow": 0.47137639936454917,
                "lutz": 0.25652642247472873,
                "axel": 0.27209717816072204
            },
            "question": "which of these figure skating jumps was invented the most recently?",
            "rate_limited": false,
            "answers": [
                "lutz",
                "salchow",
                "axel"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "salchow": 0.25311278599413306,
                "lutz": 0.03478126853699378,
                "axel": 0.20263627888869937
            },
            "integer_answers": {
                "salchow": 6,
                "lutz": 0,
                "axel": 8
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6462605326777648,
                    1.6889779678272618,
                    1.6647614994949735
                ],
                "result_count_important_words": [
                    41400.0,
                    69600.0,
                    1240000.0
                ],
                "wikipedia_search": [
                    0.053789146545541476,
                    3.903657661965097,
                    0.0425531914893617
                ],
                "answer_relation_to_question": [
                    0.3270702981229297,
                    4.106385719543614,
                    0.566543982333456
                ],
                "word_count_appended_bing": [
                    42.0,
                    62.0,
                    71.0
                ],
                "answer_relation_to_question_bing": [
                    0.16666666666666669,
                    2.5,
                    0.33333333333333337
                ],
                "cosine_similarity_raw": [
                    0.11600205302238464,
                    0.20680773258209229,
                    0.05489424616098404
                ],
                "result_count_noun_chunks": [
                    118000.0,
                    1450000.0,
                    322000.0
                ],
                "question_answer_similarity": [
                    -0.4557744115591049,
                    -0.3975550327450037,
                    0.3927432168275118
                ],
                "word_count_noun_chunks": [
                    16.0,
                    17.0,
                    37.0
                ],
                "result_count_bing": [
                    35100.0,
                    19000.0,
                    36400.0
                ],
                "word_count_raw": [
                    10.0,
                    6.0,
                    19.0
                ],
                "result_count": [
                    105000.0,
                    48900.0,
                    239000.0
                ],
                "word_count_appended": [
                    230.0,
                    209.0,
                    261.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Romaine, Iceberg and Butterhead are all varieties of what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "lettuce"
            ],
            "lines": [
                [
                    1.0,
                    1.0,
                    0.9861240000704937,
                    0.33551107167246474,
                    0.9987031403396075,
                    0.968307055263577,
                    0.998104864181933,
                    0.9982284001090216,
                    0.9895833333333334,
                    0.972707423580786,
                    0.7894736842105263,
                    0.6622008405981982,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.0,
                    0.0,
                    0.006726421257933772,
                    0.251214040558982,
                    0.0011271770880046542,
                    0.003663003663003663,
                    0.0016244021297716813,
                    0.0015331152902698284,
                    0.0,
                    0.014192139737991267,
                    0.11403508771929824,
                    0.25692494756061846,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.0,
                    0.0,
                    0.0071495786715725105,
                    0.41327488776855326,
                    0.0001696825723877974,
                    0.028029941073419336,
                    0.0002707336882952802,
                    0.00023848460070863996,
                    0.010416666666666666,
                    0.013100436681222707,
                    0.09649122807017543,
                    0.0808742118411835,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "lettuce": 0.9070674152399958,
                "race cars": 0.04642970368815608,
                "disney dwarfs": 0.046502881071848114
            },
            "question": "romaine, iceberg and butterhead are all varieties of what?",
            "rate_limited": false,
            "answers": [
                "lettuce",
                "disney dwarfs",
                "race cars"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "lettuce": 0.8577516943870164,
                "race cars": 0.1066707556515485,
                "disney dwarfs": 0.16374123926444278
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.6488033623927922,
                    1.0276997902424736,
                    0.32349684736473394
                ],
                "result_count_important_words": [
                    55300.0,
                    90.0,
                    15.0
                ],
                "wikipedia_search": [
                    3.9583333333333335,
                    0.0,
                    0.041666666666666664
                ],
                "word_count_appended_bing": [
                    90.0,
                    13.0,
                    11.0
                ],
                "answer_relation_to_question_bing": [
                    3.0,
                    0.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.7813736200332642,
                    0.005329804494976997,
                    0.005665101110935211
                ],
                "result_count_noun_chunks": [
                    58600.0,
                    90.0,
                    14.0
                ],
                "question_answer_similarity": [
                    3.1902155205607414,
                    2.3886750657111406,
                    3.9296347349882126
                ],
                "word_count_noun_chunks": [
                    676.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    182.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    304000.0,
                    1150.0,
                    8800.0
                ],
                "word_count_appended": [
                    891.0,
                    13.0,
                    12.0
                ],
                "answer_relation_to_question": [
                    4.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    82400.0,
                    93.0,
                    14.0
                ]
            },
            "integer_answers": {
                "lettuce": 13,
                "race cars": 1,
                "disney dwarfs": 0
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Rejected in the late 1700s, what was the name of the proposed 14th U.S. colony?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "vandalia"
            ],
            "lines": [
                [
                    0.26091277143908725,
                    0.2306149732620321,
                    0.2086859697451369,
                    -0.07189610347694714,
                    0.697444585627559,
                    0.24933545986177566,
                    0.6209016393442623,
                    0.8774551665243382,
                    0.008620689655172414,
                    0.5460526315789473,
                    0.5079365079365079,
                    0.42653604657199623,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.6661207213838793,
                    0.6978609625668449,
                    0.568477990349846,
                    -1.2384314651496962,
                    0.289425384723987,
                    0.3822434875066454,
                    0.2540983606557377,
                    0.08731853116994022,
                    0.24233716475095785,
                    0.40789473684210525,
                    0.4603174603174603,
                    0.2943344596166789,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.07296650717703351,
                    0.07152406417112299,
                    0.2228360399050171,
                    2.310327568626643,
                    0.013130029648454045,
                    0.3684210526315789,
                    0.125,
                    0.03522630230572161,
                    0.7490421455938697,
                    0.046052631578947366,
                    0.031746031746031744,
                    0.27912949381132485,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "roanoke": 0.3259000241478477,
                "new albion": 0.30895727622826746,
                "vandalia": 0.3651426996238848
            },
            "question": "rejected in the late 1700s, what was the name of the proposed 14th u.s. colony?",
            "rate_limited": false,
            "answers": [
                "roanoke",
                "vandalia",
                "new albion"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "roanoke": 0.32536480592355255,
                "new albion": 0.09076852570796085,
                "vandalia": 0.6667451647494494
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.5592162794319773,
                    1.7660067577000735,
                    1.674776962867949
                ],
                "result_count_important_words": [
                    6060.0,
                    2480.0,
                    1220.0
                ],
                "wikipedia_search": [
                    0.017241379310344827,
                    0.4846743295019157,
                    1.4980842911877394
                ],
                "word_count_appended_bing": [
                    32.0,
                    29.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.9224598930481284,
                    2.7914438502673797,
                    0.28609625668449196
                ],
                "cosine_similarity_raw": [
                    0.048826251178979874,
                    0.13300678133964539,
                    0.052136942744255066
                ],
                "result_count_noun_chunks": [
                    41100.0,
                    4090.0,
                    1650.0
                ],
                "question_answer_similarity": [
                    -0.12943960819393396,
                    -2.229635207913816,
                    4.159445099532604
                ],
                "word_count_noun_chunks": [
                    0.0,
                    25.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    13.0,
                    0.0
                ],
                "result_count_bing": [
                    46900.0,
                    71900.0,
                    69300.0
                ],
                "result_count": [
                    4940.0,
                    2050.0,
                    93.0
                ],
                "answer_relation_to_question": [
                    1.0436510857563488,
                    2.664482885535517,
                    0.291866028708134
                ],
                "word_count_appended": [
                    166.0,
                    124.0,
                    14.0
                ]
            },
            "integer_answers": {
                "roanoke": 6,
                "new albion": 2,
                "vandalia": 6
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which writer has stated that his/her trademark series of books would never be adapted for film?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "jeff kinney"
            ],
            "lines": [
                [
                    0.3182736552847357,
                    0.30799391385767794,
                    0.3017970395698326,
                    0.15618529231671863,
                    0.09667174796747968,
                    0.3128054740957967,
                    0.31494140625,
                    0.09267110841913989,
                    0.1946872342902522,
                    0.27238805970149255,
                    0.20370370370370372,
                    0.3157407023851537,
                    0.5,
                    0,
                    -1.0
                ],
                [
                    0.29432387836655927,
                    0.32568594502407705,
                    0.3751335519982128,
                    0.24287744907648756,
                    0.4731961382113821,
                    0.3299120234604106,
                    0.34033203125,
                    0.4613870381586917,
                    0.30531276570974775,
                    0.3694029850746269,
                    0.33333333333333337,
                    0.34348864591074935,
                    0.04999999999999999,
                    0,
                    -1.0
                ],
                [
                    0.387402466348705,
                    0.36632014111824507,
                    0.32306940843195453,
                    0.6009372586067938,
                    0.4301321138211382,
                    0.3572825024437928,
                    0.3447265625,
                    0.4459418534221684,
                    0.5,
                    0.35820895522388063,
                    0.46296296296296297,
                    0.3407706517040969,
                    0.45,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "jeff kinney": 0.17419155744865567,
                "james patterson": 0.47879087110123336,
                "sue grafton": 0.347017571450111
            },
            "question": "which writer has stated that his/her trademark series of books would never be adapted for film?",
            "rate_limited": false,
            "answers": [
                "james patterson",
                "sue grafton",
                "jeff kinney"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "jeff kinney": 0.8509479433682784,
                "james patterson": 0.1389559861347159,
                "sue grafton": 0.7406694485263279
            },
            "integer_answers": {
                "jeff kinney": 0,
                "james patterson": 11,
                "sue grafton": 2
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.579630166607848,
                    2.1911589572495087,
                    2.229210876142643
                ],
                "result_count_important_words": [
                    75800.0,
                    65400.0,
                    63600.0
                ],
                "wikipedia_search": [
                    3.6637531885169734,
                    2.3362468114830266,
                    0.0
                ],
                "word_count_appended_bing": [
                    16.0,
                    9.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.5360486891385767,
                    1.3945124398073836,
                    1.0694388710540397
                ],
                "cosine_similarity_raw": [
                    0.10679435729980469,
                    0.06727968156337738,
                    0.09533252567052841
                ],
                "result_count_noun_chunks": [
                    26900.0,
                    2550.0,
                    3570.0
                ],
                "question_answer_similarity": [
                    1.9795182928210124,
                    1.4803869109600782,
                    -0.5811477676033974
                ],
                "word_count_noun_chunks": [
                    0.0,
                    9.0,
                    1.0
                ],
                "result_count_bing": [
                    38300.0,
                    34800.0,
                    29200.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    61.0,
                    35.0,
                    38.0
                ],
                "answer_relation_to_question": [
                    2.1807161365831718,
                    2.4681134596012884,
                    1.35117040381554
                ],
                "result_count": [
                    6350.0,
                    422.0,
                    1100.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these is usually found on the ocean floor?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "sea cucumber"
            ],
            "lines": [
                [
                    0.05133928571428571,
                    0.05555555555555555,
                    0.04305281270090645,
                    0.37064613271934366,
                    0.7552708833733653,
                    0.42508917954815695,
                    0.7791458253174298,
                    0.7039372762911198,
                    0.28108974358974365,
                    0.35127478753541075,
                    0.6944444444444444,
                    0.31004563255875844,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.05357142857142857,
                    0.3055555555555556,
                    0.04959235392139037,
                    0.2753523892912719,
                    0.06965572457966374,
                    0.3388822829964328,
                    0.06810311658330127,
                    0.06084881540821544,
                    0.03541666666666667,
                    0.14730878186968838,
                    0.05555555555555555,
                    0.28335719220528505,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.8950892857142858,
                    0.6388888888888888,
                    0.9073548333777032,
                    0.3540014779893844,
                    0.17507339204697092,
                    0.23602853745541022,
                    0.15275105809926895,
                    0.23521390830066474,
                    0.6834935897435898,
                    0.5014164305949008,
                    0.25,
                    0.40659717523595645,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "sweet potato": 0.4017409632790434,
                "cherry tomato": 0.14526665526703794,
                "sea cucumber": 0.45299238145391874
            },
            "question": "which of these is usually found on the ocean floor?",
            "rate_limited": false,
            "answers": [
                "sweet potato",
                "cherry tomato",
                "sea cucumber"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sweet potato": 0.1114017696671088,
                "cherry tomato": 0.12598017450453997,
                "sea cucumber": 0.5979859450826331
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2401825302350338,
                    1.1334287688211402,
                    1.6263887009438258
                ],
                "result_count_important_words": [
                    405000.0,
                    35400.0,
                    79400.0
                ],
                "wikipedia_search": [
                    1.1243589743589744,
                    0.14166666666666666,
                    2.7339743589743586
                ],
                "word_count_appended_bing": [
                    50.0,
                    4.0,
                    18.0
                ],
                "answer_relation_to_question_bing": [
                    0.2222222222222222,
                    1.2222222222222223,
                    2.5555555555555554
                ],
                "cosine_similarity_raw": [
                    0.008864780887961388,
                    0.01021130383014679,
                    0.18682871758937836
                ],
                "result_count_noun_chunks": [
                    413000.0,
                    35700.0,
                    138000.0
                ],
                "question_answer_similarity": [
                    5.230855330824852,
                    3.8859936371445656,
                    4.995952621102333
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    1430000.0,
                    1140000.0,
                    794000.0
                ],
                "word_count_appended": [
                    124.0,
                    52.0,
                    177.0
                ],
                "answer_relation_to_question": [
                    0.20535714285714285,
                    0.21428571428571427,
                    3.5803571428571432
                ],
                "result_count": [
                    283000.0,
                    26100.0,
                    65600.0
                ]
            },
            "integer_answers": {
                "sweet potato": 6,
                "cherry tomato": 0,
                "sea cucumber": 6
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "The '90s band The Lightning Seeds took their name from which song?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "raspberry beret"
            ],
            "lines": [
                [
                    0.20424836601307192,
                    0.4202898550724638,
                    0.29905176822715146,
                    0.0985450346115554,
                    0.011519135667613355,
                    0.1963919639196392,
                    0.010167884077002344,
                    0.011617169580888266,
                    0.241156116068292,
                    0.38181818181818183,
                    0.2222222222222222,
                    0.3500363276630772,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.4950980392156863,
                    0.463768115942029,
                    0.44631082175009895,
                    0.3294429145588686,
                    0.988063792178904,
                    0.3956539565395654,
                    0.9894308720670442,
                    0.9879558746140021,
                    0.4220824843673154,
                    0.3090909090909091,
                    0.4444444444444444,
                    0.36409636357081027,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.3006535947712418,
                    0.11594202898550725,
                    0.2546374100227496,
                    0.5720120508295761,
                    0.0004170721534825525,
                    0.40795407954079543,
                    0.0004012438559534557,
                    0.00042695580510956877,
                    0.33676139956439266,
                    0.3090909090909091,
                    0.3333333333333333,
                    0.28586730876611255,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "when doves cry": 0.20839267047994026,
                "raspberry beret": 0.3176474303529399,
                "purple rain": 0.4739598991671198
            },
            "question": "the '90s band the lightning seeds took their name from which song?",
            "rate_limited": false,
            "answers": [
                "raspberry beret",
                "purple rain",
                "when doves cry"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "when doves cry": 0.1583616692212927,
                "raspberry beret": 0.6348021194555852,
                "purple rain": 0.34817434464907643
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.100217965978463,
                    2.1845781814248615,
                    1.7152038525966753
                ],
                "result_count_important_words": [
                    2230.0,
                    217000.0,
                    88.0
                ],
                "wikipedia_search": [
                    0.964624464273168,
                    1.6883299374692615,
                    1.3470455982575706
                ],
                "word_count_appended_bing": [
                    2.0,
                    4.0,
                    3.0
                ],
                "answer_relation_to_question_bing": [
                    1.2608695652173914,
                    1.391304347826087,
                    0.34782608695652173
                ],
                "cosine_similarity_raw": [
                    0.08908778429031372,
                    0.1329563856124878,
                    0.07585670799016953
                ],
                "result_count_noun_chunks": [
                    2340.0,
                    199000.0,
                    86.0
                ],
                "question_answer_similarity": [
                    2.0106428859289736,
                    6.721719212830067,
                    11.670927563216537
                ],
                "word_count_noun_chunks": [
                    3.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    95800.0,
                    193000.0,
                    199000.0
                ],
                "result_count": [
                    2320.0,
                    199000.0,
                    84.0
                ],
                "answer_relation_to_question": [
                    0.6127450980392157,
                    1.4852941176470589,
                    0.9019607843137255
                ],
                "word_count_appended": [
                    21.0,
                    17.0,
                    17.0
                ]
            },
            "integer_answers": {
                "when doves cry": 2,
                "raspberry beret": 3,
                "purple rain": 9
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these Muppets is green?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "kermit the frog"
            ],
            "lines": [
                [
                    0.2648981161091888,
                    0.18055555555555555,
                    0.3154793282152522,
                    0.19679137082721668,
                    0.026736454944122225,
                    0.020228671943711522,
                    0.11887382690302398,
                    0.0688723835246455,
                    0.021739130434782608,
                    0.2989010989010989,
                    0.31868131868131866,
                    0.31658512922747467,
                    0.15,
                    0.14285714285714285,
                    -1.0
                ],
                [
                    0.5757400999615533,
                    0.5555555555555556,
                    0.5649750429997504,
                    0.5033141093610392,
                    0.5785825434997878,
                    0.8870461113205177,
                    0.5495307612095933,
                    0.7224848075624578,
                    0.8478260869565217,
                    0.378021978021978,
                    0.2857142857142857,
                    0.29808539838975884,
                    0.3,
                    0.5714285714285714,
                    -1.0
                ],
                [
                    0.15936178392925798,
                    0.2638888888888889,
                    0.11954562878499733,
                    0.29989451981174414,
                    0.39468100155609,
                    0.09272521673577083,
                    0.3315954118873827,
                    0.2086428089128967,
                    0.13043478260869565,
                    0.3230769230769231,
                    0.3956043956043956,
                    0.3853294723827665,
                    0.55,
                    0.2857142857142857,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "swedish chef": 0.17437139486603817,
                "miss piggy": 0.28146393713529255,
                "kermit the frog": 0.5441646679986694
            },
            "question": "which of these muppets is green?",
            "rate_limited": false,
            "answers": [
                "swedish chef",
                "kermit the frog",
                "miss piggy"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "swedish chef": 0.11069017427532946,
                "miss piggy": 0.31756707766663667,
                "kermit the frog": 0.6853292456574969
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.6331702584549493,
                    0.5961707967795177,
                    0.770658944765533
                ],
                "result_count_important_words": [
                    1140000.0,
                    5270000.0,
                    3180000.0
                ],
                "wikipedia_search": [
                    0.043478260869565216,
                    1.6956521739130435,
                    0.2608695652173913
                ],
                "answer_relation_to_question": [
                    0.5297962322183776,
                    1.1514801999231066,
                    0.31872356785851597
                ],
                "answer_relation_to_question_bing": [
                    0.3611111111111111,
                    1.1111111111111112,
                    0.5277777777777778
                ],
                "word_count_appended": [
                    136.0,
                    172.0,
                    147.0
                ],
                "cosine_similarity_raw": [
                    0.06835941225290298,
                    0.12242121249437332,
                    0.02590365894138813
                ],
                "result_count_noun_chunks": [
                    1020000.0,
                    10700000.0,
                    3090000.0
                ],
                "question_answer_similarity": [
                    1.9475614503026009,
                    4.981088106986135,
                    2.9679299630224705
                ],
                "word_count_noun_chunks": [
                    3.0,
                    6.0,
                    11.0
                ],
                "word_count_raw": [
                    1.0,
                    4.0,
                    2.0
                ],
                "result_count_bing": [
                    80500.0,
                    3530000.0,
                    369000.0
                ],
                "result_count": [
                    189000.0,
                    4090000.0,
                    2790000.0
                ],
                "word_count_appended_bing": [
                    29.0,
                    26.0,
                    36.0
                ]
            },
            "integer_answers": {
                "swedish chef": 0,
                "miss piggy": 3,
                "kermit the frog": 11
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which Las Vegas hotel features a replica of the Rialto Bridge?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "the venetian"
            ],
            "lines": [
                [
                    0.2649798485288287,
                    0.29793847254335987,
                    0.03035578726570824,
                    0.10272598544463916,
                    0.27877237851662406,
                    0.4138392857142857,
                    0.029008633034654882,
                    0.2222108267952071,
                    0.19819549042736595,
                    0.5,
                    0.4084507042253521,
                    0.3108444170056817,
                    0.02702702702702703,
                    0.0,
                    -1.0
                ],
                [
                    0.23656204824594013,
                    0.2620073446071542,
                    0.8463203824244319,
                    0.5243955590488556,
                    0.6061381074168798,
                    0.153125,
                    0.961499120330592,
                    0.7726099516264123,
                    0.03243769470404984,
                    0.40804597701149425,
                    0.4225352112676056,
                    0.34898441208354747,
                    0.972972972972973,
                    0.9444444444444444,
                    -1.0
                ],
                [
                    0.4984581032252311,
                    0.44005418284948594,
                    0.12332383030985986,
                    0.37287845550650517,
                    0.11508951406649616,
                    0.4330357142857143,
                    0.00949224663475308,
                    0.0051792215783805955,
                    0.7693668148685843,
                    0.09195402298850575,
                    0.16901408450704225,
                    0.3401711709107709,
                    0.0,
                    0.05555555555555555,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "caesars palace": 0.24454092266334895,
                "luxor": 0.22031063260919534,
                "the venetian": 0.5351484447274557
            },
            "question": "which las vegas hotel features a replica of the rialto bridge?",
            "rate_limited": false,
            "answers": [
                "luxor",
                "the venetian",
                "caesars palace"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "caesars palace": 0.42233635842718914,
                "luxor": 0.2716867801592893,
                "the venetian": 0.7745553692078216
            },
            "integer_answers": {
                "caesars palace": 4,
                "luxor": 1,
                "the venetian": 9
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.1759109190397714,
                    2.442890884584832,
                    2.381198196375396
                ],
                "result_count_important_words": [
                    7090.0,
                    235000.0,
                    2320.0
                ],
                "wikipedia_search": [
                    1.1891729425641957,
                    0.19462616822429907,
                    4.616200889211505
                ],
                "word_count_appended_bing": [
                    29.0,
                    30.0,
                    12.0
                ],
                "answer_relation_to_question_bing": [
                    0.8938154176300797,
                    0.7860220338214626,
                    1.3201625485484578
                ],
                "cosine_similarity_raw": [
                    0.02265060320496559,
                    0.6314995884895325,
                    0.09202064573764801
                ],
                "result_count_noun_chunks": [
                    130000.0,
                    452000.0,
                    3030.0
                ],
                "question_answer_similarity": [
                    1.2180064041167498,
                    6.217678481712937,
                    4.421163202263415
                ],
                "word_count_noun_chunks": [
                    2.0,
                    72.0,
                    0.0
                ],
                "result_count_bing": [
                    92700.0,
                    34300.0,
                    97000.0
                ],
                "word_count_raw": [
                    0.0,
                    17.0,
                    1.0
                ],
                "result_count": [
                    3270.0,
                    7110.0,
                    1350.0
                ],
                "answer_relation_to_question": [
                    1.0599193941153149,
                    0.9462481929837605,
                    1.9938324129009244
                ],
                "word_count_appended": [
                    174.0,
                    142.0,
                    32.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What TV series derived from a nearly 20-year-old Michael Crichton screenplay?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "er"
            ],
            "lines": [
                [
                    0.2524350217877544,
                    0.3227938580460184,
                    0.1716308156937272,
                    0.7845441344844274,
                    0.06854043392504931,
                    0.20309477756286268,
                    0.06966134409545534,
                    0.08008008008008008,
                    0.022727272727272728,
                    0.016645326504481434,
                    0.006557377049180328,
                    0.32053674433006074,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.22441502912076156,
                    0.11452333247069935,
                    0.1277338981324469,
                    -0.0476483362648449,
                    0.17702169625246547,
                    0.3984526112185687,
                    0.06878399719248991,
                    0.17917917917917917,
                    0.30612627286125815,
                    0.07554417413572344,
                    0.029508196721311476,
                    0.26042323818957336,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.523149949091484,
                    0.5626828094832822,
                    0.7006352861738259,
                    0.2631042017804175,
                    0.7544378698224852,
                    0.3984526112185687,
                    0.8615546587120547,
                    0.7407407407407407,
                    0.6711464544114691,
                    0.9078104993597952,
                    0.9639344262295082,
                    0.41904001748036585,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "numb3rs": 0.13671880637211667,
                "the expanse": 0.16566051330616932,
                "er": 0.6976206803217141
            },
            "question": "what tv series derived from a nearly 20-year-old michael crichton screenplay?",
            "rate_limited": false,
            "answers": [
                "the expanse",
                "numb3rs",
                "er"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "numb3rs": 0.1560818417605178,
                "the expanse": 0.18959469099313875,
                "er": 0.7906221929744528
            },
            "integer_answers": {
                "numb3rs": 1,
                "the expanse": 1,
                "er": 12
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.884830698970547,
                    2.34380914370616,
                    3.7713601573232927
                ],
                "result_count_important_words": [
                    3970.0,
                    3920.0,
                    49100.0
                ],
                "wikipedia_search": [
                    0.09090909090909091,
                    1.2245050914450326,
                    2.6845858176458766
                ],
                "word_count_appended_bing": [
                    6.0,
                    27.0,
                    882.0
                ],
                "answer_relation_to_question_bing": [
                    1.9367631482761105,
                    0.6871399948241961,
                    3.3760968568996934
                ],
                "cosine_similarity_raw": [
                    0.04533408582210541,
                    0.033739276230335236,
                    0.18506385385990143
                ],
                "result_count_noun_chunks": [
                    16000.0,
                    35800.0,
                    148000.0
                ],
                "question_answer_similarity": [
                    5.145800360478461,
                    -0.3125239424407482,
                    1.72569220373407
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    1055.0
                ],
                "result_count_bing": [
                    10500.0,
                    20600.0,
                    20600.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    398.0
                ],
                "result_count": [
                    13900.0,
                    35900.0,
                    153000.0
                ],
                "answer_relation_to_question": [
                    1.262175108938772,
                    1.1220751456038078,
                    2.6157497454574203
                ],
                "word_count_appended": [
                    26.0,
                    118.0,
                    1418.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Marsha Bell was the model for what iconic character?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "carmen sandiego"
            ],
            "lines": [
                [
                    0.38751585623678647,
                    0.11111111111111112,
                    0.2702503698143433,
                    0.49175856099886156,
                    0.000995787054768288,
                    0.1974000962927299,
                    0.0010647977852206068,
                    0.010445205479452055,
                    0.7828525641025642,
                    0.34057971014492755,
                    0.5319148936170213,
                    0.2787969412162335,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.08096899224806202,
                    0.11111111111111112,
                    0.24618569588883193,
                    0.45629530303514226,
                    0.8655687476062811,
                    0.5825710158883004,
                    0.9970379261611135,
                    0.9726027397260274,
                    0.0,
                    0.30434782608695654,
                    0.06382978723404255,
                    0.36473298827113426,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.5315151515151515,
                    0.7777777777777778,
                    0.48356393429682476,
                    0.05194613596599621,
                    0.1334354653389506,
                    0.22002888781896968,
                    0.0018972760536658084,
                    0.016952054794520548,
                    0.2171474358974359,
                    0.35507246376811596,
                    0.40425531914893614,
                    0.35647007051263224,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "little debbie": 0.36037515237550016,
                "rosie the riveter": 0.24319184956100143,
                "carmen sandiego": 0.39643299806349835
            },
            "question": "marsha bell was the model for what iconic character?",
            "rate_limited": false,
            "answers": [
                "rosie the riveter",
                "little debbie",
                "carmen sandiego"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "little debbie": 0.17863036840751265,
                "rosie the riveter": 0.27324276508577616,
                "carmen sandiego": 0.6554243524524308
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3939847060811674,
                    1.8236649413556714,
                    1.7823503525631612
                ],
                "result_count_important_words": [
                    55.0,
                    51500.0,
                    98.0
                ],
                "wikipedia_search": [
                    3.131410256410257,
                    0.0,
                    0.8685897435897436
                ],
                "word_count_appended_bing": [
                    25.0,
                    3.0,
                    19.0
                ],
                "answer_relation_to_question_bing": [
                    0.3333333333333333,
                    0.3333333333333333,
                    2.333333333333333
                ],
                "cosine_similarity_raw": [
                    0.09575701504945755,
                    0.08723025023937225,
                    0.17133978009223938
                ],
                "result_count_noun_chunks": [
                    61.0,
                    5680.0,
                    99.0
                ],
                "question_answer_similarity": [
                    4.68056751228869,
                    4.343027535825968,
                    0.49442432867363095
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    2.0
                ],
                "result_count_bing": [
                    410000.0,
                    1210000.0,
                    457000.0
                ],
                "word_count_appended": [
                    47.0,
                    42.0,
                    49.0
                ],
                "answer_relation_to_question": [
                    1.9375792811839323,
                    0.4048449612403101,
                    2.6575757575757573
                ],
                "result_count": [
                    65.0,
                    56500.0,
                    8710.0
                ]
            },
            "integer_answers": {
                "little debbie": 5,
                "rosie the riveter": 3,
                "carmen sandiego": 6
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these video games was NOT produced by FromSoftware?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "beyond: two souls"
            ],
            "lines": [
                [
                    0.3038575596690104,
                    0.3200320512820513,
                    0.2297547951764642,
                    0.39252312988145766,
                    0.14295485636114913,
                    0.13852234989860995,
                    0.05246366536689118,
                    0.16152019002375295,
                    0.3576882323610072,
                    0.23831775700934582,
                    0.17543859649122806,
                    0.3198293397021855,
                    0.04375000000000001,
                    0.08333333333333331,
                    -1.0
                ],
                [
                    0.351972697269727,
                    0.4256410256410256,
                    0.4610706491008428,
                    0.25859885981407776,
                    0.44856361149110807,
                    0.36260910453463424,
                    0.4936192839418646,
                    0.4251781472684085,
                    0.4148591373722264,
                    0.44042056074766356,
                    0.38596491228070173,
                    0.3386460703497186,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.3441697430612627,
                    0.25432692307692306,
                    0.30917455572269303,
                    0.3488780103044646,
                    0.4084815321477428,
                    0.4988685455667558,
                    0.4539170506912442,
                    0.41330166270783847,
                    0.22745263026676638,
                    0.3212616822429907,
                    0.4385964912280702,
                    0.34152458994809587,
                    0.45625,
                    0.4166666666666667,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "dark souls": 0.5771448776347876,
                "beyond: two souls": 0.17040799145542876,
                "demon's souls": 0.25244713090978366
            },
            "question": "which of these video games was not produced by fromsoftware?",
            "rate_limited": false,
            "answers": [
                "dark souls",
                "beyond: two souls",
                "demon's souls"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "dark souls": 0.1490037922980894,
                "beyond: two souls": 0.6189866885092548,
                "demon's souls": 0.4276415149469218
            },
            "integer_answers": {
                "dark souls": 11,
                "beyond: two souls": 1,
                "demon's souls": 2
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.441365282382516,
                    1.290831437202251,
                    1.267803280415233
                ],
                "result_count_important_words": [
                    1010000.0,
                    14400.0,
                    104000.0
                ],
                "wikipedia_search": [
                    0.8538706058339567,
                    0.5108451757666417,
                    1.6352842183994016
                ],
                "word_count_appended_bing": [
                    37.0,
                    13.0,
                    7.0
                ],
                "answer_relation_to_question_bing": [
                    1.4397435897435895,
                    0.5948717948717949,
                    1.9653846153846153
                ],
                "cosine_similarity_raw": [
                    0.25428271293640137,
                    0.036629922688007355,
                    0.1795540153980255
                ],
                "result_count_noun_chunks": [
                    171000.0,
                    37800.0,
                    43800.0
                ],
                "question_answer_similarity": [
                    4.964029252529144,
                    11.149583349004388,
                    6.979864381253719
                ],
                "word_count_noun_chunks": [
                    73.0,
                    0.0,
                    7.0
                ],
                "result_count_bing": [
                    49200000.0,
                    18700000.0,
                    154000.0
                ],
                "word_count_raw": [
                    10.0,
                    0.0,
                    2.0
                ],
                "word_count_appended": [
                    224.0,
                    51.0,
                    153.0
                ],
                "answer_relation_to_question": [
                    1.1768546419859378,
                    0.8881638163816381,
                    0.9349815416324241
                ],
                "result_count": [
                    261000.0,
                    37600.0,
                    66900.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "The voice of which \u201cSimpsons\u201d character is also in the band Spinal Tap?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "kent brockman"
            ],
            "question": "the voice of which \u201csimpsons\u201d character is also in the band spinal tap?",
            "lines": [
                [
                    0.3098664423174588,
                    0.3025462962962963,
                    0.18796408495662426,
                    1.21429813061392,
                    0.33555555555555555,
                    0.3415892672858617,
                    0.010752688172043012,
                    0.005244604935087267,
                    0.6690615835777126,
                    0.28431372549019607,
                    0.2222222222222222,
                    0.33061627188846254,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.32324591851337703,
                    0.28773148148148153,
                    0.2035508085372007,
                    -0.03852630612491001,
                    0.24166666666666667,
                    0.30959752321981426,
                    0.011730205278592375,
                    0.006018399105837847,
                    0.2423264907135875,
                    0.22549019607843138,
                    0.3333333333333333,
                    0.29290291381860745,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.3668876391691641,
                    0.40972222222222227,
                    0.608485106506175,
                    -0.17577182448901008,
                    0.42277777777777775,
                    0.34881320949432404,
                    0.9775171065493646,
                    0.9887369959590749,
                    0.0886119257086999,
                    0.49019607843137253,
                    0.4444444444444444,
                    0.37648081429293007,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "comic book guy",
                "groundskeeper willie",
                "kent brockman"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "kent brockman": 0.601392443102576,
                "groundskeeper willie": 0.10918201925164114,
                "comic book guy": 0.31246912302715313
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9836976313307753,
                    1.7574174829116447,
                    2.2588848857575803
                ],
                "result_count_important_words": [
                    88.0,
                    96.0,
                    8000.0
                ],
                "wikipedia_search": [
                    3.345307917888563,
                    1.2116324535679375,
                    0.4430596285434995
                ],
                "answer_relation_to_question": [
                    1.549332211587294,
                    1.616229592566885,
                    1.8344381958458207
                ],
                "answer_relation_to_question_bing": [
                    1.210185185185185,
                    1.150925925925926,
                    1.6388888888888888
                ],
                "word_count_appended": [
                    29.0,
                    23.0,
                    50.0
                ],
                "cosine_similarity_raw": [
                    0.019615087658166885,
                    0.021241648122668266,
                    0.06349877268075943
                ],
                "result_count_noun_chunks": [
                    61.0,
                    70.0,
                    11500.0
                ],
                "question_answer_similarity": [
                    12.730281638447195,
                    -0.40389646915718913,
                    -1.8427310175611638
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    33100.0,
                    30000.0,
                    33800.0
                ],
                "result_count": [
                    6040.0,
                    4350.0,
                    7610.0
                ],
                "word_count_appended_bing": [
                    2.0,
                    3.0,
                    4.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Parmesan cheese is named for an Italian province called what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "parma"
            ],
            "question": "parmesan cheese is named for an italian province called what?",
            "lines": [
                [
                    0.14856902356902357,
                    0.1,
                    0.07014943478116492,
                    -0.16691407086727675,
                    0.09756081697428134,
                    0.24597701149425288,
                    0.6587763651310233,
                    0.20837376533436838,
                    0.0,
                    0.3994211287988423,
                    0.29473684210526313,
                    0.4647720572935388,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.07575757575757576,
                    0.0,
                    0.05808792921528038,
                    0.0,
                    1.6260136162380224e-06,
                    0.18850574712643678,
                    1.1261134446684159e-05,
                    1.0214400261488647e-05,
                    0.05555555555555555,
                    0.02894356005788712,
                    0.29473684210526313,
                    0.03876440481755826,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.7756734006734006,
                    0.9,
                    0.8717626360035547,
                    1.1669140708672767,
                    0.9024375570121024,
                    0.5655172413793104,
                    0.34121237373453,
                    0.7916160202653701,
                    0.9444444444444445,
                    0.5716353111432706,
                    0.4105263157894737,
                    0.49646353788890285,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "romana",
                "yomamma",
                "parma"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "romana": 0.1735204498871643,
                "yomamma": 0.12382071998690954,
                "parma": 0.8018631657688969
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.788632343761233,
                    0.23258642890534956,
                    2.978781227333417
                ],
                "result_count_important_words": [
                    117000.0,
                    2.0,
                    60600.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.3333333333333333,
                    5.666666666666667
                ],
                "word_count_appended_bing": [
                    28.0,
                    28.0,
                    39.0
                ],
                "answer_relation_to_question_bing": [
                    0.5,
                    0.0,
                    4.5
                ],
                "cosine_similarity_raw": [
                    0.02295972779393196,
                    0.019012028351426125,
                    0.28532564640045166
                ],
                "result_count_noun_chunks": [
                    40800.0,
                    2.0,
                    155000.0
                ],
                "question_answer_similarity": [
                    -0.10602040775120258,
                    0.0,
                    0.7411999776959419
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    87.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    14.0
                ],
                "result_count_bing": [
                    214000.0,
                    164000.0,
                    492000.0
                ],
                "word_count_appended": [
                    276.0,
                    20.0,
                    395.0
                ],
                "answer_relation_to_question": [
                    0.8914141414141414,
                    0.45454545454545453,
                    4.654040404040404
                ],
                "result_count": [
                    120000.0,
                    2.0,
                    1110000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these is NOT the title of a current TV show?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "chicago police"
            ],
            "lines": [
                [
                    0.2633317737575379,
                    0.3437700352176415,
                    0.3122553638915616,
                    0.37008623613989455,
                    0.4163905456151977,
                    0.3606632888070014,
                    0.4407812220032252,
                    0.27601156069364163,
                    0.36744209692349494,
                    0.19534883720930235,
                    0.14473684210526316,
                    0.3313825088869794,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.3050634227490123,
                    0.31583301902983774,
                    0.26008438694996977,
                    0.3478136998114085,
                    0.23381930638391873,
                    0.2759097190234915,
                    0.22227199426626054,
                    0.252528901734104,
                    0.30074818079327664,
                    0.49534883720930234,
                    0.4605263157894737,
                    0.3283556146155717,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.43160480349344976,
                    0.3403969457525208,
                    0.4276602491584686,
                    0.2821000640486969,
                    0.3497901480008836,
                    0.36342699216950713,
                    0.33694678373051423,
                    0.47145953757225434,
                    0.3318097222832285,
                    0.30930232558139537,
                    0.39473684210526316,
                    0.3402618764974489,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "chicago med": 0.36296661479154313,
                "chicago police": 0.2700839516010614,
                "chicage fire": 0.36694943360739546
            },
            "question": "which of these is not the title of a current tv show?",
            "rate_limited": false,
            "answers": [
                "chicago med",
                "chicage fire",
                "chicago police"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "chicago med": 0.10321615604592634,
                "chicago police": 0.3124954578553677,
                "chicage fire": 0.2587560772791508
            },
            "integer_answers": {
                "chicago med": 3,
                "chicago police": 1,
                "chicage fire": 8
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.0117049466781236,
                    1.0298663123065699,
                    0.9584287410153065
                ],
                "result_count_important_words": [
                    66100.0,
                    310000.0,
                    182000.0
                ],
                "wikipedia_search": [
                    0.7953474184590307,
                    1.1955109152403403,
                    1.0091416663006292
                ],
                "answer_relation_to_question": [
                    1.4200093574547723,
                    1.1696194635059263,
                    0.41037117903930137
                ],
                "word_count_appended_bing": [
                    27.0,
                    3.0,
                    8.0
                ],
                "answer_relation_to_question_bing": [
                    0.9373797886941511,
                    1.1050018858209738,
                    0.9576183254848751
                ],
                "cosine_similarity_raw": [
                    0.0888412669301033,
                    0.11352871358394623,
                    0.034231364727020264
                ],
                "result_count_noun_chunks": [
                    1240000.0,
                    1370000.0,
                    158000.0
                ],
                "question_answer_similarity": [
                    3.1205909717828035,
                    3.655587986111641,
                    5.234061062335968
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    605000.0,
                    973000.0,
                    593000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    131.0,
                    2.0,
                    82.0
                ],
                "result_count": [
                    75700.0,
                    241000.0,
                    136000.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these things is NOT found inside an atom?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "wonton"
            ],
            "lines": [
                [
                    0.3517464833254307,
                    0.23653846153846153,
                    0.3547603394099659,
                    0.22103760256197258,
                    0.4339752407152682,
                    0.3331189710610932,
                    0.2675276752767528,
                    0.2536295644522657,
                    0.32073643410852715,
                    0.313126709206928,
                    0.37371134020618557,
                    0.3021292706021742,
                    0.21774193548387094,
                    0.17857142857142855,
                    -1.0
                ],
                [
                    0.3070175438596491,
                    0.4,
                    0.4664008564073633,
                    0.4966159294720301,
                    0.1389270976616231,
                    0.3334405144694534,
                    0.48093480934809346,
                    0.4564452265728113,
                    0.29761904761904767,
                    0.37146763901549684,
                    0.4020618556701031,
                    0.39381296909621694,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.3412359728149202,
                    0.36346153846153845,
                    0.17883880418267079,
                    0.28234646796599727,
                    0.4270976616231087,
                    0.3334405144694534,
                    0.2515375153751538,
                    0.289925208974923,
                    0.38164451827242524,
                    0.3154056517775752,
                    0.22422680412371132,
                    0.3040577603016089,
                    0.282258064516129,
                    0.3214285714285714,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "wonton": 0.20789378725830168,
                "neutron": 0.3861564208160305,
                "proton": 0.4059497919256678
            },
            "question": "which of these things is not found inside an atom?",
            "rate_limited": false,
            "answers": [
                "proton",
                "wonton",
                "neutron"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "wonton": 0.4535024340847416,
                "neutron": 0.217686292498845,
                "proton": 0.17147661632941785
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1872243763869545,
                    0.6371221854226985,
                    1.1756534381903467
                ],
                "result_count_important_words": [
                    1890000.0,
                    155000.0,
                    2020000.0
                ],
                "wikipedia_search": [
                    1.0755813953488373,
                    1.2142857142857142,
                    0.7101328903654485
                ],
                "word_count_appended_bing": [
                    49.0,
                    38.0,
                    107.0
                ],
                "answer_relation_to_question_bing": [
                    1.0538461538461539,
                    0.4,
                    0.5461538461538462
                ],
                "cosine_similarity_raw": [
                    0.09750467538833618,
                    0.022556329146027565,
                    0.21560721099376678
                ],
                "result_count_noun_chunks": [
                    224000.0,
                    39600.0,
                    191000.0
                ],
                "question_answer_similarity": [
                    1.5770087577402592,
                    0.019130567088723183,
                    1.2304221978411078
                ],
                "word_count_noun_chunks": [
                    35.0,
                    0.0,
                    27.0
                ],
                "word_count_raw": [
                    18.0,
                    0.0,
                    10.0
                ],
                "result_count_bing": [
                    51900000.0,
                    51800000.0,
                    51800000.0
                ],
                "result_count": [
                    192000.0,
                    1050000.0,
                    212000.0
                ],
                "answer_relation_to_question": [
                    0.8895211000474158,
                    1.1578947368421053,
                    0.9525841631104789
                ],
                "word_count_appended": [
                    410.0,
                    282.0,
                    405.0
                ]
            },
            "integer_answers": {
                "wonton": 3,
                "neutron": 3,
                "proton": 8
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these athletes has a notably obscured glabella?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "michael phelps"
            ],
            "lines": [
                [
                    0.8666666666666667,
                    0.0,
                    0.3120464181704756,
                    0.5023348439272827,
                    0.65625,
                    0.7278358497870693,
                    0.6666666666666666,
                    0.6521739130434783,
                    0.45,
                    0.4375,
                    0.5,
                    0.41038083538083536,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.0,
                    0.5,
                    0.22852591042526785,
                    0.024076729717822265,
                    0.25,
                    0.2497096399535424,
                    0.25,
                    0.2608695652173913,
                    0.0,
                    0.3125,
                    0.42857142857142855,
                    0.3073064323064323,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.13333333333333333,
                    0.5,
                    0.4594276714042565,
                    0.4735884263548951,
                    0.09375,
                    0.02245451025938831,
                    0.08333333333333333,
                    0.08695652173913043,
                    0.55,
                    0.25,
                    0.07142857142857142,
                    0.28231273231273235,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "michael strahan": 0.23429664218265708,
                "michael phelps": 0.5151545994702063,
                "anthony davis": 0.2505487583471367
            },
            "question": "which of these athletes has a notably obscured glabella?",
            "rate_limited": false,
            "answers": [
                "michael phelps",
                "michael strahan",
                "anthony davis"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "michael strahan": 0.1230980074561317,
                "michael phelps": 0.5326835036079326,
                "anthony davis": 0.136225867234876
            },
            "integer_answers": {
                "michael strahan": 1,
                "michael phelps": 9,
                "anthony davis": 2
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6415233415233415,
                    1.2292257292257291,
                    1.1292509292509294
                ],
                "result_count_important_words": [
                    16.0,
                    6.0,
                    2.0
                ],
                "wikipedia_search": [
                    0.9,
                    0.0,
                    1.1
                ],
                "word_count_appended_bing": [
                    21.0,
                    18.0,
                    3.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    1.0,
                    1.0
                ],
                "cosine_similarity_raw": [
                    0.032896459102630615,
                    0.024091586470603943,
                    0.048433639109134674
                ],
                "result_count_noun_chunks": [
                    15.0,
                    6.0,
                    2.0
                ],
                "question_answer_similarity": [
                    0.7542836407665163,
                    0.036152545595541596,
                    0.7111192997545004
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    376000.0,
                    129000.0,
                    11600.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    7.0,
                    5.0,
                    4.0
                ],
                "answer_relation_to_question": [
                    0.8666666666666667,
                    0.0,
                    0.13333333333333333
                ],
                "result_count": [
                    21.0,
                    8.0,
                    3.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these can be counted in order to determine temperature?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "cricket chirps"
            ],
            "lines": [
                [
                    0.08223684210526316,
                    0.14575645756457564,
                    0.018364230661508534,
                    0.3812330599813103,
                    0.0,
                    0.33122119815668205,
                    0.0,
                    0.0,
                    0.5844155844155844,
                    0.047058823529411764,
                    0.2222222222222222,
                    0.11955960579465527,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.12512693371359432,
                    0.15867158671586715,
                    0.07294710411280471,
                    0.5356193886262302,
                    0.0,
                    0.33410138248847926,
                    0.0,
                    0.0,
                    0.0,
                    0.047058823529411764,
                    0.2222222222222222,
                    0.1124792590375718,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.7926362241811425,
                    0.6955719557195572,
                    0.9086886652256868,
                    0.08314755139245948,
                    1.0,
                    0.3346774193548387,
                    1.0,
                    1.0,
                    0.4155844155844156,
                    0.9058823529411765,
                    0.5555555555555556,
                    0.7679611351677729,
                    0,
                    1.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "rings on spiderweb": 0.1486206172639395,
                "cricket chirps": 0.7276696365478927,
                "ladybug flight time": 0.12370974618816778
            },
            "question": "which of these can be counted in order to determine temperature?",
            "rate_limited": false,
            "answers": [
                "rings on spiderweb",
                "ladybug flight time",
                "cricket chirps"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "rings on spiderweb": 0.1928544589888621,
                "cricket chirps": 0.7610576120966963,
                "ladybug flight time": 0.18009609183043274
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.4782384231786211,
                    0.4499170361502872,
                    3.0718445406710915
                ],
                "result_count_important_words": [
                    0,
                    0,
                    76400.0
                ],
                "wikipedia_search": [
                    0.5844155844155844,
                    0.0,
                    0.4155844155844156
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    5.0
                ],
                "answer_relation_to_question_bing": [
                    0.2915129151291513,
                    0.3173431734317343,
                    1.3911439114391144
                ],
                "cosine_similarity_raw": [
                    0.009538698010146618,
                    0.037889983505010605,
                    0.4719885587692261
                ],
                "result_count_noun_chunks": [
                    0,
                    0,
                    138000.0
                ],
                "question_answer_similarity": [
                    6.427035982720554,
                    9.029765372164547,
                    1.4017470171675086
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    4.0
                ],
                "result_count_bing": [
                    5750000.0,
                    5800000.0,
                    5810000.0
                ],
                "word_count_appended": [
                    4.0,
                    4.0,
                    77.0
                ],
                "answer_relation_to_question": [
                    0.32894736842105265,
                    0.5005077348543773,
                    3.17054489672457
                ],
                "result_count": [
                    0,
                    0,
                    71900.0
                ]
            },
            "integer_answers": {
                "rings on spiderweb": 1,
                "cricket chirps": 11,
                "ladybug flight time": 1
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What actor famously yelled \"Not the bees! Not the bees!\" in a 2006 film?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "macauley culkin"
            ],
            "lines": [
                [
                    0.18523206751054855,
                    0.2540322580645161,
                    0.0284090683113184,
                    1.4907865878808582,
                    0.08823529411764708,
                    0.33859223300970875,
                    0.31202566309365465,
                    0.2906045156591406,
                    0.3038935430478885,
                    0.25,
                    0.3731343283582089,
                    0.23952909847911785,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.4222837552742616,
                    0.3089717741935484,
                    0.48478512925171247,
                    -1.1773369883279967,
                    0.5,
                    0.22572815533980584,
                    0.49898286519051716,
                    0.4861616897305171,
                    0.3826627183370782,
                    0.4423076923076923,
                    0.4626865671641791,
                    0.40966464333376384,
                    0.5,
                    0.5,
                    1.0
                ],
                [
                    0.39248417721518986,
                    0.4369959677419355,
                    0.4868058024369692,
                    0.6865504004471386,
                    0.4117647058823529,
                    0.4356796116504854,
                    0.1889914717158282,
                    0.22323379461034232,
                    0.3134437386150333,
                    0.3076923076923077,
                    0.16417910447761191,
                    0.3508062581871183,
                    0.5,
                    0.5,
                    1.0
                ]
            ],
            "fraction_answers": {
                "macauley culkin": 0.3647288568864172,
                "nicolas cage": 0.4065036203524847,
                "oprah winfrey": 0.22876752276109813
            },
            "question": "what actor famously yelled \"not the bees! not the bees!\" in a 2006 film?",
            "rate_limited": false,
            "answers": [
                "nicolas cage",
                "macauley culkin",
                "oprah winfrey"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "macauley culkin": 0.5241321790420751,
                "nicolas cage": 0.1016053595147207,
                "oprah winfrey": 0.460608549387696
            },
            "integer_answers": {
                "macauley culkin": 1,
                "nicolas cage": 10,
                "oprah winfrey": 3
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.64659262129235,
                    1.2646949933273064,
                    2.0887123853803438
                ],
                "result_count_important_words": [
                    9610.0,
                    52.0,
                    15900.0
                ],
                "wikipedia_search": [
                    1.176638741712669,
                    0.7040236899775306,
                    1.1193375683098006
                ],
                "word_count_appended_bing": [
                    17.0,
                    5.0,
                    45.0
                ],
                "answer_relation_to_question_bing": [
                    0.9838709677419355,
                    0.7641129032258065,
                    0.25201612903225806
                ],
                "cosine_similarity_raw": [
                    0.5552423000335693,
                    0.017913702875375748,
                    0.015534600242972374
                ],
                "result_count_noun_chunks": [
                    230000.0,
                    15200.0,
                    304000.0
                ],
                "question_answer_similarity": [
                    2.9484580010175705,
                    -4.991546841803938,
                    0.5551508544012904
                ],
                "word_count_noun_chunks": [
                    10.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    266000.0,
                    452000.0,
                    106000.0
                ],
                "word_count_raw": [
                    4.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    14.0,
                    0,
                    3.0
                ],
                "answer_relation_to_question": [
                    3.1476793248945145,
                    0.7771624472573839,
                    1.0751582278481013
                ],
                "word_count_appended": [
                    13.0,
                    3.0,
                    10.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Who defeated Napoleon at the Battle of Waterloo?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "the duke of wellington"
            ],
            "lines": [
                [
                    0.0,
                    0.0,
                    0.024504883285301784,
                    0.05217046836658575,
                    0.007798713496897592,
                    0.33696729435084244,
                    0.0003361344537815126,
                    0.00018295285914662654,
                    0.0375,
                    0.14096916299559473,
                    0.16666666666666666,
                    0.14229139358668327,
                    0.0,
                    0.0,
                    0.0
                ],
                [
                    0.967548076923077,
                    1.0,
                    0.9295390261704148,
                    0.7759516233692595,
                    0.9791085558148802,
                    0.3419226957383548,
                    0.983957219251337,
                    0.9838798202996362,
                    0.9625,
                    0.42731277533039647,
                    0.6666666666666666,
                    0.5494388978647906,
                    1.0,
                    1.0,
                    0.0
                ],
                [
                    0.03245192307692308,
                    0.0,
                    0.045956090544283366,
                    0.17187790826415475,
                    0.013092730688222235,
                    0.3211100099108028,
                    0.01570664629488159,
                    0.015937226841217247,
                    0.0,
                    0.43171806167400884,
                    0.16666666666666666,
                    0.3082697085485262,
                    0.0,
                    0.0,
                    0.0
                ]
            ],
            "fraction_answers": {
                "jack skellington": 0.06495626214725002,
                "beef wellington": 0.10877049803640618,
                "the duke of wellington": 0.8262732398163438
            },
            "question": "who defeated napoleon at the battle of waterloo?",
            "rate_limited": false,
            "answers": [
                "jack skellington",
                "the duke of wellington",
                "beef wellington"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "jack skellington": 0.10170845874711071,
                "beef wellington": 0.3137481915293897,
                "the duke of wellington": 0.776432132067992
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.5691655743467331,
                    2.1977555914591624,
                    1.2330788341941048
                ],
                "result_count_important_words": [
                    55.0,
                    161000.0,
                    2570.0
                ],
                "wikipedia_search": [
                    0.15,
                    3.85,
                    0.0
                ],
                "word_count_appended_bing": [
                    2.0,
                    8.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    4.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.008136066608130932,
                    0.3086238503456116,
                    0.015258257277309895
                ],
                "result_count_noun_chunks": [
                    45.0,
                    242000.0,
                    3920.0
                ],
                "question_answer_similarity": [
                    0.6204546950757504,
                    9.228263478260487,
                    2.0441153491847217
                ],
                "word_count_noun_chunks": [
                    0.0,
                    13.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    5.0,
                    0.0
                ],
                "result_count_bing": [
                    340000.0,
                    345000.0,
                    324000.0
                ],
                "result_count": [
                    1370.0,
                    172000.0,
                    2300.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    3.870192307692308,
                    0.12980769230769232
                ],
                "word_count_appended": [
                    32.0,
                    97.0,
                    98.0
                ]
            },
            "integer_answers": {
                "jack skellington": 0,
                "beef wellington": 1,
                "the duke of wellington": 13
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Actually taking place in Bethel, NY, which of these things is misnamed?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "the woodstock festival"
            ],
            "question": "actually taking place in bethel, ny, which of these things is misnamed?",
            "lines": [
                [
                    0.5173198146882357,
                    0.3157894736842105,
                    0.6094573567203921,
                    0.2559929752011973,
                    0.34375,
                    0.09925064994647499,
                    0.34146341463414637,
                    0.25256322624743677,
                    0.49596771203345963,
                    0.30434782608695654,
                    0.3333333333333333,
                    0.26527270502449757,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.16974823422191843,
                    0.5371710526315789,
                    0.26700248417600725,
                    0.29474721527131714,
                    0.0,
                    0.5780700412907173,
                    0.6585365853658537,
                    0.5194805194805194,
                    0.23003940209666615,
                    0.13043478260869565,
                    0.5,
                    0.22509618485621496,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.3129319510898458,
                    0.14703947368421053,
                    0.12354015910360058,
                    0.4492598095274855,
                    0.65625,
                    0.32267930876280776,
                    0.0,
                    0.22795625427204375,
                    0.2739928858698742,
                    0.5652173913043478,
                    0.16666666666666666,
                    0.5096311101192874,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "the woodstock festival",
                "the manhattan project",
                "the battle of brooklyn"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "the woodstock festival": 0.3314132448251912,
                "the battle of brooklyn": 0.13563340087707879,
                "the manhattan project": 0.13191081138608415
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.8569089351714831,
                    1.5756732939935048,
                    3.567417770835012
                ],
                "result_count_important_words": [
                    14.0,
                    27.0,
                    0
                ],
                "wikipedia_search": [
                    2.975806272200758,
                    1.3802364125799969,
                    1.6439573152192453
                ],
                "word_count_appended_bing": [
                    4.0,
                    6.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.5789473684210527,
                    2.6858552631578947,
                    0.7351973684210527
                ],
                "cosine_similarity_raw": [
                    0.10905491560697556,
                    0.04777681827545166,
                    0.022105995565652847
                ],
                "result_count_noun_chunks": [
                    73900.0,
                    152000.0,
                    66700.0
                ],
                "question_answer_similarity": [
                    9.58661448827479,
                    11.037911966443062,
                    16.824214006774127
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    649000.0,
                    3780000.0,
                    2110000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    7.0,
                    3.0,
                    13.0
                ],
                "answer_relation_to_question": [
                    3.1039188881294146,
                    1.0184894053315106,
                    1.8775917065390748
                ],
                "result_count": [
                    11.0,
                    0,
                    21.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Mardi Gras is celebrated right before what other observance?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "lent"
            ],
            "lines": [
                [
                    0.18793715419146048,
                    0.3141289437585734,
                    0.8675481987330943,
                    0.6132297093779424,
                    0.4172813487881981,
                    0.4517241379310345,
                    0.1971557853910795,
                    0.6467661691542289,
                    0.4016225749559083,
                    0.4667458432304038,
                    0.3902439024390244,
                    0.3974166208856349,
                    0.9850746268656716,
                    1.0,
                    1.0
                ],
                [
                    0.4645752862163761,
                    0.4513031550068587,
                    0.08071054284794756,
                    0.1351850487461589,
                    0.48472075869336145,
                    0.27241379310344827,
                    0.28183581124757595,
                    0.11201567917985829,
                    0.28557319223985894,
                    0.29572446555819476,
                    0.35772357723577236,
                    0.303896861981727,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.34748755959216343,
                    0.23456790123456792,
                    0.05174125841895818,
                    0.2515852418758987,
                    0.09799789251844046,
                    0.27586206896551724,
                    0.5210084033613446,
                    0.24121815166591287,
                    0.3128042328042328,
                    0.2375296912114014,
                    0.25203252032520324,
                    0.2986865171326381,
                    0.014925373134328358,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "kwanzaa": 0.25183415514693847,
                "ramadan": 0.22410334373147192,
                "lent": 0.5240625011215897
            },
            "question": "mardi gras is celebrated right before what other observance?",
            "rate_limited": false,
            "answers": [
                "lent",
                "kwanzaa",
                "ramadan"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "kwanzaa": 0.3026178103322456,
                "ramadan": 0.03263025571071399,
                "lent": 0.563976529319204
            },
            "integer_answers": {
                "kwanzaa": 3,
                "ramadan": 1,
                "lent": 10
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9870831044281745,
                    1.519484309908635,
                    1.4934325856631905
                ],
                "result_count_important_words": [
                    30500.0,
                    43600.0,
                    80600.0
                ],
                "wikipedia_search": [
                    1.2048677248677249,
                    0.8567195767195768,
                    0.9384126984126984
                ],
                "answer_relation_to_question": [
                    0.5638114625743814,
                    1.3937258586491283,
                    1.0424626787764903
                ],
                "word_count_appended_bing": [
                    48.0,
                    44.0,
                    31.0
                ],
                "answer_relation_to_question_bing": [
                    0.9423868312757202,
                    1.353909465020576,
                    0.7037037037037037
                ],
                "cosine_similarity_raw": [
                    0.40218690037727356,
                    0.037416622042655945,
                    0.02398674376308918
                ],
                "result_count_noun_chunks": [
                    42900.0,
                    7430.0,
                    16000.0
                ],
                "question_answer_similarity": [
                    1.887045793235302,
                    0.4159948118031025,
                    0.774184396257624
                ],
                "word_count_noun_chunks": [
                    66.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    262000.0,
                    158000.0,
                    160000.0
                ],
                "word_count_raw": [
                    26.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    39600.0,
                    46000.0,
                    9300.0
                ],
                "word_count_appended": [
                    393.0,
                    249.0,
                    200.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "In which state is happy hour currently banned?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "illinois"
            ],
            "lines": [
                [
                    0.4535248672139063,
                    0.27708333333333335,
                    0.6031405784280088,
                    0.25970209133061684,
                    0.40414507772020725,
                    0.3800183879865155,
                    0.41920374707259955,
                    0.3855507868383405,
                    0.39244089834515367,
                    0.5191873589164786,
                    0.5619047619047619,
                    0.3805536582073918,
                    1.0,
                    0.75,
                    -1.0
                ],
                [
                    0.30583051665861904,
                    0.3319444444444445,
                    0.07892177834667964,
                    0.23225752984005626,
                    0.4450777202072539,
                    0.43518234753294516,
                    0.5199063231850117,
                    0.38412017167381973,
                    0.2851418439716312,
                    0.3905191873589165,
                    0.3142857142857143,
                    0.2732887798068684,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.24064461612747465,
                    0.3909722222222222,
                    0.31793764322531154,
                    0.5080403788293268,
                    0.15077720207253886,
                    0.18479926448053938,
                    0.06088992974238876,
                    0.23032904148783978,
                    0.32241725768321516,
                    0.09029345372460497,
                    0.12380952380952381,
                    0.34615756198573977,
                    0.0,
                    0.25,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "arizona": 0.2854625969508543,
                "illinois": 0.48474682480695097,
                "rhode island": 0.22979057824219468
            },
            "question": "in which state is happy hour currently banned?",
            "rate_limited": false,
            "answers": [
                "illinois",
                "arizona",
                "rhode island"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "arizona": 0.1599056350514332,
                "illinois": 0.7745553692078216,
                "rhode island": 0.34373755675368045
            },
            "integer_answers": {
                "arizona": 3,
                "illinois": 9,
                "rhode island": 2
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.902768291036959,
                    1.366443899034342,
                    1.7307878099286989
                ],
                "result_count_important_words": [
                    1790000.0,
                    2220000.0,
                    260000.0
                ],
                "wikipedia_search": [
                    1.5697635933806147,
                    1.140567375886525,
                    1.2896690307328607
                ],
                "answer_relation_to_question": [
                    0.9070497344278126,
                    0.6116610333172381,
                    0.4812892322549493
                ],
                "word_count_appended_bing": [
                    59.0,
                    33.0,
                    13.0
                ],
                "answer_relation_to_question_bing": [
                    0.5541666666666667,
                    0.663888888888889,
                    0.7819444444444444
                ],
                "cosine_similarity_raw": [
                    0.24079617857933044,
                    0.031508512794971466,
                    0.1269325464963913
                ],
                "result_count_noun_chunks": [
                    5390000.0,
                    5370000.0,
                    3220000.0
                ],
                "question_answer_similarity": [
                    1.4304169341921806,
                    1.2792546339333057,
                    2.798243007622659
                ],
                "word_count_noun_chunks": [
                    31.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    1240000.0,
                    1420000.0,
                    603000.0
                ],
                "word_count_raw": [
                    6.0,
                    0.0,
                    2.0
                ],
                "result_count": [
                    780000.0,
                    859000.0,
                    291000.0
                ],
                "word_count_appended": [
                    460.0,
                    346.0,
                    80.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "In baking, yeast helps bread rise, but scientifically yeast is what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "fungus"
            ],
            "lines": [
                [
                    0.7098039215686274,
                    0.5,
                    0.44314645141350506,
                    0.3043206998380932,
                    0.6001805054151624,
                    0.2630350194552529,
                    0.6012658227848101,
                    0.12354113293481354,
                    0.8807241145950824,
                    0.35097493036211697,
                    0.29464285714285715,
                    0.3334024161636499,
                    0.4117647058823529,
                    0.7142857142857143,
                    1.0
                ],
                [
                    0.2,
                    0.0,
                    0.213702139932093,
                    0.32993410956901953,
                    0.24819494584837545,
                    0.40077821011673154,
                    0.24773960216998192,
                    0.71733561058924,
                    0.06468531468531469,
                    0.318941504178273,
                    0.32142857142857145,
                    0.3232375110821559,
                    0.47058823529411764,
                    0.14285714285714285,
                    1.0
                ],
                [
                    0.09019607843137255,
                    0.5,
                    0.34315140865440197,
                    0.3657451905928873,
                    0.15162454873646208,
                    0.3361867704280156,
                    0.15099457504520797,
                    0.1591232564759465,
                    0.05459057071960298,
                    0.33008356545961004,
                    0.38392857142857145,
                    0.34336007275419417,
                    0.11764705882352941,
                    0.14285714285714285,
                    1.0
                ]
            ],
            "fraction_answers": {
                "fungus": 0.4665063065601456,
                "plant": 0.28567306412507265,
                "bacteria": 0.24782062931478174
            },
            "question": "in baking, yeast helps bread rise, but scientifically yeast is what?",
            "rate_limited": false,
            "answers": [
                "fungus",
                "plant",
                "bacteria"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "fungus": 0.5520455203070409,
                "plant": 0.15447812185319845,
                "bacteria": 0.36688537791827897
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.3338169131455495,
                    2.2626625775750915,
                    2.403520509279359
                ],
                "result_count_important_words": [
                    1330000.0,
                    548000.0,
                    334000.0
                ],
                "wikipedia_search": [
                    3.5228964583803295,
                    0.25874125874125875,
                    0.21836228287841192
                ],
                "answer_relation_to_question": [
                    3.549019607843137,
                    1.0,
                    0.45098039215686275
                ],
                "result_count": [
                    1330000.0,
                    550000.0,
                    336000.0
                ],
                "answer_relation_to_question_bing": [
                    2.0,
                    0.0,
                    2.0
                ],
                "cosine_similarity_raw": [
                    0.10530021041631699,
                    0.050779782235622406,
                    0.08153944462537766
                ],
                "result_count_noun_chunks": [
                    434000.0,
                    2520000.0,
                    559000.0
                ],
                "question_answer_similarity": [
                    3.1372757628560066,
                    3.4013272374868393,
                    3.7705076336860657
                ],
                "word_count_noun_chunks": [
                    7.0,
                    8.0,
                    2.0
                ],
                "word_count_raw": [
                    5.0,
                    1.0,
                    1.0
                ],
                "result_count_bing": [
                    676000.0,
                    1030000.0,
                    864000.0
                ],
                "word_count_appended": [
                    252.0,
                    229.0,
                    237.0
                ],
                "word_count_appended_bing": [
                    33.0,
                    36.0,
                    43.0
                ]
            },
            "integer_answers": {
                "fungus": 8,
                "plant": 3,
                "bacteria": 3
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these is classified as a neurological condition or disorder?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "multiple sclerosis"
            ],
            "lines": [
                [
                    0.179516317016317,
                    0.22142857142857142,
                    0.025085304869273985,
                    0.09134108998673256,
                    0.0037449662326266473,
                    0.16727716727716727,
                    0.6586447118429386,
                    0.004173267900614398,
                    0.28756674294431733,
                    0.5117056856187291,
                    0.3950617283950617,
                    0.2984097426851436,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.23237179487179485,
                    0.5404761904761904,
                    0.017508679259076853,
                    0.3633260241673973,
                    0.032592174744198434,
                    0.25274725274725274,
                    0.12824572514249524,
                    0.02206422195602612,
                    0.20404271548436306,
                    0.24581939799331104,
                    0.37037037037037035,
                    0.3113347178961925,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.5881118881118881,
                    0.23809523809523808,
                    0.9574060158716492,
                    0.5453328858458701,
                    0.963662859023175,
                    0.57997557997558,
                    0.21310956301456618,
                    0.9737625101433595,
                    0.5083905415713196,
                    0.24247491638795987,
                    0.2345679012345679,
                    0.390255539418664,
                    1.0,
                    1.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "halitosis": 0.20313966401410669,
                "cystic fibrosis": 0.19434994750776208,
                "multiple sclerosis": 0.6025103884781312
            },
            "question": "which of these is classified as a neurological condition or disorder?",
            "rate_limited": false,
            "answers": [
                "halitosis",
                "cystic fibrosis",
                "multiple sclerosis"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "halitosis": 0.15893564954791747,
                "cystic fibrosis": 0.09132043844649664,
                "multiple sclerosis": 0.2484951865528776
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1936389707405743,
                    1.24533887158477,
                    1.561022157674656
                ],
                "result_count_important_words": [
                    2080000.0,
                    405000.0,
                    673000.0
                ],
                "wikipedia_search": [
                    0.8627002288329519,
                    0.6121281464530892,
                    1.5251716247139586
                ],
                "answer_relation_to_question": [
                    0.5385489510489511,
                    0.6971153846153846,
                    1.7643356643356642
                ],
                "answer_relation_to_question_bing": [
                    0.44285714285714284,
                    1.0809523809523809,
                    0.47619047619047616
                ],
                "word_count_appended": [
                    306.0,
                    147.0,
                    145.0
                ],
                "cosine_similarity_raw": [
                    0.011235890910029411,
                    0.00784226506948471,
                    0.4288291335105896
                ],
                "result_count_noun_chunks": [
                    108000.0,
                    571000.0,
                    25200000.0
                ],
                "question_answer_similarity": [
                    1.1146197230555117,
                    4.433605428785086,
                    6.654604081064463
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    4.0
                ],
                "result_count_bing": [
                    1370000.0,
                    2070000.0,
                    4750000.0
                ],
                "result_count": [
                    95600.0,
                    832000.0,
                    24600000.0
                ],
                "word_count_appended_bing": [
                    32.0,
                    30.0,
                    19.0
                ]
            },
            "integer_answers": {
                "halitosis": 3,
                "cystic fibrosis": 1,
                "multiple sclerosis": 10
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these Uranus moons is NOT named after a Shakespearean character?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "trinculo"
            ],
            "lines": [
                [
                    0.46838280766852197,
                    0.3398809523809524,
                    0.32080231739233567,
                    0.0,
                    0.12394303515798843,
                    0.06865848832607835,
                    0.11527672023404001,
                    0.11034198221325753,
                    0.4713541666666667,
                    0.282122905027933,
                    0.34782608695652173,
                    0.32328521754461165,
                    0.18181818181818182,
                    0.1764705882352941,
                    -1.0
                ],
                [
                    0.21732374768089052,
                    0.20625,
                    0.22612638718788894,
                    0.5,
                    0.40431686693368935,
                    0.445191927186387,
                    0.3972067486875326,
                    0.40075181076372973,
                    0.20208333333333334,
                    0.33379888268156427,
                    0.30434782608695654,
                    0.33153043017037004,
                    0.35454545454545455,
                    0.32352941176470584,
                    -1.0
                ],
                [
                    0.3142934446505875,
                    0.4538690476190476,
                    0.4530712954197755,
                    0.5,
                    0.4717400979083222,
                    0.48614958448753465,
                    0.48751653107842746,
                    0.48890620702301274,
                    0.3265625,
                    0.3840782122905028,
                    0.34782608695652173,
                    0.3451843522850183,
                    0.4636363636363636,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "oberon": 0.5242623643396596,
                "trinculo": 0.13959518237784085,
                "umbriel": 0.3361424532824996
            },
            "question": "which of these uranus moons is not named after a shakespearean character?",
            "rate_limited": false,
            "answers": [
                "oberon",
                "umbriel",
                "trinculo"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "oberon": 0.23142756255358532,
                "trinculo": 0.4564511175848187,
                "umbriel": 0.22395605793825202
            },
            "integer_answers": {
                "oberon": 8,
                "trinculo": 0,
                "umbriel": 5
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7671478245538834,
                    1.6846956982962995,
                    1.548156477149817
                ],
                "result_count_important_words": [
                    19200.0,
                    5130.0,
                    623.0
                ],
                "wikipedia_search": [
                    0.22916666666666666,
                    2.3833333333333333,
                    1.3875
                ],
                "word_count_appended_bing": [
                    35.0,
                    45.0,
                    35.0
                ],
                "answer_relation_to_question_bing": [
                    1.280952380952381,
                    2.35,
                    0.369047619047619
                ],
                "cosine_similarity_raw": [
                    0.11752565205097198,
                    0.1796182543039322,
                    0.030777890235185623
                ],
                "result_count_noun_chunks": [
                    17000.0,
                    4330.0,
                    484.0
                ],
                "question_answer_similarity": [
                    -0.7806879468262196,
                    0.0,
                    0.0
                ],
                "word_count_noun_chunks": [
                    35.0,
                    16.0,
                    4.0
                ],
                "result_count_bing": [
                    218000.0,
                    27700.0,
                    7000.0
                ],
                "word_count_raw": [
                    11.0,
                    6.0,
                    0.0
                ],
                "result_count": [
                    16900.0,
                    4300.0,
                    1270.0
                ],
                "answer_relation_to_question": [
                    0.31617192331478045,
                    2.826762523191095,
                    1.8570655534941247
                ],
                "word_count_appended": [
                    312.0,
                    238.0,
                    166.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "In 1973, Arabella and Anita became the first members of what species to go into space?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "spider"
            ],
            "question": "in 1973, arabella and anita became the first members of what species to go into space?",
            "lines": [
                [
                    0.5134966688158178,
                    0.1445378151260504,
                    0.8629942598893077,
                    0.23661198064976566,
                    0.9201596806387226,
                    0.48434283237620546,
                    0.4787664307381193,
                    0.7177681473456121,
                    0.73,
                    0.8362068965517241,
                    0.6627906976744186,
                    0.6084402151091959,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.2793036750483559,
                    0.5815126050420169,
                    0.09981199890366943,
                    0.7008173485808812,
                    0.033932135728542916,
                    0.03185610575360277,
                    0.021739130434782608,
                    0.0866738894907909,
                    0.09142857142857143,
                    0.01293103448275862,
                    0.023255813953488372,
                    0.1883994111022505,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.20719965613582633,
                    0.2739495798319328,
                    0.037193741207022867,
                    0.06257067076935317,
                    0.04590818363273453,
                    0.4838010618701918,
                    0.4994944388270981,
                    0.19555796316359697,
                    0.17857142857142858,
                    0.15086206896551724,
                    0.313953488372093,
                    0.20316037378855362,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "spider",
                "fruit fly",
                "ladybug"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "ladybug": 0.1049890814725439,
                "fruit fly": 0.22890571217015276,
                "spider": 0.7723488789400673
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.650641290655175,
                    1.130396466613503,
                    1.2189622427313216
                ],
                "result_count_important_words": [
                    9470.0,
                    430.0,
                    9880.0
                ],
                "wikipedia_search": [
                    2.92,
                    0.3657142857142857,
                    0.7142857142857143
                ],
                "word_count_appended_bing": [
                    57.0,
                    2.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    0.2890756302521008,
                    1.1630252100840337,
                    0.5478991596638656
                ],
                "cosine_similarity_raw": [
                    0.4063692092895508,
                    0.04699975997209549,
                    0.01751389540731907
                ],
                "result_count_noun_chunks": [
                    26500.0,
                    3200.0,
                    7220.0
                ],
                "question_answer_similarity": [
                    2.5772198028862476,
                    7.633427284657955,
                    0.6815308816730976
                ],
                "word_count_noun_chunks": [
                    158.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    53.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    8940000.0,
                    588000.0,
                    8930000.0
                ],
                "word_count_appended": [
                    194.0,
                    3.0,
                    35.0
                ],
                "answer_relation_to_question": [
                    1.5404900064474534,
                    0.8379110251450677,
                    0.621598968407479
                ],
                "result_count": [
                    461.0,
                    17.0,
                    23.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these universities is named after a former owner of the Staten Island Ferry?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "vanderbilt"
            ],
            "lines": [
                [
                    0.09411764705882353,
                    0.22916666666666666,
                    0.08325810743630245,
                    0.34088983427100966,
                    0.1580226904376013,
                    0.3154875717017208,
                    0.39796747967479673,
                    0.1149843912591051,
                    0.5925925925925926,
                    0.05,
                    0.045454545454545456,
                    0.2701491914179058,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.17294117647058824,
                    0.16666666666666669,
                    0.06116102621641002,
                    0.6633045211824571,
                    0.1774716369529984,
                    0.3422562141491396,
                    0.012601626016260163,
                    0.4172736732570239,
                    0.07407407407407407,
                    0.2980769230769231,
                    0.17532467532467533,
                    0.29761337371863744,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.7329411764705883,
                    0.6041666666666667,
                    0.8555808663472875,
                    -0.00419435545346676,
                    0.6645056726094003,
                    0.3422562141491396,
                    0.5894308943089431,
                    0.46774193548387094,
                    0.3333333333333333,
                    0.6519230769230769,
                    0.7792207792207793,
                    0.4322374348634567,
                    1.0,
                    1.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "carnegie mellon": 0.1922921941407907,
                "stetson": 0.20419754193613243,
                "vanderbilt": 0.6035102639230769
            },
            "question": "which of these universities is named after a former owner of the staten island ferry?",
            "rate_limited": false,
            "answers": [
                "carnegie mellon",
                "stetson",
                "vanderbilt"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "carnegie mellon": 0.13856824379534213,
                "stetson": 0.10498133419932126,
                "vanderbilt": 0.7906221929744528
            },
            "integer_answers": {
                "carnegie mellon": 1,
                "stetson": 1,
                "vanderbilt": 12
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6208951485074348,
                    1.7856802423118245,
                    2.5934246091807402
                ],
                "result_count_important_words": [
                    979000.0,
                    31000.0,
                    1450000.0
                ],
                "wikipedia_search": [
                    1.7777777777777777,
                    0.2222222222222222,
                    1.0
                ],
                "word_count_appended_bing": [
                    7.0,
                    27.0,
                    120.0
                ],
                "answer_relation_to_question_bing": [
                    0.9166666666666665,
                    0.6666666666666666,
                    2.4166666666666665
                ],
                "cosine_similarity_raw": [
                    0.006080477498471737,
                    0.004466691054403782,
                    0.062484487891197205
                ],
                "result_count_noun_chunks": [
                    22100.0,
                    80200.0,
                    89900.0
                ],
                "question_answer_similarity": [
                    -0.8124950705096126,
                    -1.580955486278981,
                    0.00999705120921135
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    2.0
                ],
                "result_count_bing": [
                    165000.0,
                    179000.0,
                    179000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    5.0
                ],
                "result_count": [
                    19500.0,
                    21900.0,
                    82000.0
                ],
                "answer_relation_to_question": [
                    0.47058823529411764,
                    0.8647058823529412,
                    3.6647058823529415
                ],
                "word_count_appended": [
                    26.0,
                    155.0,
                    339.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these celebrities is known for having aviophobia?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "angelina jolie"
            ],
            "lines": [
                [
                    0.19583333333333333,
                    0.0,
                    0.2558089911117708,
                    0.26241121544127877,
                    0.4304635761589404,
                    0.33112582781456956,
                    0.4378698224852071,
                    0.5035460992907801,
                    0.8344907407407408,
                    0.417910447761194,
                    0.3225806451612903,
                    0.3764582854268468,
                    0.3333333333333333,
                    0.16666666666666666,
                    -1.0
                ],
                [
                    0.4583333333333333,
                    0.4375,
                    0.46393681148048377,
                    0.4589561755030482,
                    0.3443708609271523,
                    0.3355408388520971,
                    0.1893491124260355,
                    0.22695035460992907,
                    0.07423941798941798,
                    0.3880597014925373,
                    0.25806451612903225,
                    0.38809388473699336,
                    0.3333333333333333,
                    0.3333333333333333,
                    -1.0
                ],
                [
                    0.3458333333333334,
                    0.5625,
                    0.2802541974077455,
                    0.27863260905567305,
                    0.2251655629139073,
                    0.3333333333333333,
                    0.3727810650887574,
                    0.2695035460992908,
                    0.09126984126984126,
                    0.19402985074626866,
                    0.41935483870967744,
                    0.23544782983615983,
                    0.3333333333333333,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "angelina jolie": 0.3477499274804251,
                "john travolta": 0.31724566722338005,
                "john madden": 0.3350044052961948
            },
            "question": "which of these celebrities is known for having aviophobia?",
            "rate_limited": false,
            "answers": [
                "angelina jolie",
                "john madden",
                "john travolta"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "angelina jolie": 0.46224809399276434,
                "john travolta": 0.21834245662727622,
                "john madden": 0.4535024340847416
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.5058331417073871,
                    1.5523755389479734,
                    0.9417913193446393
                ],
                "result_count_important_words": [
                    74.0,
                    32.0,
                    63.0
                ],
                "wikipedia_search": [
                    2.5034722222222223,
                    0.22271825396825395,
                    0.2738095238095238
                ],
                "word_count_appended_bing": [
                    10.0,
                    8.0,
                    13.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.875,
                    1.125
                ],
                "cosine_similarity_raw": [
                    0.11690175533294678,
                    0.21201376616954803,
                    0.12807293236255646
                ],
                "result_count_noun_chunks": [
                    71.0,
                    32.0,
                    38.0
                ],
                "question_answer_similarity": [
                    1.0088321383518633,
                    1.7644434105604887,
                    1.0711948052048683
                ],
                "word_count_noun_chunks": [
                    4.0,
                    4.0,
                    4.0
                ],
                "word_count_raw": [
                    1.0,
                    2.0,
                    3.0
                ],
                "result_count_bing": [
                    4500000.0,
                    4560000.0,
                    4530000.0
                ],
                "word_count_appended": [
                    28.0,
                    26.0,
                    13.0
                ],
                "answer_relation_to_question": [
                    0.5875,
                    1.375,
                    1.0375
                ],
                "result_count": [
                    65.0,
                    52.0,
                    34.0
                ]
            },
            "integer_answers": {
                "angelina jolie": 6,
                "john travolta": 3,
                "john madden": 5
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "How did Mason jars get their name?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "named after inventor"
            ],
            "lines": [
                [
                    0.0,
                    0.0,
                    0.15369554125848628,
                    0.25523113929291164,
                    1.1825046888881578e-05,
                    0.3235955056179775,
                    3.795049850826078e-06,
                    4.292908695640097e-06,
                    0.0,
                    0.56,
                    0.3333333333333333,
                    0.08947846179237498,
                    0,
                    0,
                    5.0
                ],
                [
                    0.3608644144921624,
                    0.3767123287671233,
                    0.4403464473519183,
                    0.39869682594891903,
                    0.9999876608206377,
                    0.31235955056179776,
                    0.9999956356926716,
                    0.9999952020432226,
                    0.0,
                    0.24,
                    0.3333333333333333,
                    0.8456345931671894,
                    0,
                    0,
                    5.0
                ],
                [
                    0.6391355855078377,
                    0.6232876712328768,
                    0.4059580113895955,
                    0.34607203475816933,
                    5.141324734296338e-07,
                    0.36404494382022473,
                    5.692574776239117e-07,
                    5.050480818400113e-07,
                    1.0,
                    0.2,
                    0.3333333333333333,
                    0.06488694504043571,
                    0,
                    0,
                    5.0
                ]
            ],
            "fraction_answers": {
                "named after inventor": 0.14294615785837658,
                "invented in mason, al": 0.525660499348248,
                "masons used them": 0.3313933427933755
            },
            "question": "how did mason jars get their name?",
            "rate_limited": false,
            "answers": [
                "named after inventor",
                "invented in mason, al",
                "masons used them"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "named after inventor": 0.2952405103978026,
                "invented in mason, al": 0.12681941571163238,
                "masons used them": 0.18317508568592022
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 5
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.17895692358474996,
                    1.6912691863343787,
                    0.12977389008087142
                ],
                "result_count_important_words": [
                    20.0,
                    5270000.0,
                    3.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.3767123287671233,
                    0.6232876712328768
                ],
                "cosine_similarity_raw": [
                    0.0738634318113327,
                    0.2116229236125946,
                    0.1950964331626892
                ],
                "result_count_noun_chunks": [
                    17.0,
                    3960000.0,
                    2.0
                ],
                "question_answer_similarity": [
                    6.142987018451095,
                    9.59596635773778,
                    8.329375572502613
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    1440000.0,
                    1390000.0,
                    1620000.0
                ],
                "word_count_appended": [
                    14.0,
                    6.0,
                    5.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    0.7217288289843247,
                    1.2782711710156751
                ],
                "result_count": [
                    46.0,
                    3890000.0,
                    2.0
                ]
            },
            "integer_answers": {
                "named after inventor": 2,
                "invented in mason, al": 6,
                "masons used them": 4
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Who wrote a #1 hit song for the Monkees?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "james taylor"
            ],
            "lines": [
                [
                    0.24800136225357855,
                    0.2977935613958844,
                    0.5125856982477803,
                    0.4592827845818843,
                    0.3333333333333333,
                    0.33270142180094786,
                    0.24833702882483372,
                    0.3333333333333333,
                    0.5372058867787953,
                    0,
                    0,
                    0.3336143901068016,
                    0,
                    0,
                    0.0
                ],
                [
                    0.35061644633631217,
                    0.35315643909999556,
                    0.14416734866222453,
                    0.3607110767257043,
                    0.3333333333333333,
                    0.33364928909952607,
                    0.5964523281596452,
                    0.3333333333333333,
                    0.2949306756646522,
                    0,
                    0,
                    0.3326306913996627,
                    0,
                    0,
                    0.0
                ],
                [
                    0.40138219141010933,
                    0.3490499995041199,
                    0.3432469530899951,
                    0.1800061386924114,
                    0.3333333333333333,
                    0.33364928909952607,
                    0.15521064301552107,
                    0.3333333333333333,
                    0.16786343755655248,
                    0,
                    0,
                    0.3337549184935357,
                    0,
                    0,
                    0.0
                ]
            ],
            "fraction_answers": {
                "neil diamond": 0.34329809618143897,
                "james taylor": 0.3636188800657173,
                "jackson browne": 0.2930830237528438
            },
            "question": "who wrote a #1 hit song for the monkees?",
            "rate_limited": false,
            "answers": [
                "james taylor",
                "neil diamond",
                "jackson browne"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "neil diamond": 0.10190819660860118,
                "james taylor": 0.3330912675920115,
                "jackson browne": 0.11226047315010274
            },
            "integer_answers": {
                "neil diamond": 3,
                "james taylor": 5,
                "jackson browne": 2
            },
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3344575604272062,
                    1.3305227655986507,
                    1.3350196739741427
                ],
                "result_count_important_words": [
                    112000.0,
                    269000.0,
                    70000.0
                ],
                "wikipedia_search": [
                    2.1488235471151813,
                    1.1797227026586088,
                    0.6714537502262099
                ],
                "word_count_appended_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "answer_relation_to_question_bing": [
                    0.8933806841876533,
                    1.0594693172999867,
                    1.0471499985123598
                ],
                "cosine_similarity_raw": [
                    0.055027689784765244,
                    0.01547681912779808,
                    0.03684864193201065
                ],
                "result_count_noun_chunks": [
                    7280000000.0,
                    7280000000.0,
                    7280000000.0
                ],
                "question_answer_similarity": [
                    3.3408411890268326,
                    2.623826676979661,
                    1.309371791430749
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    35100000.0,
                    35200000.0,
                    35200000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    65100000.0,
                    65100000.0,
                    65100000.0
                ],
                "answer_relation_to_question": [
                    0.9920054490143142,
                    1.4024657853452487,
                    1.6055287656404373
                ],
                "word_count_appended": [
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Anne of Green Gables literally means Anne of what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "green pastures"
            ],
            "lines": [
                [
                    0.47450420168067226,
                    0.32885530366638677,
                    0.5141846846509843,
                    0.29398007696491074,
                    0.5529411764705883,
                    0.33042635658914726,
                    0.6147540983606558,
                    0.22439331862590609,
                    0.5,
                    0.46511627906976744,
                    0.36585365853658536,
                    0.5083634718075225,
                    0,
                    0,
                    1.0
                ],
                [
                    0.2872268907563026,
                    0.32213825916596694,
                    0.3051330210084451,
                    0.31345494110442695,
                    0.047058823529411764,
                    0.3313953488372093,
                    0.03278688524590164,
                    0.0003151591553734636,
                    0.07446808510638298,
                    0.06976744186046512,
                    0.3170731707317073,
                    0.1351712241593088,
                    0,
                    0,
                    1.0
                ],
                [
                    0.2382689075630252,
                    0.34900643716764623,
                    0.18068229434057054,
                    0.3925649819306623,
                    0.4,
                    0.3381782945736434,
                    0.3524590163934426,
                    0.7752915222187204,
                    0.425531914893617,
                    0.46511627906976744,
                    0.3170731707317073,
                    0.3564653040331687,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "green pastures": 0.43111438553526055,
                "green walls": 0.3825531769096642,
                "green jars": 0.18633243755507514
            },
            "question": "anne of green gables literally means anne of what?",
            "rate_limited": false,
            "answers": [
                "green pastures",
                "green jars",
                "green walls"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "green pastures": 0.5061159046529368,
                "green walls": 0.3619938809295726,
                "green jars": 0.309172930751728
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.050180830845135,
                    0.8110273449558527,
                    2.138791824199012
                ],
                "result_count_important_words": [
                    75.0,
                    4.0,
                    43.0
                ],
                "wikipedia_search": [
                    2.0,
                    0.2978723404255319,
                    1.702127659574468
                ],
                "word_count_appended_bing": [
                    15.0,
                    13.0,
                    13.0
                ],
                "answer_relation_to_question_bing": [
                    0.9865659109991602,
                    0.9664147774979008,
                    1.0470193115029387
                ],
                "cosine_similarity_raw": [
                    0.05005403608083725,
                    0.02970360592007637,
                    0.017588773742318153
                ],
                "result_count_noun_chunks": [
                    17800.0,
                    25.0,
                    61500.0
                ],
                "question_answer_similarity": [
                    4.595996670424938,
                    4.900460876524448,
                    6.137243613600731
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    341000.0,
                    342000.0,
                    349000.0
                ],
                "word_count_appended": [
                    20.0,
                    3.0,
                    20.0
                ],
                "answer_relation_to_question": [
                    1.8980168067226888,
                    1.1489075630252101,
                    0.9530756302521007
                ],
                "result_count": [
                    47.0,
                    4.0,
                    34.0
                ]
            },
            "integer_answers": {
                "green pastures": 8,
                "green walls": 4,
                "green jars": 0
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these modes of transportation has only one wheel?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "bus"
            ],
            "lines": [
                [
                    0.08870967741935484,
                    0.0,
                    0.036324522614702086,
                    0.6515017310824474,
                    0.30093457943925234,
                    0.1325898389095415,
                    0.020722084960548337,
                    0.3081081081081081,
                    0.09574468085106383,
                    0.11617312072892938,
                    0.018691588785046728,
                    0.26991804524045726,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.42699490662139217,
                    0.5,
                    0.5526133784043324,
                    0.33917109880097696,
                    0.4672897196261682,
                    0.7001239157372986,
                    0.9703514784410616,
                    0.4720720720720721,
                    0.5425531914893617,
                    0.39635535307517084,
                    0.411214953271028,
                    0.3701184386789127,
                    0.9259259259259259,
                    0.46153846153846156,
                    -1.0
                ],
                [
                    0.48429541595925296,
                    0.5,
                    0.4110620989809654,
                    0.009327170116575717,
                    0.23177570093457944,
                    0.16728624535315986,
                    0.008926436598390054,
                    0.21981981981981982,
                    0.3617021276595745,
                    0.4874715261958998,
                    0.5700934579439252,
                    0.35996351608063004,
                    0.07407407407407407,
                    0.5384615384615384,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "unicycle": 0.316018509155599,
                "bus": 0.5383087781201545,
                "monster truck": 0.14567271272424653
            },
            "question": "which of these modes of transportation has only one wheel?",
            "rate_limited": false,
            "answers": [
                "monster truck",
                "bus",
                "unicycle"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "unicycle": 0.7260559555895089,
                "bus": 0.8324687186410895,
                "monster truck": 0.044372874260536724
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.8097541357213718,
                    1.110355316036738,
                    1.0798905482418901
                ],
                "result_count_important_words": [
                    1040000.0,
                    48700000.0,
                    448000.0
                ],
                "wikipedia_search": [
                    0.19148936170212766,
                    1.0851063829787233,
                    0.723404255319149
                ],
                "word_count_appended_bing": [
                    2.0,
                    44.0,
                    61.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    1.0,
                    1.0
                ],
                "cosine_similarity_raw": [
                    0.006783303339034319,
                    0.10319596529006958,
                    0.07676243782043457
                ],
                "result_count_noun_chunks": [
                    1710000.0,
                    2620000.0,
                    1220000.0
                ],
                "question_answer_similarity": [
                    5.47492903470993,
                    2.8502421528100967,
                    0.07838136423379183
                ],
                "word_count_noun_chunks": [
                    0.0,
                    25.0,
                    2.0
                ],
                "word_count_raw": [
                    0.0,
                    6.0,
                    7.0
                ],
                "result_count_bing": [
                    428000.0,
                    2260000.0,
                    540000.0
                ],
                "word_count_appended": [
                    102.0,
                    348.0,
                    428.0
                ],
                "answer_relation_to_question": [
                    0.1774193548387097,
                    0.8539898132427843,
                    0.9685908319185059
                ],
                "result_count": [
                    1610000.0,
                    2500000.0,
                    1240000.0
                ]
            },
            "integer_answers": {
                "unicycle": 4,
                "bus": 9,
                "monster truck": 1
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What is Telluride, Colorado named after?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "an element"
            ],
            "lines": [
                [
                    0.3157894736842105,
                    0,
                    0.7544189322250512,
                    0.27549462008946013,
                    0.9783109414105724,
                    0.7690322580645161,
                    0.9974174214449254,
                    0.8642440218414611,
                    0,
                    0.5121951219512195,
                    0.3333333333333333,
                    0.4696113008738994,
                    0,
                    0,
                    1.0
                ],
                [
                    0.21052631578947367,
                    0,
                    0.10785356850828243,
                    0.2947672410525409,
                    0.02156083336895145,
                    0.11638709677419355,
                    0.0025639826370476896,
                    0.08981359442666165,
                    0,
                    0.3902439024390244,
                    0.3333333333333333,
                    0.4274734694379188,
                    0,
                    0,
                    1.0
                ],
                [
                    0.47368421052631576,
                    0,
                    0.13772749926666636,
                    0.429738138857999,
                    0.000128225220476143,
                    0.11458064516129032,
                    1.8595918026939285e-05,
                    0.045942383731877236,
                    0,
                    0.0975609756097561,
                    0.3333333333333333,
                    0.10291522968818187,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "a european city": 0.17356292373139232,
                "a governor": 0.19945233377674276,
                "an element": 0.626984742491865
            },
            "question": "what is telluride, colorado named after?",
            "rate_limited": false,
            "answers": [
                "an element",
                "a governor",
                "a european city"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "a european city": 0.2219388861570518,
                "a governor": 0.3143178110234338,
                "an element": 0.5319921901361423
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.408833902621698,
                    1.2824204083137563,
                    0.3087456890645456
                ],
                "result_count_important_words": [
                    1770000.0,
                    4550.0,
                    33.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    0.0
                ],
                "answer_relation_to_question": [
                    0.3157894736842105,
                    0.21052631578947367,
                    0.47368421052631576
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    63.0,
                    48.0,
                    12.0
                ],
                "cosine_similarity_raw": [
                    0.3329974412918091,
                    0.047606125473976135,
                    0.060792356729507446
                ],
                "result_count_noun_chunks": [
                    45900000.0,
                    4770000.0,
                    2440000.0
                ],
                "question_answer_similarity": [
                    3.8979606702923775,
                    4.170648095197976,
                    6.08034510165453
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    2980000.0,
                    451000.0,
                    444000.0
                ],
                "result_count": [
                    206000.0,
                    4540.0,
                    27.0
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ]
            },
            "integer_answers": {
                "a european city": 2,
                "a governor": 0,
                "an element": 8
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these is NOT a suit in a traditional Tarot deck?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "gloves"
            ],
            "lines": [
                [
                    0.46964285714285714,
                    0,
                    0.48762140217600425,
                    0.32013387901234147,
                    0.4029706790123457,
                    0.30505331478905884,
                    0.49998055631169896,
                    0.39884743706399756,
                    0.24043062200956938,
                    0.4130218687872763,
                    0.41346153846153844,
                    0.38283634578197157,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.10535714285714282,
                    0,
                    0.15083160261162326,
                    0.3262927253056208,
                    0.3130787037037037,
                    0.4049605934167826,
                    0.29961096750999366,
                    0.313466787989081,
                    0.3181818181818182,
                    0.279324055666004,
                    0.2692307692307692,
                    0.30598477022859005,
                    0.19072164948453607,
                    0.2088607594936709,
                    -1.0
                ],
                [
                    0.425,
                    0,
                    0.36154699521237255,
                    0.3535733956820377,
                    0.28395061728395066,
                    0.2899860917941586,
                    0.20040847617830737,
                    0.28768577494692144,
                    0.44138755980861244,
                    0.30765407554671964,
                    0.3173076923076923,
                    0.3111788839894384,
                    0.30927835051546393,
                    0.2911392405063291,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "gloves": 0.1793845383771293,
                "cups": 0.3569081301889225,
                "swords": 0.4637073314339482
            },
            "question": "which of these is not a suit in a traditional tarot deck?",
            "rate_limited": false,
            "answers": [
                "gloves",
                "swords",
                "cups"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "gloves": 0.5191723276961996,
                "cups": 0.09527561993876078,
                "swords": 0.045090162435045
            },
            "integer_answers": {
                "gloves": 2,
                "cups": 4,
                "swords": 7
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.9373092337442275,
                    1.5521218381712794,
                    1.5105689280844932
                ],
                "result_count_important_words": [
                    98.0,
                    1010000.0,
                    1510000.0
                ],
                "wikipedia_search": [
                    1.0382775119617225,
                    0.7272727272727273,
                    0.23444976076555024
                ],
                "answer_relation_to_question": [
                    0.24285714285714285,
                    3.1571428571428575,
                    0.6
                ],
                "word_count_appended_bing": [
                    27.0,
                    72.0,
                    57.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.004843301139771938,
                    0.13661706447601318,
                    0.054171692579984665
                ],
                "result_count_noun_chunks": [
                    667000.0,
                    1230000.0,
                    1400000.0
                ],
                "question_answer_similarity": [
                    2.3634153008461,
                    2.2824889346957207,
                    1.924024797976017
                ],
                "word_count_noun_chunks": [
                    0.0,
                    60.0,
                    37.0
                ],
                "result_count_bing": [
                    841000.0,
                    410000.0,
                    906000.0
                ],
                "word_count_raw": [
                    0.0,
                    46.0,
                    33.0
                ],
                "word_count_appended": [
                    175.0,
                    444.0,
                    387.0
                ],
                "result_count": [
                    503000.0,
                    969000.0,
                    1120000.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which company named an iconic sports car after an untranslatable local expression?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "porsche"
            ],
            "question": "which company named an iconic sports car after an untranslatable local expression?",
            "lines": [
                [
                    0.46550559737372926,
                    0.32325485336637755,
                    0.5601214348190032,
                    0.4181473416329346,
                    0.5004433060782189,
                    0.3645224171539961,
                    0.5022591967731301,
                    0.5013807555964669,
                    0.3363833434160113,
                    0.3848797250859107,
                    0.38181818181818183,
                    0.37012637293154094,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.24524988355443617,
                    0.3958364312267658,
                    0.3020355906730387,
                    0.41247114506604515,
                    0.49847305684169047,
                    0.3333333333333333,
                    0.49644152267923286,
                    0.49746371844336945,
                    0.23772097869194056,
                    0.38831615120274915,
                    0.26666666666666666,
                    0.40935090964582765,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.28924451907183457,
                    0.28090871540685663,
                    0.1378429745079581,
                    0.16938151330102022,
                    0.0010836370800906315,
                    0.30214424951267055,
                    0.0012992805476370547,
                    0.001155525960163732,
                    0.42589567789204813,
                    0.2268041237113402,
                    0.3515151515151515,
                    0.22052271742263144,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "porsche",
                "ferrari",
                "lamborghini"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "porsche": 0.774688303286301,
                "ferrari": 0.25581529317364143,
                "lamborghini": 0.09977118142716283
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.961010983452327,
                    3.274807277166621,
                    1.7641817393810513
                ],
                "result_count_important_words": [
                    25900.0,
                    25600.0,
                    67.0
                ],
                "wikipedia_search": [
                    2.0183000604960677,
                    1.4263258721516434,
                    2.5553740673522887
                ],
                "word_count_appended_bing": [
                    63.0,
                    44.0,
                    58.0
                ],
                "answer_relation_to_question_bing": [
                    1.6162742668318877,
                    1.979182156133829,
                    1.4045435770342831
                ],
                "cosine_similarity_raw": [
                    0.1252407282590866,
                    0.06753385066986084,
                    0.030821092426776886
                ],
                "result_count_noun_chunks": [
                    25600.0,
                    25400.0,
                    59.0
                ],
                "question_answer_similarity": [
                    1.267041533254087,
                    1.2498419098556042,
                    0.5132483001798391
                ],
                "word_count_noun_chunks": [
                    5.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    7.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    187000.0,
                    171000.0,
                    155000.0
                ],
                "word_count_appended": [
                    112.0,
                    113.0,
                    66.0
                ],
                "answer_relation_to_question": [
                    3.2585391816161047,
                    1.7167491848810532,
                    2.024711633502842
                ],
                "result_count": [
                    25400.0,
                    25300.0,
                    55.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Table tennis is also known as what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ping-pong"
            ],
            "lines": [
                [
                    0.6816993464052287,
                    0.8666666666666667,
                    0.9367748736424744,
                    0.7563207514471897,
                    0.9877157055138339,
                    0.4334862385321101,
                    0.9885883331654598,
                    0.9886928648715577,
                    0.20535714285714285,
                    0.494195688225539,
                    0.43157894736842106,
                    0.6217619681172665,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.14901960784313725,
                    0.09824561403508773,
                    0.020094665425424006,
                    0.3266652415397998,
                    0.012206579264846557,
                    0.13302752293577982,
                    0.011191566035835394,
                    0.011088144278933357,
                    0.6785714285714286,
                    0.2902155887230514,
                    0.28421052631578947,
                    0.2122935390297178,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.16928104575163397,
                    0.03508771929824561,
                    0.04313046093210163,
                    -0.08298599298698954,
                    7.771522131952308e-05,
                    0.4334862385321101,
                    0.00022010079870476277,
                    0.0002189908495089338,
                    0.11607142857142858,
                    0.2155887230514096,
                    0.28421052631578947,
                    0.1659444928530156,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "hunky-dory": 0.1590592874284879,
                "argle-bargle": 0.09859510351344847,
                "ping-pong": 0.7423456090580637
            },
            "question": "table tennis is also known as what?",
            "rate_limited": false,
            "answers": [
                "ping-pong",
                "hunky-dory",
                "argle-bargle"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "hunky-dory": 0.1331820401012603,
                "argle-bargle": 0.06903399818773011,
                "ping-pong": 0.6230036992319048
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.8652859043517998,
                    0.6368806170891534,
                    0.4978334785590469
                ],
                "result_count_important_words": [
                    10600000.0,
                    120000.0,
                    2360.0
                ],
                "wikipedia_search": [
                    0.4107142857142857,
                    1.3571428571428572,
                    0.23214285714285715
                ],
                "word_count_appended_bing": [
                    41.0,
                    27.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    2.6,
                    0.2947368421052632,
                    0.10526315789473684
                ],
                "cosine_similarity_raw": [
                    0.501369833946228,
                    0.010754834860563278,
                    0.02308378741145134
                ],
                "result_count_noun_chunks": [
                    10700000.0,
                    120000.0,
                    2370.0
                ],
                "question_answer_similarity": [
                    3.905524665489793,
                    1.6868493370711803,
                    -0.4285269733518362
                ],
                "word_count_noun_chunks": [
                    24.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    36.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    3780000.0,
                    1160000.0,
                    3780000.0
                ],
                "word_count_appended": [
                    298.0,
                    175.0,
                    130.0
                ],
                "answer_relation_to_question": [
                    2.045098039215686,
                    0.44705882352941173,
                    0.5078431372549019
                ],
                "result_count": [
                    9710000.0,
                    120000.0,
                    764.0
                ]
            },
            "integer_answers": {
                "hunky-dory": 1,
                "argle-bargle": 0,
                "ping-pong": 13
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these countries is closest to the International Date Line?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "japan"
            ],
            "lines": [
                [
                    0.32908212010919013,
                    0.17856300141576217,
                    0.5359196305173226,
                    0.4010674837634104,
                    0.43844856661045534,
                    0.33116619260463226,
                    0.3206724782067248,
                    0.19943555973659455,
                    0.32252483171803087,
                    0.31831255992329816,
                    0.3333333333333333,
                    0.3468759052836018,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.31242569002123144,
                    0.5297703319175713,
                    0.29885548316194366,
                    0.2642045304237973,
                    0.19856661045531196,
                    0.33441690369768384,
                    0.29140722291407223,
                    0.4590780809031044,
                    0.37791367531208453,
                    0.3221476510067114,
                    0.3333333333333333,
                    0.32939995651990345,
                    0.5,
                    0,
                    -1.0
                ],
                [
                    0.35849218986957837,
                    0.2916666666666667,
                    0.1652248863207337,
                    0.3347279858127923,
                    0.36298482293423273,
                    0.33441690369768384,
                    0.387920298879203,
                    0.34148635936030103,
                    0.2995614929698846,
                    0.3595397890699904,
                    0.3333333333333333,
                    0.32372413819649476,
                    0.5,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "brazil": 0.3501168822820576,
                "japan": 0.31195397409402736,
                "spain": 0.33792914362391496
            },
            "question": "which of these countries is closest to the international date line?",
            "rate_limited": false,
            "answers": [
                "japan",
                "brazil",
                "spain"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "brazil": 0.19459574406323787,
                "japan": 0.3069154346179429,
                "spain": 0.1805632185619136
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7343795264180089,
                    1.6469997825995173,
                    1.6186206909824739
                ],
                "result_count_important_words": [
                    5150000.0,
                    4680000.0,
                    6230000.0
                ],
                "wikipedia_search": [
                    1.2900993268721235,
                    1.5116547012483381,
                    1.1982459718795384
                ],
                "word_count_appended_bing": [
                    29.0,
                    29.0,
                    29.0
                ],
                "answer_relation_to_question_bing": [
                    0.7142520056630486,
                    2.1190813276702847,
                    1.1666666666666665
                ],
                "cosine_similarity_raw": [
                    0.07343737035989761,
                    0.04095233604311943,
                    0.022640859708189964
                ],
                "result_count_noun_chunks": [
                    2120000.0,
                    4880000.0,
                    3630000.0
                ],
                "question_answer_similarity": [
                    1.9795766919851303,
                    1.3040526881814003,
                    1.6521402150392532
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    81500000.0,
                    82300000.0,
                    82300000.0
                ],
                "word_count_appended": [
                    332.0,
                    336.0,
                    375.0
                ],
                "answer_relation_to_question": [
                    1.3163284804367605,
                    1.2497027600849258,
                    1.4339687594783135
                ],
                "result_count": [
                    10400000.0,
                    4710000.0,
                    8610000.0
                ]
            },
            "integer_answers": {
                "brazil": 5,
                "japan": 5,
                "spain": 3
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these is NOT a name of one of the Florida Keys?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "turtle key"
            ],
            "lines": [
                [
                    0.24224341871556304,
                    0.3744789693065555,
                    0.38610875008731615,
                    0.2792305500054321,
                    0.4297752808988764,
                    0.3723331039229181,
                    0.476364522417154,
                    0.4999927356804696,
                    0.33333333333333337,
                    0.40625,
                    0.46511627906976744,
                    0.3818392025212387,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.47206119101786,
                    0.35297461159530125,
                    0.3987978594784563,
                    0.3551920914320528,
                    0.2359550561797753,
                    0.3719889883000688,
                    0.364766081871345,
                    0.48429523301515615,
                    0.5,
                    0.3506944444444444,
                    0.4534883720930233,
                    0.35663309582534025,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.285695390266577,
                    0.27254641909814326,
                    0.21509339043422754,
                    0.36557735856251505,
                    0.3342696629213483,
                    0.25567790777701305,
                    0.158869395711501,
                    0.01571203130437432,
                    0.16666666666666669,
                    0.24305555555555558,
                    0.08139534883720928,
                    0.26152770165342104,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "pigeon key": 0.5573188618685746,
                "turtle key": 0.21719216245786274,
                "fat deer key": 0.2254889756735626
            },
            "question": "which of these is not a name of one of the florida keys?",
            "rate_limited": false,
            "answers": [
                "fat deer key",
                "turtle key",
                "pigeon key"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "pigeon key": 0.13851532755811163,
                "turtle key": 0.3934851968464327,
                "fat deer key": 0.3796236363014837
            },
            "integer_answers": {
                "pigeon key": 9,
                "turtle key": 1,
                "fat deer key": 2
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.47264318991504517,
                    0.573467616698639,
                    0.9538891933863158
                ],
                "result_count_important_words": [
                    7760.0,
                    44400.0,
                    112000.0
                ],
                "wikipedia_search": [
                    0.3333333333333333,
                    0.0,
                    0.6666666666666666
                ],
                "word_count_appended_bing": [
                    3.0,
                    4.0,
                    36.0
                ],
                "answer_relation_to_question_bing": [
                    0.502084122773778,
                    0.588101553618795,
                    0.9098143236074271
                ],
                "cosine_similarity_raw": [
                    0.03496728837490082,
                    0.031071433797478676,
                    0.08747301995754242
                ],
                "result_count_noun_chunks": [
                    21.0,
                    45400.0,
                    1400000.0
                ],
                "question_answer_similarity": [
                    11.634681515395641,
                    7.631463035941124,
                    7.084153272211552
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    3710000.0,
                    3720000.0,
                    7100000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    25.0,
                    94.0,
                    59.0
                ],
                "answer_relation_to_question": [
                    1.0310263251377478,
                    0.1117552359285601,
                    0.8572184389336921
                ],
                "word_count_appended": [
                    27.0,
                    43.0,
                    74.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Traditionally, an \u201camuse-bouche\u201d arrives right before what part of the meal?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "dessert"
            ],
            "question": "traditionally, an \u201camuse-bouche\u201d arrives right before what part of the meal?",
            "lines": [
                [
                    0.19495005549389566,
                    0.1062874251497006,
                    0.41297466660303817,
                    0.314079863600959,
                    0.3494132985658409,
                    0.2937937937937938,
                    0.3418803418803419,
                    0.9998527508307221,
                    0.46359268650740687,
                    0.515970515970516,
                    0.5571428571428572,
                    0.3297285339950779,
                    0.6666666666666666,
                    0.0,
                    1.0
                ],
                [
                    0.6619866814650388,
                    0.7139649272882806,
                    0.2735294802261939,
                    0.2233335916676549,
                    0.14471968709256844,
                    0.26426426426426425,
                    0.14846470402025957,
                    7.310972740368823e-05,
                    0.170470216646648,
                    0.0,
                    0.0,
                    0.315628180335398,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.14306326304106548,
                    0.17974764756201883,
                    0.31349585317076795,
                    0.4625865447313861,
                    0.5058670143415906,
                    0.4419419419419419,
                    0.5096549540993985,
                    7.413944187416271e-05,
                    0.3659370968459451,
                    0.48402948402948404,
                    0.44285714285714284,
                    0.3546432856695241,
                    0.3333333333333333,
                    1.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "appetizers",
                "entr\u00e9e",
                "dessert"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "dessert": 0.5008476108277418,
                "appetizers": 0.3457898325365812,
                "entr\u00e9e": 0.10959829621476558
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9783712039704673,
                    1.8937690820123882,
                    2.1278597140171445
                ],
                "result_count_important_words": [
                    108000.0,
                    46900.0,
                    161000.0
                ],
                "wikipedia_search": [
                    2.3179634325370344,
                    0.8523510832332399,
                    1.8296854842297257
                ],
                "word_count_appended_bing": [
                    39.0,
                    0.0,
                    31.0
                ],
                "answer_relation_to_question_bing": [
                    0.4251497005988024,
                    2.8558597091531226,
                    0.7189905902480753
                ],
                "cosine_similarity_raw": [
                    0.12385908514261246,
                    0.08203677833080292,
                    0.09402346611022949
                ],
                "result_count_noun_chunks": [
                    971000.0,
                    71.0,
                    72.0
                ],
                "question_answer_similarity": [
                    2.7654968285933137,
                    1.966469076985959,
                    4.073109328746796
                ],
                "word_count_noun_chunks": [
                    18.0,
                    0.0,
                    9.0
                ],
                "result_count_bing": [
                    58700.0,
                    52800.0,
                    88300.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    2.0
                ],
                "result_count": [
                    53600.0,
                    22200.0,
                    77600.0
                ],
                "answer_relation_to_question": [
                    0.7798002219755826,
                    2.6479467258601552,
                    0.5722530521642619
                ],
                "word_count_appended": [
                    210.0,
                    0.0,
                    197.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What Ivy League school is located in a city with the same name?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "cornell"
            ],
            "lines": [
                [
                    0.2826873758291473,
                    0.4459287054409006,
                    0.47222055885406344,
                    0.39747811540313027,
                    0.4258804258804259,
                    0.2925311203319502,
                    0.2671957671957672,
                    0.3447098976109215,
                    0.6660353130016051,
                    0.35403726708074534,
                    0.34394904458598724,
                    0.34544495837007483,
                    0.42424242424242425,
                    0.5263157894736842,
                    1.0
                ],
                [
                    0.3985636373795022,
                    0.31129455909943715,
                    0.12675445363353802,
                    -0.40157997214475094,
                    0.13185913185913187,
                    0.25311203319502074,
                    0.3201058201058201,
                    0.2525597269624573,
                    0.05523809523809524,
                    0.3007985803016859,
                    0.34394904458598724,
                    0.32988130656591746,
                    0.30303030303030304,
                    0.15789473684210525,
                    1.0
                ],
                [
                    0.31874898679135055,
                    0.2427767354596623,
                    0.4010249875123985,
                    1.0041018567416207,
                    0.44226044226044225,
                    0.45435684647302904,
                    0.4126984126984127,
                    0.40273037542662116,
                    0.27872659176029957,
                    0.3451641526175688,
                    0.31210191082802546,
                    0.3246737350640077,
                    0.2727272727272727,
                    0.3157894736842105,
                    1.0
                ]
            ],
            "fraction_answers": {
                "cornell": 0.399189768807202,
                "dartmouth": 0.20596153261816072,
                "princeton": 0.3948486985746373
            },
            "question": "what ivy league school is located in a city with the same name?",
            "rate_limited": false,
            "answers": [
                "cornell",
                "dartmouth",
                "princeton"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "cornell": 0.460608549387696,
                "dartmouth": 0.1691569715695715,
                "princeton": 0.21285173058259743
            },
            "integer_answers": {
                "cornell": 8,
                "dartmouth": 1,
                "princeton": 5
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7272247918503743,
                    1.6494065328295873,
                    1.6233686753200385
                ],
                "result_count_important_words": [
                    1010000.0,
                    1210000.0,
                    1560000.0
                ],
                "wikipedia_search": [
                    3.3301765650080255,
                    0.2761904761904762,
                    1.393632958801498
                ],
                "word_count_appended_bing": [
                    54.0,
                    54.0,
                    49.0
                ],
                "answer_relation_to_question_bing": [
                    2.229643527204503,
                    1.5564727954971858,
                    1.2138836772983115
                ],
                "cosine_similarity_raw": [
                    0.1520576924085617,
                    0.040815651416778564,
                    0.12913231551647186
                ],
                "result_count_noun_chunks": [
                    2020000.0,
                    1480000.0,
                    2360000.0
                ],
                "question_answer_similarity": [
                    0.3621192193822935,
                    -0.36585618276149035,
                    0.9147788682021201
                ],
                "word_count_noun_chunks": [
                    14.0,
                    10.0,
                    9.0
                ],
                "result_count_bing": [
                    1410000.0,
                    1220000.0,
                    2190000.0
                ],
                "word_count_raw": [
                    10.0,
                    3.0,
                    6.0
                ],
                "result_count": [
                    1040000.0,
                    322000.0,
                    1080000.0
                ],
                "answer_relation_to_question": [
                    1.4134368791457366,
                    1.9928181868975108,
                    1.5937449339567527
                ],
                "word_count_appended": [
                    399.0,
                    339.0,
                    389.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "In the original Angry Birds game, what did the pigs do that made the birds so angry?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "steal their eggs"
            ],
            "question": "in the original angry birds game, what did the pigs do that made the birds so angry?",
            "lines": [
                [
                    0.026455026455026454,
                    0.3333333333333333,
                    0.058306735929637164,
                    0.41168828543950503,
                    0.9989799148756552,
                    0.28857715430861725,
                    0.004374134285922578,
                    0.9987354720233254,
                    0.2,
                    0.1864406779661017,
                    0.3333333333333333,
                    0.2851576041037584,
                    0,
                    0,
                    1.0
                ],
                [
                    0.8915343915343915,
                    0.5541125541125541,
                    0.874549581520976,
                    0.24105594950681863,
                    0.0006894368426606634,
                    0.3777555110220441,
                    0.991470438142451,
                    0.0007007256940808814,
                    0.8,
                    0.711864406779661,
                    0.3333333333333333,
                    0.44387931384708984,
                    0,
                    0,
                    1.0
                ],
                [
                    0.082010582010582,
                    0.11255411255411255,
                    0.06714368254938688,
                    0.34725576505367634,
                    0.0003306482816841957,
                    0.3336673346693387,
                    0.004155427571626449,
                    0.0005638022825938127,
                    0.0,
                    0.1016949152542373,
                    0.3333333333333333,
                    0.2709630820491518,
                    0,
                    0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "make too much noise",
                "steal their eggs",
                "knock down their tree"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "steal their eggs": 0.6826851169024626,
                "make too much noise": 0.07896891312850449,
                "knock down their tree": 0.07587283668186315
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9961032287263087,
                    3.1071551969296287,
                    1.8967415743440628
                ],
                "result_count_important_words": [
                    60.0,
                    13600.0,
                    57.0
                ],
                "wikipedia_search": [
                    0.4,
                    1.6,
                    0.0
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    2.333333333333333,
                    3.878787878787879,
                    0.7878787878787878
                ],
                "cosine_similarity_raw": [
                    0.01271515991538763,
                    0.19071617722511292,
                    0.014642264693975449
                ],
                "result_count_noun_chunks": [
                    124000.0,
                    87.0,
                    70.0
                ],
                "question_answer_similarity": [
                    32.83790588378906,
                    19.22758762538433,
                    27.698510095477104
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    288000.0,
                    377000.0,
                    333000.0
                ],
                "word_count_appended": [
                    11.0,
                    42.0,
                    6.0
                ],
                "answer_relation_to_question": [
                    0.18518518518518517,
                    6.2407407407407405,
                    0.5740740740740741
                ],
                "result_count": [
                    142000.0,
                    98.0,
                    47.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "By definition, an Anglophile would be most interested in which of these things?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "geometry"
            ],
            "lines": [
                [
                    0.3846153846153846,
                    1.0,
                    0.4353186990502371,
                    0.3023327550698545,
                    0.41313620105961785,
                    0.9471673810211197,
                    0.09572935319514939,
                    0.41552019037703575,
                    0.26666666666666666,
                    0.6735537190082644,
                    0.7575757575757576,
                    0.40635178845756753,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.5384615384615384,
                    0.0,
                    0.40309302202234426,
                    0.011893953872856482,
                    0.5862326186332077,
                    0.006418086122276755,
                    0.903840593619177,
                    0.5837733323417863,
                    0.7333333333333334,
                    0.2768595041322314,
                    0.18181818181818182,
                    0.4200617336577158,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.07692307692307693,
                    0.0,
                    0.1615882789274186,
                    0.685773291057289,
                    0.0006311803071744162,
                    0.046414532856603526,
                    0.00043005318567364064,
                    0.0007064772811779579,
                    0.0,
                    0.049586776859504134,
                    0.06060606060606061,
                    0.17358647788471665,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "geometry": 0.5081639913413879,
                "downton abbey": 0.3871488248345541,
                "trout fishing": 0.10468718382405795
            },
            "question": "by definition, an anglophile would be most interested in which of these things?",
            "rate_limited": false,
            "answers": [
                "geometry",
                "downton abbey",
                "trout fishing"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "geometry": 0.7970866997107331,
                "downton abbey": 0.2861712414301517,
                "trout fishing": 0.12191230027485045
            },
            "integer_answers": {
                "geometry": 5,
                "downton abbey": 6,
                "trout fishing": 1
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6254071538302701,
                    1.6802469346308633,
                    0.6943459115388666
                ],
                "result_count_important_words": [
                    197000.0,
                    1860000.0,
                    885.0
                ],
                "wikipedia_search": [
                    0.8,
                    2.2,
                    0.0
                ],
                "word_count_appended_bing": [
                    50.0,
                    12.0,
                    4.0
                ],
                "answer_relation_to_question_bing": [
                    1.0,
                    0.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.05387365072965622,
                    0.04988550394773483,
                    0.019997648894786835
                ],
                "result_count_noun_chunks": [
                    44700.0,
                    62800.0,
                    76.0
                ],
                "question_answer_similarity": [
                    2.0908243283629417,
                    0.08225429663434625,
                    4.74256082624197
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    8530000.0,
                    57800.0,
                    418000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    43200.0,
                    61300.0,
                    66.0
                ],
                "answer_relation_to_question": [
                    1.1538461538461537,
                    1.6153846153846154,
                    0.23076923076923078
                ],
                "word_count_appended": [
                    163.0,
                    67.0,
                    12.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Who is NOT considered an official member of the Eagles?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "j.d. souther"
            ],
            "lines": [
                [
                    0.36151646919959024,
                    0.41406810035842295,
                    0.40449237177288755,
                    0.6971256234132697,
                    0.3385964912280702,
                    0.315,
                    0.4998632970541206,
                    0.3861702127659574,
                    0.33333333333333337,
                    0.43607954545454547,
                    0.4666666666666667,
                    0.3609486919861699,
                    0.5,
                    0.5,
                    0.0
                ],
                [
                    0.2918350890086559,
                    0.3153449820788531,
                    0.34157071393291344,
                    0.0861530535134809,
                    0.33333333333333337,
                    0.3416666666666667,
                    0.21830908121811993,
                    0.28191489361702127,
                    0.2691658223573117,
                    0.26846590909090906,
                    0.18,
                    0.3130254311201448,
                    0.25,
                    0.125,
                    0.0
                ],
                [
                    0.3466484417917539,
                    0.270586917562724,
                    0.253936914294199,
                    0.21672132307324943,
                    0.3280701754385965,
                    0.3433333333333333,
                    0.28182762172775955,
                    0.3319148936170213,
                    0.39750084430935495,
                    0.2954545454545454,
                    0.35333333333333333,
                    0.3260258768936853,
                    0.25,
                    0.375,
                    0.0
                ]
            ],
            "fraction_answers": {
                "j.d. souther": 0.14087702810956657,
                "randy meisner": 0.4834592891517985,
                "bernie leadon": 0.3756636827386349
            },
            "question": "who is not considered an official member of the eagles?",
            "rate_limited": false,
            "answers": [
                "j.d. souther",
                "randy meisner",
                "bernie leadon"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "j.d. souther": 0.5255211406635183,
                "randy meisner": 0.15532879528263782,
                "bernie leadon": 0.1890288279777383
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1124104641106407,
                    1.4957965510388416,
                    1.3917929848505177
                ],
                "result_count_important_words": [
                    99.0,
                    204000.0,
                    158000.0
                ],
                "wikipedia_search": [
                    1.0,
                    1.3850050658561297,
                    0.6149949341438703
                ],
                "word_count_appended_bing": [
                    5.0,
                    48.0,
                    22.0
                ],
                "answer_relation_to_question_bing": [
                    0.5155913978494624,
                    1.1079301075268817,
                    1.376478494623656
                ],
                "cosine_similarity_raw": [
                    0.057853784412145615,
                    0.09596860408782959,
                    0.14905281364917755
                ],
                "result_count_noun_chunks": [
                    107000.0,
                    205000.0,
                    158000.0
                ],
                "question_answer_similarity": [
                    0.4345264742150903,
                    -0.9122479939833283,
                    -0.6244347263127565
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    3.0,
                    1.0
                ],
                "result_count_bing": [
                    3330000.0,
                    2850000.0,
                    2820000.0
                ],
                "word_count_appended": [
                    45.0,
                    163.0,
                    144.0
                ],
                "answer_relation_to_question": [
                    1.1078682464032783,
                    1.6653192879307528,
                    1.226812465665969
                ],
                "result_count": [
                    92.0,
                    95.0,
                    98.0
                ]
            },
            "integer_answers": {
                "j.d. souther": 2,
                "randy meisner": 9,
                "bernie leadon": 3
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Catherine O'Hara and Eugene Levy do NOT kiss in which Christopher Guest film?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "best in show"
            ],
            "lines": [
                [
                    0.398059126608381,
                    0.36015159021584975,
                    0.4589707075583702,
                    0.28663031782274806,
                    0.027974053052241377,
                    0.08983536148890481,
                    0.008101270905696012,
                    0.08478639930252835,
                    0.4393939393939394,
                    0.3559870550161812,
                    0.35454545454545455,
                    0.3340292429779208,
                    0.28169014084507044,
                    0.35714285714285715,
                    -1.0
                ],
                [
                    0.30131202652706157,
                    0.31047240193507025,
                    0.12911083377363192,
                    0.34781547124473333,
                    0.47586779412332525,
                    0.4363636363636364,
                    0.4959142289236453,
                    0.4660527462946818,
                    0.0896464646464647,
                    0.25889967637540456,
                    0.32727272727272727,
                    0.32885008134631233,
                    0.323943661971831,
                    0.19047619047619047,
                    -1.0
                ],
                [
                    0.3006288468645575,
                    0.3293760078490801,
                    0.41191845866799787,
                    0.3655542109325186,
                    0.4961581528244334,
                    0.47380100214745885,
                    0.49598450017065876,
                    0.4491608544027899,
                    0.47095959595959597,
                    0.3851132686084142,
                    0.3181818181818182,
                    0.33712067567576687,
                    0.3943661971830986,
                    0.4523809523809524,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "a mighty wind": 0.3597145798178978,
                "best in show": 0.45181464044626524,
                "waiting for guffman": 0.188470779735837
            },
            "question": "catherine o'hara and eugene levy do not kiss in which christopher guest film?",
            "rate_limited": false,
            "answers": [
                "best in show",
                "a mighty wind",
                "waiting for guffman"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "a mighty wind": 0.1691569715695715,
                "best in show": 0.34629323055895384,
                "waiting for guffman": 0.3312447007039207
            },
            "integer_answers": {
                "a mighty wind": 6,
                "best in show": 6,
                "waiting for guffman": 2
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.655532112353267,
                    2.7383986984590023,
                    2.60606918918773
                ],
                "result_count_important_words": [
                    490000.0,
                    4070.0,
                    4000.0
                ],
                "wikipedia_search": [
                    0.36363636363636365,
                    2.462121212121212,
                    0.17424242424242425
                ],
                "word_count_appended_bing": [
                    16.0,
                    19.0,
                    20.0
                ],
                "answer_relation_to_question_bing": [
                    1.9578777369781042,
                    2.653386372909017,
                    2.3887358901128795
                ],
                "cosine_similarity_raw": [
                    0.05103388801217079,
                    0.4613268971443176,
                    0.10955937206745148
                ],
                "result_count_noun_chunks": [
                    762000.0,
                    62300.0,
                    93300.0
                ],
                "question_answer_similarity": [
                    10.79942603642121,
                    7.702619910240173,
                    6.8047969145700336
                ],
                "word_count_noun_chunks": [
                    31.0,
                    25.0,
                    15.0
                ],
                "result_count_bing": [
                    573000.0,
                    88900.0,
                    36600.0
                ],
                "word_count_raw": [
                    6.0,
                    13.0,
                    2.0
                ],
                "result_count": [
                    489000.0,
                    25000.0,
                    3980.0
                ],
                "answer_relation_to_question": [
                    1.631053974265904,
                    3.1790075755670153,
                    3.1899384501670807
                ],
                "word_count_appended": [
                    89.0,
                    149.0,
                    71.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these bottled water brands is headquartered in France?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "evian"
            ],
            "question": "which of these bottled water brands is headquartered in france?",
            "lines": [
                [
                    0.4287037373473315,
                    0.1084474885844749,
                    0.6670029075153712,
                    -0.38915296334888616,
                    0.5663915978994749,
                    0.39444306623666586,
                    0.9001161440185831,
                    0.565987833906374,
                    0.3619047619047619,
                    0.5102040816326531,
                    0.4296875,
                    0.3548631769381644,
                    0.8571428571428571,
                    1.0,
                    -1.0
                ],
                [
                    0.07291397404745903,
                    0.3578767123287671,
                    0.10981613969543488,
                    2.6099350124097516,
                    0.2726931732933233,
                    0.21111386752666833,
                    0.06090301974448316,
                    0.2581327691087014,
                    0.0,
                    0.11352040816326531,
                    0.0390625,
                    0.2756309270244439,
                    0.047619047619047616,
                    0.0,
                    -1.0
                ],
                [
                    0.4983822886052094,
                    0.533675799086758,
                    0.2231809527891939,
                    -1.2207820490608652,
                    0.1609152288072018,
                    0.39444306623666586,
                    0.0389808362369338,
                    0.17587939698492464,
                    0.638095238095238,
                    0.3762755102040816,
                    0.53125,
                    0.36950589603739165,
                    0.09523809523809523,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "evian",
                "la croix",
                "dasani"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "la croix": 0.05363553658454364,
                "dasani": 0.526000926187827,
                "evian": 0.903379503015968
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.774315884690822,
                    1.3781546351222196,
                    1.8475294801869582
                ],
                "result_count_important_words": [
                    1240000.0,
                    83900.0,
                    53700.0
                ],
                "wikipedia_search": [
                    1.0857142857142856,
                    0.0,
                    1.9142857142857141
                ],
                "word_count_appended_bing": [
                    55.0,
                    5.0,
                    68.0
                ],
                "answer_relation_to_question_bing": [
                    0.4337899543378996,
                    1.4315068493150684,
                    2.134703196347032
                ],
                "cosine_similarity_raw": [
                    0.25088149309158325,
                    0.04130542278289795,
                    0.0839456170797348
                ],
                "result_count_noun_chunks": [
                    214000.0,
                    97600.0,
                    66500.0
                ],
                "question_answer_similarity": [
                    -0.18803596124053001,
                    1.261102150194347,
                    -0.5898732572532026
                ],
                "word_count_noun_chunks": [
                    54.0,
                    3.0,
                    6.0
                ],
                "result_count_bing": [
                    1590000.0,
                    851000.0,
                    1590000.0
                ],
                "word_count_raw": [
                    3.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    151000.0,
                    72700.0,
                    42900.0
                ],
                "answer_relation_to_question": [
                    1.714814949389326,
                    0.2916558961898361,
                    1.9935291544208376
                ],
                "word_count_appended": [
                    400.0,
                    89.0,
                    295.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "The \u201cS\u201d in \u201cUSB cable\u201d shares its name with a well-known what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "podcast"
            ],
            "lines": [
                [
                    0.1111111111111111,
                    0.0,
                    0.19900551311865466,
                    0.1889586613394305,
                    0.15083043646195443,
                    0.14730878186968838,
                    0.28619528619528617,
                    0.28978622327790976,
                    0.4215686274509804,
                    0.38875305623471884,
                    0.42105263157894735,
                    0.3713543902936298,
                    0,
                    0,
                    1.0
                ],
                [
                    0.4444444444444444,
                    1.0,
                    0.4349176203265923,
                    0.22808708332736752,
                    0.8265739667825415,
                    0.2507082152974504,
                    0.47474747474747475,
                    0.3681710213776722,
                    0.5686274509803921,
                    0.5843520782396088,
                    0.5263157894736842,
                    0.4383028117197107,
                    0,
                    0,
                    1.0
                ],
                [
                    0.4444444444444444,
                    0.0,
                    0.366076866554753,
                    0.582954255333202,
                    0.022595596755504054,
                    0.6019830028328612,
                    0.23905723905723905,
                    0.342042755344418,
                    0.00980392156862745,
                    0.02689486552567237,
                    0.05263157894736842,
                    0.19034279798665948,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "sports team": 0.2399022770292291,
                "podcast": 0.5121039963930782,
                "airline": 0.2479937265776926
            },
            "question": "the \u201cs\u201d in \u201cusb cable\u201d shares its name with a well-known what?",
            "rate_limited": false,
            "answers": [
                "airline",
                "podcast",
                "sports team"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sports team": 0.11730857945046533,
                "podcast": 0.5203059655593976,
                "airline": 0.30614754159833957
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.4854175611745193,
                    1.7532112468788428,
                    0.7613711919466379
                ],
                "result_count_important_words": [
                    170000.0,
                    282000.0,
                    142000.0
                ],
                "wikipedia_search": [
                    0.4215686274509804,
                    0.5686274509803921,
                    0.00980392156862745
                ],
                "word_count_appended_bing": [
                    40.0,
                    50.0,
                    5.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    1.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.037452664226293564,
                    0.08185111731290817,
                    0.06889534741640091
                ],
                "result_count_noun_chunks": [
                    122000.0,
                    155000.0,
                    144000.0
                ],
                "question_answer_similarity": [
                    2.8294338546693325,
                    3.4153359830379486,
                    8.729054778814316
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    1040000.0,
                    1770000.0,
                    4250000.0
                ],
                "word_count_appended": [
                    159.0,
                    239.0,
                    11.0
                ],
                "answer_relation_to_question": [
                    0.3333333333333333,
                    1.3333333333333333,
                    1.3333333333333333
                ],
                "result_count": [
                    7810.0,
                    42800.0,
                    1170.0
                ]
            },
            "integer_answers": {
                "sports team": 2,
                "podcast": 10,
                "airline": 0
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Queen Victoria is credited with starting what fashion trend?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "white wedding dress"
            ],
            "lines": [
                [
                    0.18149717514124294,
                    0.3055555555555555,
                    0.03736106737732742,
                    0.22234368675636973,
                    0.4153846153846154,
                    0.34602829162132753,
                    0.9903169014084507,
                    0.12735012415750266,
                    0.5936507936507937,
                    0.2524271844660194,
                    0.29545454545454547,
                    0.37742603764297167,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.28115969415937436,
                    0.36111111111111116,
                    0.11452274249913501,
                    0.40554760076526175,
                    0.2,
                    0.352557127312296,
                    0.0044014084507042256,
                    0.24122029088329194,
                    0.11904761904761905,
                    0.3106796116504854,
                    0.3409090909090909,
                    0.2093147586555503,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.5373431306993827,
                    0.33333333333333337,
                    0.8481161901235376,
                    0.3721087124783685,
                    0.38461538461538464,
                    0.3014145810663765,
                    0.00528169014084507,
                    0.6314295849592054,
                    0.2873015873015873,
                    0.4368932038834951,
                    0.36363636363636365,
                    0.413259203701478,
                    0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "white wedding dress": 0.45497945891841207,
                "mini dress": 0.318830459893594,
                "little black dress": 0.22619008118799386
            },
            "question": "queen victoria is credited with starting what fashion trend?",
            "rate_limited": false,
            "answers": [
                "mini dress",
                "little black dress",
                "white wedding dress"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "white wedding dress": 0.6291834999797511,
                "mini dress": 0.07658598927411717,
                "little black dress": 0.037049364938785204
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.26455622585783,
                    1.2558885519333018,
                    2.479555222208868
                ],
                "result_count_important_words": [
                    18000.0,
                    80.0,
                    96.0
                ],
                "wikipedia_search": [
                    2.3746031746031746,
                    0.4761904761904762,
                    1.1492063492063491
                ],
                "word_count_appended_bing": [
                    13.0,
                    15.0,
                    16.0
                ],
                "answer_relation_to_question_bing": [
                    1.833333333333333,
                    2.1666666666666665,
                    1.9999999999999998
                ],
                "cosine_similarity_raw": [
                    0.015078761614859104,
                    0.046220872551202774,
                    0.34229594469070435
                ],
                "result_count_noun_chunks": [
                    718000.0,
                    1360000.0,
                    3560000.0
                ],
                "question_answer_similarity": [
                    4.946093179285526,
                    9.021511927247047,
                    8.277655154466629
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    2.0
                ],
                "result_count_bing": [
                    318000.0,
                    324000.0,
                    277000.0
                ],
                "word_count_appended": [
                    26.0,
                    32.0,
                    45.0
                ],
                "answer_relation_to_question": [
                    1.0889830508474576,
                    1.6869581649562462,
                    3.224058784196296
                ],
                "result_count": [
                    54.0,
                    26.0,
                    50.0
                ]
            },
            "integer_answers": {
                "white wedding dress": 7,
                "mini dress": 3,
                "little black dress": 3
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "How do you let someone on Tinder know you're interested?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "swipe right"
            ],
            "lines": [
                [
                    0.5175868593051256,
                    0.5357142857142857,
                    0.806155830002902,
                    0.2599849600624926,
                    0.999567116603046,
                    0.4589028936623469,
                    0.09090523846562894,
                    0.5343855784977872,
                    0.9128571428571428,
                    0.8333333333333334,
                    0.7142857142857143,
                    0.7148147806196916,
                    1.0,
                    1.0,
                    5.0
                ],
                [
                    0.16546267629858963,
                    0.11904761904761905,
                    0.09372904866618545,
                    0.5174167924346847,
                    0.0,
                    0.0857447186224037,
                    0.9090523846562893,
                    0.46530151131556385,
                    0.06469387755102039,
                    0.027777777777777776,
                    0.14285714285714285,
                    0.07023841214029874,
                    0.0,
                    0.0,
                    5.0
                ],
                [
                    0.3169504643962849,
                    0.3452380952380953,
                    0.10011512133091251,
                    0.22259824750282267,
                    0.00043288339695407503,
                    0.45535238771524944,
                    4.2376878081721756e-05,
                    0.00031291018664889443,
                    0.02244897959183673,
                    0.1388888888888889,
                    0.14285714285714285,
                    0.21494680724000959,
                    0.0,
                    0.0,
                    5.0
                ]
            ],
            "fraction_answers": {
                "draw circle around face": 0.19009442581196967,
                "shake phone": 0.14001316465878053,
                "swipe right": 0.6698924095292498
            },
            "question": "how do you let someone on tinder know you're interested?",
            "rate_limited": false,
            "answers": [
                "swipe right",
                "draw circle around face",
                "shake phone"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "draw circle around face": 0.10333065815224987,
                "shake phone": 0.11315285427801795,
                "swipe right": 0.7906221929744528
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 5
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.574073903098458,
                    0.3511920607014937,
                    1.074734036200048
                ],
                "result_count_important_words": [
                    133000.0,
                    1330000.0,
                    62.0
                ],
                "wikipedia_search": [
                    4.564285714285715,
                    0.32346938775510203,
                    0.11224489795918367
                ],
                "answer_relation_to_question": [
                    2.070347437220502,
                    0.6618507051943584,
                    1.2678018575851393
                ],
                "answer_relation_to_question_bing": [
                    1.6071428571428572,
                    0.35714285714285715,
                    1.0357142857142858
                ],
                "word_count_appended": [
                    90.0,
                    3.0,
                    15.0
                ],
                "cosine_similarity_raw": [
                    0.25540950894355774,
                    0.02969561144709587,
                    0.03171887248754501
                ],
                "result_count_noun_chunks": [
                    2630000.0,
                    2290000.0,
                    1540.0
                ],
                "question_answer_similarity": [
                    8.666833512485027,
                    17.248556206934154,
                    7.420513674383983
                ],
                "word_count_noun_chunks": [
                    14.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    3.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    51700000.0,
                    9660000.0,
                    51300000.0
                ],
                "result_count": [
                    127000.0,
                    0,
                    55.0
                ],
                "word_count_appended_bing": [
                    10.0,
                    2.0,
                    2.0
                ]
            },
            "integer_answers": {
                "draw circle around face": 2,
                "shake phone": 0,
                "swipe right": 12
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these utensils is tined?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "fork"
            ],
            "lines": [
                [
                    0.0,
                    0.0,
                    0.25349380597795457,
                    0.32500422247505123,
                    0.057580332111278845,
                    0.8702290076335878,
                    0.4068965517241379,
                    0.01954921225217981,
                    0.8333333333333334,
                    0.4087423312883436,
                    0.4695121951219512,
                    0.3348555407597997,
                    0.3582089552238806,
                    0.6179775280898876,
                    -1.0
                ],
                [
                    1.0,
                    1.0,
                    0.5245143213330276,
                    0.35042849358865497,
                    0.46150528358852705,
                    0.09648854961832061,
                    0.28620689655172415,
                    0.9598713470864638,
                    0.16666666666666669,
                    0.27223926380368096,
                    0.18902439024390244,
                    0.32662634196251905,
                    0.04477611940298507,
                    0.14606741573033707,
                    -1.0
                ],
                [
                    0.0,
                    0.0,
                    0.22199187268901785,
                    0.3245672839362938,
                    0.48091438430019406,
                    0.033282442748091605,
                    0.30689655172413793,
                    0.020579440661356384,
                    0.0,
                    0.31901840490797545,
                    0.34146341463414637,
                    0.3385181172776812,
                    0.5970149253731343,
                    0.23595505617977527,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "fork": 0.3539559297136705,
                "spoon": 0.2300144210308432,
                "knife": 0.41602964925548636
            },
            "question": "which of these utensils is tined?",
            "rate_limited": false,
            "answers": [
                "fork",
                "knife",
                "spoon"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "fork": 0.4266555500301792,
                "spoon": 0.11395905726861419,
                "knife": 0.24686074842117361
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.6697110815195993,
                    0.6532526839250381,
                    0.6770362345553624
                ],
                "result_count_important_words": [
                    354000.0,
                    249000.0,
                    267000.0
                ],
                "wikipedia_search": [
                    1.6666666666666665,
                    0.3333333333333333,
                    0.0
                ],
                "word_count_appended_bing": [
                    77.0,
                    31.0,
                    56.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    1.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.11733929812908173,
                    0.24279150366783142,
                    0.10275742411613464
                ],
                "result_count_noun_chunks": [
                    7780000.0,
                    382000000.0,
                    8190000.0
                ],
                "question_answer_similarity": [
                    1.3906403183937073,
                    1.4994266480207443,
                    1.38877072930336
                ],
                "word_count_noun_chunks": [
                    24.0,
                    3.0,
                    40.0
                ],
                "word_count_raw": [
                    55.0,
                    13.0,
                    21.0
                ],
                "result_count_bing": [
                    2850000.0,
                    316000.0,
                    109000.0
                ],
                "word_count_appended": [
                    533.0,
                    355.0,
                    416.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count": [
                    267000.0,
                    2140000.0,
                    2230000.0
                ]
            },
            "integer_answers": {
                "fork": 6,
                "spoon": 3,
                "knife": 5
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Albert Einstein won a Nobel Prize for his work on what scientific phenomenon?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "photoelectric effect"
            ],
            "question": "albert einstein won a nobel prize for his work on what scientific phenomenon?",
            "lines": [
                [
                    0.24420961207163305,
                    0.18571170606873705,
                    0.2414393050938356,
                    0.3884370563351825,
                    0.33016627078384797,
                    0.6264119942493325,
                    0.014077425842131725,
                    0.9835947876655777,
                    0.37366452991453,
                    0.22058823529411764,
                    0.09090909090909091,
                    0.33345313660053405,
                    0.1891891891891892,
                    0.16666666666666666,
                    1.0
                ],
                [
                    0.43826859466842716,
                    0.6338330187374972,
                    0.6278431142090111,
                    0.23097750770975134,
                    0.24782264449722882,
                    0.18874512220168413,
                    0.08597285067873303,
                    0.005276950138205837,
                    0.43643162393162394,
                    0.5833333333333334,
                    0.8181818181818182,
                    0.3392285411048448,
                    0.7837837837837838,
                    0.75,
                    1.0
                ],
                [
                    0.3175217932599398,
                    0.18045527519376564,
                    0.13071758069715336,
                    0.38058543595506616,
                    0.4220110847189232,
                    0.18484288354898337,
                    0.8999497234791353,
                    0.01112826219621639,
                    0.18990384615384617,
                    0.19607843137254902,
                    0.09090909090909091,
                    0.32731832229462116,
                    0.02702702702702703,
                    0.08333333333333333,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "general relativity",
                "photoelectric effect",
                "special relativity"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "special relativity": 0.14673835108995562,
                "general relativity": 0.23311381279711046,
                "photoelectric effect": 0.8941198245920544
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.6676250928042724,
                    2.7138283288387584,
                    2.6185465783569692
                ],
                "result_count_important_words": [
                    28000.0,
                    171000.0,
                    1790000.0
                ],
                "wikipedia_search": [
                    2.9893162393162394,
                    3.491452991452991,
                    1.5192307692307692
                ],
                "answer_relation_to_question": [
                    1.9536768965730644,
                    3.5061487573474173,
                    2.5401743460795183
                ],
                "word_count_appended_bing": [
                    2.0,
                    18.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.1142702364124224,
                    3.802998112424984,
                    1.082731651162594
                ],
                "cosine_similarity_raw": [
                    0.11090030521154404,
                    0.28838714957237244,
                    0.060042500495910645
                ],
                "result_count_noun_chunks": [
                    2740000.0,
                    14700.0,
                    31000.0
                ],
                "question_answer_similarity": [
                    6.774048951454461,
                    4.028073322027922,
                    6.63712262082845
                ],
                "word_count_noun_chunks": [
                    7.0,
                    29.0,
                    1.0
                ],
                "result_count_bing": [
                    305000.0,
                    91900.0,
                    90000.0
                ],
                "word_count_raw": [
                    2.0,
                    9.0,
                    1.0
                ],
                "result_count": [
                    41700.0,
                    31300.0,
                    53300.0
                ],
                "word_count_appended": [
                    45.0,
                    119.0,
                    40.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What did Yankee Doodle stick in his cap?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "feather"
            ],
            "question": "what did yankee doodle stick in his cap?",
            "lines": [
                [
                    0.32,
                    0.16428571428571428,
                    0.9076932769663477,
                    0.257263365662809,
                    0.41334873259984195,
                    0.30090819564503773,
                    0.43812170860152133,
                    0.38424657534246576,
                    0.2567567567567568,
                    0.5780730897009967,
                    0.647887323943662,
                    0.47220876589901845,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.023809523809523808,
                    0.16428571428571428,
                    0.03136735602537429,
                    0.44615722471499186,
                    0.003100115494498815,
                    0.06116642958748222,
                    0.05280866003510825,
                    0.2232876712328767,
                    0.6216216216216217,
                    0.044850498338870434,
                    0.08450704225352113,
                    0.20556515483722265,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.6561904761904762,
                    0.6714285714285715,
                    0.060939367008278005,
                    0.29657940962219914,
                    0.5835511519056592,
                    0.63792537476748,
                    0.5090696313633704,
                    0.39246575342465756,
                    0.12162162162162164,
                    0.3770764119601329,
                    0.2676056338028169,
                    0.3222260792637589,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "feather",
                "noodle soup",
                "duck"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "feather": 0.7783667384251826,
                "noodle soup": 0.12028618117953958,
                "duck": 0.18127460351028826
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.8888350635960738,
                    0.8222606193488906,
                    1.2889043170550356
                ],
                "result_count_important_words": [
                    59900.0,
                    7220.0,
                    69600.0
                ],
                "wikipedia_search": [
                    0.5135135135135135,
                    1.2432432432432432,
                    0.24324324324324326
                ],
                "word_count_appended_bing": [
                    92.0,
                    12.0,
                    38.0
                ],
                "answer_relation_to_question_bing": [
                    0.32857142857142857,
                    0.32857142857142857,
                    1.342857142857143
                ],
                "cosine_similarity_raw": [
                    0.6004437804222107,
                    0.020749667659401894,
                    0.04031170532107353
                ],
                "result_count_noun_chunks": [
                    561000.0,
                    326000.0,
                    573000.0
                ],
                "question_answer_similarity": [
                    1.9004967287182808,
                    3.2959233969449997,
                    2.1909384429454803
                ],
                "word_count_noun_chunks": [
                    153.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    275000.0,
                    55900.0,
                    583000.0
                ],
                "word_count_raw": [
                    56.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    1020000.0,
                    7650.0,
                    1440000.0
                ],
                "answer_relation_to_question": [
                    0.64,
                    0.047619047619047616,
                    1.3123809523809524
                ],
                "word_count_appended": [
                    348.0,
                    27.0,
                    227.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these is NOT a skin care brand?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "kristin ess"
            ],
            "lines": [
                [
                    0.2249806164943976,
                    0.21261582323592304,
                    0.30846336459861035,
                    0.15931768978302185,
                    0.3918032786885246,
                    0.3328173374613003,
                    0.21455756422454803,
                    0.21793635486981677,
                    0.5,
                    0.339323467230444,
                    0.3854166666666667,
                    0.2944018896934909,
                    0.5,
                    0,
                    -1.0
                ],
                [
                    0.4554379303618448,
                    0.4641304347826087,
                    0.40008827932463287,
                    0.524452446262377,
                    0.2169398907103825,
                    0.33436532507739936,
                    0.35965746907706947,
                    0.35776277724204436,
                    0.3232456140350878,
                    0.3583509513742072,
                    0.30208333333333337,
                    0.3987892849375494,
                    0.5,
                    0,
                    -1.0
                ],
                [
                    0.31958145314375763,
                    0.3232537419814683,
                    0.2914483560767568,
                    0.3162298639546012,
                    0.3912568306010929,
                    0.3328173374613003,
                    0.4257849666983825,
                    0.42430086788813887,
                    0.17675438596491228,
                    0.3023255813953488,
                    0.3125,
                    0.3068088253689597,
                    0.0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "kristin ess": 0.23149173284330204,
                "dr. dennis gross": 0.37205629954665476,
                "drunk elephant": 0.39645196761004325
            },
            "question": "which of these is not a skin care brand?",
            "rate_limited": false,
            "answers": [
                "dr. dennis gross",
                "kristin ess",
                "drunk elephant"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "kristin ess": 0.42267916692500057,
                "dr. dennis gross": 0.19464335480584577,
                "drunk elephant": 0.13445441139038655
            },
            "integer_answers": {
                "kristin ess": 2,
                "dr. dennis gross": 7,
                "drunk elephant": 4
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2335886618390544,
                    0.6072642903747035,
                    1.159147047786242
                ],
                "result_count_important_words": [
                    1200000.0,
                    590000.0,
                    312000.0
                ],
                "wikipedia_search": [
                    0.0,
                    1.0605263157894735,
                    1.9394736842105262
                ],
                "word_count_appended_bing": [
                    11.0,
                    19.0,
                    18.0
                ],
                "answer_relation_to_question_bing": [
                    1.7243050605844616,
                    0.2152173913043478,
                    1.0604775481111903
                ],
                "cosine_similarity_raw": [
                    0.054375119507312775,
                    0.028363825753331184,
                    0.059205491095781326
                ],
                "result_count_noun_chunks": [
                    1170000.0,
                    590000.0,
                    314000.0
                ],
                "question_answer_similarity": [
                    7.580420333892107,
                    -0.5440840786322951,
                    4.089014410972595
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    2.0
                ],
                "result_count_bing": [
                    10800000.0,
                    10700000.0,
                    10800000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    198000.0,
                    518000.0,
                    199000.0
                ],
                "answer_relation_to_question": [
                    1.6501163010336144,
                    0.2673724178289312,
                    1.0825112811374544
                ],
                "word_count_appended": [
                    152.0,
                    134.0,
                    187.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which famous math equation remains unsolved?": {
        "raw_data": {
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7179575789003594,
                    1.6946555297491697,
                    1.5873868913504707
                ],
                "result_count_important_words": [
                    8100.0,
                    3420.0,
                    6740.0
                ],
                "wikipedia_search": [
                    1.6473228873929808,
                    1.6787832494608197,
                    1.6738938631461995
                ],
                "word_count_appended_bing": [
                    6.0,
                    0.0,
                    9.0
                ],
                "answer_relation_to_question_bing": [
                    1.3912393162393162,
                    1.0217948717948717,
                    1.5869658119658119
                ],
                "word_count_appended": [
                    2.0,
                    0.0,
                    118.0
                ],
                "cosine_similarity_raw": [
                    0.18747636675834656,
                    0.10419890284538269,
                    0.1942000538110733
                ],
                "result_count_noun_chunks": [
                    69900.0,
                    125000.0,
                    63900.0
                ],
                "question_answer_similarity": [
                    1.8765289783477783,
                    1.6885697767138481,
                    5.415198208764195
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    71800.0,
                    69400.0,
                    181000.0
                ],
                "result_count": [
                    8120.0,
                    1400.0,
                    6720.0
                ],
                "answer_relation_to_question": [
                    2.436777981322443,
                    0.5304567430239268,
                    2.0327652756536305
                ]
            },
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "z-best_answer_by_ml": [
                "fermat's last theorem"
            ],
            "lines": [
                [
                    0.4873555962644886,
                    0.34780982905982905,
                    0.3858528262780158,
                    0.20896068202496634,
                    0.5,
                    0.2228429546865301,
                    0.44359255202628695,
                    0.27009273570324577,
                    0.32946457747859614,
                    0.016666666666666666,
                    0.4,
                    0.34359151578007185,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.10609134860478535,
                    0.25544871794871793,
                    0.2144560503979861,
                    0.18803050539594593,
                    0.08620689655172414,
                    0.2153941651148355,
                    0.18729463307776562,
                    0.48299845440494593,
                    0.3357566498921639,
                    0.0,
                    0.0,
                    0.33893110594983394,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.4065530551307261,
                    0.39674145299145297,
                    0.3996911233239981,
                    0.6030088125790877,
                    0.41379310344827586,
                    0.5617628801986344,
                    0.36911281489594744,
                    0.24690880989180836,
                    0.3347787726292399,
                    0.9833333333333333,
                    0.6,
                    0.31747737827009415,
                    1.0,
                    1.0,
                    -1.0
                ]
            ],
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "question": "which famous math equation remains unsolved?",
            "rate_limited": false,
            "answers": [
                "reimann hypothesis",
                "poincar\u00e9 conjecture",
                "fermat's last theorem"
            ],
            "ml_answers": {
                "reimann hypothesis": 0.5433490951142239,
                "poincar\u00e9 conjecture": 0.08626331860235377,
                "fermat's last theorem": 0.6917769203799655
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these is a U.S. postage stamp?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "forever stamp"
            ],
            "lines": [
                [
                    0.09170591313448456,
                    0.20782312925170068,
                    0.021421084261492605,
                    0.19778120577025707,
                    0.010238224038789044,
                    0.4454609429978888,
                    3.1763246708964083e-05,
                    6.19083928189399e-05,
                    0.046468401486988845,
                    0.28308823529411764,
                    0.3888888888888889,
                    0.23965737444036073,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.7736787022501308,
                    0.5792517006802721,
                    0.8400866635238591,
                    0.4370629939845183,
                    0.2885135541263449,
                    0.09289232934553132,
                    0.054724629872070656,
                    0.04388443035266625,
                    0.6988847583643123,
                    0.33455882352941174,
                    0.4444444444444444,
                    0.39091027805334017,
                    1.0,
                    0,
                    -1.0
                ],
                [
                    0.1346153846153846,
                    0.21292517006802722,
                    0.13849225221464834,
                    0.3651558002452246,
                    0.701248221834866,
                    0.4616467276565799,
                    0.9452436068812203,
                    0.9560536612545149,
                    0.25464684014869887,
                    0.38235294117647056,
                    0.16666666666666666,
                    0.36943234750629905,
                    0.0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "terence stamp": 0.14866362086188437,
                "forever stamp": 0.4599148698866848,
                "rubber stamp": 0.3914215092514309
            },
            "question": "which of these is a u.s. postage stamp?",
            "rate_limited": false,
            "answers": [
                "terence stamp",
                "forever stamp",
                "rubber stamp"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "terence stamp": 0.1260103156444987,
                "forever stamp": 0.5377733909217588,
                "rubber stamp": 0.25373361824932844
            },
            "integer_answers": {
                "terence stamp": 0,
                "forever stamp": 8,
                "rubber stamp": 5
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.7189721233210822,
                    1.1727308341600204,
                    1.1082970425188972
                ],
                "result_count_important_words": [
                    83.0,
                    143000.0,
                    2470000.0
                ],
                "wikipedia_search": [
                    0.09293680297397769,
                    1.3977695167286246,
                    0.5092936802973977
                ],
                "word_count_appended_bing": [
                    7.0,
                    8.0,
                    3.0
                ],
                "answer_relation_to_question_bing": [
                    0.6234693877551021,
                    1.7377551020408162,
                    0.6387755102040816
                ],
                "cosine_similarity_raw": [
                    0.016785720363259315,
                    0.658298134803772,
                    0.10852355509996414
                ],
                "result_count_noun_chunks": [
                    79.0,
                    56000.0,
                    1220000.0
                ],
                "question_answer_similarity": [
                    2.799171890132129,
                    6.185696169734001,
                    5.168002933263779
                ],
                "word_count_noun_chunks": [
                    0.0,
                    5.0,
                    0.0
                ],
                "result_count_bing": [
                    6330000.0,
                    1320000.0,
                    6560000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    51100.0,
                    1440000.0,
                    3500000.0
                ],
                "answer_relation_to_question": [
                    0.2751177394034537,
                    2.3210361067503924,
                    0.40384615384615385
                ],
                "word_count_appended": [
                    77.0,
                    91.0,
                    104.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which scientific plant name is one of the 11 herbs and spices in Colonel Sanders' \u201coriginal recipe\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "zingiber officinale"
            ],
            "question": "which scientific plant name is one of the 11 herbs and spices in colonel sanders' \u201coriginal recipe\u201d?",
            "lines": [
                [
                    0.33903870665939745,
                    0.4805068226120858,
                    0.4586243544942367,
                    0.1860688788809157,
                    0.5675675675675675,
                    0.13251961639058413,
                    0.7736804049168474,
                    0.20610687022900764,
                    0.45453839288527637,
                    0.4166666666666667,
                    0.2222222222222222,
                    0.385605745306912,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.29662918995084064,
                    0.21393762183235865,
                    0.3520517295530894,
                    0.8139311211190843,
                    0.13513513513513514,
                    0.5841325196163906,
                    0.028199566160520606,
                    0.7022900763358778,
                    0.34032917637253685,
                    0.25,
                    0.5555555555555556,
                    0.270270061562166,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.36433210338976196,
                    0.3055555555555555,
                    0.1893239159526739,
                    -0.0,
                    0.2972972972972973,
                    0.2833478639930253,
                    0.19812002892263195,
                    0.0916030534351145,
                    0.20513243074218682,
                    0.3333333333333333,
                    0.2222222222222222,
                    0.3441241931309219,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "zingiber officinale",
                "pimenta dioica",
                "petroselinum crispum"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "pimenta dioica": 0.13505240119505785,
                "petroselinum crispum": 0.255793758027924,
                "zingiber officinale": 0.6646840021837105
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.470451707762208,
                    2.432430554059494,
                    3.097117738178297
                ],
                "result_count_important_words": [
                    2140.0,
                    78.0,
                    548.0
                ],
                "wikipedia_search": [
                    2.727230357311658,
                    2.041975058235221,
                    1.230794584453121
                ],
                "word_count_appended_bing": [
                    2.0,
                    5.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    2.883040935672515,
                    1.283625730994152,
                    1.833333333333333
                ],
                "cosine_similarity_raw": [
                    0.027250373736023903,
                    0.02091808058321476,
                    0.011249179020524025
                ],
                "result_count_noun_chunks": [
                    27.0,
                    92.0,
                    12.0
                ],
                "question_answer_similarity": [
                    -0.8044166648760438,
                    -3.518803154118359,
                    0.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    3040.0,
                    13400.0,
                    6500.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    21.0,
                    5.0,
                    11.0
                ],
                "answer_relation_to_question": [
                    2.034232239956385,
                    1.7797751397050439,
                    2.185992620338572
                ],
                "word_count_appended": [
                    5.0,
                    3.0,
                    4.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Blue spruce, red cedar & white pine are all kinds of what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "evergreen trees"
            ],
            "lines": [
                [
                    0.25914379399402443,
                    0.1629115947931279,
                    0.03544457386853066,
                    0.24184512436559077,
                    0.3342541436464088,
                    0.15117014547754587,
                    0.0010601158816325785,
                    0.048764066557660864,
                    0.298035298035298,
                    0.0,
                    0,
                    0.3333333333333333,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.05233706385780119,
                    0.11730659004178166,
                    0.05462827215823001,
                    0.13596316450215923,
                    0.3314917127071823,
                    0.6957621758380772,
                    0.0009687265814918389,
                    0.0009618159084351255,
                    0.06953046953046953,
                    0.0,
                    0,
                    0.3333333333333333,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.6885191421481744,
                    0.7197818151650904,
                    0.9099271539732393,
                    0.62219171113225,
                    0.3342541436464088,
                    0.15306767868437698,
                    0.9979711575368756,
                    0.950274117533904,
                    0.6324342324342325,
                    1.0,
                    0,
                    0.3333333333333333,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "marvel supervillains": 0.13786794803530472,
                "evergreen trees": 0.7185964988913759,
                "colgate flavors": 0.1435355530733195
            },
            "question": "blue spruce, red cedar & white pine are all kinds of what?",
            "rate_limited": false,
            "answers": [
                "colgate flavors",
                "marvel supervillains",
                "evergreen trees"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "marvel supervillains": 0.17843876458768138,
                "evergreen trees": 0.9232052724579368,
                "colgate flavors": 0.2811653746051813
            },
            "integer_answers": {
                "marvel supervillains": 1,
                "evergreen trees": 10,
                "colgate flavors": 2
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.333333333333333,
                    2.333333333333333,
                    2.333333333333333
                ],
                "result_count_important_words": [
                    58.0,
                    53.0,
                    54600.0
                ],
                "wikipedia_search": [
                    2.086247086247086,
                    0.48671328671328673,
                    4.427039627039627
                ],
                "word_count_appended_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "answer_relation_to_question_bing": [
                    1.1403811635518952,
                    0.8211461302924716,
                    5.038472706155633
                ],
                "cosine_similarity_raw": [
                    0.010836290195584297,
                    0.01670122519135475,
                    0.27818742394447327
                ],
                "result_count_noun_chunks": [
                    5070.0,
                    100.0,
                    98800.0
                ],
                "question_answer_similarity": [
                    3.01923155086115,
                    1.6973849569913,
                    7.76753655821085
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    2.0
                ],
                "result_count_bing": [
                    239000.0,
                    1100000.0,
                    242000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    3.0
                ],
                "result_count": [
                    1210000.0,
                    1200000.0,
                    1210000.0
                ],
                "answer_relation_to_question": [
                    1.8140065579581708,
                    0.36635944700460826,
                    4.81963399503722
                ],
                "word_count_appended": [
                    0.0,
                    0.0,
                    9.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these actors was a high school cheerleader?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "michael douglas"
            ],
            "lines": [
                [
                    0.3644688644688645,
                    0.2,
                    0.2447598026784328,
                    0.2644841029754693,
                    0.13405047579644186,
                    0.3338632750397456,
                    0.30392156862745096,
                    0.14762623720852205,
                    0.36481481481481487,
                    0.3973509933774834,
                    0.23529411764705882,
                    0.3191827414903692,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.21520146520146521,
                    0.275,
                    0.5605042145066768,
                    0.4041569489050635,
                    0.06495655771617707,
                    0.3338632750397456,
                    0.22450980392156863,
                    0.07062573393725885,
                    0.10555555555555556,
                    0.33112582781456956,
                    0.0392156862745098,
                    0.3153674310773668,
                    1.0,
                    0,
                    -1.0
                ],
                [
                    0.42032967032967034,
                    0.525,
                    0.19473598281489043,
                    0.3313589481194672,
                    0.800992966487381,
                    0.3322734499205087,
                    0.4715686274509804,
                    0.7817480288542191,
                    0.5296296296296297,
                    0.271523178807947,
                    0.7254901960784313,
                    0.36544982743226406,
                    0.0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "john travolta": 0.4423154235327222,
                "george clooney": 0.25460130724035795,
                "michael douglas": 0.3030832692269198
            },
            "question": "which of these actors was a high school cheerleader?",
            "rate_limited": false,
            "answers": [
                "george clooney",
                "michael douglas",
                "john travolta"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "john travolta": 0.29179235509667106,
                "george clooney": 0.21354534493052876,
                "michael douglas": 0.3146608826667013
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2767309659614767,
                    1.2614697243094672,
                    1.4617993097290563
                ],
                "result_count_important_words": [
                    310000.0,
                    229000.0,
                    481000.0
                ],
                "wikipedia_search": [
                    1.0944444444444446,
                    0.31666666666666665,
                    1.588888888888889
                ],
                "word_count_appended_bing": [
                    12.0,
                    2.0,
                    37.0
                ],
                "answer_relation_to_question_bing": [
                    0.4,
                    0.55,
                    1.05
                ],
                "cosine_similarity_raw": [
                    0.05280975252389908,
                    0.12093525379896164,
                    0.04201653599739075
                ],
                "result_count_noun_chunks": [
                    176000.0,
                    84200.0,
                    932000.0
                ],
                "question_answer_similarity": [
                    1.4658440416678786,
                    2.239949580281973,
                    1.8364829276688397
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    21000000.0,
                    21000000.0,
                    20900000.0
                ],
                "result_count": [
                    162000.0,
                    78500.0,
                    968000.0
                ],
                "answer_relation_to_question": [
                    1.0934065934065935,
                    0.6456043956043956,
                    1.260989010989011
                ],
                "word_count_appended": [
                    60.0,
                    50.0,
                    41.0
                ]
            },
            "integer_answers": {
                "john travolta": 8,
                "george clooney": 2,
                "michael douglas": 3
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What did the first-ever State of the Union \u201cHero in the Balcony\u201d do to earn his invitation?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "rescue a drowning woman"
            ],
            "lines": [
                [
                    0.3883228840125392,
                    0.14492753623188406,
                    0.14997854762669421,
                    0.3212687211876556,
                    0.4166666666666667,
                    0.33519553072625696,
                    0.08333333333333333,
                    0.09806295399515738,
                    0.0,
                    0.3,
                    0.3333333333333333,
                    0.35855319193783125,
                    0,
                    0,
                    1.0
                ],
                [
                    0.40287729511867443,
                    0.4492753623188406,
                    0.5707637904043806,
                    0.2893956254529456,
                    0.0,
                    0.28212290502793297,
                    0.8333333333333334,
                    0.2602905569007264,
                    0.75,
                    0.15,
                    0.3333333333333333,
                    0.18126523968156794,
                    0,
                    0,
                    1.0
                ],
                [
                    0.2087998208687864,
                    0.4057971014492754,
                    0.2792576619689252,
                    0.38933565335939874,
                    0.5833333333333334,
                    0.38268156424581007,
                    0.08333333333333333,
                    0.6416464891041163,
                    0.25,
                    0.55,
                    0.3333333333333333,
                    0.4601815683806009,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "serve as pow": 0.3752214534643113,
                "build a hospital": 0.24413689158761268,
                "rescue a drowning woman": 0.38064165494807606
            },
            "question": "what did the first-ever state of the union \u201chero in the balcony\u201d do to earn his invitation?",
            "rate_limited": false,
            "answers": [
                "build a hospital",
                "serve as pow",
                "rescue a drowning woman"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "serve as pow": 0.31932680869065144,
                "build a hospital": 0.14888710827133908,
                "rescue a drowning woman": 0.3727603250328644
            },
            "integer_answers": {
                "serve as pow": 5,
                "build a hospital": 1,
                "rescue a drowning woman": 6
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.1513191516269874,
                    1.0875914380894076,
                    2.7610894102836054
                ],
                "result_count_important_words": [
                    10.0,
                    100.0,
                    10.0
                ],
                "wikipedia_search": [
                    0.0,
                    3.0,
                    1.0
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.43478260869565216,
                    1.3478260869565217,
                    1.2173913043478262
                ],
                "cosine_similarity_raw": [
                    0.01194948423653841,
                    0.04547538980841637,
                    0.022249748930335045
                ],
                "result_count_noun_chunks": [
                    1620000.0,
                    4300000.0,
                    10600000.0
                ],
                "question_answer_similarity": [
                    19.938595544546843,
                    17.96048587281257,
                    24.162968914955854
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    120000.0,
                    101000.0,
                    137000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    10.0,
                    0,
                    14.0
                ],
                "answer_relation_to_question": [
                    1.553291536050157,
                    1.6115091804746977,
                    0.8351992834751456
                ],
                "word_count_appended": [
                    6.0,
                    3.0,
                    11.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What nickname is stamped on official NFL footballs?": {
        "raw_data": {
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2969527856081215,
                    1.853030085650859,
                    1.8500171287410194
                ],
                "result_count_important_words": [
                    8750.0,
                    652000.0,
                    726000.0
                ],
                "wikipedia_search": [
                    0.5,
                    0.0,
                    1.5
                ],
                "word_count_appended_bing": [
                    2.0,
                    24.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.2857142857142857,
                    1.7142857142857144,
                    1.0
                ],
                "cosine_similarity_raw": [
                    0.04566202312707901,
                    0.06860609352588654,
                    0.2817654311656952
                ],
                "result_count_noun_chunks": [
                    210000.0,
                    855000.0,
                    373000.0
                ],
                "question_answer_similarity": [
                    4.18517349101603,
                    4.24658327922225,
                    4.738580897450447
                ],
                "word_count_noun_chunks": [
                    0.0,
                    37.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    22.0,
                    0.0
                ],
                "result_count_bing": [
                    1340000.0,
                    1510000.0,
                    2760000.0
                ],
                "word_count_appended": [
                    31.0,
                    82.0,
                    64.0
                ],
                "answer_relation_to_question": [
                    0.6818181818181819,
                    1.0587878787878788,
                    1.2593939393939393
                ],
                "result_count": [
                    7270.0,
                    81300.0,
                    646000.0
                ]
            },
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "z-best_answer_by_ml": [
                "the duke"
            ],
            "lines": [
                [
                    0.2272727272727273,
                    0.09523809523809523,
                    0.11529837151065579,
                    0.3177726795330138,
                    0.00989694651292593,
                    0.23885918003565063,
                    0.006309716964124752,
                    0.14603616133518776,
                    0.25,
                    0.1751412429378531,
                    0.07142857142857142,
                    0.2593905571216243,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.3529292929292929,
                    0.5714285714285715,
                    0.17323303519049474,
                    0.3224354140623553,
                    0.11067699470438488,
                    0.26916221033868093,
                    0.47016405264106725,
                    0.5945757997218358,
                    0.0,
                    0.4632768361581921,
                    0.8571428571428571,
                    0.3706060171301718,
                    0.9736842105263158,
                    1.0,
                    1.0
                ],
                [
                    0.4197979797979798,
                    0.3333333333333333,
                    0.7114685932988495,
                    0.3597919064046309,
                    0.8794260587826892,
                    0.4919786096256685,
                    0.5235262303948081,
                    0.25938803894297635,
                    0.75,
                    0.3615819209039548,
                    0.07142857142857142,
                    0.3700034257482039,
                    0.02631578947368421,
                    0.0,
                    1.0
                ]
            ],
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "question": "what nickname is stamped on official nfl footballs?",
            "rate_limited": false,
            "answers": [
                "the champ",
                "the duke",
                "the pro"
            ],
            "ml_answers": {
                "the duke": 0.8882594897636996,
                "the pro": 0.6576331025465756,
                "the champ": 0.0391776239531341
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these restaurant brands has its original location in Europe?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "benihana"
            ],
            "lines": [
                [
                    0.1669364881693649,
                    0.3786375661375661,
                    0.29348677355022645,
                    -0.09341174832817872,
                    0.23267008985879334,
                    0.3333333333333333,
                    0.5505026328386788,
                    0.06634382566585957,
                    0.6653508771929825,
                    0.8195652173913044,
                    0.8809523809523809,
                    0.3713908657467212,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.5424934274249342,
                    0.3568121693121693,
                    0.3659790038192885,
                    0.6188005737291824,
                    0.6370346598202824,
                    0.3333333333333333,
                    0.15509813307802778,
                    0.8958837772397095,
                    0.2655701754385965,
                    0.11304347826086956,
                    0.07142857142857142,
                    0.3153106754225738,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.2905700844057008,
                    0.2645502645502645,
                    0.34053422263048505,
                    0.4746111745989963,
                    0.13029525032092426,
                    0.3333333333333333,
                    0.29439923408329344,
                    0.03777239709443099,
                    0.06907894736842105,
                    0.06739130434782609,
                    0.047619047619047616,
                    0.31329845883070506,
                    0.5,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "mr. chow": 0.2616752656559592,
                "benihana": 0.40469702160778814,
                "p.f. chang's": 0.33362771273625275
            },
            "question": "which of these restaurant brands has its original location in europe?",
            "rate_limited": false,
            "answers": [
                "benihana",
                "p.f. chang's",
                "mr. chow"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "mr. chow": 0.09501769441273002,
                "benihana": 0.45154518088222373,
                "p.f. chang's": 0.2154564648448365
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.8569543287336059,
                    1.576553377112869,
                    1.5664922941535253
                ],
                "result_count_important_words": [
                    115000.0,
                    32400.0,
                    61500.0
                ],
                "wikipedia_search": [
                    2.66140350877193,
                    1.062280701754386,
                    0.2763157894736842
                ],
                "word_count_appended_bing": [
                    37.0,
                    3.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.1359126984126984,
                    1.070436507936508,
                    0.7936507936507935
                ],
                "cosine_similarity_raw": [
                    0.06169611215591431,
                    0.07693526148796082,
                    0.07158631831407547
                ],
                "result_count_noun_chunks": [
                    137000.0,
                    1850000.0,
                    78000.0
                ],
                "question_answer_similarity": [
                    -1.2132769133895636,
                    8.037280786782503,
                    6.164479214698076
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    409000.0,
                    409000.0,
                    409000.0
                ],
                "result_count": [
                    145000.0,
                    397000.0,
                    81200.0
                ],
                "answer_relation_to_question": [
                    0.6677459526774596,
                    2.169973709699737,
                    1.1622803376228033
                ],
                "word_count_appended": [
                    377.0,
                    52.0,
                    31.0
                ]
            },
            "integer_answers": {
                "mr. chow": 0,
                "benihana": 9,
                "p.f. chang's": 5
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "In which of these movies is the title NOT spoken by any character?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "gravity"
            ],
            "lines": [
                [
                    0.19723895582329315,
                    0.16753393665158367,
                    0.18580023072053725,
                    0.35123211291886114,
                    0.41085423197492166,
                    0.29486023444544635,
                    0.3724007561436673,
                    0.2791828793774319,
                    0.10241739211831885,
                    0.3138913624220837,
                    0.22535211267605632,
                    0.3261281676018138,
                    0.5,
                    0.0,
                    -1.0
                ],
                [
                    0.4156124497991968,
                    0.40486425339366516,
                    0.40048114583837374,
                    0.34398438693097644,
                    0.3947884012539185,
                    0.4102795311091073,
                    0.3402646502835539,
                    0.3438715953307393,
                    0.42334784236637646,
                    0.3192341941228851,
                    0.3427230046948357,
                    0.3374832560074836,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.38714859437751004,
                    0.4276018099547511,
                    0.413718623441089,
                    0.3047835001501624,
                    0.1943573667711599,
                    0.29486023444544635,
                    0.2873345935727788,
                    0.3769455252918288,
                    0.4742347655153047,
                    0.3668744434550312,
                    0.431924882629108,
                    0.3363885763907027,
                    0.0,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "inception": 0.4675868038751407,
                "speed": 0.31483244057216103,
                "gravity": 0.2175807555526983
            },
            "question": "in which of these movies is the title not spoken by any character?",
            "rate_limited": false,
            "answers": [
                "inception",
                "gravity",
                "speed"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "inception": 0.15720197078447753,
                "speed": 0.24462968968857096,
                "gravity": 0.3075486066324411
            },
            "integer_answers": {
                "inception": 10,
                "speed": 4,
                "gravity": 0
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.39097465918549,
                    1.3001339519401314,
                    1.3088913888743785
                ],
                "result_count_important_words": [
                    1350000.0,
                    1690000.0,
                    2250000.0
                ],
                "wikipedia_search": [
                    2.385495647290087,
                    0.4599129458017411,
                    0.15459140690817186
                ],
                "word_count_appended_bing": [
                    117.0,
                    67.0,
                    29.0
                ],
                "answer_relation_to_question_bing": [
                    1.3298642533936653,
                    0.38054298642533935,
                    0.2895927601809955
                ],
                "cosine_similarity_raw": [
                    0.14731481671333313,
                    0.046660128980875015,
                    0.04045364260673523
                ],
                "result_count_noun_chunks": [
                    4540000.0,
                    3210000.0,
                    2530000.0
                ],
                "question_answer_similarity": [
                    2.5318306535482407,
                    2.655177265405655,
                    3.322323977947235
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    18200000.0,
                    7960000.0,
                    18200000.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    4550000.0,
                    5370000.0,
                    15600000.0
                ],
                "answer_relation_to_question": [
                    1.816566265060241,
                    0.5063253012048192,
                    0.6771084337349398
                ],
                "word_count_appended": [
                    418.0,
                    406.0,
                    299.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Who holds the record as the youngest solo artist with a Billboard #1 hit?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "stevie wonder"
            ],
            "lines": [
                [
                    0.2653737594151765,
                    0.2957977332170881,
                    0.20224554473515405,
                    0.23162578867103845,
                    0.3333333333333333,
                    0.3414425949637217,
                    0.12635869565217392,
                    0.3331193838254172,
                    0.33001806965710034,
                    0.35,
                    0.0,
                    0.3334803857344841,
                    0.45,
                    0.25,
                    0.0
                ],
                [
                    0.3873242700910759,
                    0.3220895088637024,
                    0.23148459699203763,
                    0.33327754450893626,
                    0.3333333333333333,
                    0.3183952198036705,
                    0.552536231884058,
                    0.3331193838254172,
                    0.5176102262687609,
                    0.25,
                    0.0,
                    0.333937578814628,
                    0.3,
                    0.0,
                    0.0
                ],
                [
                    0.3473019704937476,
                    0.3821127579192095,
                    0.5662698582728083,
                    0.4350966668200253,
                    0.3333333333333333,
                    0.34016218523260777,
                    0.3211050724637681,
                    0.3337612323491656,
                    0.1523717040741389,
                    0.4,
                    1.0,
                    0.3325820354508879,
                    0.25,
                    0.75,
                    0.0
                ]
            ],
            "fraction_answers": {
                "justin bieber": 0.27448537780033483,
                "stevie wonder": 0.42457834402926375,
                "michael jackson": 0.30093627817040147
            },
            "question": "who holds the record as the youngest solo artist with a billboard #1 hit?",
            "rate_limited": false,
            "answers": [
                "justin bieber",
                "michael jackson",
                "stevie wonder"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "justin bieber": 0.1923237100459994,
                "stevie wonder": 0.7278206031490109,
                "michael jackson": 0.1691569715695715
            },
            "integer_answers": {
                "justin bieber": 3,
                "stevie wonder": 7,
                "michael jackson": 4
            },
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.6678430858758726,
                    2.671500630517024,
                    2.660656283607103
                ],
                "result_count_important_words": [
                    27900.0,
                    122000.0,
                    70900.0
                ],
                "wikipedia_search": [
                    2.6401445572568023,
                    4.140881810150086,
                    1.2189736325931109
                ],
                "word_count_appended_bing": [
                    0.0,
                    0.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.4789886660854403,
                    1.610447544318512,
                    1.9105637895960477
                ],
                "cosine_similarity_raw": [
                    0.08675868809223175,
                    0.09930156916379929,
                    0.2429167479276657
                ],
                "result_count_noun_chunks": [
                    519000.0,
                    519000.0,
                    520000.0
                ],
                "question_answer_similarity": [
                    3.0850419979542494,
                    4.438949685543776,
                    5.795086540281773
                ],
                "word_count_noun_chunks": [
                    9.0,
                    6.0,
                    5.0
                ],
                "result_count_bing": [
                    8000000.0,
                    7460000.0,
                    7970000.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    3.0
                ],
                "result_count": [
                    506000.0,
                    506000.0,
                    506000.0
                ],
                "answer_relation_to_question": [
                    1.8576163159062355,
                    2.7112698906375314,
                    2.4311137934562335
                ],
                "word_count_appended": [
                    7.0,
                    5.0,
                    8.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these mountain ranges is found in Mexico?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "sierra madre"
            ],
            "question": "which of these mountain ranges is found in mexico?",
            "lines": [
                [
                    0.2189917483238783,
                    0.09418103448275862,
                    0.04523466573820833,
                    0.031195195463956896,
                    0.4861910241657077,
                    0.18507157464212678,
                    0.3865979381443299,
                    0.4748247291268324,
                    0.22105757196495618,
                    0.5582010582010583,
                    0.5625,
                    0.2844789388585083,
                    0.05434782608695652,
                    0.1,
                    -1.0
                ],
                [
                    0.32876784726965236,
                    0.2084530651340996,
                    0.9247335757587569,
                    0.17585925287804688,
                    0.08227848101265822,
                    0.11247443762781185,
                    0.19845360824742267,
                    0.07265774378585087,
                    0.4530378882694277,
                    0.27380952380952384,
                    0.29375,
                    0.35226228403034127,
                    0.8260869565217391,
                    0.8666666666666667,
                    -1.0
                ],
                [
                    0.4522404044064694,
                    0.6973659003831418,
                    0.03003175850303475,
                    0.7929455516579962,
                    0.4315304948216341,
                    0.7024539877300614,
                    0.41494845360824745,
                    0.45251752708731674,
                    0.3259045397656161,
                    0.167989417989418,
                    0.14375,
                    0.3632587771111505,
                    0.11956521739130435,
                    0.03333333333333333,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "andes",
                "sierra madre",
                "rocky mountains"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "andes": 0.06370675800104161,
                "sierra madre": 0.9303553719010506,
                "rocky mountains": 0.5532209854290354
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1379157554340331,
                    1.409049136121365,
                    1.453035108444602
                ],
                "result_count_important_words": [
                    600000.0,
                    308000.0,
                    644000.0
                ],
                "wikipedia_search": [
                    0.8842302878598247,
                    1.8121515530777108,
                    1.3036181590624645
                ],
                "word_count_appended_bing": [
                    90.0,
                    47.0,
                    23.0
                ],
                "answer_relation_to_question_bing": [
                    0.3767241379310345,
                    0.8338122605363985,
                    2.7894636015325673
                ],
                "cosine_similarity_raw": [
                    0.040949560701847076,
                    0.8371330499649048,
                    0.027186833322048187
                ],
                "result_count_noun_chunks": [
                    1490000.0,
                    228000.0,
                    1420000.0
                ],
                "question_answer_similarity": [
                    0.23089898098260164,
                    1.3016659034183249,
                    5.869183287024498
                ],
                "word_count_noun_chunks": [
                    5.0,
                    76.0,
                    11.0
                ],
                "word_count_raw": [
                    3.0,
                    26.0,
                    1.0
                ],
                "result_count_bing": [
                    1810000.0,
                    1100000.0,
                    6870000.0
                ],
                "word_count_appended": [
                    422.0,
                    207.0,
                    127.0
                ],
                "answer_relation_to_question": [
                    0.8759669932955132,
                    1.3150713890786094,
                    1.8089616176258776
                ],
                "result_count": [
                    1690000.0,
                    286000.0,
                    1500000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which player won Rookie of the Year in their sport most recently?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "mike trout"
            ],
            "lines": [
                [
                    0.5322469861287864,
                    0.37511050353155617,
                    0.4826854771140381,
                    0.5754058205595037,
                    0.3434343434343434,
                    0.40166975881261596,
                    0.40298507462686567,
                    0.2423270728355474,
                    0.39216705261113155,
                    0.24864864864864866,
                    0.35802469135802467,
                    0.3532286779305869,
                    0.4,
                    0,
                    -1.0
                ],
                [
                    0.2186241001546023,
                    0.1723490544543176,
                    0.14312604916084481,
                    0.300034466776248,
                    0.37373737373737376,
                    0.2782931354359926,
                    0.26865671641791045,
                    0.0842876775080165,
                    0.16188290419375945,
                    0.44324324324324327,
                    0.2716049382716049,
                    0.3327628155953313,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.24912891371661128,
                    0.45254044201412624,
                    0.3741884737251171,
                    0.12455971266424831,
                    0.2828282828282828,
                    0.3200371057513915,
                    0.3283582089552239,
                    0.6733852496564361,
                    0.445950043195109,
                    0.3081081081081081,
                    0.37037037037037035,
                    0.3140085064740819,
                    0.6,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "mike trout": 0.3929180082762806,
                "von miller": 0.23450788268840345,
                "blake griffin": 0.37257410903531585
            },
            "question": "which player won rookie of the year in their sport most recently?",
            "rate_limited": false,
            "answers": [
                "mike trout",
                "von miller",
                "blake griffin"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "mike trout": 0.4757194791850875,
                "von miller": 0.3008174886492512,
                "blake griffin": 0.18283525257915206
            },
            "integer_answers": {
                "mike trout": 6,
                "von miller": 2,
                "blake griffin": 5
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.1193720675835213,
                    1.9965768935719876,
                    1.8840510388444913
                ],
                "result_count_important_words": [
                    27.0,
                    18.0,
                    22.0
                ],
                "wikipedia_search": [
                    1.9608352630556578,
                    0.8094145209687973,
                    2.229750215975545
                ],
                "word_count_appended_bing": [
                    29.0,
                    22.0,
                    30.0
                ],
                "answer_relation_to_question_bing": [
                    1.875552517657781,
                    0.861745272271588,
                    2.262702210070631
                ],
                "cosine_similarity_raw": [
                    0.09483902901411057,
                    0.028121698647737503,
                    0.07352131605148315
                ],
                "result_count_noun_chunks": [
                    529000.0,
                    184000.0,
                    1470000.0
                ],
                "question_answer_similarity": [
                    3.5639198571443558,
                    1.858338507823646,
                    0.7714917324483395
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    3.0
                ],
                "result_count_bing": [
                    433000.0,
                    300000.0,
                    345000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    34.0,
                    37.0,
                    28.0
                ],
                "answer_relation_to_question": [
                    3.1934819167727184,
                    1.3117446009276137,
                    1.4947734822996677
                ],
                "word_count_appended": [
                    46.0,
                    82.0,
                    57.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What form of transportation counts Jay-Z as a prominent investor?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "boats"
            ],
            "lines": [
                [
                    0.3796718724313661,
                    0.3510876538399474,
                    0.30944513530966794,
                    0.21880172288869149,
                    0.47813777917584144,
                    0.12349183818310859,
                    0.5104895104895105,
                    0.575013068478829,
                    0.16511018786127168,
                    0.4396984924623116,
                    0.49557522123893805,
                    0.3818280258385283,
                    0.3333333333333333,
                    0.3333333333333333,
                    1.0
                ],
                [
                    0.2757722614937805,
                    0.33317196619948913,
                    0.38933913702850587,
                    0.6017750880559021,
                    0.06259830135262662,
                    0.30872959545777146,
                    0.08251748251748252,
                    0.014874304994535,
                    0.4782153179190752,
                    0.1984924623115578,
                    0.23893805309734514,
                    0.26684015787744364,
                    0.3333333333333333,
                    0.3333333333333333,
                    1.0
                ],
                [
                    0.34455586607485345,
                    0.3157403799605634,
                    0.30121572766182625,
                    0.17942318905540647,
                    0.45926391947153195,
                    0.5677785663591199,
                    0.406993006993007,
                    0.41011262652663594,
                    0.35667449421965314,
                    0.36180904522613067,
                    0.26548672566371684,
                    0.35133181628402793,
                    0.3333333333333333,
                    0.3333333333333333,
                    1.0
                ]
            ],
            "fraction_answers": {
                "boats": 0.35621800215451,
                "aviation": 0.36392979820461996,
                "e-bikes": 0.27985219964087016
            },
            "question": "what form of transportation counts jay-z as a prominent investor?",
            "rate_limited": false,
            "answers": [
                "aviation",
                "e-bikes",
                "boats"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "boats": 0.5498699963744416,
                "aviation": 0.39387107934162624,
                "e-bikes": 0.08691115724032182
            },
            "integer_answers": {
                "boats": 1,
                "aviation": 10,
                "e-bikes": 3
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.6727961808696983,
                    1.8678811051421056,
                    2.4593227139881955
                ],
                "result_count_important_words": [
                    365000.0,
                    59000.0,
                    291000.0
                ],
                "wikipedia_search": [
                    0.6604407514450867,
                    1.9128612716763007,
                    1.4266979768786126
                ],
                "word_count_appended_bing": [
                    56.0,
                    27.0,
                    30.0
                ],
                "answer_relation_to_question_bing": [
                    1.0532629615198421,
                    0.9995158985984673,
                    0.9472211398816903
                ],
                "cosine_similarity_raw": [
                    0.053956739604473114,
                    0.06788754463195801,
                    0.05252180993556976
                ],
                "result_count_noun_chunks": [
                    1210000.0,
                    31300.0,
                    863000.0
                ],
                "question_answer_similarity": [
                    2.1645172073040158,
                    5.953118265373632,
                    1.7749612524639815
                ],
                "word_count_noun_chunks": [
                    1.0,
                    1.0,
                    1.0
                ],
                "result_count_bing": [
                    34800.0,
                    87000.0,
                    160000.0
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    1.0
                ],
                "word_count_appended": [
                    175.0,
                    79.0,
                    144.0
                ],
                "answer_relation_to_question": [
                    1.8983593621568304,
                    1.3788613074689025,
                    1.7227793303742671
                ],
                "result_count": [
                    456000.0,
                    59700.0,
                    438000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these has NEVER been named Pantone\u2019s Color of the Year?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "cucumber"
            ],
            "lines": [
                [
                    0.43311072242539883,
                    0.3464646464646465,
                    0.31457762529392175,
                    0.4497752310993193,
                    0.35684730379071006,
                    0.2836474648261216,
                    0.22712146422628954,
                    0.24744245524296676,
                    0.3277777777777778,
                    0.17931034482758623,
                    0.17857142857142855,
                    0.3757656669292377,
                    0,
                    0.33333333333333337,
                    -1.0
                ],
                [
                    0.2985600961113761,
                    0.3242424242424242,
                    0.37173026454525193,
                    0.22207050061374806,
                    0.14628937533368924,
                    0.4393416511813114,
                    0.3826955074875208,
                    0.43371696504688834,
                    0.40416666666666673,
                    0.4,
                    0.39285714285714285,
                    0.31300362577055485,
                    0,
                    0.33333333333333337,
                    -1.0
                ],
                [
                    0.26832918146322504,
                    0.3292929292929293,
                    0.31369211016082627,
                    0.32815426828693267,
                    0.49686332087560064,
                    0.277010883992567,
                    0.39018302828618967,
                    0.3188405797101449,
                    0.26805555555555555,
                    0.4206896551724138,
                    0.4285714285714286,
                    0.31123070730020747,
                    0,
                    0.33333333333333337,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "chili pepper": 0.3101158489228686,
                "cucumber": 0.3763468515678865,
                "sand dollar": 0.3135372995092449
            },
            "question": "which of these has never been named pantone\u2019s color of the year?",
            "rate_limited": false,
            "answers": [
                "cucumber",
                "sand dollar",
                "chili pepper"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "chili pepper": 0.2587560772791508,
                "cucumber": 0.28543370597102813,
                "sand dollar": 0.22568283237120018
            },
            "integer_answers": {
                "chili pepper": 5,
                "cucumber": 5,
                "sand dollar": 3
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.9938746645660984,
                    1.4959709938355612,
                    1.5101543415983403
                ],
                "result_count_important_words": [
                    328000.0,
                    141000.0,
                    132000.0
                ],
                "wikipedia_search": [
                    1.0333333333333334,
                    0.575,
                    1.3916666666666668
                ],
                "word_count_appended_bing": [
                    27.0,
                    9.0,
                    6.0
                ],
                "answer_relation_to_question_bing": [
                    0.30707070707070705,
                    0.3515151515151515,
                    0.3414141414141414
                ],
                "cosine_similarity_raw": [
                    0.06203324347734451,
                    0.04291277006268501,
                    0.0623294934630394
                ],
                "result_count_noun_chunks": [
                    23700000.0,
                    6220000.0,
                    17000000.0
                ],
                "question_answer_similarity": [
                    1.1156974867917597,
                    6.173950637457892,
                    3.817396380007267
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    16300000.0,
                    4570000.0,
                    16800000.0
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    1.0
                ],
                "word_count_appended": [
                    186.0,
                    58.0,
                    46.0
                ],
                "answer_relation_to_question": [
                    0.4013356654476069,
                    1.2086394233317432,
                    1.39002491122065
                ],
                "result_count": [
                    4290000.0,
                    10600000.0,
                    94000.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What was the first theatrical feature film to be completely computer-animated?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "toy story"
            ],
            "question": "what was the first theatrical feature film to be completely computer-animated?",
            "lines": [
                [
                    0.20711562540356313,
                    0.174,
                    0.0990090281091075,
                    0.5100711548917874,
                    0.05457746478873239,
                    0.2772002772002772,
                    0.00012751278179985647,
                    0.18932038834951456,
                    0.009407195286527765,
                    0.12589928057553956,
                    0.2631578947368421,
                    0.30472328176338487,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.4841258611686628,
                    0.28802197802197804,
                    0.36178155839360215,
                    0.27205263135546953,
                    0.7423708920187794,
                    0.4857934857934858,
                    0.00011801714911263312,
                    0.5844660194174758,
                    0.3812912431868358,
                    0.30935251798561153,
                    0.10526315789473684,
                    0.32763719021177434,
                    0,
                    0.14285714285714285,
                    1.0
                ],
                [
                    0.3087585134277741,
                    0.5379780219780219,
                    0.5392094134972903,
                    0.21787621375274302,
                    0.20305164319248825,
                    0.23700623700623702,
                    0.9997544700690875,
                    0.2262135922330097,
                    0.6093015615266364,
                    0.564748201438849,
                    0.631578947368421,
                    0.36763952802484073,
                    0,
                    0.8571428571428571,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "gone with the wind",
                "toy story 2",
                "toy story"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "gone with the wind": 0.0258505317047327,
                "toy story 2": 0.1670030430261531,
                "toy story": 0.6847258929636958
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.8283396905803093,
                    1.9658231412706462,
                    2.2058371681490443
                ],
                "result_count_important_words": [
                    94.0,
                    87.0,
                    737000.0
                ],
                "wikipedia_search": [
                    0.05644317171916659,
                    2.287747459121015,
                    3.6558093691598184
                ],
                "word_count_appended_bing": [
                    5.0,
                    2.0,
                    12.0
                ],
                "answer_relation_to_question_bing": [
                    0.87,
                    1.44010989010989,
                    2.6898901098901096
                ],
                "cosine_similarity_raw": [
                    0.033866893500089645,
                    0.12375050783157349,
                    0.1844412386417389
                ],
                "result_count_noun_chunks": [
                    1950000.0,
                    6020000.0,
                    2330000.0
                ],
                "question_answer_similarity": [
                    19.574651796370745,
                    10.44037773553282,
                    8.361286416649818
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    400000.0,
                    701000.0,
                    342000.0
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    6.0
                ],
                "word_count_appended": [
                    35.0,
                    86.0,
                    157.0
                ],
                "answer_relation_to_question": [
                    1.2426937524213788,
                    2.904755167011977,
                    1.8525510805666445
                ],
                "result_count": [
                    186000.0,
                    2530000.0,
                    692000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "In the popular comic strip, what color is Hagar the Horrible\u2019s beard?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "red"
            ],
            "question": "in the popular comic strip, what color is hagar the horrible\u2019s beard?",
            "lines": [
                [
                    0.08380281690140845,
                    0.3329911019849418,
                    0.2934721284471434,
                    0.38096408261723425,
                    0.49990959081868175,
                    0.3333333333333333,
                    0.5519288963325276,
                    0.016463281003618716,
                    0.15693043667884207,
                    0.35904255319148937,
                    0.35428571428571426,
                    0.39267374266666,
                    0.2711864406779661,
                    0.5416666666666666,
                    1.0
                ],
                [
                    0.66908953722334,
                    0.35249828884325807,
                    0.17029625279153196,
                    0.24562824606653122,
                    0.00018081836263654445,
                    0.3333333333333333,
                    0.012237595873855698,
                    1.6035663315213034e-05,
                    0.444854985553072,
                    0.125,
                    0.28,
                    0.15806328270488848,
                    0.01694915254237288,
                    0.0,
                    1.0
                ],
                [
                    0.24710764587525152,
                    0.3145106091718001,
                    0.5362316187613246,
                    0.37340767131623454,
                    0.49990959081868175,
                    0.3333333333333333,
                    0.4358335077936167,
                    0.9835206833330661,
                    0.3982145777680859,
                    0.5159574468085106,
                    0.3657142857142857,
                    0.4492629746284515,
                    0.711864406779661,
                    0.4583333333333333,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "white",
                "blond",
                "red"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "white": 0.30654450937226263,
                "red": 0.665050164530748,
                "blond": 0.13938322408443143
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.74871619866662,
                    1.1064429789342194,
                    3.1448408223991606
                ],
                "result_count_important_words": [
                    290000.0,
                    6430.0,
                    229000.0
                ],
                "wikipedia_search": [
                    0.6277217467153683,
                    1.779419942212288,
                    1.5928583110723435
                ],
                "word_count_appended_bing": [
                    62.0,
                    49.0,
                    64.0
                ],
                "answer_relation_to_question_bing": [
                    0.6659822039698836,
                    0.7049965776865161,
                    0.6290212183436003
                ],
                "cosine_similarity_raw": [
                    0.05255678668618202,
                    0.03049769625067711,
                    0.09603164345026016
                ],
                "result_count_noun_chunks": [
                    46200.0,
                    45.0,
                    2760000.0
                ],
                "question_answer_similarity": [
                    4.233473824337125,
                    2.729550626128912,
                    4.149502996355295
                ],
                "word_count_noun_chunks": [
                    16.0,
                    1.0,
                    42.0
                ],
                "result_count_bing": [
                    136000.0,
                    136000.0,
                    136000.0
                ],
                "word_count_raw": [
                    13.0,
                    0.0,
                    11.0
                ],
                "word_count_appended": [
                    135.0,
                    47.0,
                    194.0
                ],
                "answer_relation_to_question": [
                    0.3352112676056338,
                    2.67635814889336,
                    0.9884305835010061
                ],
                "result_count": [
                    141000.0,
                    51.0,
                    141000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "The son of the first NBA slam dunk champion now plays for what team?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "chicago bulls"
            ],
            "question": "the son of the first nba slam dunk champion now plays for what team?",
            "lines": [
                [
                    0.2639802954211927,
                    0.24334180573541833,
                    0.36242018169563706,
                    0.2503760371675074,
                    0.15869642224583777,
                    0.29893778452200304,
                    0.1622746185852982,
                    0.16483516483516483,
                    0.18348241212547675,
                    0.26519337016574585,
                    0.42105263157894735,
                    0.3294190354079828,
                    0.46153846153846156,
                    0.5,
                    1.0
                ],
                [
                    0.3942385492946321,
                    0.2703643904687539,
                    0.36353892952467204,
                    0.4175843282483777,
                    0.3790294013460857,
                    0.30576631259484066,
                    0.3786407766990291,
                    0.36106750392464676,
                    0.371192433517062,
                    0.3867403314917127,
                    0.2631578947368421,
                    0.3219000885808221,
                    0.38461538461538464,
                    0.0,
                    1.0
                ],
                [
                    0.34178115528417513,
                    0.4862938037958277,
                    0.2740408887796909,
                    0.3320396345841149,
                    0.4622741764080765,
                    0.3952959028831563,
                    0.4590846047156727,
                    0.4740973312401884,
                    0.44532515435746123,
                    0.34806629834254144,
                    0.3157894736842105,
                    0.3486808760111951,
                    0.15384615384615385,
                    0.5,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "atlanta hawks",
                "los angeles lakers",
                "chicago bulls"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "atlanta hawks": 0.21551626632663823,
                "los angeles lakers": -0.0017751623756384807,
                "chicago bulls": 0.4494609507359033
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.3059332478558794,
                    2.2533006200657546,
                    2.4407661320783656
                ],
                "result_count_important_words": [
                    117000.0,
                    273000.0,
                    331000.0
                ],
                "wikipedia_search": [
                    1.1008944727528605,
                    2.227154601102372,
                    2.6719509261447674
                ],
                "word_count_appended_bing": [
                    8.0,
                    5.0,
                    6.0
                ],
                "answer_relation_to_question_bing": [
                    1.2167090286770916,
                    1.3518219523437693,
                    2.4314690189791386
                ],
                "cosine_similarity_raw": [
                    0.11341878026723862,
                    0.11376889050006866,
                    0.08576063066720963
                ],
                "result_count_noun_chunks": [
                    105000.0,
                    230000.0,
                    302000.0
                ],
                "question_answer_similarity": [
                    4.11739744618535,
                    6.867113427259028,
                    5.4603434056043625
                ],
                "word_count_noun_chunks": [
                    6.0,
                    5.0,
                    2.0
                ],
                "result_count_bing": [
                    394000.0,
                    403000.0,
                    521000.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    1.0
                ],
                "result_count": [
                    89600.0,
                    214000.0,
                    261000.0
                ],
                "answer_relation_to_question": [
                    1.5838817725271563,
                    2.3654312957677925,
                    2.0506869317050507
                ],
                "word_count_appended": [
                    48.0,
                    70.0,
                    63.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these things was created by a person who chose to remain anonymous?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "bitcoin"
            ],
            "lines": [
                [
                    0.5504761904761905,
                    0.6444444444444445,
                    0.5006813433715943,
                    0.24620562545757632,
                    0.9232929612076531,
                    0.4629433000637078,
                    0.8390255009107468,
                    0.9700889248181084,
                    0.8973684210526316,
                    0.7586854460093897,
                    0.6293103448275862,
                    0.3950380167633712,
                    0,
                    0,
                    0.0
                ],
                [
                    0.17857142857142855,
                    0.2518518518518518,
                    0.2761173352090164,
                    0.7823076718961841,
                    0.042127435492364404,
                    0.4586961138245912,
                    0.053506375227686705,
                    0.00669823305231551,
                    0.08947368421052632,
                    0.18779342723004694,
                    0.25,
                    0.3233623000707564,
                    0,
                    0,
                    0.0
                ],
                [
                    0.27095238095238094,
                    0.1037037037037037,
                    0.22320132141938925,
                    -0.02851329735376043,
                    0.03457960329998245,
                    0.07836058611170099,
                    0.10746812386156648,
                    0.023212842129576163,
                    0.013157894736842105,
                    0.05352112676056338,
                    0.1206896551724138,
                    0.2815996831658724,
                    0,
                    0,
                    0.0
                ]
            ],
            "fraction_answers": {
                "hoverboards": 0.24170882138639738,
                "fidget spinners": 0.10682780199668591,
                "bitcoin": 0.6514633766169168
            },
            "question": "which of these things was created by a person who chose to remain anonymous?",
            "rate_limited": false,
            "answers": [
                "bitcoin",
                "hoverboards",
                "fidget spinners"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "hoverboards": 0.2406346588261331,
                "fidget spinners": 0.14760358985098584,
                "bitcoin": 0.4908365697962334
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.3702281005802273,
                    1.9401738004245384,
                    1.6895980989952346
                ],
                "result_count_important_words": [
                    737000.0,
                    47000.0,
                    94400.0
                ],
                "wikipedia_search": [
                    3.5894736842105264,
                    0.35789473684210527,
                    0.05263157894736842
                ],
                "answer_relation_to_question": [
                    2.7523809523809524,
                    0.8928571428571428,
                    1.3547619047619048
                ],
                "answer_relation_to_question_bing": [
                    1.9333333333333333,
                    0.7555555555555555,
                    0.3111111111111111
                ],
                "word_count_appended": [
                    808.0,
                    200.0,
                    57.0
                ],
                "cosine_similarity_raw": [
                    0.04489924758672714,
                    0.024761179462075233,
                    0.020015867426991463
                ],
                "result_count_noun_chunks": [
                    16800000.0,
                    116000.0,
                    402000.0
                ],
                "question_answer_similarity": [
                    -0.5448476430028677,
                    -1.7312297001481056,
                    0.0630993009544909
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    2180000.0,
                    2160000.0,
                    369000.0
                ],
                "result_count": [
                    263000.0,
                    12000.0,
                    9850.0
                ],
                "word_count_appended_bing": [
                    73.0,
                    29.0,
                    14.0
                ]
            },
            "integer_answers": {
                "hoverboards": 0,
                "fidget spinners": 1,
                "bitcoin": 11
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which former NFL star does NOT have a football video game named after him?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "brett favre"
            ],
            "lines": [
                [
                    0.32637875415405604,
                    0.3192033732665628,
                    0.3535789698954476,
                    0.3717581617080084,
                    0.462521055586749,
                    0.3381294964028777,
                    0.4610874200426439,
                    0.4733650221275201,
                    0.414966373785995,
                    0.3588082901554404,
                    0.4491525423728814,
                    0.334961989489444,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.36983581196118165,
                    0.3516931149235413,
                    0.35226734504023033,
                    0.32728874703125244,
                    0.08169567658618754,
                    0.3237410071942446,
                    0.08315565031982941,
                    0.05908867398787082,
                    0.2899205822012069,
                    0.3354922279792746,
                    0.2711864406779661,
                    0.3293562249588342,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.3037854338847623,
                    0.329103511809896,
                    0.29415368506432205,
                    0.3009530912607392,
                    0.45578326782706347,
                    0.3381294964028777,
                    0.4557569296375267,
                    0.4675463038846091,
                    0.2951130440127982,
                    0.30569948186528495,
                    0.2796610169491526,
                    0.3356817855517218,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "kurt warner": 0.30643882530820765,
                "brett favre": 0.4708797495230634,
                "emmitt smith": 0.22268142516872894
            },
            "question": "which former nfl star does not have a football video game named after him?",
            "rate_limited": false,
            "answers": [
                "emmitt smith",
                "brett favre",
                "kurt warner"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "kurt warner": 0.10641979500881776,
                "brett favre": 0.29751448084320553,
                "emmitt smith": 0.12401293458063398
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.980456126126672,
                    2.0477253004939895,
                    1.971818573379339
                ],
                "result_count_important_words": [
                    36500.0,
                    391000.0,
                    41500.0
                ],
                "wikipedia_search": [
                    1.020403514568061,
                    2.5209530135855176,
                    2.458643471846422
                ],
                "word_count_appended_bing": [
                    6.0,
                    27.0,
                    26.0
                ],
                "answer_relation_to_question_bing": [
                    2.1695595208012466,
                    1.779682620917505,
                    2.050757858281248
                ],
                "cosine_similarity_raw": [
                    0.031085189431905746,
                    0.03136364743113518,
                    0.0437011793255806
                ],
                "result_count_noun_chunks": [
                    32500.0,
                    538000.0,
                    39600.0
                ],
                "question_answer_similarity": [
                    1.8515626415610313,
                    2.4936144711682573,
                    2.8738501026527956
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    1800000.0,
                    1960000.0,
                    1800000.0
                ],
                "word_count_appended": [
                    109.0,
                    127.0,
                    150.0
                ],
                "answer_relation_to_question": [
                    2.083454950151328,
                    1.56197025646582,
                    2.354574793382852
                ],
                "result_count": [
                    26700.0,
                    298000.0,
                    31500.0
                ]
            },
            "integer_answers": {
                "kurt warner": 4,
                "brett favre": 7,
                "emmitt smith": 1
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "In which town were a President, Governor, Senator, NFL owner and late night host all born?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "hope, ar"
            ],
            "lines": [
                [
                    0.23983871740885915,
                    0.2887989203778677,
                    0.12925113865931756,
                    0.22482535151494046,
                    0.4020930873037731,
                    0.3395784543325527,
                    0.24769433465085638,
                    0.24038772213247173,
                    0.38285929032272115,
                    0.4,
                    0.3333333333333333,
                    0.40906282063733546,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.341157020069985,
                    0.26855600539811064,
                    0.5120595743595259,
                    0.425805971447376,
                    0.022307904158633984,
                    0.32201405152224827,
                    0.024374176548089592,
                    0.02617124394184168,
                    0.17637664562531583,
                    0.2,
                    0.3333333333333333,
                    0.26035476113529715,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.4190042625211559,
                    0.4426450742240216,
                    0.3586892869811566,
                    0.34936867703768354,
                    0.5755990085375929,
                    0.33840749414519905,
                    0.7279314888010541,
                    0.7334410339256866,
                    0.44076406405196294,
                    0.4,
                    0.3333333333333333,
                    0.33058241822736734,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "muncie, in": 0.45414717848218444,
                "hope, ar": 0.24270922396164643,
                "brookline, ma": 0.3031435975561691
            },
            "question": "in which town were a president, governor, senator, nfl owner and late night host all born?",
            "rate_limited": false,
            "answers": [
                "brookline, ma",
                "hope, ar",
                "muncie, in"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "muncie, in": 0.07468058506442075,
                "hope, ar": 0.30089804766837025,
                "brookline, ma": 0.12630205429851368
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    4.090628206373355,
                    2.6035476113529716,
                    3.3058241822736734
                ],
                "result_count_important_words": [
                    752.0,
                    74.0,
                    2210.0
                ],
                "wikipedia_search": [
                    3.062874322581769,
                    1.4110131650025266,
                    3.5261125124157036
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.4439946018893386,
                    1.3427800269905532,
                    2.213225371120108
                ],
                "cosine_similarity_raw": [
                    0.01591062732040882,
                    0.06303378939628601,
                    0.044154129922389984
                ],
                "result_count_noun_chunks": [
                    744.0,
                    81.0,
                    2270.0
                ],
                "question_answer_similarity": [
                    8.583991593215615,
                    16.25757439993322,
                    13.33914421312511
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    290000.0,
                    275000.0,
                    289000.0
                ],
                "result_count": [
                    1460.0,
                    81.0,
                    2090.0
                ],
                "answer_relation_to_question": [
                    1.678871021862014,
                    2.388099140489895,
                    2.933029837648091
                ],
                "word_count_appended": [
                    6.0,
                    3.0,
                    6.0
                ]
            },
            "integer_answers": {
                "muncie, in": 6,
                "hope, ar": 2,
                "brookline, ma": 4
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Though it now conveys something different, which of these words originally meant \u201cparrot\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "popinjay"
            ],
            "lines": [
                [
                    0.12571428571428572,
                    0.0,
                    0.3420904241359691,
                    0.009540421605749829,
                    0.21902017291066284,
                    0.3464788732394366,
                    0.48578811369509045,
                    0.3480355819125278,
                    0.0,
                    0.352112676056338,
                    0.32926829268292684,
                    0.37956639526736424,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.1957142857142857,
                    0.0,
                    0.11780364221230614,
                    -0.04067507928221927,
                    0.5,
                    0.3464788732394366,
                    0.32299741602067183,
                    0.5707931801334322,
                    0.041666666666666664,
                    0.3436619718309859,
                    0.32926829268292684,
                    0.3290363433986637,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.6785714285714286,
                    1.0,
                    0.5401059336517248,
                    1.0311346576764695,
                    0.28097982708933716,
                    0.30704225352112674,
                    0.19121447028423771,
                    0.08117123795404003,
                    0.9583333333333334,
                    0.30422535211267604,
                    0.34146341463414637,
                    0.291397261333972,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "thespian": 0.254728799384763,
                "popinjay": 0.5004699308468743,
                "warble": 0.24480126976836267
            },
            "question": "though it now conveys something different, which of these words originally meant \u201cparrot\u201d?",
            "rate_limited": false,
            "answers": [
                "warble",
                "thespian",
                "popinjay"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "thespian": 0.0675362787562466,
                "popinjay": 0.2821995322550263,
                "warble": 0.17225926874830394
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.2773983716041855,
                    1.9742180603919823,
                    1.748383568003832
                ],
                "result_count_important_words": [
                    18800.0,
                    12500.0,
                    7400.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.125,
                    2.875
                ],
                "answer_relation_to_question": [
                    0.6285714285714286,
                    0.9785714285714285,
                    3.392857142857143
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_count_appended": [
                    125.0,
                    122.0,
                    108.0
                ],
                "cosine_similarity_raw": [
                    0.04515066742897034,
                    0.015548266470432281,
                    0.0712856650352478
                ],
                "result_count_noun_chunks": [
                    939000.0,
                    1540000.0,
                    219000.0
                ],
                "question_answer_similarity": [
                    -0.01999802637146786,
                    0.08526052010711282,
                    -2.161399037577212
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    24600.0,
                    24600.0,
                    21800.0
                ],
                "result_count": [
                    15200.0,
                    34700.0,
                    19500.0
                ],
                "word_count_appended_bing": [
                    27.0,
                    27.0,
                    28.0
                ]
            },
            "integer_answers": {
                "thespian": 3,
                "popinjay": 5,
                "warble": 4
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What gargantuan fruit is the subject of a Roald Dahl children's book?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "peach"
            ],
            "lines": [
                [
                    0.3961882306709893,
                    0.4006700167504188,
                    0.12663841414988747,
                    -0.798390738273242,
                    0.004606172270842929,
                    0.011715797430083144,
                    0.0013271400132714001,
                    0.005522319374137137,
                    0.18108318034175963,
                    0.22040816326530613,
                    0.15,
                    0.1652210650193729,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.04383534728362314,
                    0.20845896147403686,
                    0.2331953660256818,
                    -0.6644885296576125,
                    0.005066789497927222,
                    0.1946334089191232,
                    0.0033178500331785005,
                    0.0050621260929590425,
                    0.33319799463352634,
                    0.12244897959183673,
                    0.15555555555555556,
                    0.148452136854994,
                    0.14285714285714285,
                    0.0,
                    1.0
                ],
                [
                    0.5599764220453876,
                    0.39087102177554434,
                    0.6401662198244308,
                    2.4628792679308544,
                    0.9903270382312298,
                    0.7936507936507936,
                    0.9953550099535501,
                    0.9894155545329039,
                    0.48571882502471403,
                    0.6571428571428571,
                    0.6944444444444444,
                    0.6863267981256331,
                    0.8571428571428571,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "loquat": 0.06654236636871234,
                "dragonfruit": 0.06178498292948763,
                "peach": 0.8716726507018001
            },
            "question": "what gargantuan fruit is the subject of a roald dahl children's book?",
            "rate_limited": false,
            "answers": [
                "dragonfruit",
                "loquat",
                "peach"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "loquat": 0.18348756255580995,
                "dragonfruit": 0.13283455919738485,
                "peach": 0.7906221929744528
            },
            "integer_answers": {
                "loquat": 0,
                "dragonfruit": 1,
                "peach": 13
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1565474551356103,
                    1.039164957984958,
                    4.804287586879432
                ],
                "result_count_important_words": [
                    14.0,
                    35.0,
                    10500.0
                ],
                "wikipedia_search": [
                    0.7243327213670385,
                    1.3327919785341054,
                    1.9428753000988561
                ],
                "word_count_appended_bing": [
                    27.0,
                    28.0,
                    125.0
                ],
                "answer_relation_to_question_bing": [
                    1.6026800670016752,
                    0.8338358458961475,
                    1.5634840871021773
                ],
                "cosine_similarity_raw": [
                    0.016420908272266388,
                    0.030237900093197823,
                    0.08300886303186417
                ],
                "result_count_noun_chunks": [
                    12.0,
                    11.0,
                    2150.0
                ],
                "question_answer_similarity": [
                    -0.7061498463153839,
                    -0.5877178311347961,
                    2.1783341579139233
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    6.0
                ],
                "result_count_bing": [
                    1860.0,
                    30900.0,
                    126000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    2.0
                ],
                "result_count": [
                    10.0,
                    11.0,
                    2150.0
                ],
                "answer_relation_to_question": [
                    1.9809411533549464,
                    0.21917673641811572,
                    2.799882110226938
                ],
                "word_count_appended": [
                    54.0,
                    30.0,
                    161.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "The only person who owns more U.S. land than Ted Turner made his fortune in what business?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "cable tv"
            ],
            "lines": [
                [
                    0.26963307763498107,
                    0.3969607843137255,
                    0.13856251083272977,
                    0.13011617462424702,
                    0.4096204434423149,
                    0.44957290573954745,
                    0.3826491092176607,
                    0.36607142857142855,
                    0.34123238463663996,
                    0.8352272727272727,
                    0.8125,
                    0.3162403781825374,
                    0,
                    0,
                    1.0
                ],
                [
                    0.5933827317743366,
                    0.23284313725490194,
                    0.6780507750846306,
                    0.34588049508251706,
                    0.23412251033446074,
                    0.09635845946350967,
                    0.2877614252517428,
                    0.20535714285714285,
                    0.3936787326149028,
                    0.09090909090909091,
                    0.125,
                    0.3688726546300934,
                    0,
                    0,
                    1.0
                ],
                [
                    0.13698419059068226,
                    0.3701960784313726,
                    0.18338671408263957,
                    0.524003330293236,
                    0.35625704622322435,
                    0.4540686347969429,
                    0.3295894655305964,
                    0.42857142857142855,
                    0.2650888827484572,
                    0.07386363636363637,
                    0.0625,
                    0.3148869671873692,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "pharmaceuticals": 0.40403220582692373,
                "cable tv": 0.30435142960477746,
                "fast food": 0.2916163645682988
            },
            "question": "the only person who owns more u.s. land than ted turner made his fortune in what business?",
            "rate_limited": false,
            "answers": [
                "pharmaceuticals",
                "cable tv",
                "fast food"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "pharmaceuticals": 0.29979428918187434,
                "cable tv": 0.48102098213170347,
                "fast food": 0.13183604734444246
            },
            "integer_answers": {
                "pharmaceuticals": 5,
                "cable tv": 4,
                "fast food": 3
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.213682647277762,
                    2.5821085824106538,
                    2.2042087703115842
                ],
                "result_count_important_words": [
                    98800.0,
                    74300.0,
                    85100.0
                ],
                "wikipedia_search": [
                    1.7061619231831997,
                    1.968393663074514,
                    1.325444413742286
                ],
                "word_count_appended_bing": [
                    26.0,
                    4.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.9848039215686275,
                    1.1642156862745097,
                    1.8509803921568628
                ],
                "cosine_similarity_raw": [
                    0.03379317745566368,
                    0.16536572575569153,
                    0.04472508281469345
                ],
                "result_count_noun_chunks": [
                    123000.0,
                    69000.0,
                    144000.0
                ],
                "question_answer_similarity": [
                    2.6720909513533115,
                    7.103068806231022,
                    10.761033833026886
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    300000.0,
                    64300.0,
                    303000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    147.0,
                    16.0,
                    13.0
                ],
                "answer_relation_to_question": [
                    1.6177984658098863,
                    3.5602963906460197,
                    0.8219051435440936
                ],
                "result_count": [
                    109000.0,
                    62300.0,
                    94800.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which game is an example of combinatorics?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "sudoku"
            ],
            "lines": [
                [
                    0.6029106029106029,
                    0.5730337078651686,
                    0.37484961160174485,
                    0.42868967912442885,
                    0.11132306465848589,
                    0.010336969803718668,
                    0.28583928831850663,
                    0.11006953241561805,
                    0.14516129032258066,
                    0.3173173173173173,
                    0.3630573248407643,
                    0.37087723869177963,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.13721413721413722,
                    0.2921348314606742,
                    0.1950946896881642,
                    0.4580737238810853,
                    0.8712239842838027,
                    0.49648278822972514,
                    0.6693889456030334,
                    0.8726740421698618,
                    0.3544142614601019,
                    0.06106106106106106,
                    0.07643312101910828,
                    0.29729976120530205,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.2598752598752599,
                    0.13483146067415733,
                    0.4300556987100909,
                    0.11323659699448582,
                    0.017452951057711472,
                    0.4931802419665562,
                    0.044771766078459965,
                    0.01725642541452017,
                    0.5004244482173175,
                    0.6216216216216216,
                    0.5605095541401274,
                    0.33182300010291826,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "sudoku": 0.2937532520711022,
                "risk": 0.3077888023225597,
                "crossword puzzles": 0.3984579456063381
            },
            "question": "which game is an example of combinatorics?",
            "rate_limited": false,
            "answers": [
                "risk",
                "crossword puzzles",
                "sudoku"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sudoku": 0.5609330330327317,
                "risk": 0.3012202217787225,
                "crossword puzzles": 0.047967999689000895
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.112631716075339,
                    0.8918992836159062,
                    0.9954690003087548
                ],
                "result_count_important_words": [
                    392000.0,
                    918000.0,
                    61400.0
                ],
                "wikipedia_search": [
                    0.2903225806451613,
                    0.7088285229202038,
                    1.000848896434635
                ],
                "answer_relation_to_question": [
                    1.2058212058212059,
                    0.27442827442827444,
                    0.5197505197505198
                ],
                "answer_relation_to_question_bing": [
                    0.5730337078651685,
                    0.29213483146067415,
                    0.1348314606741573
                ],
                "word_count_appended": [
                    317.0,
                    61.0,
                    621.0
                ],
                "cosine_similarity_raw": [
                    0.04157327115535736,
                    0.02163727581501007,
                    0.047695986926555634
                ],
                "result_count_noun_chunks": [
                    391000.0,
                    3100000.0,
                    61300.0
                ],
                "question_answer_similarity": [
                    2.276065156329423,
                    2.4320754446089268,
                    0.6012131511233747
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    939000.0,
                    45100000.0,
                    44800000.0
                ],
                "result_count": [
                    391000.0,
                    3060000.0,
                    61300.0
                ],
                "word_count_appended_bing": [
                    57.0,
                    12.0,
                    88.0
                ]
            },
            "integer_answers": {
                "sudoku": 4,
                "risk": 3,
                "crossword puzzles": 5
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "The political practice known as gerrymandering is named for a governor of what state?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "massachusetts"
            ],
            "question": "the political practice known as gerrymandering is named for a governor of what state?",
            "lines": [
                [
                    0.519345238095238,
                    0.4074603174603175,
                    0.7724659785295726,
                    0.29558404045661757,
                    0.9678263144127649,
                    0.3258064516129032,
                    0.3191881918819188,
                    0.01474051460968164,
                    0.30564192777421273,
                    0.36257309941520466,
                    0.3867924528301887,
                    0.3326011184122643,
                    0.7083333333333334,
                    1.0,
                    1.0
                ],
                [
                    0.35019841269841273,
                    0.5411111111111111,
                    0.14064491447596086,
                    0.39835612865888514,
                    0.017525503531258174,
                    0.33548387096774196,
                    0.37084870848708484,
                    0.017095508068033144,
                    0.284124662863485,
                    0.3435672514619883,
                    0.33962264150943394,
                    0.3275069063764647,
                    0.2916666666666667,
                    0.0,
                    1.0
                ],
                [
                    0.1304563492063492,
                    0.051428571428571435,
                    0.08688910699446654,
                    0.3060598308844973,
                    0.014648182055976981,
                    0.3387096774193548,
                    0.30996309963099633,
                    0.9681639773222852,
                    0.41023340936230235,
                    0.29385964912280704,
                    0.27358490566037735,
                    0.339891975211271,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "massachusetts",
                "virginia",
                "alabama"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "alabama": 0.010517325141006152,
                "massachusetts": 0.9979126011306716,
                "virginia": 0.20033734714555146
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.32820782888585,
                    2.292548344635253,
                    2.379243826478897
                ],
                "result_count_important_words": [
                    173000.0,
                    201000.0,
                    168000.0
                ],
                "wikipedia_search": [
                    1.8338515666452762,
                    1.7047479771809095,
                    2.4614004561738136
                ],
                "word_count_appended_bing": [
                    41.0,
                    36.0,
                    29.0
                ],
                "answer_relation_to_question_bing": [
                    2.037301587301587,
                    2.7055555555555553,
                    0.2571428571428571
                ],
                "cosine_similarity_raw": [
                    0.16196520626544952,
                    0.029489431530237198,
                    0.01821829378604889
                ],
                "result_count_noun_chunks": [
                    169000.0,
                    196000.0,
                    11100000.0
                ],
                "question_answer_similarity": [
                    1.7056816490367055,
                    2.2987328320741653,
                    1.7661326918751001
                ],
                "word_count_noun_chunks": [
                    34.0,
                    14.0,
                    0.0
                ],
                "result_count_bing": [
                    1010000.0,
                    1040000.0,
                    1050000.0
                ],
                "word_count_raw": [
                    13.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    11100000.0,
                    201000.0,
                    168000.0
                ],
                "answer_relation_to_question": [
                    3.1160714285714284,
                    2.1011904761904763,
                    0.7827380952380951
                ],
                "word_count_appended": [
                    248.0,
                    235.0,
                    201.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these mammals averages the largest litter?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "jackrabbit"
            ],
            "lines": [
                [
                    0.27777777777777773,
                    0.3333333333333333,
                    0.2075082169275096,
                    0.687111442664507,
                    0.6780952380952381,
                    0.5259360218071033,
                    0.6843033509700176,
                    0.2470004137360364,
                    0.7863247863247863,
                    0.15481171548117154,
                    0.18085106382978725,
                    0.3315972849042629,
                    1.0,
                    0,
                    -1.0
                ],
                [
                    0.11904761904761903,
                    0.0,
                    0.4525528060973408,
                    0.319605813404062,
                    0.11428571428571428,
                    0.037120179587909884,
                    0.11675485008818343,
                    0.2772031443938767,
                    0.1111111111111111,
                    0.10669456066945607,
                    0.2127659574468085,
                    0.23099898690731213,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.6031746031746031,
                    0.6666666666666666,
                    0.33993897697514963,
                    -0.006717256068568913,
                    0.20761904761904762,
                    0.4369437986049868,
                    0.19894179894179895,
                    0.4757964418700869,
                    0.10256410256410257,
                    0.7384937238493724,
                    0.6063829787234043,
                    0.437403728188425,
                    0.0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "naked mole rat": 0.4688192804501178,
                "burmese cat": 0.16139544177226106,
                "jackrabbit": 0.3697852777776211
            },
            "question": "which of these mammals averages the largest litter?",
            "rate_limited": false,
            "answers": [
                "naked mole rat",
                "burmese cat",
                "jackrabbit"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "naked mole rat": 0.035285328494094034,
                "burmese cat": 0.03439178423615458,
                "jackrabbit": 0.7818273252691595
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3263891396170515,
                    0.9239959476292485,
                    1.7496149127537
                ],
                "result_count_important_words": [
                    194000.0,
                    33100.0,
                    56400.0
                ],
                "wikipedia_search": [
                    2.358974358974359,
                    0.3333333333333333,
                    0.3076923076923077
                ],
                "word_count_appended_bing": [
                    17.0,
                    20.0,
                    57.0
                ],
                "answer_relation_to_question_bing": [
                    1.0,
                    0.0,
                    2.0
                ],
                "cosine_similarity_raw": [
                    0.031003842130303383,
                    0.06761600077152252,
                    0.050790347158908844
                ],
                "result_count_noun_chunks": [
                    59700.0,
                    67000.0,
                    115000.0
                ],
                "question_answer_similarity": [
                    4.587753164814785,
                    2.133966182038421,
                    -0.04485023953020573
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    656000.0,
                    46300.0,
                    545000.0
                ],
                "word_count_appended": [
                    74.0,
                    51.0,
                    353.0
                ],
                "answer_relation_to_question": [
                    0.8333333333333333,
                    0.3571428571428571,
                    1.8095238095238095
                ],
                "result_count": [
                    178000.0,
                    30000.0,
                    54500.0
                ]
            },
            "integer_answers": {
                "naked mole rat": 6,
                "burmese cat": 1,
                "jackrabbit": 6
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What SportsCenter anchor shares their last name with Linus & Lucy from \u201cPeanuts\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "scott van pelt"
            ],
            "lines": [
                [
                    0.338162623539982,
                    0.3512639623750735,
                    0.12380415743913603,
                    0.2873738733448794,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.449438202247191,
                    0.0033981393794217593,
                    0.34523809523809523,
                    0.3333333333333333,
                    1.0,
                    0.33207371758541177,
                    0.6666666666666666,
                    0.2857142857142857,
                    1.0
                ],
                [
                    0.27560022960966357,
                    0.3724279835390946,
                    0.6296858665488221,
                    0.5281511140389037,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.34831460674157305,
                    0.2612667817948861,
                    0.19047619047619047,
                    0.3333333333333333,
                    0.0,
                    0.3339631412072941,
                    0.0,
                    0.5714285714285714,
                    1.0
                ],
                [
                    0.38623714685035443,
                    0.27630805408583187,
                    0.24650997601204183,
                    0.18447501261621685,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.20224719101123595,
                    0.7353350788256922,
                    0.4642857142857143,
                    0.3333333333333333,
                    0.0,
                    0.3339631412072941,
                    0.3333333333333333,
                    0.14285714285714285,
                    1.0
                ]
            ],
            "fraction_answers": {
                "jemele hill": 0.37022383739501025,
                "kenny mayne": 0.30753941364891846,
                "scott van pelt": 0.3222367489560714
            },
            "question": "what sportscenter anchor shares their last name with linus & lucy from \u201cpeanuts\u201d?",
            "rate_limited": false,
            "answers": [
                "jemele hill",
                "scott van pelt",
                "kenny mayne"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "jemele hill": 0.2655068640541657,
                "kenny mayne": 0.1923237100459994,
                "scott van pelt": 0.38837152559445953
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9924423055124707,
                    2.0037788472437645,
                    2.0037788472437645
                ],
                "result_count_important_words": [
                    40.0,
                    31.0,
                    18.0
                ],
                "wikipedia_search": [
                    0.6904761904761905,
                    0.38095238095238093,
                    0.9285714285714286
                ],
                "word_count_appended_bing": [
                    3.0,
                    0.0,
                    0.0
                ],
                "answer_relation_to_question_bing": [
                    1.0537918871252205,
                    1.1172839506172838,
                    0.8289241622574955
                ],
                "cosine_similarity_raw": [
                    0.07089301943778992,
                    0.3605721592903137,
                    0.1411571055650711
                ],
                "result_count_noun_chunks": [
                    61.0,
                    4690.0,
                    13200.0
                ],
                "question_answer_similarity": [
                    2.472496133297682,
                    4.544085974339396,
                    1.5871789250522852
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    2.0,
                    4.0,
                    1.0
                ],
                "result_count_bing": [
                    302000.0,
                    302000.0,
                    302000.0
                ],
                "word_count_appended": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question": [
                    1.014487870619946,
                    0.8268006888289907,
                    1.1587114405510632
                ],
                "result_count": [
                    175000.0,
                    175000.0,
                    175000.0
                ]
            },
            "integer_answers": {
                "jemele hill": 6,
                "kenny mayne": 3,
                "scott van pelt": 5
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Who is the only actor to appear as both a student and a guest on \u201cInside the Actors Studio\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "bradley cooper"
            ],
            "lines": [
                [
                    0.19552384824536884,
                    0.34481534090909094,
                    0.3815108602083751,
                    -0.5029677575685773,
                    0.26548672566371684,
                    0.3340961098398169,
                    0.2777161862527716,
                    0.19710144927536233,
                    0.288332481689661,
                    0.23214285714285715,
                    0.21739130434782608,
                    0.2646036711669378,
                    0.0,
                    0.0,
                    0.0
                ],
                [
                    0.5069029044799633,
                    0.14879261363636365,
                    0.2590060142601899,
                    1.9558769515532988,
                    0.336283185840708,
                    0.34096109839816935,
                    0.32483370288248337,
                    0.3507246376811594,
                    0.12284321239993186,
                    0.16071428571428573,
                    0.13043478260869565,
                    0.2836629401570631,
                    0.0,
                    0.0,
                    0.0
                ],
                [
                    0.2975732472746678,
                    0.5063920454545454,
                    0.359483125531435,
                    -0.45290919398472157,
                    0.39823008849557523,
                    0.32494279176201374,
                    0.397450110864745,
                    0.45217391304347826,
                    0.5888243059104071,
                    0.6071428571428571,
                    0.6521739130434783,
                    0.4517333886759991,
                    1.0,
                    1.0,
                    0.0
                ]
            ],
            "fraction_answers": {
                "tobey maguire": 0.35150259497230796,
                "bradley cooper": 0.4702293280867486,
                "ryan gosling": 0.1782680769409434
            },
            "question": "who is the only actor to appear as both a student and a guest on \u201cinside the actors studio\u201d?",
            "rate_limited": false,
            "answers": [
                "ryan gosling",
                "tobey maguire",
                "bradley cooper"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "tobey maguire": 0.1877323290332801,
                "bradley cooper": 0.6580514912745106,
                "ryan gosling": 0.139938983953725
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.5876220270016268,
                    1.7019776409423788,
                    2.710400332055995
                ],
                "result_count_important_words": [
                    50100.0,
                    58600.0,
                    71700.0
                ],
                "wikipedia_search": [
                    1.7299948901379663,
                    0.7370592743995912,
                    3.5329458354624426
                ],
                "word_count_appended_bing": [
                    5.0,
                    3.0,
                    15.0
                ],
                "answer_relation_to_question_bing": [
                    1.3792613636363638,
                    0.5951704545454546,
                    2.0255681818181817
                ],
                "cosine_similarity_raw": [
                    0.028064705431461334,
                    0.019053002819418907,
                    0.02644430100917816
                ],
                "result_count_noun_chunks": [
                    68000.0,
                    121000.0,
                    156000.0
                ],
                "question_answer_similarity": [
                    1.295287961140275,
                    -5.0369508396834135,
                    1.1663726305123419
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    3.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    146000.0,
                    149000.0,
                    142000.0
                ],
                "word_count_appended": [
                    13.0,
                    9.0,
                    34.0
                ],
                "answer_relation_to_question": [
                    0.9776192412268442,
                    2.534514522399817,
                    1.4878662363733388
                ],
                "result_count": [
                    30.0,
                    38.0,
                    45.0
                ]
            },
            "integer_answers": {
                "tobey maguire": 2,
                "bradley cooper": 10,
                "ryan gosling": 2
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What country recently added two red stripes to its flag?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "venezuela"
            ],
            "lines": [
                [
                    0.17427248677248677,
                    0.2911290322580645,
                    0.17852270575154056,
                    0.6540067805717774,
                    0.9999825853372893,
                    0.29927007299270075,
                    0.9998991252209954,
                    0.4858339048609736,
                    0.3776198275335099,
                    0.494910941475827,
                    0.24786324786324787,
                    0.4361395138007124,
                    0,
                    0.07142857142857142,
                    1.0
                ],
                [
                    0.13315696649029982,
                    0.3887096774193548,
                    0.22936722435581622,
                    0.34599321942822264,
                    1.360520524268421e-05,
                    0.3445255474452555,
                    6.901958563472357e-05,
                    0.5141624124330421,
                    0.6223801724664901,
                    0.4618320610687023,
                    0.5384615384615384,
                    0.47354203666632194,
                    0,
                    0.9285714285714286,
                    1.0
                ],
                [
                    0.6925705467372134,
                    0.32016129032258067,
                    0.5921100698926433,
                    0.0,
                    3.8094574679515785e-06,
                    0.3562043795620438,
                    3.185519336987242e-05,
                    3.682705984368896e-06,
                    0.0,
                    0.043256997455470736,
                    0.21367521367521367,
                    0.09031844953296568,
                    0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "jamaica": 0.4392983689128998,
                "venezuela": 0.38313730073825764,
                "muaritania": 0.17756433034884256
            },
            "question": "what country recently added two red stripes to its flag?",
            "rate_limited": false,
            "answers": [
                "jamaica",
                "venezuela",
                "muaritania"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "jamaica": 0.2722751603793054,
                "venezuela": 0.7318992725222034,
                "muaritania": 0.19439748459447748
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.6168370828042744,
                    2.8412522199979318,
                    0.5419106971977941
                ],
                "result_count_important_words": [
                    1130000.0,
                    78.0,
                    36.0
                ],
                "wikipedia_search": [
                    1.5104793101340397,
                    2.4895206898659605,
                    0.0
                ],
                "word_count_appended_bing": [
                    29.0,
                    63.0,
                    25.0
                ],
                "answer_relation_to_question_bing": [
                    0.582258064516129,
                    0.7774193548387096,
                    0.6403225806451613
                ],
                "cosine_similarity_raw": [
                    0.04639171063899994,
                    0.05960439518094063,
                    0.15386837720870972
                ],
                "result_count_noun_chunks": [
                    3430000.0,
                    3630000.0,
                    26.0
                ],
                "question_answer_similarity": [
                    1.0611465505789965,
                    0.5613848697394133,
                    0.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    13.0,
                    0.0
                ],
                "result_count_bing": [
                    205000.0,
                    236000.0,
                    244000.0
                ],
                "word_count_appended": [
                    389.0,
                    363.0,
                    34.0
                ],
                "answer_relation_to_question": [
                    0.6970899470899471,
                    0.5326278659611993,
                    2.7702821869488536
                ],
                "result_count": [
                    7350000.0,
                    100.0,
                    28.0
                ]
            },
            "integer_answers": {
                "jamaica": 4,
                "venezuela": 6,
                "muaritania": 3
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "According to the old saying, what kind of animal can NOT change its spots?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "tiger"
            ],
            "lines": [
                [
                    0.3519868173258004,
                    0.4278692810457516,
                    0.48297840972934664,
                    0.3649204933253706,
                    0.47639653815892996,
                    0.3651741293532338,
                    0.4842488935173132,
                    0.4802057054143217,
                    0.4863121874177415,
                    0.35756302521008404,
                    0.36250000000000004,
                    0.33741115652829057,
                    0.48175182481751827,
                    0.5,
                    1.0
                ],
                [
                    0.24612366603892027,
                    0.16626143790849673,
                    0.056054565337792794,
                    0.3445936692250927,
                    0.4760031471282455,
                    0.3756218905472637,
                    0.2721947409528769,
                    0.4758393169027751,
                    0.3476472195281712,
                    0.3021008403361345,
                    0.2833333333333334,
                    0.32904961566497587,
                    0.07664233576642338,
                    0.01315789473684209,
                    1.0
                ],
                [
                    0.40188951663527933,
                    0.4058692810457516,
                    0.46096702493286057,
                    0.2904858374495367,
                    0.047600314712824554,
                    0.2592039800995025,
                    0.24355636552980997,
                    0.043954977682903185,
                    0.1660405930540873,
                    0.3403361344537815,
                    0.3541666666666667,
                    0.3335392278067337,
                    0.4416058394160584,
                    0.4868421052631579,
                    1.0
                ]
            ],
            "fraction_answers": {
                "tiger": 0.3891345907501494,
                "leopard": 0.46219661808466517,
                "zebra": 0.1486687911651854
            },
            "question": "according to the old saying, what kind of animal can not change its spots?",
            "rate_limited": false,
            "answers": [
                "zebra",
                "leopard",
                "tiger"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "tiger": 0.34629323055895384,
                "leopard": 0.11531274684001594,
                "zebra": 0.330715345108869
            },
            "integer_answers": {
                "tiger": 6,
                "leopard": 8,
                "zebra": 0
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.2762438086039327,
                    2.393305380690338,
                    2.330450810705729
                ],
                "result_count_important_words": [
                    1210000.0,
                    17500000.0,
                    19700000.0
                ],
                "wikipedia_search": [
                    0.16425375098710188,
                    1.8282333656619458,
                    4.007512883350953
                ],
                "answer_relation_to_question": [
                    1.7761581920903955,
                    3.046516007532957,
                    1.1773258003766478
                ],
                "word_count_appended_bing": [
                    33.0,
                    52.0,
                    35.0
                ],
                "answer_relation_to_question_bing": [
                    0.7213071895424836,
                    3.3373856209150325,
                    0.9413071895424837
                ],
                "cosine_similarity_raw": [
                    0.025274302810430527,
                    0.6591870188713074,
                    0.057957641780376434
                ],
                "result_count_noun_chunks": [
                    816000.0,
                    996000.0,
                    18800000.0
                ],
                "question_answer_similarity": [
                    2.0059347860515118,
                    2.3077887427061796,
                    3.111291691660881
                ],
                "word_count_noun_chunks": [
                    5.0,
                    116.0,
                    16.0
                ],
                "result_count_bing": [
                    27100000.0,
                    25000000.0,
                    48400000.0
                ],
                "word_count_raw": [
                    0.0,
                    74.0,
                    2.0
                ],
                "result_count": [
                    1200000.0,
                    1220000.0,
                    23000000.0
                ],
                "word_count_appended": [
                    339.0,
                    471.0,
                    380.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these astronomical objects orbits the Earth?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "sun"
            ],
            "lines": [
                [
                    0.29500000000000004,
                    0.6861702127659575,
                    0.42211034075305975,
                    0.28026798016523896,
                    0.32674772036474165,
                    0.08620689655172414,
                    0.27139507620164127,
                    0.26063829787234044,
                    0.40514200711569137,
                    0.4921875,
                    0.3448275862068966,
                    0.3389424976476997,
                    0.4918032786885246,
                    0.3333333333333333,
                    -1.0
                ],
                [
                    0.545,
                    0.30319148936170215,
                    0.37529086926307464,
                    0.4728802127287828,
                    0.2066869300911854,
                    0.2150635208711434,
                    0.16588511137162953,
                    0.20638297872340425,
                    0.3276821862348178,
                    0.09040178571428571,
                    0.2413793103448276,
                    0.3254996673373903,
                    0.06557377049180328,
                    0.0,
                    -1.0
                ],
                [
                    0.16000000000000003,
                    0.010638297872340425,
                    0.2025987899838656,
                    0.2468518071059783,
                    0.46656534954407297,
                    0.6987295825771325,
                    0.5627198124267292,
                    0.5329787234042553,
                    0.26717580664949087,
                    0.4174107142857143,
                    0.41379310344827586,
                    0.33555783501491,
                    0.4426229508196721,
                    0.6666666666666666,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "sun": 0.3874506742713646,
                "milky way": 0.25292270232386044,
                "moon": 0.3596266234047749
            },
            "question": "which of these astronomical objects orbits the earth?",
            "rate_limited": false,
            "answers": [
                "moon",
                "milky way",
                "sun"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sun": 0.6351742441216272,
                "milky way": 0.18299687276582882,
                "moon": 0.6189866885092548
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3557699905907987,
                    1.301998669349561,
                    1.34223134005964
                ],
                "result_count_important_words": [
                    926000.0,
                    566000.0,
                    1920000.0
                ],
                "wikipedia_search": [
                    1.6205680284627655,
                    1.3107287449392713,
                    1.0687032265979635
                ],
                "answer_relation_to_question": [
                    1.18,
                    2.1799999999999997,
                    0.64
                ],
                "result_count": [
                    2150000.0,
                    1360000.0,
                    3070000.0
                ],
                "answer_relation_to_question_bing": [
                    2.74468085106383,
                    1.2127659574468086,
                    0.0425531914893617
                ],
                "cosine_similarity_raw": [
                    0.08714041858911514,
                    0.07747501134872437,
                    0.041824474930763245
                ],
                "result_count_noun_chunks": [
                    2450000.0,
                    1940000.0,
                    5010000.0
                ],
                "question_answer_similarity": [
                    3.1645723432302475,
                    5.3394028171896935,
                    2.7872623950242996
                ],
                "word_count_noun_chunks": [
                    60.0,
                    8.0,
                    54.0
                ],
                "word_count_raw": [
                    13.0,
                    0.0,
                    26.0
                ],
                "result_count_bing": [
                    190000.0,
                    474000.0,
                    1540000.0
                ],
                "word_count_appended": [
                    441.0,
                    81.0,
                    374.0
                ],
                "word_count_appended_bing": [
                    50.0,
                    35.0,
                    60.0
                ]
            },
            "integer_answers": {
                "sun": 6,
                "milky way": 2,
                "moon": 6
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these famous fictional animals is a ruminant?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "bambi"
            ],
            "question": "which of these famous fictional animals is a ruminant?",
            "lines": [
                [
                    0.28174603174603174,
                    0.5,
                    0.47394566932414944,
                    0.022271896373905094,
                    0.9651076466221232,
                    0.42735042735042733,
                    0.4187844512849882,
                    0.9653647752394989,
                    0.3333333333333333,
                    0.5661016949152542,
                    0.37623762376237624,
                    0.4727225002107549,
                    1.0,
                    0,
                    -1.0
                ],
                [
                    0.35714285714285715,
                    0.0,
                    0.2658534546588037,
                    1.4944997655711405,
                    0.028062360801781736,
                    0.10945685139233527,
                    0.015677571766053403,
                    0.02800294767870302,
                    0.5333333333333333,
                    0.06779661016949153,
                    0.13861386138613863,
                    0.2230409356662501,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.3611111111111111,
                    0.5,
                    0.26020087601704683,
                    -0.5167716619450456,
                    0.006829992576095026,
                    0.46319272125723737,
                    0.5655379769489584,
                    0.006632277081798084,
                    0.13333333333333333,
                    0.36610169491525424,
                    0.48514851485148514,
                    0.30423656412299505,
                    0.0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "bambi",
                "mister ed",
                "eeyore"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "mister ed": 0.12789234060698845,
                "bambi": 0.6459955209948428,
                "eeyore": 0.2559255276754089
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.8908900008430196,
                    0.8921637426650004,
                    1.2169462564919802
                ],
                "result_count_important_words": [
                    11700.0,
                    438.0,
                    15800.0
                ],
                "wikipedia_search": [
                    0.6666666666666666,
                    1.0666666666666667,
                    0.26666666666666666
                ],
                "word_count_appended_bing": [
                    38.0,
                    14.0,
                    49.0
                ],
                "answer_relation_to_question_bing": [
                    1.0,
                    0.0,
                    1.0
                ],
                "cosine_similarity_raw": [
                    0.05643796920776367,
                    0.0316581204533577,
                    0.03098500519990921
                ],
                "result_count_noun_chunks": [
                    13100.0,
                    380.0,
                    90.0
                ],
                "question_answer_similarity": [
                    0.028029044391587377,
                    1.8808187488466501,
                    -0.6503539532423019
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    1550000.0,
                    397000.0,
                    1680000.0
                ],
                "word_count_appended": [
                    167.0,
                    20.0,
                    108.0
                ],
                "answer_relation_to_question": [
                    0.8452380952380952,
                    1.0714285714285714,
                    1.0833333333333333
                ],
                "result_count": [
                    13000.0,
                    378.0,
                    92.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these is NOT a geometric shape?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "tarragon"
            ],
            "lines": [
                [
                    0.4285714285714286,
                    0.23529411764705882,
                    0.2987363433453165,
                    0.24774906815628073,
                    0.326530612244898,
                    0.284688995215311,
                    0.4805429483946882,
                    0.48153499189513005,
                    0.35,
                    0.33739837398373984,
                    0.27808988764044945,
                    0.28835668158731786,
                    0.2857142857142857,
                    0,
                    -1.0
                ],
                [
                    0.5,
                    0.5,
                    0.36178761729647885,
                    0.5086250853281601,
                    0.47959183673469385,
                    0.3656299840510367,
                    0.4943267776096823,
                    0.49418563676087107,
                    0.4,
                    0.3902439024390244,
                    0.42696629213483145,
                    0.400116073260825,
                    0.5,
                    0,
                    -1.0
                ],
                [
                    0.0714285714285714,
                    0.2647058823529412,
                    0.33947603935820464,
                    0.24362584651555919,
                    0.19387755102040816,
                    0.3496810207336523,
                    0.025130273995629504,
                    0.02427937134399888,
                    0.25,
                    0.27235772357723576,
                    0.2949438202247191,
                    0.31152724515185715,
                    0.2142857142857143,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "hexagon": 0.5607201446171551,
                "tarragon": 0.10438873759759944,
                "octagon": 0.33489111778524544
            },
            "question": "which of these is not a geometric shape?",
            "rate_limited": false,
            "answers": [
                "octagon",
                "tarragon",
                "hexagon"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "hexagon": 0.2887702952589943,
                "tarragon": 0.43914396811896894,
                "octagon": 0.19464335480584577
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.8465732736507285,
                    0.39953570695670015,
                    0.7538910193925713
                ],
                "result_count_important_words": [
                    463000.0,
                    135000.0,
                    11300000.0
                ],
                "wikipedia_search": [
                    0.3,
                    0.2,
                    0.5
                ],
                "answer_relation_to_question": [
                    0.2857142857142857,
                    0.0,
                    1.7142857142857144
                ],
                "answer_relation_to_question_bing": [
                    0.5294117647058824,
                    0.0,
                    0.47058823529411764
                ],
                "word_count_appended": [
                    360.0,
                    243.0,
                    504.0
                ],
                "cosine_similarity_raw": [
                    0.051331717520952225,
                    0.035250671207904816,
                    0.04094117507338524
                ],
                "result_count_noun_chunks": [
                    524000.0,
                    165000.0,
                    13500000.0
                ],
                "question_answer_similarity": [
                    0.9916807818226516,
                    -0.03390802681678906,
                    1.0078905124682933
                ],
                "word_count_noun_chunks": [
                    3.0,
                    0.0,
                    4.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    5400000.0,
                    3370000.0,
                    3770000.0
                ],
                "result_count": [
                    1360000.0,
                    160000.0,
                    2400000.0
                ],
                "word_count_appended_bing": [
                    79.0,
                    26.0,
                    73.0
                ]
            },
            "integer_answers": {
                "hexagon": 8,
                "tarragon": 0,
                "octagon": 5
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What tech mogul became a billionaire the youngest?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "evan spiegel"
            ],
            "lines": [
                [
                    0.7347562808029812,
                    0.6233974358974359,
                    0.44254662695241587,
                    -0.03201012515269249,
                    0.008794910179640719,
                    0.24464668094218414,
                    0.010310517930682057,
                    0.06176735798016231,
                    0.4163832199546485,
                    0.42735042735042733,
                    0.5454545454545454,
                    0.3266936451626309,
                    0.1111111111111111,
                    0.3,
                    1.0
                ],
                [
                    0.14565302319990384,
                    0.16506410256410256,
                    0.2451562391435868,
                    0.4607635381042576,
                    0.966816367265469,
                    0.5182012847965739,
                    0.9649201911804491,
                    0.8295761947700632,
                    0.34268707482993194,
                    0.3803418803418803,
                    0.3939393939393939,
                    0.3547618730571236,
                    0.8888888888888888,
                    0.4,
                    1.0
                ],
                [
                    0.11959069599711504,
                    0.21153846153846154,
                    0.3122971339039973,
                    0.5712465870484349,
                    0.02438872255489022,
                    0.23715203426124196,
                    0.02476929088886885,
                    0.10865644724977457,
                    0.24092970521541948,
                    0.19230769230769232,
                    0.06060606060606061,
                    0.3185444817802455,
                    0.0,
                    0.3,
                    1.0
                ]
            ],
            "fraction_answers": {
                "mark zuckerberg": 0.504055003720116,
                "evan spiegel": 0.3015144738975838,
                "larry page": 0.19443052238230019
            },
            "question": "what tech mogul became a billionaire the youngest?",
            "rate_limited": false,
            "answers": [
                "evan spiegel",
                "mark zuckerberg",
                "larry page"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "mark zuckerberg": 0.4234840831440445,
                "evan spiegel": 0.46310304549881814,
                "larry page": 0.17930681769438886
            },
            "integer_answers": {
                "mark zuckerberg": 7,
                "evan spiegel": 6,
                "larry page": 1
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3067745806505235,
                    1.4190474922284944,
                    1.274177927120982
                ],
                "result_count_important_words": [
                    3430.0,
                    321000.0,
                    8240.0
                ],
                "wikipedia_search": [
                    1.2491496598639455,
                    1.0280612244897958,
                    0.7227891156462585
                ],
                "word_count_appended_bing": [
                    18.0,
                    13.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.8701923076923077,
                    0.4951923076923077,
                    0.6346153846153846
                ],
                "cosine_similarity_raw": [
                    0.2256964147090912,
                    0.12502837181091309,
                    0.15926986932754517
                ],
                "result_count_noun_chunks": [
                    13700.0,
                    184000.0,
                    24100.0
                ],
                "question_answer_similarity": [
                    -0.14885316602885723,
                    2.1426380281336606,
                    2.6564052049070597
                ],
                "word_count_noun_chunks": [
                    1.0,
                    8.0,
                    0.0
                ],
                "result_count_bing": [
                    45700.0,
                    96800.0,
                    44300.0
                ],
                "word_count_raw": [
                    3.0,
                    4.0,
                    3.0
                ],
                "result_count": [
                    2820.0,
                    310000.0,
                    7820.0
                ],
                "answer_relation_to_question": [
                    2.9390251232119247,
                    0.5826120927996153,
                    0.47836278398846016
                ],
                "word_count_appended": [
                    100.0,
                    89.0,
                    45.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Who is the only person to win both the Super Bowl and an Olympic gold medal?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "bob hayes"
            ],
            "question": "who is the only person to win both the super bowl and an olympic gold medal?",
            "lines": [
                [
                    0.42160717217845634,
                    0.40118367346938777,
                    0.7667769611830259,
                    0.492300018874787,
                    0.45402014198448076,
                    0.8489208633093526,
                    0.39375,
                    0.508419689119171,
                    0.592528735632184,
                    0.4626334519572954,
                    0.5862068965517241,
                    0.36751536200114177,
                    0.8055555555555556,
                    0.92,
                    0.0
                ],
                [
                    0.3219137465724928,
                    0.30718367346938774,
                    0.07095274366524126,
                    -0.28014865154386,
                    0.02922238732045567,
                    0.021377183967112023,
                    0.15875,
                    0.03011658031088083,
                    0.3646141215106732,
                    0.21352313167259787,
                    0.20689655172413793,
                    0.29349479641570775,
                    0.0,
                    0.0,
                    0.0
                ],
                [
                    0.2564790812490509,
                    0.2916326530612245,
                    0.1622702951517329,
                    0.7878486326690729,
                    0.5167574706950636,
                    0.12970195272353546,
                    0.4475,
                    0.46146373056994816,
                    0.04285714285714286,
                    0.3238434163701068,
                    0.20689655172413793,
                    0.33898984158315043,
                    0.19444444444444445,
                    0.08,
                    0.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "bob hayes",
                "willie gault",
                "michael carter"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "bob hayes": 0.7979488672384695,
                "willie gault": 0.05252119473885747,
                "michael carter": 0.07152597354095637
            },
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.5726075340079926,
                    2.0544635749099545,
                    2.3729288910820534
                ],
                "result_count_important_words": [
                    31500.0,
                    12700.0,
                    35800.0
                ],
                "wikipedia_search": [
                    4.147701149425288,
                    2.5522988505747124,
                    0.3
                ],
                "word_count_appended_bing": [
                    17.0,
                    6.0,
                    6.0
                ],
                "answer_relation_to_question_bing": [
                    2.8082857142857143,
                    2.1502857142857144,
                    2.0414285714285714
                ],
                "cosine_similarity_raw": [
                    0.5169486403465271,
                    0.04783519357442856,
                    0.10940001159906387
                ],
                "result_count_noun_chunks": [
                    31400.0,
                    1860.0,
                    28500.0
                ],
                "question_answer_similarity": [
                    2.845189206302166,
                    -1.619085697690025,
                    4.553277147933841
                ],
                "word_count_noun_chunks": [
                    29.0,
                    0.0,
                    7.0
                ],
                "word_count_raw": [
                    23.0,
                    0.0,
                    2.0
                ],
                "result_count_bing": [
                    4130000.0,
                    104000.0,
                    631000.0
                ],
                "result_count": [
                    27500.0,
                    1770.0,
                    31300.0
                ],
                "answer_relation_to_question": [
                    2.9512502052491945,
                    2.2533962260074496,
                    1.7953535687433564
                ],
                "word_count_appended": [
                    130.0,
                    60.0,
                    91.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these is NOT a marsupial?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "quintana roo"
            ],
            "lines": [
                [
                    0.5,
                    0.5,
                    0.4727917637133819,
                    0.18376836484494402,
                    0.4926317577610736,
                    0.33599290780141844,
                    0.45655399835119537,
                    0.4577927278551978,
                    0.5,
                    0.4683333333333333,
                    0.4619883040935673,
                    0.4084577114427861,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.2013888888888889,
                    0.33333333333333337,
                    0.36049582888042675,
                    0.17720885310834011,
                    0.4660253960164777,
                    0.38475177304964536,
                    0.4572959604286892,
                    0.45466922953708155,
                    0.5,
                    0.28791666666666665,
                    0.3128654970760234,
                    0.30199004975124377,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.2986111111111111,
                    0.16666666666666669,
                    0.16671240740619137,
                    0.6390227820467158,
                    0.041342846222448715,
                    0.2792553191489362,
                    0.08615004122011544,
                    0.08753804260772063,
                    0.0,
                    0.24375000000000002,
                    0.22514619883040937,
                    0.28955223880597014,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "cuscus": 0.25172264618045476,
                "quintana roo": 0.10881273297187175,
                "wombat": 0.6394646208476736
            },
            "question": "which of these is not a marsupial?",
            "rate_limited": false,
            "answers": [
                "quintana roo",
                "cuscus",
                "wombat"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "cuscus": 0.15946509319464014,
                "quintana roo": 0.8501752383736512,
                "wombat": 0.10004507654825694
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.18308457711442785,
                    0.39601990049751246,
                    0.4208955223880597
                ],
                "result_count_important_words": [
                    52700.0,
                    51800.0,
                    502000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    1.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    0.5972222222222222,
                    0.4027777777777778
                ],
                "result_count": [
                    34700.0,
                    160000.0,
                    2160000.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.3333333333333333,
                    0.6666666666666666
                ],
                "cosine_similarity_raw": [
                    0.011475966311991215,
                    0.05884046107530594,
                    0.14057497680187225
                ],
                "result_count_noun_chunks": [
                    52700.0,
                    56600.0,
                    515000.0
                ],
                "question_answer_similarity": [
                    -0.7520406674593687,
                    -0.7676400542259216,
                    0.33061456913128495
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    14.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    11.0
                ],
                "result_count_bing": [
                    1850000.0,
                    1300000.0,
                    2490000.0
                ],
                "word_count_appended": [
                    76.0,
                    509.0,
                    615.0
                ],
                "word_count_appended_bing": [
                    13.0,
                    64.0,
                    94.0
                ]
            },
            "integer_answers": {
                "cuscus": 1,
                "quintana roo": 0,
                "wombat": 13
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Appropriately, science-themed \u201cMole Day\u201d occurs in what month?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "october"
            ],
            "question": "appropriately, science-themed \u201cmole day\u201d occurs in what month?",
            "lines": [
                [
                    0.3839638565361659,
                    0.22222828459188126,
                    0.29559937998771996,
                    0.342164539306822,
                    0.3153153153153153,
                    0.321011673151751,
                    0.34134419551934825,
                    0.3473389355742297,
                    0.1267010064478419,
                    0.2799043062200957,
                    0.25663716814159293,
                    0.3199997047050357,
                    0.0,
                    0.3333333333333333,
                    1.0
                ],
                [
                    0.29822707993893055,
                    0.4919794849410738,
                    0.3565282442850053,
                    0.3149441061282392,
                    0.30180180180180183,
                    0.33949416342412453,
                    0.32260692464358454,
                    0.3137254901960784,
                    0.4225437010247137,
                    0.2942583732057416,
                    0.3185840707964602,
                    0.31311125087694125,
                    0.011235955056179775,
                    0.0,
                    1.0
                ],
                [
                    0.31780906352490357,
                    0.28579223046704494,
                    0.34787237572727475,
                    0.3428913545649388,
                    0.38288288288288286,
                    0.33949416342412453,
                    0.3360488798370672,
                    0.3389355742296919,
                    0.45075529252744445,
                    0.4258373205741627,
                    0.4247787610619469,
                    0.36688904441802306,
                    0.9887640449438202,
                    0.6666666666666666,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "july",
                "february",
                "october"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "july": 0.015706595592299845,
                "october": 0.6975681348231264,
                "february": 0.11100097711990284
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.2399979329352497,
                    2.191778756138589,
                    2.5682233109261614
                ],
                "result_count_important_words": [
                    838000.0,
                    792000.0,
                    825000.0
                ],
                "wikipedia_search": [
                    0.5068040257913675,
                    1.6901748040988547,
                    1.8030211701097776
                ],
                "word_count_appended_bing": [
                    29.0,
                    36.0,
                    48.0
                ],
                "answer_relation_to_question_bing": [
                    0.44445656918376253,
                    0.9839589698821476,
                    0.5715844609340899
                ],
                "cosine_similarity_raw": [
                    0.08442221581935883,
                    0.10182330012321472,
                    0.09935121238231659
                ],
                "result_count_noun_chunks": [
                    124000.0,
                    112000.0,
                    121000.0
                ],
                "question_answer_similarity": [
                    2.0873295377241448,
                    1.921274883672595,
                    2.091763378120959
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    88.0
                ],
                "result_count_bing": [
                    330000.0,
                    349000.0,
                    349000.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    2.0
                ],
                "result_count": [
                    70.0,
                    67.0,
                    85.0
                ],
                "answer_relation_to_question": [
                    1.1518915696084977,
                    0.8946812398167916,
                    0.9534271905747107
                ],
                "word_count_appended": [
                    117.0,
                    123.0,
                    178.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "New Mexico exempts which group of people from paying state income taxes?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "registered lobbyists"
            ],
            "question": "new mexico exempts which group of people from paying state income taxes?",
            "lines": [
                [
                    0.6514820367593981,
                    0.19928030303030306,
                    0.4585537966193516,
                    0.6084988759297891,
                    0.0009277099835866696,
                    0.2728442728442728,
                    0.003223406893131664,
                    0.004267425320056899,
                    0.5150498622001857,
                    0.04316546762589928,
                    0.05714285714285714,
                    0.10487410649449141,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.20676439557998777,
                    0.3728409090909091,
                    0.19206939601758388,
                    0.011887963488703676,
                    0.899165061014772,
                    0.34491634491634493,
                    0.6496404661542277,
                    0.6448553816974869,
                    0.20149161026463167,
                    0.841726618705036,
                    0.8857142857142857,
                    0.41084792965239303,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.14175356766061414,
                    0.4278787878787879,
                    0.34937680736306453,
                    0.3796131605815072,
                    0.09990722900164133,
                    0.38223938223938225,
                    0.34713612695264073,
                    0.3508771929824561,
                    0.2834585275351826,
                    0.11510791366906475,
                    0.05714285714285714,
                    0.48427796385311567,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "hybrid car owners",
                "centenarians",
                "registered lobbyists"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "hybrid car owners": 0.32940694968852385,
                "registered lobbyists": 0.38624147345183246,
                "centenarians": 0.3053503561175735
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.9438669584504227,
                    3.697631366871537,
                    4.358501674678041
                ],
                "result_count_important_words": [
                    13.0,
                    2620.0,
                    1400.0
                ],
                "wikipedia_search": [
                    3.6053490354013,
                    1.4104412718524217,
                    1.984209692746278
                ],
                "word_count_appended_bing": [
                    2.0,
                    31.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.7971212121212121,
                    1.4913636363636362,
                    1.7115151515151514
                ],
                "cosine_similarity_raw": [
                    0.026421042159199715,
                    0.01106669194996357,
                    0.02013046108186245
                ],
                "result_count_noun_chunks": [
                    18.0,
                    2720.0,
                    1480.0
                ],
                "question_answer_similarity": [
                    9.64158707903698,
                    0.18836326524615288,
                    6.014922112226486
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    424000.0,
                    536000.0,
                    594000.0
                ],
                "word_count_appended": [
                    6.0,
                    117.0,
                    16.0
                ],
                "answer_relation_to_question": [
                    5.211856294075185,
                    1.6541151646399022,
                    1.1340285412849131
                ],
                "result_count": [
                    13.0,
                    12600.0,
                    1400.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which human sense is most closely associated with the bony labyrinth?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "hearing"
            ],
            "lines": [
                [
                    0.4443371943371943,
                    0.2303921568627451,
                    0.13707029914483904,
                    0.3330204235621606,
                    0.4420459877991553,
                    0.18975332068311196,
                    0.34513274336283184,
                    0.3004694835680751,
                    0.3904761904761905,
                    0.30383480825958703,
                    0.35135135135135137,
                    0.33482126809050206,
                    0.4,
                    0.0,
                    -1.0
                ],
                [
                    0.3844916344916345,
                    0.6813725490196079,
                    0.6765664652398113,
                    0.3099602330094307,
                    0.44533083059596434,
                    0.6204933586337761,
                    0.3303834808259587,
                    0.37715179968701096,
                    0.5,
                    0.40707964601769914,
                    0.3783783783783784,
                    0.33063610415274153,
                    0.28888888888888886,
                    1.0,
                    -1.0
                ],
                [
                    0.17117117117117117,
                    0.08823529411764706,
                    0.1863632356153496,
                    0.3570193434284087,
                    0.11262318160488034,
                    0.18975332068311196,
                    0.32448377581120946,
                    0.3223787167449139,
                    0.10952380952380951,
                    0.2890855457227139,
                    0.2702702702702703,
                    0.3345426277567564,
                    0.3111111111111111,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "touch": 0.30019323053555313,
                "sight": 0.21904010025438236,
                "hearing": 0.48076666921006445
            },
            "question": "which human sense is most closely associated with the bony labyrinth?",
            "rate_limited": false,
            "answers": [
                "touch",
                "hearing",
                "sight"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "touch": 0.11523992049450761,
                "sight": 0.05328575333416943,
                "hearing": 0.8638491898393257
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.0089276085430123,
                    1.9838166249164493,
                    2.0072557665405384
                ],
                "result_count_important_words": [
                    234000.0,
                    224000.0,
                    220000.0
                ],
                "wikipedia_search": [
                    1.9523809523809523,
                    2.5,
                    0.5476190476190476
                ],
                "word_count_appended_bing": [
                    39.0,
                    42.0,
                    30.0
                ],
                "answer_relation_to_question_bing": [
                    0.4607843137254902,
                    1.3627450980392157,
                    0.17647058823529413
                ],
                "cosine_similarity_raw": [
                    0.03414810448884964,
                    0.16855192184448242,
                    0.0464283749461174
                ],
                "result_count_noun_chunks": [
                    192000000.0,
                    241000000.0,
                    206000000.0
                ],
                "question_answer_similarity": [
                    3.739047773182392,
                    3.4801352620124817,
                    4.008500039577484
                ],
                "word_count_noun_chunks": [
                    36.0,
                    26.0,
                    28.0
                ],
                "word_count_raw": [
                    0.0,
                    6.0,
                    0.0
                ],
                "result_count_bing": [
                    30000.0,
                    98100.0,
                    30000.0
                ],
                "word_count_appended": [
                    206.0,
                    276.0,
                    196.0
                ],
                "answer_relation_to_question": [
                    1.333011583011583,
                    1.1534749034749034,
                    0.5135135135135135
                ],
                "result_count": [
                    94200.0,
                    94900.0,
                    24000.0
                ]
            },
            "integer_answers": {
                "touch": 4,
                "sight": 1,
                "hearing": 9
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these is the name of a machine used to punish prisoners in the 1800s?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "treadmill"
            ],
            "question": "which of these is the name of a machine used to punish prisoners in the 1800s?",
            "lines": [
                [
                    0.5429553264604811,
                    0.5681818181818181,
                    0.743190559080613,
                    0.14184763585797272,
                    0.9995769771678993,
                    0.42096597145993414,
                    0.9979187486298191,
                    0.2980347261972906,
                    0.8296296296296296,
                    0.9619450317124736,
                    0.9512195121951219,
                    0.6544911767863982,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.37285223367697595,
                    0.22727272727272727,
                    0.08321998091689486,
                    0.25525397708776704,
                    0.0004174567422046911,
                    0.42371020856201974,
                    0.002049932950109757,
                    0.690707880175539,
                    0.1111111111111111,
                    0.02536997885835095,
                    0.024390243902439025,
                    0.23055751876265063,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.08419243986254296,
                    0.20454545454545456,
                    0.17358946000249215,
                    0.6028983870542602,
                    5.566089896062548e-06,
                    0.1553238199780461,
                    3.131842007112129e-05,
                    0.011257393627170388,
                    0.05925925925925926,
                    0.012684989429175475,
                    0.024390243902439025,
                    0.11495130445095104,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "treadmill",
                "elliptical trainer",
                "bowflex home gym"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "bowflex home gym": 0.197914836896855,
                "elliptical trainer": 0.18733881260735896,
                "treadmill": 0.7808898297787332
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.6179647071455934,
                    0.9222300750506027,
                    0.4598052178038043
                ],
                "result_count_important_words": [
                    701000.0,
                    1440.0,
                    22.0
                ],
                "wikipedia_search": [
                    2.488888888888889,
                    0.3333333333333333,
                    0.17777777777777778
                ],
                "word_count_appended_bing": [
                    78.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.1363636363636362,
                    0.45454545454545453,
                    0.4090909090909091
                ],
                "cosine_similarity_raw": [
                    0.15480893850326538,
                    0.017334984615445137,
                    0.03615923225879669
                ],
                "result_count_noun_chunks": [
                    781000.0,
                    1810000.0,
                    29500.0
                ],
                "question_answer_similarity": [
                    1.943046879954636,
                    3.4965013042092323,
                    8.258578458568081
                ],
                "word_count_noun_chunks": [
                    28.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    76700.0,
                    77200.0,
                    28300.0
                ],
                "word_count_raw": [
                    41.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    4310000.0,
                    1800.0,
                    24.0
                ],
                "answer_relation_to_question": [
                    1.0859106529209621,
                    0.7457044673539519,
                    0.16838487972508592
                ],
                "word_count_appended": [
                    455.0,
                    12.0,
                    6.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Buying a can of soda will incur a higher tax in which city?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "seattle, wa"
            ],
            "question": "buying a can of soda will incur a higher tax in which city?",
            "lines": [
                [
                    0.4216417910447761,
                    0.2542372881355932,
                    0.2529294057713967,
                    0.44241537538975556,
                    0.1412536259301299,
                    0.1505488761108207,
                    0.2912256742915671,
                    0.0007211748016769296,
                    0.11364227877385773,
                    0.3387096774193548,
                    0.5263157894736842,
                    0.2852641526215574,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.23400852878464817,
                    0.3728813559322034,
                    0.323776209793746,
                    0.2886675495227158,
                    0.8134695421869088,
                    0.4547830632514375,
                    0.3789689313758962,
                    0.0007290997994975552,
                    0.31582562174667445,
                    0.3387096774193548,
                    0.21052631578947367,
                    0.35504795453041793,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.34434968017057566,
                    0.3728813559322034,
                    0.4232943844348573,
                    0.26891707508752866,
                    0.045276831882961284,
                    0.3946680606377418,
                    0.3298053943325367,
                    0.9985497253988255,
                    0.5705320994794679,
                    0.3225806451612903,
                    0.2631578947368421,
                    0.3596878928480247,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "santa barbara, ca",
                "denver, co",
                "seattle, wa"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "denver, co": 0.3462605807467021,
                "santa barbara, ca": 0.05206766096814396,
                "seattle, wa": 0.6605688207902691
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7115849157293443,
                    2.1302877271825076,
                    2.1581273570881483
                ],
                "result_count_important_words": [
                    8530.0,
                    11100.0,
                    9660.0
                ],
                "wikipedia_search": [
                    0.45456911509543085,
                    1.2633024869866976,
                    2.2821283979178713
                ],
                "answer_relation_to_question": [
                    0.8432835820895522,
                    0.46801705756929635,
                    0.6886993603411513
                ],
                "word_count_appended_bing": [
                    10.0,
                    4.0,
                    5.0
                ],
                "answer_relation_to_question_bing": [
                    0.2542372881355932,
                    0.3728813559322034,
                    0.3728813559322034
                ],
                "cosine_similarity_raw": [
                    0.03213835507631302,
                    0.041140470653772354,
                    0.05378570035099983
                ],
                "result_count_noun_chunks": [
                    91.0,
                    92.0,
                    126000.0
                ],
                "question_answer_similarity": [
                    11.573573343455791,
                    7.551534693688154,
                    7.034862857311964
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    576000.0,
                    1740000.0,
                    1510000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    112000.0,
                    645000.0,
                    35900.0
                ],
                "word_count_appended": [
                    21.0,
                    21.0,
                    20.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which TV comedy centers on a vice president?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "veep"
            ],
            "lines": [
                [
                    0.14932600732600732,
                    0.2682685334969599,
                    0.0560686828040383,
                    0.6525233818279421,
                    0.055091054861986025,
                    0.09076227390180878,
                    0.06997863247863248,
                    0.06993795826283136,
                    0.31792355371900827,
                    0.0791974656810982,
                    0.08396946564885496,
                    0.32383877163397756,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.16888278388278388,
                    0.19097590227031852,
                    0.04037540835477659,
                    0.17565075979398295,
                    0.4346581147634864,
                    0.8107235142118863,
                    0.390491452991453,
                    0.39593908629441626,
                    0.02706611570247934,
                    0.2576557550158395,
                    0.3053435114503817,
                    0.29775496033904914,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.6817912087912088,
                    0.5407555642327215,
                    0.9035559088411851,
                    0.17182585837807487,
                    0.5102508303745276,
                    0.09851421188630491,
                    0.5395299145299145,
                    0.5341229554427523,
                    0.6550103305785124,
                    0.6631467793030623,
                    0.6106870229007634,
                    0.3784062680269733,
                    1.0,
                    1.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "young sheldon": 0.15834898440308182,
                "veep": 0.5919712038061429,
                "superstore": 0.24967981179077525
            },
            "question": "which tv comedy centers on a vice president?",
            "rate_limited": false,
            "answers": [
                "young sheldon",
                "superstore",
                "veep"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "young sheldon": 0.14270027711526276,
                "veep": 0.8616135232371673,
                "superstore": 0.14364367220201885
            },
            "integer_answers": {
                "young sheldon": 1,
                "veep": 12,
                "superstore": 1
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6191938581698877,
                    1.4887748016952458,
                    1.8920313401348665
                ],
                "result_count_important_words": [
                    13100.0,
                    73100.0,
                    101000.0
                ],
                "wikipedia_search": [
                    1.271694214876033,
                    0.10826446280991736,
                    2.6200413223140497
                ],
                "word_count_appended_bing": [
                    11.0,
                    40.0,
                    80.0
                ],
                "answer_relation_to_question_bing": [
                    1.0730741339878396,
                    0.7639036090812741,
                    2.163022256930886
                ],
                "cosine_similarity_raw": [
                    0.023849716410040855,
                    0.017174329608678818,
                    0.3843420445919037
                ],
                "result_count_noun_chunks": [
                    12400.0,
                    70200.0,
                    94700.0
                ],
                "question_answer_similarity": [
                    2.3016564340214245,
                    0.6195758078247309,
                    0.6060841702856123
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    22.0
                ],
                "result_count_bing": [
                    2810000.0,
                    25100000.0,
                    3050000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    9.0
                ],
                "word_count_appended": [
                    75.0,
                    244.0,
                    628.0
                ],
                "answer_relation_to_question": [
                    0.7466300366300366,
                    0.8444139194139194,
                    3.408956043956044
                ],
                "result_count": [
                    9620.0,
                    75900.0,
                    89100.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which organization began as the North West Police Agency?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "pinkerton"
            ],
            "lines": [
                [
                    0.29746219176221794,
                    0.49924242424242427,
                    0.3919071067240065,
                    4.226847840769482,
                    0.5827505827505828,
                    0.2744630071599045,
                    0.5978994748687172,
                    0.5567164179104478,
                    1.0,
                    0.31272210376687987,
                    0.3333333333333333,
                    0.34657301730441575,
                    0.047619047619047616,
                    0.6666666666666666,
                    -1.0
                ],
                [
                    0.11588200628601363,
                    0.1590909090909091,
                    0.17296991991317662,
                    0.2703570749405698,
                    0.3341103341103341,
                    0.37708830548926014,
                    0.3218304576144036,
                    0.3044776119402985,
                    0.0,
                    0.31982942430703626,
                    0.3333333333333333,
                    0.2895314611973883,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.5866558019517684,
                    0.3416666666666667,
                    0.43512297336281686,
                    -3.4972049157100518,
                    0.08313908313908314,
                    0.34844868735083534,
                    0.08027006751687922,
                    0.13880597014925372,
                    0.0,
                    0.3674484719260839,
                    0.3333333333333333,
                    0.3638955214981959,
                    0.9523809523809523,
                    0.3333333333333333,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "pinkerton": 0.06194971049279647,
                "fbi": 0.7238716582055804,
                "nra": 0.2141786313016231
            },
            "question": "which organization began as the north west police agency?",
            "rate_limited": false,
            "answers": [
                "fbi",
                "nra",
                "pinkerton"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "pinkerton": 0.5057061298780129,
                "fbi": 0.4767826224263234,
                "nra": 0.09366398117748181
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.0794381038264946,
                    1.73718876718433,
                    2.1833731289891753
                ],
                "result_count_important_words": [
                    797000.0,
                    429000.0,
                    107000.0
                ],
                "wikipedia_search": [
                    5.0,
                    0.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    65.0,
                    65.0,
                    65.0
                ],
                "answer_relation_to_question_bing": [
                    1.996969696969697,
                    0.6363636363636364,
                    1.3666666666666667
                ],
                "cosine_similarity_raw": [
                    0.05844486877322197,
                    0.025794899091124535,
                    0.06488962471485138
                ],
                "result_count_noun_chunks": [
                    746000.0,
                    408000.0,
                    186000.0
                ],
                "question_answer_similarity": [
                    1.0856999319512397,
                    0.06944339349865913,
                    -0.8982852664776146
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    20.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    1150000.0,
                    1580000.0,
                    1460000.0
                ],
                "result_count": [
                    750000.0,
                    430000.0,
                    107000.0
                ],
                "answer_relation_to_question": [
                    1.7847731505733078,
                    0.6952920377160817,
                    3.5199348117106104
                ],
                "word_count_appended": [
                    440.0,
                    450.0,
                    517.0
                ]
            },
            "integer_answers": {
                "pinkerton": 5,
                "fbi": 8,
                "nra": 1
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Where is the Hershey Company headquarters located?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "pennsylvania"
            ],
            "question": "where is the hershey company headquarters located?",
            "lines": [
                [
                    0.13333333333333336,
                    0.19999999999999998,
                    0.1156583017423948,
                    -0.16885281601350965,
                    0.051601423487544484,
                    0.04058490002984184,
                    0.09905254091300603,
                    0.13996193035494345,
                    0.030303030303030304,
                    0.32386363636363635,
                    0.2692307692307692,
                    0.23733338976435586,
                    0.0,
                    0.0,
                    3.0
                ],
                [
                    0.6888888888888889,
                    0.7666666666666666,
                    0.7160531876816902,
                    1.194958885556722,
                    0.8629893238434164,
                    0.8027454491196657,
                    0.602928509905254,
                    0.8274549322584257,
                    0.7818181818181819,
                    0.40767045454545453,
                    0.47115384615384615,
                    0.47498223795604333,
                    1.0,
                    1.0,
                    3.0
                ],
                [
                    0.1777777777777778,
                    0.03333333333333333,
                    0.1682885105759149,
                    -0.02610606954321237,
                    0.08540925266903915,
                    0.1566696508504924,
                    0.2980189491817399,
                    0.03258313738663084,
                    0.1878787878787879,
                    0.2684659090909091,
                    0.25961538461538464,
                    0.28768437227960086,
                    0.0,
                    0.0,
                    3.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "transylvania",
                "pennsylvania",
                "lithuania"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "pennsylvania": 0.8137961965446759,
                "lithuania": 0.07411326805684869,
                "transylvania": 0.06379133950745564
            },
            "categorical_data": {
                "question_type": 3
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.9493335590574234,
                    1.8999289518241733,
                    1.1507374891184035
                ],
                "result_count_important_words": [
                    115000.0,
                    700000.0,
                    346000.0
                ],
                "wikipedia_search": [
                    0.09090909090909091,
                    2.3454545454545457,
                    0.5636363636363637
                ],
                "word_count_appended_bing": [
                    28.0,
                    49.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    0.6,
                    2.3,
                    0.1
                ],
                "cosine_similarity_raw": [
                    0.02039094641804695,
                    0.12624257802963257,
                    0.029669828712940216
                ],
                "result_count_noun_chunks": [
                    125000.0,
                    739000.0,
                    29100.0
                ],
                "question_answer_similarity": [
                    -0.19127713702619076,
                    1.353654146194458,
                    -0.02957305870950222
                ],
                "word_count_noun_chunks": [
                    0.0,
                    46.0,
                    0.0
                ],
                "result_count_bing": [
                    2720000.0,
                    53800000.0,
                    10500000.0
                ],
                "word_count_raw": [
                    0.0,
                    24.0,
                    0.0
                ],
                "word_count_appended": [
                    228.0,
                    287.0,
                    189.0
                ],
                "answer_relation_to_question": [
                    0.4,
                    2.0666666666666664,
                    0.5333333333333333
                ],
                "result_count": [
                    23200.0,
                    388000.0,
                    38400.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What is the total cost of all the vowels on \u201cWheel of Fortune\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "$2,500"
            ],
            "lines": [
                [
                    0.26452991452991453,
                    0.2777777777777778,
                    0.2011574129089494,
                    0.29829337871079203,
                    0.30714285714285716,
                    0.3333333333333333,
                    0.26151560178306094,
                    0.08942675159235669,
                    0.0,
                    0.325,
                    0.35294117647058826,
                    0.347989934430612,
                    0,
                    0,
                    1.0
                ],
                [
                    0.5256410256410257,
                    0.2777777777777778,
                    0.5589035217223928,
                    0.4713793088949262,
                    0.40714285714285714,
                    0.3333333333333333,
                    0.6322436849925706,
                    0.8738853503184714,
                    1.0,
                    0.35,
                    0.35294117647058826,
                    0.4273002506625877,
                    0,
                    0,
                    1.0
                ],
                [
                    0.20982905982905986,
                    0.4444444444444444,
                    0.23993906536865783,
                    0.2303273123942818,
                    0.2857142857142857,
                    0.3333333333333333,
                    0.1062407132243685,
                    0.03668789808917197,
                    0.0,
                    0.325,
                    0.29411764705882354,
                    0.22470981490680025,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "$2,500": 0.5175456905797109,
                "$1,250": 0.25492567822335344,
                "$2,900": 0.22752863119693556
            },
            "question": "what is the total cost of all the vowels on \u201cwheel of fortune\u201d?",
            "rate_limited": false,
            "answers": [
                "$1,250",
                "$2,500",
                "$2,900"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "$2,500": 0.7879067029384634,
                "$1,250": 0.15575988041489416,
                "$2,900": 0.26352356426179974
            },
            "integer_answers": {
                "$2,500": 9,
                "$1,250": 2,
                "$2,900": 1
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.73994967215306,
                    2.1365012533129386,
                    1.1235490745340013
                ],
                "result_count_important_words": [
                    3520.0,
                    8510.0,
                    1430.0
                ],
                "wikipedia_search": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    6.0,
                    6.0,
                    5.0
                ],
                "answer_relation_to_question_bing": [
                    0.5555555555555556,
                    0.5555555555555556,
                    0.8888888888888888
                ],
                "cosine_similarity_raw": [
                    0.044646892696619034,
                    0.12404865026473999,
                    0.05325448140501976
                ],
                "result_count_noun_chunks": [
                    3510.0,
                    34300.0,
                    1440.0
                ],
                "question_answer_similarity": [
                    2.751216939795995,
                    4.34762161099934,
                    2.1243528981285635
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    363000.0,
                    363000.0,
                    363000.0
                ],
                "result_count": [
                    43.0,
                    57.0,
                    40.0
                ],
                "answer_relation_to_question": [
                    1.0581196581196581,
                    2.1025641025641026,
                    0.8393162393162394
                ],
                "word_count_appended": [
                    13.0,
                    14.0,
                    13.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these Supreme Court cases dealt with the First Amendment?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "miranda v. arizona"
            ],
            "lines": [
                [
                    0.26881199507036646,
                    0.18949701619778347,
                    0.484741766196998,
                    0.698490878534633,
                    0.07011289364230541,
                    0.34737678855325915,
                    0.043017456359102244,
                    0.07066508313539192,
                    0.4826877637130802,
                    0.04081632653061224,
                    0.06666666666666667,
                    0.2832305367363727,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.3531315729505095,
                    0.33958104981122883,
                    0.22482815824586386,
                    0.2839919389250533,
                    0.44860368389780153,
                    0.29411764705882354,
                    0.47381546134663344,
                    0.4483372921615202,
                    0.2826107594936709,
                    0.47619047619047616,
                    0.43333333333333335,
                    0.3569367332533441,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.378056431979124,
                    0.47092193399098764,
                    0.2904300755571382,
                    0.017517182540313685,
                    0.48128342245989303,
                    0.3585055643879173,
                    0.48316708229426436,
                    0.4809976247030879,
                    0.23470147679324893,
                    0.48299319727891155,
                    0.5,
                    0.35983273001028326,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "miranda v. arizona": 0.36795650888902154,
                "plessy v. ferguson": 0.37820056016626413,
                "ny times co. v. sullivan": 0.2538429309447143
            },
            "question": "which of these supreme court cases dealt with the first amendment?",
            "rate_limited": false,
            "answers": [
                "ny times co. v. sullivan",
                "miranda v. arizona",
                "plessy v. ferguson"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "miranda v. arizona": 0.4415111113278891,
                "plessy v. ferguson": 0.39423120353260965,
                "ny times co. v. sullivan": 0.26200279281204586
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.4161526836818636,
                    1.7846836662667203,
                    1.7991636500514163
                ],
                "result_count_important_words": [
                    13800.0,
                    152000.0,
                    155000.0
                ],
                "wikipedia_search": [
                    2.413438818565401,
                    1.4130537974683546,
                    1.1735073839662447
                ],
                "word_count_appended_bing": [
                    2.0,
                    13.0,
                    15.0
                ],
                "answer_relation_to_question_bing": [
                    0.9474850809889174,
                    1.6979052490561442,
                    2.354609669954938
                ],
                "cosine_similarity_raw": [
                    0.08185433596372604,
                    0.037964873015880585,
                    0.04904252663254738
                ],
                "result_count_noun_chunks": [
                    23800.0,
                    151000.0,
                    162000.0
                ],
                "question_answer_similarity": [
                    7.861860077828169,
                    3.1964696400100365,
                    0.1971645476296544
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    4370000.0,
                    3700000.0,
                    4510000.0
                ],
                "word_count_appended": [
                    6.0,
                    70.0,
                    71.0
                ],
                "answer_relation_to_question": [
                    1.3440599753518323,
                    1.7656578647525476,
                    1.8902821598956199
                ],
                "result_count": [
                    23600.0,
                    151000.0,
                    162000.0
                ]
            },
            "integer_answers": {
                "miranda v. arizona": 0,
                "plessy v. ferguson": 9,
                "ny times co. v. sullivan": 3
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these does a plant typically need to grow?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "water"
            ],
            "question": "which of these does a plant typically need to grow?",
            "lines": [
                [
                    0.14299242424242423,
                    0.0,
                    0.1959616641951237,
                    0.4146927176758664,
                    0.018615424208630008,
                    0.12723845428840716,
                    0.18087356450639483,
                    0.04859861146824376,
                    0.36105769230769236,
                    0.0986267166042447,
                    0.07142857142857142,
                    0.2955991855001849,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.5274621212121212,
                    1.0,
                    0.44951682354639605,
                    0.30671077305885336,
                    0.5027022389980269,
                    0.6833176248821866,
                    0.8191075564346836,
                    0.7932630496271535,
                    0.6100961538461539,
                    0.4756554307116105,
                    0.6071428571428571,
                    0.43538798254717703,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.3295454545454545,
                    0.0,
                    0.3545215122584803,
                    0.27859650926528023,
                    0.47868233679334304,
                    0.18944392082940623,
                    1.8879058921542894e-05,
                    0.15813833890460272,
                    0.02884615384615385,
                    0.4257178526841448,
                    0.32142857142857145,
                    0.2690128319526381,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "diesel fuel",
                "water",
                "confidence"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "water": 0.7920869848797062,
                "confidence": 0.225744125048077,
                "diesel fuel": 0.08258256023782763
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1823967420007395,
                    1.7415519301887081,
                    1.0760513278105523
                ],
                "result_count_important_words": [
                    594000.0,
                    2690000.0,
                    62.0
                ],
                "wikipedia_search": [
                    1.4442307692307692,
                    2.440384615384615,
                    0.11538461538461539
                ],
                "word_count_appended_bing": [
                    6.0,
                    51.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    4.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.01458599790930748,
                    0.03345884755253792,
                    0.026388069614768028
                ],
                "result_count_noun_chunks": [
                    3780000.0,
                    61700000.0,
                    12300000.0
                ],
                "question_answer_similarity": [
                    5.036822967231274,
                    3.7252833247184753,
                    3.3838098347187042
                ],
                "word_count_noun_chunks": [
                    0.0,
                    8.0,
                    0.0
                ],
                "result_count_bing": [
                    13500000.0,
                    72500000.0,
                    20100000.0
                ],
                "word_count_raw": [
                    0.0,
                    2.0,
                    0.0
                ],
                "word_count_appended": [
                    79.0,
                    381.0,
                    341.0
                ],
                "answer_relation_to_question": [
                    0.4289772727272727,
                    1.5823863636363638,
                    0.9886363636363636
                ],
                "result_count": [
                    217000.0,
                    5860000.0,
                    5580000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which brand mascot was NOT a real person?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "sara lee"
            ],
            "lines": [
                [
                    0.4166970507211149,
                    0.2641129032258065,
                    0.4417259369515351,
                    0.18523063987353944,
                    0.48225715889949466,
                    0.2698653929656969,
                    0.4190721649484536,
                    0.482256760398886,
                    0.3032407407407408,
                    0.2849002849002849,
                    0.2857142857142857,
                    0.32695603612018154,
                    0.25,
                    0.16666666666666669,
                    -1.0
                ],
                [
                    0.3042465092097445,
                    0.2943548387096774,
                    0.4234232729841575,
                    0.36753576994789994,
                    0.4893879842784952,
                    0.37277464177160224,
                    0.43247422680412373,
                    0.4893989758332585,
                    0.4207175925925926,
                    0.3532763532763533,
                    0.39285714285714285,
                    0.3731249463777423,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.27905644006914065,
                    0.4415322580645161,
                    0.1348507900643074,
                    0.4472335901785606,
                    0.02835485682201011,
                    0.35735996526270086,
                    0.1484536082474227,
                    0.028344263767855538,
                    0.2760416666666667,
                    0.36182336182336183,
                    0.3214285714285714,
                    0.29991901750207617,
                    0.25,
                    0.33333333333333337,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "betty crocker": 0.47032403953849666,
                "sara lee": 0.1837753921938872,
                "little debbie": 0.3459005682676162
            },
            "question": "which brand mascot was not a real person?",
            "rate_limited": false,
            "answers": [
                "little debbie",
                "sara lee",
                "betty crocker"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "betty crocker": 0.21835044253510733,
                "sara lee": 0.3682877588442318,
                "little debbie": 0.2629862673755379
            },
            "integer_answers": {
                "betty crocker": 7,
                "sara lee": 0,
                "little debbie": 7
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3843517110385475,
                    1.0150004289780616,
                    1.6006478599833907
                ],
                "result_count_important_words": [
                    157000.0,
                    131000.0,
                    682000.0
                ],
                "wikipedia_search": [
                    1.574074074074074,
                    0.6342592592592593,
                    1.7916666666666667
                ],
                "word_count_appended_bing": [
                    24.0,
                    12.0,
                    20.0
                ],
                "answer_relation_to_question_bing": [
                    1.4153225806451613,
                    1.2338709677419355,
                    0.35080645161290325
                ],
                "cosine_similarity_raw": [
                    0.025272458791732788,
                    0.03321000933647156,
                    0.15835893154144287
                ],
                "result_count_noun_chunks": [
                    158000.0,
                    94400.0,
                    4200000.0
                ],
                "question_answer_similarity": [
                    4.103627513162792,
                    1.7269274834543467,
                    0.6879122257232666
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    2.0
                ],
                "result_count_bing": [
                    1060000.0,
                    586000.0,
                    657000.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    1.0
                ],
                "word_count_appended": [
                    151.0,
                    103.0,
                    97.0
                ],
                "answer_relation_to_question": [
                    0.4998176956733107,
                    1.174520944741533,
                    1.3256613595851563
                ],
                "result_count": [
                    158000.0,
                    94500.0,
                    4200000.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What iconic painting once hung in Napoleon's bedroom?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "mona lisa"
            ],
            "lines": [
                [
                    0.358128078817734,
                    0.21428571428571427,
                    0.12707721482301051,
                    0.4054201490299891,
                    0.7073030477285797,
                    0.5577689243027888,
                    0.7334785766158315,
                    0.5553218342448465,
                    0.7308965102286402,
                    0.13569321533923304,
                    0.056338028169014086,
                    0.27612363787901995,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.1022167487684729,
                    0.1326530612244898,
                    0.8152481720274101,
                    0.10499486390772106,
                    0.05462909718228867,
                    0.26145418326693226,
                    0.10675381263616558,
                    0.12536811106436685,
                    0.20532490974729242,
                    0.6430678466076696,
                    0.7605633802816901,
                    0.4057631836310163,
                    0.9565217391304348,
                    1.0,
                    1.0
                ],
                [
                    0.539655172413793,
                    0.653061224489796,
                    0.05767461314957937,
                    0.48958498706228987,
                    0.2380678550891317,
                    0.1807768924302789,
                    0.15976761074800291,
                    0.3193100546907867,
                    0.06377858002406739,
                    0.22123893805309736,
                    0.18309859154929578,
                    0.31811317848996373,
                    0.043478260869565216,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "the birth of venus": 0.247686139932832,
                "mona lisa": 0.40532565067685355,
                "the starry night": 0.3469882093903144
            },
            "question": "what iconic painting once hung in napoleon's bedroom?",
            "rate_limited": false,
            "answers": [
                "the starry night",
                "mona lisa",
                "the birth of venus"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "the birth of venus": 0.20888592458796984,
                "mona lisa": 0.7906221929744528,
                "the starry night": 0.096086251899523
            },
            "integer_answers": {
                "the birth of venus": 3,
                "mona lisa": 6,
                "the starry night": 5
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3806181893950997,
                    2.0288159181550816,
                    1.5905658924498187
                ],
                "result_count_important_words": [
                    1010000.0,
                    147000.0,
                    220000.0
                ],
                "wikipedia_search": [
                    2.1926895306859207,
                    0.6159747292418772,
                    0.19133574007220217
                ],
                "word_count_appended_bing": [
                    4.0,
                    54.0,
                    13.0
                ],
                "answer_relation_to_question_bing": [
                    0.42857142857142855,
                    0.2653061224489796,
                    1.306122448979592
                ],
                "cosine_similarity_raw": [
                    0.06755483895540237,
                    0.43338972330093384,
                    0.03066009283065796
                ],
                "result_count_noun_chunks": [
                    1320000.0,
                    298000.0,
                    759000.0
                ],
                "question_answer_similarity": [
                    8.063914388883859,
                    2.0883757155388594,
                    9.737975360127166
                ],
                "word_count_noun_chunks": [
                    0.0,
                    22.0,
                    1.0
                ],
                "result_count_bing": [
                    112000.0,
                    52500.0,
                    36300.0
                ],
                "word_count_raw": [
                    0.0,
                    15.0,
                    0.0
                ],
                "result_count": [
                    2460000.0,
                    190000.0,
                    828000.0
                ],
                "answer_relation_to_question": [
                    1.79064039408867,
                    0.5110837438423645,
                    2.6982758620689653
                ],
                "word_count_appended": [
                    46.0,
                    218.0,
                    75.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these is NOT among the four \u201cC\u2019s\u201d of diamond buying?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "core"
            ],
            "lines": [
                [
                    0.3316157525838369,
                    0.33982247327012305,
                    0.17434280047174477,
                    0.35298536436153694,
                    0.17524841915085815,
                    0.33156911581569115,
                    0.4631233203941475,
                    0.3084763948497854,
                    0.15657439199123635,
                    0.27001127395715896,
                    0.1942307692307692,
                    0.3268599271732113,
                    0.2053941908713693,
                    0.06837606837606836,
                    -1.0
                ],
                [
                    0.2693957754450962,
                    0.3099657050635465,
                    0.36882906581024966,
                    0.3104077788750854,
                    0.4083107497741644,
                    0.3334371108343711,
                    0.15362794864138551,
                    0.30686695278969955,
                    0.45210846887589246,
                    0.31454340473506204,
                    0.4096153846153846,
                    0.32140668087293695,
                    0.2946058091286307,
                    0.43162393162393164,
                    -1.0
                ],
                [
                    0.3989884719710669,
                    0.35021182166633047,
                    0.45682813371800557,
                    0.3366068567633777,
                    0.4164408310749774,
                    0.33499377334993774,
                    0.383248730964467,
                    0.38465665236051505,
                    0.39131713913287125,
                    0.415445321307779,
                    0.39615384615384613,
                    0.35173339195385184,
                    0.5,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "color": 0.4716242482146375,
                "core": 0.19762500422613916,
                "cut": 0.3307507475592233
            },
            "question": "which of these is not among the four \u201cc\u2019s\u201d of diamond buying?",
            "rate_limited": false,
            "answers": [
                "color",
                "cut",
                "core"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "color": 0.15739648008548568,
                "core": 0.587405225712829,
                "cut": 0.18696114317489043
            },
            "integer_answers": {
                "color": 8,
                "core": 0,
                "cut": 6
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.0388404369607325,
                    1.0715599147623784,
                    0.8895996482768893
                ],
                "result_count_important_words": [
                    2470000.0,
                    23200000.0,
                    7820000.0
                ],
                "wikipedia_search": [
                    1.3737024320350546,
                    0.19156612449643015,
                    0.4347314434685151
                ],
                "word_count_appended_bing": [
                    159.0,
                    47.0,
                    54.0
                ],
                "answer_relation_to_question_bing": [
                    0.3203550534597539,
                    0.380068589872907,
                    0.2995763566673391
                ],
                "cosine_similarity_raw": [
                    0.21935856342315674,
                    0.08835507929325104,
                    0.029080022126436234
                ],
                "result_count_noun_chunks": [
                    3570000.0,
                    3600000.0,
                    2150000.0
                ],
                "question_answer_similarity": [
                    3.8351904675364494,
                    4.945917636156082,
                    4.26245878636837
                ],
                "word_count_noun_chunks": [
                    142.0,
                    99.0,
                    0.0
                ],
                "result_count_bing": [
                    54100000.0,
                    53500000.0,
                    53000000.0
                ],
                "word_count_raw": [
                    101.0,
                    16.0,
                    0.0
                ],
                "word_count_appended": [
                    408.0,
                    329.0,
                    150.0
                ],
                "answer_relation_to_question": [
                    1.0103054844969788,
                    1.3836253473294227,
                    0.6060691681735986
                ],
                "result_count": [
                    719000.0,
                    203000.0,
                    185000.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What is the correct pronunciation of the performer who sings \u201cSmooth Operator\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "shah-day"
            ],
            "lines": [
                [
                    0.8888888888888888,
                    0.3333333333333333,
                    0.7603270936853664,
                    0.44677802128757055,
                    0.8085106382978723,
                    0.24945837202104612,
                    0.639344262295082,
                    0.45259461568474446,
                    0.0,
                    0.7608695652173914,
                    0.34146341463414637,
                    0.7063092510146416,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.0,
                    0.3333333333333333,
                    0.14261963323269905,
                    -0.02854204455048535,
                    0.13829787234042554,
                    0.5478180129990715,
                    0.22950819672131148,
                    0.0031213421771361686,
                    0.0,
                    0.125,
                    0.32926829268292684,
                    0.1632074105768559,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.1111111111111111,
                    0.3333333333333333,
                    0.09705327308193447,
                    0.5817640232629148,
                    0.05319148936170213,
                    0.2027236149798824,
                    0.13114754098360656,
                    0.5442840421381194,
                    1.0,
                    0.11413043478260869,
                    0.32926829268292684,
                    0.1304833384085024,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "sayd": 0.14168800353666244,
                "shah-day": 0.5991341040257202,
                "say-dee": 0.2591778924376173
            },
            "question": "what is the correct pronunciation of the performer who sings \u201csmooth operator\u201d?",
            "rate_limited": false,
            "answers": [
                "shah-day",
                "sayd",
                "say-dee"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sayd": 0.09523895959228616,
                "shah-day": 0.7906221929744528,
                "say-dee": 0.08195424085485645
            },
            "integer_answers": {
                "sayd": 1,
                "shah-day": 10,
                "say-dee": 3
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    4.23785550608785,
                    0.9792444634611356,
                    0.7829000304510145
                ],
                "result_count_important_words": [
                    39.0,
                    14.0,
                    8.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_appended_bing": [
                    28.0,
                    27.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "cosine_similarity_raw": [
                    0.13098712265491486,
                    0.024570129811763763,
                    0.016720078885555267
                ],
                "result_count_noun_chunks": [
                    2320.0,
                    16.0,
                    2790.0
                ],
                "question_answer_similarity": [
                    6.507756527513266,
                    -0.41574264597147703,
                    8.473958967253566
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    80600.0,
                    177000.0,
                    65500.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    140.0,
                    23.0,
                    21.0
                ],
                "answer_relation_to_question": [
                    2.6666666666666665,
                    0.0,
                    0.3333333333333333
                ],
                "result_count": [
                    76.0,
                    13.0,
                    5.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What\u2019s another name for a garbanzo bean?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "chickpea"
            ],
            "lines": [
                [
                    0.4696969696969697,
                    0.4738095238095238,
                    0.0756876236539241,
                    0.31820440511081716,
                    0.08891389983117615,
                    0.2731535756154748,
                    0.05610724925521351,
                    0.01748367821179595,
                    0.25751485694843307,
                    0.1092896174863388,
                    0.047619047619047616,
                    0.38281645768353184,
                    0.007380073800738007,
                    0.0,
                    1.0
                ],
                [
                    0.509926854754441,
                    0.5071428571428571,
                    0.855025534685342,
                    0.09901777012832769,
                    0.7428249859313449,
                    0.45076201641266117,
                    0.8192651439920556,
                    0.9494301206152485,
                    0.5519706721810582,
                    0.7639344262295082,
                    0.9206349206349206,
                    0.37158422473885827,
                    0.988929889298893,
                    1.0,
                    1.0
                ],
                [
                    0.02037617554858934,
                    0.01904761904761905,
                    0.06928684166073387,
                    0.5827778247608552,
                    0.1682611142374789,
                    0.276084407971864,
                    0.12462760675273088,
                    0.033086201172955625,
                    0.19051447087050874,
                    0.126775956284153,
                    0.031746031746031744,
                    0.24559931757760983,
                    0.0036900369003690036,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "lima bean": 0.1841197841944989,
                "chickpea": 0.680746386910394,
                "black-eyed pea": 0.13513382889510708
            },
            "question": "what\u2019s another name for a garbanzo bean?",
            "rate_limited": false,
            "answers": [
                "lima bean",
                "chickpea",
                "black-eyed pea"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "lima bean": 0.42149974196037354,
                "chickpea": 0.7906221929744528,
                "black-eyed pea": 0.10789242084873253
            },
            "integer_answers": {
                "lima bean": 1,
                "chickpea": 12,
                "black-eyed pea": 1
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.7656329153670637,
                    0.7431684494777165,
                    0.49119863515521967
                ],
                "result_count_important_words": [
                    113000.0,
                    1650000.0,
                    251000.0
                ],
                "wikipedia_search": [
                    0.7725445708452992,
                    1.6559120165431747,
                    0.5715434126115262
                ],
                "word_count_appended_bing": [
                    6.0,
                    116.0,
                    4.0
                ],
                "answer_relation_to_question_bing": [
                    0.9476190476190476,
                    1.0142857142857142,
                    0.0380952380952381
                ],
                "cosine_similarity_raw": [
                    0.024287283420562744,
                    0.2743678092956543,
                    0.022233346477150917
                ],
                "result_count_noun_chunks": [
                    158000.0,
                    8580000.0,
                    299000.0
                ],
                "question_answer_similarity": [
                    3.672154745378066,
                    1.1426886888220906,
                    6.7253951244056225
                ],
                "word_count_noun_chunks": [
                    2.0,
                    268.0,
                    1.0
                ],
                "result_count_bing": [
                    466000.0,
                    769000.0,
                    471000.0
                ],
                "word_count_raw": [
                    0.0,
                    78.0,
                    0.0
                ],
                "result_count": [
                    158000.0,
                    1320000.0,
                    299000.0
                ],
                "answer_relation_to_question": [
                    0.9393939393939394,
                    1.019853709508882,
                    0.04075235109717868
                ],
                "word_count_appended": [
                    100.0,
                    699.0,
                    116.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "In which version of \u201cDragnet\u201d is the line \u201cJust the facts, ma\u2019am\u201d first said?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "'50s tv show"
            ],
            "lines": [
                [
                    0.3982233872142129,
                    0.3159420289855073,
                    0.5147012162615172,
                    0.4047773689742971,
                    0.2777777777777778,
                    0.1827768014059754,
                    0.2222222222222222,
                    0.39789997236805746,
                    0.012077294685990338,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.37486165339823874,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.1681010630551915,
                    0.3405797101449275,
                    0.2856342090534163,
                    0.2857209373967589,
                    0.16666666666666666,
                    0.22495606326889278,
                    0.18518518518518517,
                    0.41171594363083724,
                    0.8140096618357487,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3077698054527323,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.4336755497305956,
                    0.34347826086956523,
                    0.1996645746850665,
                    0.309501693628944,
                    0.5555555555555556,
                    0.5922671353251318,
                    0.5925925925925926,
                    0.1903840840011053,
                    0.17391304347826086,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3173685411490289,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "'80s movie": 0.3645889748068761,
                "'50s movie": 0.32141715936308535,
                "'50s tv show": 0.31399386583003863
            },
            "question": "in which version of \u201cdragnet\u201d is the line \u201cjust the facts, ma\u2019am\u201d first said?",
            "rate_limited": false,
            "answers": [
                "'50s tv show",
                "'50s movie",
                "'80s movie"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "'80s movie": 0.1244002590289454,
                "'50s movie": 0.12298677217676027,
                "'50s tv show": 0.223660283735346
            },
            "integer_answers": {
                "'80s movie": 5,
                "'50s movie": 2,
                "'50s tv show": 5
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.8743082669911937,
                    1.5388490272636615,
                    1.5868427057451446
                ],
                "result_count_important_words": [
                    12.0,
                    10.0,
                    32.0
                ],
                "wikipedia_search": [
                    0.036231884057971016,
                    2.442028985507246,
                    0.5217391304347826
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.6318840579710145,
                    0.681159420289855,
                    0.6869565217391305
                ],
                "cosine_similarity_raw": [
                    0.12402669340372086,
                    0.06882879883050919,
                    0.048112839460372925
                ],
                "result_count_noun_chunks": [
                    144000.0,
                    149000.0,
                    68900.0
                ],
                "question_answer_similarity": [
                    16.162886361591518,
                    11.408925983123481,
                    12.358498983085155
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    104000.0,
                    128000.0,
                    337000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    3.0,
                    3.0,
                    3.0
                ],
                "answer_relation_to_question": [
                    1.9911169360710645,
                    0.8405053152759575,
                    2.168377748652978
                ],
                "result_count": [
                    10.0,
                    6.0,
                    20.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which one of these was Georgia O\u2019Keeffe most likely to paint?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "cynomorphism"
            ],
            "lines": [
                [
                    0.0,
                    0.3333333333333333,
                    0.00399286694906737,
                    1.0,
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.0,
                    0,
                    0.06153846153846154,
                    0.03571428571428571,
                    0.19033958094266648,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.9333333333333332,
                    0.3333333333333333,
                    0.481354231467438,
                    -0.0,
                    0.075,
                    0.3333333333333333,
                    0.15384615384615385,
                    0.05128205128205128,
                    0,
                    0.35384615384615387,
                    0.48214285714285715,
                    0.30665821374096275,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.06666666666666667,
                    0.3333333333333333,
                    0.5146529015834946,
                    -0.0,
                    0.925,
                    0.3333333333333333,
                    0.8461538461538461,
                    0.9487179487179487,
                    0,
                    0.5846153846153846,
                    0.48214285714285715,
                    0.5030022053163709,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "ongle du doigt": 0.17802289652828615,
                "cynomorphism": 0.31855724193869245,
                "bromeliaceae": 0.5034198615330214
            },
            "question": "which one of these was georgia o\u2019keeffe most likely to paint?",
            "rate_limited": false,
            "answers": [
                "ongle du doigt",
                "cynomorphism",
                "bromeliaceae"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "ongle du doigt": 0.0890295821093803,
                "cynomorphism": 0.7160875612916177,
                "bromeliaceae": 0.5328584689211768
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.5710187428279995,
                    0.9199746412228882,
                    1.5090066159491125
                ],
                "result_count_important_words": [
                    0,
                    2.0,
                    11.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    2.0,
                    27.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "cosine_similarity_raw": [
                    0.0030207252129912376,
                    0.36415910720825195,
                    0.389350563287735
                ],
                "result_count_noun_chunks": [
                    0,
                    2.0,
                    37.0
                ],
                "question_answer_similarity": [
                    -1.3974651565076783,
                    0.0,
                    0.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    172000.0,
                    172000.0,
                    172000.0
                ],
                "word_count_appended": [
                    4.0,
                    23.0,
                    38.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    2.8,
                    0.2
                ],
                "result_count": [
                    0,
                    3.0,
                    37.0
                ]
            },
            "integer_answers": {
                "ongle du doigt": 2,
                "cynomorphism": 2,
                "bromeliaceae": 6
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these companies went public first?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "facebook"
            ],
            "lines": [
                [
                    0.5333333333333333,
                    0.0,
                    0.5020786388215926,
                    1.1639659299124,
                    0.9722467739084321,
                    0.4289013526888816,
                    0.985604205244609,
                    0.9875756610385473,
                    0.5581804281345565,
                    0.2953281423804227,
                    0.22815533980582525,
                    0.35743442882295096,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.23333333333333334,
                    0.0,
                    0.1486442693495346,
                    0.3346810499462215,
                    0.016174650875022095,
                    0.17518970636753547,
                    0.007645899289170301,
                    0.007327174259318254,
                    0.12904106596767148,
                    0.3364849833147942,
                    0.3932038834951456,
                    0.30708240727986746,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.23333333333333334,
                    1.0,
                    0.34927709182887273,
                    -0.4986469798586215,
                    0.011578575216545872,
                    0.395908940943583,
                    0.0067498954662206555,
                    0.005097164702134438,
                    0.31277850589777195,
                    0.3681868743047831,
                    0.3786407766990291,
                    0.3354831638971815,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "ferrari": 0.14920060167697247,
                "facebook": 0.6437717310065395,
                "alibaba": 0.20702766731648814
            },
            "question": "which of these companies went public first?",
            "rate_limited": false,
            "answers": [
                "facebook",
                "ferrari",
                "alibaba"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "ferrari": 0.04982586533813474,
                "facebook": 0.816051231526463,
                "alibaba": 0.14102925790457016
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.0723032864688529,
                    0.9212472218396024,
                    1.0064494916915445
                ],
                "result_count_important_words": [
                    330000000.0,
                    2560000.0,
                    2260000.0
                ],
                "wikipedia_search": [
                    1.6745412844036696,
                    0.3871231979030144,
                    0.9383355176933159
                ],
                "word_count_appended_bing": [
                    47.0,
                    81.0,
                    78.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "cosine_similarity_raw": [
                    0.0997619479894638,
                    0.02953529730439186,
                    0.0694006085395813
                ],
                "result_count_noun_chunks": [
                    403000000.0,
                    2990000.0,
                    2080000.0
                ],
                "question_answer_similarity": [
                    1.5726340487599373,
                    0.4521874748170376,
                    -0.6737217977643013
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    130000000.0,
                    53100000.0,
                    120000000.0
                ],
                "word_count_appended": [
                    531.0,
                    605.0,
                    662.0
                ],
                "answer_relation_to_question": [
                    1.0666666666666667,
                    0.4666666666666667,
                    0.4666666666666667
                ],
                "result_count": [
                    330000000.0,
                    5490000.0,
                    3930000.0
                ]
            },
            "integer_answers": {
                "ferrari": 1,
                "facebook": 11,
                "alibaba": 2
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What topic would a herpetologist study?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "venereal disease"
            ],
            "lines": [
                [
                    0.36467236467236464,
                    1.0,
                    0.3619837626283285,
                    0.326491276716233,
                    0.9838201759148659,
                    0.6821282401091405,
                    0.9039657853810265,
                    0.7178871548619448,
                    0.7333333333333334,
                    0.5,
                    0.42857142857142855,
                    0.4597317825803489,
                    0,
                    0,
                    1.0
                ],
                [
                    0.12962962962962962,
                    0.0,
                    0.28175971418203505,
                    0.3448202958839333,
                    0.005755239439678575,
                    0.17326057298772168,
                    0.053654743390357695,
                    0.10108043217286915,
                    0.0,
                    0.16216216216216217,
                    0.2857142857142857,
                    0.19753480801633794,
                    0,
                    0,
                    1.0
                ],
                [
                    0.5056980056980057,
                    0.0,
                    0.3562565231896365,
                    0.32868842739983367,
                    0.010424584645455533,
                    0.1446111869031378,
                    0.042379471228615864,
                    0.18103241296518607,
                    0.26666666666666666,
                    0.33783783783783783,
                    0.2857142857142857,
                    0.34273340940331304,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "venereal disease": 0.6218821087307512,
                "mushroom farming": 0.14461432363158422,
                "crocodile teeth": 0.23350356763766456
            },
            "question": "what topic would a herpetologist study?",
            "rate_limited": false,
            "answers": [
                "venereal disease",
                "mushroom farming",
                "crocodile teeth"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "venereal disease": 0.3727603250328644,
                "mushroom farming": 0.04810635733270982,
                "crocodile teeth": 0.10420843573584748
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3791953477410468,
                    0.5926044240490138,
                    1.0282002282099392
                ],
                "result_count_important_words": [
                    46500.0,
                    2760.0,
                    2180.0
                ],
                "wikipedia_search": [
                    1.4666666666666668,
                    0.0,
                    0.5333333333333333
                ],
                "word_count_appended_bing": [
                    3.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.0,
                    0.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.032720405608415604,
                    0.025468800216913223,
                    0.03220270946621895
                ],
                "result_count_noun_chunks": [
                    299000.0,
                    42100.0,
                    75400.0
                ],
                "question_answer_similarity": [
                    2.05005219951272,
                    2.1651408672332764,
                    2.0638481993228197
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    500000.0,
                    127000.0,
                    106000.0
                ],
                "word_count_appended": [
                    37.0,
                    12.0,
                    25.0
                ],
                "answer_relation_to_question": [
                    0.7293447293447293,
                    0.25925925925925924,
                    1.0113960113960114
                ],
                "result_count": [
                    9060.0,
                    53.0,
                    96.0
                ]
            },
            "integer_answers": {
                "venereal disease": 10,
                "mushroom farming": 1,
                "crocodile teeth": 1
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these holidays does NOT involve fasting?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ramadan"
            ],
            "lines": [
                [
                    0.46616541353383456,
                    0.389957264957265,
                    0.4029195054409304,
                    0.08287465216945383,
                    0.21240576931456706,
                    0.20087609511889865,
                    0.25895022108115817,
                    0.15726227795193315,
                    0.38084816418149753,
                    0.29777070063694266,
                    0.32552083333333337,
                    0.3330353491089687,
                    0.0,
                    0.33333333333333337,
                    -1.0
                ],
                [
                    0.1797827903091061,
                    0.1955128205128205,
                    0.35727088161045173,
                    0.3758552752161449,
                    0.3071811407904484,
                    0.39549436795994997,
                    0.31172443303380404,
                    0.4101358411703239,
                    0.29062049062049067,
                    0.2814490445859873,
                    0.28385416666666663,
                    0.33912072565072715,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.3540517961570593,
                    0.41452991452991456,
                    0.23980961294861786,
                    0.5412700726144013,
                    0.48041308989498455,
                    0.40362953692115144,
                    0.4293253458850378,
                    0.43260188087774293,
                    0.32853134519801186,
                    0.42078025477707004,
                    0.390625,
                    0.32784392524030415,
                    0.5,
                    0.16666666666666669,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "yom kippur": 0.22427450832700538,
                "easter": 0.4511543456911262,
                "ramadan": 0.3245711459818685
            },
            "question": "which of these holidays does not involve fasting?",
            "rate_limited": false,
            "answers": [
                "easter",
                "ramadan",
                "yom kippur"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "yom kippur": 0.30254693022365486,
                "easter": 0.18923307663490713,
                "ramadan": 0.4276415149469218
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.0017879053461876,
                    0.9652756460956371,
                    1.0329364485581753
                ],
                "result_count_important_words": [
                    338000.0,
                    264000.0,
                    99100.0
                ],
                "wikipedia_search": [
                    0.7149110149110149,
                    1.2562770562770562,
                    1.0288119288119286
                ],
                "answer_relation_to_question": [
                    0.20300751879699247,
                    1.9213032581453633,
                    0.8756892230576441
                ],
                "result_count": [
                    1320000.0,
                    885000.0,
                    89900.0
                ],
                "answer_relation_to_question_bing": [
                    0.44017094017094016,
                    1.217948717948718,
                    0.3418803418803419
                ],
                "cosine_similarity_raw": [
                    0.07834909856319427,
                    0.11518995463848114,
                    0.20998741686344147
                ],
                "result_count_noun_chunks": [
                    6560000.0,
                    1720000.0,
                    1290000.0
                ],
                "question_answer_similarity": [
                    1.67487733066082,
                    0.49847650434821844,
                    -0.16571112116798759
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    4.0
                ],
                "result_count_bing": [
                    47800000.0,
                    16700000.0,
                    15400000.0
                ],
                "word_count_appended": [
                    508.0,
                    549.0,
                    199.0
                ],
                "word_count_appended_bing": [
                    67.0,
                    83.0,
                    42.0
                ]
            },
            "integer_answers": {
                "yom kippur": 3,
                "easter": 6,
                "ramadan": 5
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "In the U.K., who appoints the Prime Minister?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "the queen"
            ],
            "lines": [
                [
                    0.08333333333333333,
                    0.0,
                    0.23007680203898415,
                    0.3687968130790466,
                    0.4738562091503268,
                    0.4454225352112676,
                    0.35352730171383023,
                    0.9238802972484434,
                    0.0,
                    0.25,
                    0.22727272727272727,
                    0.3332990332799033,
                    0.4,
                    0.0,
                    0.0
                ],
                [
                    0.7833333333333333,
                    0.0,
                    0.29401264765789115,
                    0.33006858967631847,
                    0.2995642701525055,
                    0.25264084507042256,
                    0.34834595456357115,
                    0.04800160674834304,
                    0.393939393939394,
                    0.24193548387096775,
                    0.22727272727272727,
                    0.32112897193316814,
                    0.0,
                    0.0,
                    0.0
                ],
                [
                    0.13333333333333333,
                    1.0,
                    0.4759105503031247,
                    0.301134597244635,
                    0.22657952069716775,
                    0.3019366197183099,
                    0.2981267437225986,
                    0.028118096003213498,
                    0.6060606060606061,
                    0.5080645161290323,
                    0.5454545454545454,
                    0.34557199478692857,
                    0.6,
                    1.0,
                    0.0
                ]
            ],
            "fraction_answers": {
                "the people": 0.2921046465948473,
                "the parliament": 0.25287455887276017,
                "the queen": 0.4550207945323925
            },
            "question": "in the u.k., who appoints the prime minister?",
            "rate_limited": false,
            "answers": [
                "the people",
                "the parliament",
                "the queen"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "the people": 0.16896246226856335,
                "the parliament": 0.33732834831409336,
                "the queen": 0.7362868318204314
            },
            "integer_answers": {
                "the people": 5,
                "the parliament": 1,
                "the queen": 8
            },
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3331961331196132,
                    1.2845158877326726,
                    1.3822879791477143
                ],
                "result_count_important_words": [
                    887000.0,
                    874000.0,
                    748000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.7878787878787878,
                    1.212121212121212
                ],
                "word_count_appended_bing": [
                    5.0,
                    5.0,
                    12.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    2.0
                ],
                "cosine_similarity_raw": [
                    0.07257325947284698,
                    0.09274058043956757,
                    0.15011674165725708
                ],
                "result_count_noun_chunks": [
                    46000000.0,
                    2390000.0,
                    1400000.0
                ],
                "question_answer_similarity": [
                    7.725611565634608,
                    6.914326868951321,
                    6.308213207870722
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    3.0
                ],
                "result_count_bing": [
                    5060000.0,
                    2870000.0,
                    3430000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    5.0
                ],
                "result_count": [
                    1740000.0,
                    1100000.0,
                    832000.0
                ],
                "answer_relation_to_question": [
                    0.16666666666666666,
                    1.5666666666666667,
                    0.26666666666666666
                ],
                "word_count_appended": [
                    62.0,
                    60.0,
                    126.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these instruments is considered a woodwind?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "clarinet"
            ],
            "lines": [
                [
                    0.6907551775730373,
                    0.6785714285714286,
                    0.7678421442422232,
                    0.29790399908216253,
                    0.3568521031207598,
                    0.02351295543861986,
                    0.878341516639389,
                    0.3287760416666667,
                    0.6588693957115009,
                    0.4081237911025145,
                    0.37719298245614036,
                    0.35471272489320366,
                    0.23076923076923078,
                    0.9166666666666666,
                    -1.0
                ],
                [
                    0.22885266556421768,
                    0.21428571428571427,
                    0.15857742017740364,
                    0.3463386996564129,
                    0.29579375848032563,
                    0.9525078265503231,
                    0.058919803600654665,
                    0.3216145833333333,
                    0.16680590364800887,
                    0.28433268858800775,
                    0.2631578947368421,
                    0.32978828739966176,
                    0.15384615384615385,
                    0.08333333333333333,
                    -1.0
                ],
                [
                    0.0803921568627451,
                    0.10714285714285714,
                    0.07358043558037321,
                    0.3557573012614246,
                    0.3473541383989145,
                    0.023979218011057082,
                    0.06273867975995635,
                    0.349609375,
                    0.1743247006404901,
                    0.30754352030947774,
                    0.35964912280701755,
                    0.3154989877071346,
                    0.6153846153846154,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "violin": 0.22663965063329025,
                "clarinet": 0.4977778684238246,
                "trumpet": 0.2755824809428852
            },
            "question": "which of these instruments is considered a woodwind?",
            "rate_limited": false,
            "answers": [
                "clarinet",
                "trumpet",
                "violin"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "violin": 0.1486469829039032,
                "clarinet": 0.7420902821268696,
                "trumpet": 0.03578236720362811
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.064138174679611,
                    0.9893648621989852,
                    0.9464969631214037
                ],
                "result_count_important_words": [
                    16100000.0,
                    1080000.0,
                    1150000.0
                ],
                "wikipedia_search": [
                    1.976608187134503,
                    0.5004177109440267,
                    0.5229741019214703
                ],
                "word_count_appended_bing": [
                    43.0,
                    30.0,
                    41.0
                ],
                "answer_relation_to_question_bing": [
                    1.3571428571428572,
                    0.42857142857142855,
                    0.21428571428571427
                ],
                "cosine_similarity_raw": [
                    0.32002708315849304,
                    0.06609310209751129,
                    0.030667413026094437
                ],
                "result_count_noun_chunks": [
                    5050000.0,
                    4940000.0,
                    5370000.0
                ],
                "question_answer_similarity": [
                    1.703945191577077,
                    1.9809809997677803,
                    2.0348533242940903
                ],
                "word_count_noun_chunks": [
                    3.0,
                    2.0,
                    8.0
                ],
                "word_count_raw": [
                    11.0,
                    1.0,
                    0.0
                ],
                "result_count_bing": [
                    353000.0,
                    14300000.0,
                    360000.0
                ],
                "result_count": [
                    2630000.0,
                    2180000.0,
                    2560000.0
                ],
                "answer_relation_to_question": [
                    2.0722655327191117,
                    0.686557996692653,
                    0.2411764705882353
                ],
                "word_count_appended": [
                    422.0,
                    294.0,
                    318.0
                ]
            },
            "integer_answers": {
                "violin": 3,
                "clarinet": 10,
                "trumpet": 1
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these are you most likely to find in a toolbox?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "hammer"
            ],
            "lines": [
                [
                    0.21428571428571427,
                    0.2549019607843137,
                    0.7188154521485658,
                    0.3402481905319875,
                    0.9472527472527472,
                    0.47614737032678556,
                    0.7423245614035088,
                    0.9699411217849395,
                    0.05555555555555555,
                    0.7647058823529411,
                    0.6263736263736264,
                    0.45727549695954783,
                    1.0,
                    0,
                    -1.0
                ],
                [
                    0.16666666666666666,
                    0.3137254901960784,
                    0.21204193873537788,
                    0.42028756764684777,
                    0.02879120879120879,
                    0.47614737032678556,
                    0.1118421052631579,
                    0.017250284061563887,
                    0.8888888888888888,
                    0.11990950226244344,
                    0.03296703296703297,
                    0.2807133528330583,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.6190476190476191,
                    0.43137254901960786,
                    0.06914260911605627,
                    0.23946424182116474,
                    0.023956043956043956,
                    0.04770525934642889,
                    0.14583333333333334,
                    0.01280859415349654,
                    0.05555555555555555,
                    0.11538461538461539,
                    0.34065934065934067,
                    0.2620111502073939,
                    0.0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "hammerhead shark": 0.18176468550774275,
                "hammer": 0.5821405907507872,
                "mc hammer": 0.23609472374147003
            },
            "question": "which of these are you most likely to find in a toolbox?",
            "rate_limited": false,
            "answers": [
                "hammer",
                "mc hammer",
                "hammerhead shark"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "hammerhead shark": 0.22172976760634577,
                "hammer": 0.579304156662912,
                "mc hammer": 0.11719082810183122
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3718264908786435,
                    0.842140058499175,
                    0.7860334506221817
                ],
                "result_count_important_words": [
                    677000.0,
                    102000.0,
                    133000.0
                ],
                "wikipedia_search": [
                    0.1111111111111111,
                    1.7777777777777777,
                    0.1111111111111111
                ],
                "word_count_appended_bing": [
                    57.0,
                    3.0,
                    31.0
                ],
                "answer_relation_to_question_bing": [
                    0.5098039215686274,
                    0.6274509803921569,
                    0.8627450980392157
                ],
                "cosine_similarity_raw": [
                    0.1629670113325119,
                    0.048073314130306244,
                    0.0156757403165102
                ],
                "result_count_noun_chunks": [
                    9390000.0,
                    167000.0,
                    124000.0
                ],
                "question_answer_similarity": [
                    2.6737168580293655,
                    3.3026772400480695,
                    1.8817427926696837
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    52600000.0,
                    52600000.0,
                    5270000.0
                ],
                "word_count_appended": [
                    338.0,
                    53.0,
                    51.0
                ],
                "answer_relation_to_question": [
                    0.42857142857142855,
                    0.3333333333333333,
                    1.2380952380952381
                ],
                "result_count": [
                    4310000.0,
                    131000.0,
                    109000.0
                ]
            },
            "integer_answers": {
                "hammerhead shark": 2,
                "hammer": 9,
                "mc hammer": 2
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "When did the Super Bowl-winning 1985 Chicago Bears record \u201cThe Super Bowl Shuffle\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "during regular season"
            ],
            "question": "when did the super bowl-winning 1985 chicago bears record \u201cthe super bowl shuffle\u201d?",
            "lines": [
                [
                    0.561811264058455,
                    0.15,
                    0.19707032747699327,
                    0.32138428698041366,
                    0.0,
                    0.2570514684501308,
                    0.49647160023150977,
                    0.6825196450897846,
                    0.20054945054945053,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.2539399377522556,
                    0,
                    0,
                    2.0
                ],
                [
                    0.18777394058292934,
                    0.2,
                    0.5517359999318381,
                    0.3692033414894501,
                    1.0,
                    0.05670252980517592,
                    0.00015398584392207253,
                    0.0003805198021297029,
                    0.5989010989010989,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.4921201244954888,
                    0,
                    0,
                    2.0
                ],
                [
                    0.2504147953586156,
                    0.65,
                    0.2511936725911687,
                    0.30941237153013623,
                    0.0,
                    0.6862460017446932,
                    0.5033744139245682,
                    0.31709983510808576,
                    0.20054945054945053,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.2539399377522556,
                    0,
                    0,
                    2.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "week before sb",
                "during regular season",
                "week after sb"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "week after sb": 0.19015107291173972,
                "week before sb": 0.1668984862825972,
                "during regular season": 0.6435820696314984
            },
            "categorical_data": {
                "question_type": 2
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.539399377522556,
                    4.921201244954888,
                    2.539399377522556
                ],
                "result_count_important_words": [
                    93500.0,
                    29.0,
                    94800.0
                ],
                "wikipedia_search": [
                    1.4038461538461537,
                    4.1923076923076925,
                    1.4038461538461537
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.75,
                    1.0,
                    3.25
                ],
                "cosine_similarity_raw": [
                    0.026711050420999527,
                    0.07478268444538116,
                    0.0340469665825367
                ],
                "result_count_noun_chunks": [
                    113000.0,
                    63.0,
                    52500.0
                ],
                "question_answer_similarity": [
                    12.433622437529266,
                    14.283632202073932,
                    11.970456431619823
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    884000.0,
                    195000.0,
                    2360000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    0,
                    11.0,
                    0
                ],
                "answer_relation_to_question": [
                    5.056301376526095,
                    1.689965465246364,
                    2.2537331582275404
                ],
                "word_count_appended": [
                    3.0,
                    3.0,
                    3.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "In Harry Potter's Quidditch, what ALWAYS happens when one team catches the snitch?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "the game ends"
            ],
            "lines": [
                [
                    0.25995222929936307,
                    0.6182795698924731,
                    0.5593625366402655,
                    0.3338759673731476,
                    0.1794871794871795,
                    0.26026200873362443,
                    0.025879917184265012,
                    0.012534818941504178,
                    0.0625,
                    0.11666666666666667,
                    0.3333333333333333,
                    0.1597846980283918,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.5300955414012739,
                    0.3763440860215054,
                    0.1679551768897467,
                    0.34585338582943714,
                    0.042735042735042736,
                    0.26026200873362443,
                    0.006211180124223602,
                    0.0032497678737233053,
                    0.9375,
                    0.08333333333333333,
                    0.3333333333333333,
                    0.09806138661387281,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.20995222929936308,
                    0.005376344086021506,
                    0.2726822864699878,
                    0.32027064679741524,
                    0.7777777777777778,
                    0.4794759825327511,
                    0.9679089026915114,
                    0.9842154131847726,
                    0.0,
                    0.8,
                    0.3333333333333333,
                    0.7421539153577354,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "the game ends": 0.5637962022521906,
                "that team loses": 0.2274953030635083,
                "that team wins": 0.208708494684301
            },
            "question": "in harry potter's quidditch, what always happens when one team catches the snitch?",
            "rate_limited": false,
            "answers": [
                "that team wins",
                "that team loses",
                "the game ends"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "the game ends": 0.4869861986582607,
                "that team loses": 0.16466010493570699,
                "that team wins": 0.11306868011177049
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1184928861987424,
                    0.6864297062971096,
                    5.195077407504147
                ],
                "result_count_important_words": [
                    25.0,
                    6.0,
                    935.0
                ],
                "wikipedia_search": [
                    0.0625,
                    0.9375,
                    0.0
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.6182795698924731,
                    0.3763440860215054,
                    0.005376344086021506
                ],
                "cosine_similarity_raw": [
                    0.2583844065666199,
                    0.0775829553604126,
                    0.12595918774604797
                ],
                "result_count_noun_chunks": [
                    27.0,
                    7.0,
                    2120.0
                ],
                "question_answer_similarity": [
                    15.678421485237777,
                    16.240866923704743,
                    15.039531684014946
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    4.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    2.0
                ],
                "result_count_bing": [
                    29800.0,
                    29800.0,
                    54900.0
                ],
                "word_count_appended": [
                    7.0,
                    5.0,
                    48.0
                ],
                "answer_relation_to_question": [
                    0.5199044585987261,
                    1.0601910828025478,
                    0.41990445859872616
                ],
                "result_count": [
                    21.0,
                    5.0,
                    91.0
                ]
            },
            "integer_answers": {
                "the game ends": 8,
                "that team loses": 3,
                "that team wins": 3
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "One of Apple\u2019s biggest flops was a product named after a man who did what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "discovered saturn"
            ],
            "lines": [
                [
                    0.32602843166223444,
                    0.5249433106575964,
                    0.14752903191592426,
                    0.2589584497031569,
                    0.5434782608695652,
                    0.333843797856049,
                    0.5633802816901409,
                    0.5208333333333334,
                    0.3467086834733893,
                    0.35294117647058826,
                    0.42857142857142855,
                    0.3563749329172821,
                    0,
                    0,
                    1.0
                ],
                [
                    0.5307257296693916,
                    0.3038548752834467,
                    0.3120795819795179,
                    0.500655128221995,
                    0.13043478260869565,
                    0.3315467075038285,
                    0.14788732394366197,
                    0.1597222222222222,
                    0.2574229691876751,
                    0.35294117647058826,
                    0.14285714285714285,
                    0.3055139458285154,
                    0,
                    0,
                    1.0
                ],
                [
                    0.14324583866837387,
                    0.17120181405895693,
                    0.5403913861045578,
                    0.2403864220748481,
                    0.32608695652173914,
                    0.3346094946401225,
                    0.2887323943661972,
                    0.3194444444444444,
                    0.39586834733893556,
                    0.29411764705882354,
                    0.42857142857142855,
                    0.33811112125420245,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "invented the transistor": 0.28963679881472343,
                "developed calculus": 0.3183972745918858,
                "discovered saturn": 0.3919659265933908
            },
            "question": "one of apple\u2019s biggest flops was a product named after a man who did what?",
            "rate_limited": false,
            "answers": [
                "discovered saturn",
                "invented the transistor",
                "developed calculus"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "invented the transistor": 0.23959110245507648,
                "developed calculus": 0.3569632692530953,
                "discovered saturn": 0.36868467009045297
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.138249597503693,
                    1.8330836749710921,
                    2.0286667275252146
                ],
                "result_count_important_words": [
                    80.0,
                    21.0,
                    41.0
                ],
                "wikipedia_search": [
                    1.040126050420168,
                    0.7722689075630252,
                    1.1876050420168067
                ],
                "word_count_appended_bing": [
                    6.0,
                    2.0,
                    6.0
                ],
                "answer_relation_to_question_bing": [
                    1.5748299319727892,
                    0.91156462585034,
                    0.5136054421768708
                ],
                "cosine_similarity_raw": [
                    0.02318807877600193,
                    0.04905153810977936,
                    0.08493676036596298
                ],
                "result_count_noun_chunks": [
                    75.0,
                    23.0,
                    46.0
                ],
                "question_answer_similarity": [
                    6.830728069879115,
                    13.2061303332448,
                    6.340840713120997
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    436000.0,
                    433000.0,
                    437000.0
                ],
                "word_count_appended": [
                    6.0,
                    6.0,
                    5.0
                ],
                "answer_relation_to_question": [
                    1.3041137266489378,
                    2.1229029186775663,
                    0.5729833546734955
                ],
                "result_count": [
                    75.0,
                    18.0,
                    45.0
                ]
            },
            "integer_answers": {
                "invented the transistor": 2,
                "developed calculus": 3,
                "discovered saturn": 7
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "In the classic novel \u201cA Confederacy of Dunces,\u201d where does the main character work?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "petting zoo"
            ],
            "question": "in the classic novel \u201ca confederacy of dunces,\u201d where does the main character work?",
            "lines": [
                [
                    0.45686274509803926,
                    0.3333333333333333,
                    0.39658234111834495,
                    0.2521594144739166,
                    0.5157894736842106,
                    0.4022346368715084,
                    0.027594665031427257,
                    0.028156073561446927,
                    0.0,
                    0.4583333333333333,
                    0.25,
                    0.3512417111144043,
                    0,
                    0,
                    3.0
                ],
                [
                    0.19215686274509802,
                    0.3333333333333333,
                    0.286029679333732,
                    0.33213411016006916,
                    0.0,
                    0.26256983240223464,
                    0.9535489805304308,
                    0.9706435885656703,
                    0.13020833333333331,
                    0.125,
                    0.25,
                    0.08396261612730603,
                    0,
                    0,
                    3.0
                ],
                [
                    0.3509803921568627,
                    0.3333333333333333,
                    0.317387979547923,
                    0.4157064753660142,
                    0.4842105263157895,
                    0.33519553072625696,
                    0.01885635443814196,
                    0.0012003378728827373,
                    0.8697916666666666,
                    0.4166666666666667,
                    0.5,
                    0.5647956727582898,
                    0,
                    0,
                    3.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "petting zoo",
                "canning facility",
                "pants factory"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "petting zoo": 0.4656809683578133,
                "pants factory": 0.41861011801300607,
                "canning facility": 0.10627979946466078
            },
            "categorical_data": {
                "question_type": 3
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.45869197780083,
                    0.5877383128911422,
                    3.953569709308028
                ],
                "result_count_important_words": [
                    1800.0,
                    62200.0,
                    1230.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.5208333333333333,
                    3.4791666666666665
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    4.0
                ],
                "answer_relation_to_question_bing": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "cosine_similarity_raw": [
                    0.039399608969688416,
                    0.028416438028216362,
                    0.031531818211078644
                ],
                "result_count_noun_chunks": [
                    1900.0,
                    65500.0,
                    81.0
                ],
                "question_answer_similarity": [
                    3.7646648790687323,
                    4.9586632419377565,
                    6.206373738124967
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    43200.0,
                    28200.0,
                    36000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    11.0,
                    3.0,
                    10.0
                ],
                "answer_relation_to_question": [
                    2.2843137254901964,
                    0.9607843137254901,
                    1.7549019607843137
                ],
                "result_count": [
                    49.0,
                    0,
                    46.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Laurie Metcalf, Amy Morton and Tracy Letts are members of a theatre company from what city?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "chicago"
            ],
            "lines": [
                [
                    0.4354618473895583,
                    0.2949685534591195,
                    0.1970899495652382,
                    0.43169498387416644,
                    0.3888419273034658,
                    0.38092909535452324,
                    0.3898690950483779,
                    0.38513513513513514,
                    0.23854824469347158,
                    0.14402173913043478,
                    0.10416666666666667,
                    0.3436545663355241,
                    0.0962962962962963,
                    0.06451612903225806,
                    1.0
                ],
                [
                    0.25850066934404287,
                    0.44716981132075473,
                    0.06519089273322184,
                    0.3406846974583432,
                    0.2589461820231051,
                    0.4078239608801956,
                    0.2686397268070575,
                    0.26576576576576577,
                    0.28420241234874344,
                    0.07880434782608696,
                    0.10416666666666667,
                    0.3143246362966611,
                    0.022222222222222223,
                    0.0,
                    1.0
                ],
                [
                    0.3060374832663989,
                    0.2578616352201258,
                    0.73771915770154,
                    0.2276203186674904,
                    0.3522118906734291,
                    0.21124694376528116,
                    0.3414911781445646,
                    0.3490990990990991,
                    0.47724934295778504,
                    0.7771739130434783,
                    0.7916666666666666,
                    0.3420207973678148,
                    0.8814814814814815,
                    0.9354838709677419,
                    1.0
                ]
            ],
            "fraction_answers": {
                "new york": 0.2782281592345882,
                "los angeles": 0.22260299940663333,
                "chicago": 0.4991688413587784
            },
            "question": "laurie metcalf, amy morton and tracy letts are members of a theatre company from what city?",
            "rate_limited": false,
            "answers": [
                "new york",
                "los angeles",
                "chicago"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "new york": 0.10333804451297891,
                "los angeles": 0.10616857723550685,
                "chicago": 0.8232899199413797
            },
            "integer_answers": {
                "new york": 6,
                "los angeles": 2,
                "chicago": 6
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.436545663355241,
                    3.1432463629666105,
                    3.4202079736781483
                ],
                "result_count_important_words": [
                    1370.0,
                    944.0,
                    1200.0
                ],
                "wikipedia_search": [
                    0.9541929787738863,
                    1.1368096493949738,
                    1.9089973718311402
                ],
                "word_count_appended_bing": [
                    5.0,
                    5.0,
                    38.0
                ],
                "answer_relation_to_question_bing": [
                    1.4748427672955975,
                    2.2358490566037736,
                    1.2893081761006289
                ],
                "cosine_similarity_raw": [
                    0.04374731332063675,
                    0.01447017677128315,
                    0.16374874114990234
                ],
                "result_count_noun_chunks": [
                    1710.0,
                    1180.0,
                    1550.0
                ],
                "question_answer_similarity": [
                    8.457883653230965,
                    6.6747857658192515,
                    4.459598198533058
                ],
                "word_count_noun_chunks": [
                    13.0,
                    3.0,
                    119.0
                ],
                "result_count_bing": [
                    77900.0,
                    83400.0,
                    43200.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    29.0
                ],
                "word_count_appended": [
                    53.0,
                    29.0,
                    286.0
                ],
                "answer_relation_to_question": [
                    2.1773092369477913,
                    1.2925033467202143,
                    1.5301874163319946
                ],
                "result_count": [
                    1380.0,
                    919.0,
                    1250.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What product\u2019s packaging specifically states, \u201cDo not insert swab into ear canal\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "rice-a-roni"
            ],
            "lines": [
                [
                    0.37194441726196303,
                    0.3566616390145802,
                    0.49279680806077214,
                    0.2584289049670668,
                    0.49150743099787686,
                    0.35830527497194165,
                    0.48785292186474066,
                    0.49975658080951035,
                    0.3685064935064935,
                    0.4407114624505929,
                    0.33870967741935487,
                    0.4354069439465768,
                    0.5,
                    0.5,
                    1.0
                ],
                [
                    0.20570531795975822,
                    0.3238185017596782,
                    0.012396865853598393,
                    0.32467495659869555,
                    0.031847133757961776,
                    0.32491582491582494,
                    0.0370978332239002,
                    0.0007076139258420655,
                    0.3327922077922078,
                    0.07509881422924902,
                    0.17204301075268819,
                    0.1499716984125229,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.42235026477827875,
                    0.3195198592257416,
                    0.49480632608562947,
                    0.4168961384342377,
                    0.47664543524416136,
                    0.3167789001122334,
                    0.47504924491135914,
                    0.4995358052646476,
                    0.2987012987012987,
                    0.4841897233201581,
                    0.489247311827957,
                    0.4146213576409003,
                    0.5,
                    0.5,
                    1.0
                ]
            ],
            "fraction_answers": {
                "rice-a-roni": 0.15705877781836147,
                "oreo cookies": 0.12737976206477097,
                "q-tips": 0.7155614601168675
            },
            "question": "what product\u2019s packaging specifically states, \u201cdo not insert swab into ear canal\u201d?",
            "rate_limited": false,
            "answers": [
                "rice-a-roni",
                "q-tips",
                "oreo cookies"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "rice-a-roni": 0.4980065775170711,
                "oreo cookies": 0.42594445424846233,
                "q-tips": 0.09505945033210296
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.0334888968547709,
                    5.600452825399634,
                    1.3660582777455954
                ],
                "result_count_important_words": [
                    37.0,
                    1410.0,
                    76.0
                ],
                "wikipedia_search": [
                    1.051948051948052,
                    1.3376623376623376,
                    1.6103896103896103
                ],
                "word_count_appended_bing": [
                    30.0,
                    61.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.433383609854198,
                    1.7618149824032179,
                    1.804801407742584
                ],
                "cosine_similarity_raw": [
                    0.00928478129208088,
                    0.6285114288330078,
                    0.006694549694657326
                ],
                "result_count_noun_chunks": [
                    43.0,
                    88200.0,
                    82.0
                ],
                "question_answer_similarity": [
                    10.437109580263495,
                    7.5749405776150525,
                    3.5905128036392853
                ],
                "word_count_noun_chunks": [
                    0.0,
                    30.0,
                    0.0
                ],
                "result_count_bing": [
                    50500.0,
                    62400.0,
                    65300.0
                ],
                "word_count_raw": [
                    0.0,
                    38.0,
                    0.0
                ],
                "word_count_appended": [
                    30.0,
                    215.0,
                    8.0
                ],
                "answer_relation_to_question": [
                    2.048889323808592,
                    4.7087149126438685,
                    1.2423957635475396
                ],
                "result_count": [
                    16.0,
                    882.0,
                    44.0
                ]
            },
            "integer_answers": {
                "rice-a-roni": 1,
                "oreo cookies": 3,
                "q-tips": 10
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which artist painted the ceiling of one of France's most iconic opera houses?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "marc chagall"
            ],
            "lines": [
                [
                    0.5969455544455544,
                    0.4246575342465753,
                    0.8725147596527358,
                    -0.25652443449254586,
                    0.5778175313059034,
                    0.2639022822243652,
                    0.009991020584457654,
                    0.17382342837512882,
                    0.8431401931401932,
                    0.6036036036036037,
                    0.7,
                    0.38608760999360736,
                    0.9629629629629629,
                    1.0,
                    -1.0
                ],
                [
                    0.12130369630369632,
                    0.3647260273972603,
                    0.0738361845052557,
                    0.2935615048420961,
                    0.24865831842576028,
                    0.388942462230794,
                    0.631968992600507,
                    0.4878048780487805,
                    0.07062937062937064,
                    0.2747747747747748,
                    0.2,
                    0.3094331229554043,
                    0.037037037037037035,
                    0.0,
                    -1.0
                ],
                [
                    0.2817507492507492,
                    0.2106164383561644,
                    0.053649055842008464,
                    0.9629629296504498,
                    0.1735241502683363,
                    0.3471552555448409,
                    0.3580399868150354,
                    0.3383716935760907,
                    0.08623043623043623,
                    0.12162162162162163,
                    0.1,
                    0.3044792670509883,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "marc chagall": 0.5113515747173245,
                "edgar degas": 0.2501911692679098,
                "auguste renoir": 0.2384572560147658
            },
            "question": "which artist painted the ceiling of one of france's most iconic opera houses?",
            "rate_limited": false,
            "answers": [
                "marc chagall",
                "edgar degas",
                "auguste renoir"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "marc chagall": 0.7906221929744528,
                "edgar degas": 0.09628375534246894,
                "auguste renoir": 0.08734073138702975
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.7026132699552514,
                    2.16603186068783,
                    2.1313548693569184
                ],
                "result_count_important_words": [
                    87900.0,
                    5560000.0,
                    3150000.0
                ],
                "wikipedia_search": [
                    5.058841158841159,
                    0.4237762237762238,
                    0.5173826173826174
                ],
                "word_count_appended_bing": [
                    35.0,
                    10.0,
                    5.0
                ],
                "answer_relation_to_question_bing": [
                    1.273972602739726,
                    1.0941780821917808,
                    0.6318493150684932
                ],
                "cosine_similarity_raw": [
                    0.22796885669231415,
                    0.019291765987873077,
                    0.014017314650118351
                ],
                "result_count_noun_chunks": [
                    50600.0,
                    142000.0,
                    98500.0
                ],
                "question_answer_similarity": [
                    0.3011175722349435,
                    -0.344593012414407,
                    -1.1303603888736689
                ],
                "word_count_noun_chunks": [
                    26.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    7.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    82100.0,
                    121000.0,
                    108000.0
                ],
                "word_count_appended": [
                    134.0,
                    61.0,
                    27.0
                ],
                "answer_relation_to_question": [
                    4.178618881118881,
                    0.8491258741258743,
                    1.9722552447552446
                ],
                "result_count": [
                    323000.0,
                    139000.0,
                    97000.0
                ]
            },
            "integer_answers": {
                "marc chagall": 11,
                "edgar degas": 3,
                "auguste renoir": 0
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What soda is named for a medical condition?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "pepsi"
            ],
            "lines": [
                [
                    0.28270874424720577,
                    0.37142857142857144,
                    0.17036371173248932,
                    4.749959552994105,
                    0.07273802483737433,
                    0.012075817792723938,
                    0.10181958962446767,
                    0.014821915787921245,
                    0.024390243902439025,
                    0.2558613659531091,
                    0.5024390243902439,
                    0.2948559141601118,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.5950032873109796,
                    0.5,
                    0.6093064097383513,
                    -6.197112683480115,
                    0.41395623891188643,
                    0.02491592785081015,
                    0.4039230868499161,
                    0.8166801858270039,
                    0.8048780487804879,
                    0.46992864424057085,
                    0.24878048780487805,
                    0.39136588779980946,
                    0,
                    1.0,
                    1.0
                ],
                [
                    0.1222879684418146,
                    0.12857142857142856,
                    0.22032987852915947,
                    2.4471531304860106,
                    0.5133057362507392,
                    0.9630082543564659,
                    0.4942573235256162,
                    0.16849789838507484,
                    0.17073170731707318,
                    0.2742099898063201,
                    0.24878048780487805,
                    0.31377819804007867,
                    0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "fanta": 0.4665316924242045,
                "pepsi": 0.006278886279583035,
                "faygo": 0.5271894212962124
            },
            "question": "what soda is named for a medical condition?",
            "rate_limited": false,
            "answers": [
                "faygo",
                "pepsi",
                "fanta"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "fanta": 0.07844826540933159,
                "pepsi": 0.7468675511902355,
                "faygo": 0.16288566345960515
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1794236566404472,
                    1.5654635511992379,
                    1.2551127921603147
                ],
                "result_count_important_words": [
                    78900.0,
                    313000.0,
                    383000.0
                ],
                "wikipedia_search": [
                    0.04878048780487805,
                    1.6097560975609757,
                    0.34146341463414637
                ],
                "word_count_appended_bing": [
                    103.0,
                    51.0,
                    51.0
                ],
                "answer_relation_to_question_bing": [
                    0.7428571428571429,
                    1.0,
                    0.2571428571428571
                ],
                "cosine_similarity_raw": [
                    0.040354788303375244,
                    0.1443290412425995,
                    0.05219049006700516
                ],
                "result_count_noun_chunks": [
                    8040.0,
                    443000.0,
                    91400.0
                ],
                "question_answer_similarity": [
                    -0.7330479547381401,
                    0.9563830443657935,
                    -0.3776622889563441
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    12.0,
                    0.0
                ],
                "result_count_bing": [
                    158000.0,
                    326000.0,
                    12600000.0
                ],
                "word_count_appended": [
                    251.0,
                    461.0,
                    269.0
                ],
                "answer_relation_to_question": [
                    0.8481262327416172,
                    1.7850098619329389,
                    0.3668639053254438
                ],
                "result_count": [
                    61500.0,
                    350000.0,
                    434000.0
                ]
            },
            "integer_answers": {
                "fanta": 3,
                "pepsi": 9,
                "faygo": 1
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What was the first registered trademark for a breakfast cereal?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "quaker oats"
            ],
            "lines": [
                [
                    0.6029516164263872,
                    0.3086297760210803,
                    0.9678192454021143,
                    0.26757112384816656,
                    0.8014735165272799,
                    0.47330222913426645,
                    0.2917744438696327,
                    0.5828002842928216,
                    0.7261904761904762,
                    0.2742200328407225,
                    0.21518987341772153,
                    0.33867956194028614,
                    0.9354838709677419,
                    1.0,
                    1.0
                ],
                [
                    0.22516655744866754,
                    0.4334650856389987,
                    0.020598658147501604,
                    0.7605214761464008,
                    0.0730784547988849,
                    0.37636080870917576,
                    0.22038282462493533,
                    0.00023691068467187872,
                    0.10833333333333334,
                    0.0,
                    0.0,
                    0.327912578508771,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.1718818261249454,
                    0.25790513833992096,
                    0.01158209645038408,
                    -0.028092599994567334,
                    0.12544802867383512,
                    0.1503369621565578,
                    0.48784273150543195,
                    0.4169628050225065,
                    0.1654761904761905,
                    0.7257799671592775,
                    0.7848101265822784,
                    0.3334078595509429,
                    0.06451612903225806,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "quaker oats": 0.5561490036341927,
                "wheaties": 0.2619898043628544,
                "kellogg\u2019s corn flakes": 0.18186119200295292
            },
            "question": "what was the first registered trademark for a breakfast cereal?",
            "rate_limited": false,
            "answers": [
                "quaker oats",
                "kellogg\u2019s corn flakes",
                "wheaties"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "quaker oats": 0.8271532799205323,
                "wheaties": 0.2500713443875775,
                "kellogg\u2019s corn flakes": 0.14612362962884304
            },
            "integer_answers": {
                "quaker oats": 9,
                "wheaties": 3,
                "kellogg\u2019s corn flakes": 2
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3547182477611446,
                    1.311650314035084,
                    1.3336314382037715
                ],
                "result_count_important_words": [
                    56400.0,
                    42600.0,
                    94300.0
                ],
                "wikipedia_search": [
                    2.9047619047619047,
                    0.43333333333333335,
                    0.661904761904762
                ],
                "word_count_appended_bing": [
                    17.0,
                    0.0,
                    62.0
                ],
                "answer_relation_to_question_bing": [
                    1.2345191040843213,
                    1.7338603425559949,
                    1.0316205533596838
                ],
                "cosine_similarity_raw": [
                    0.5257837176322937,
                    0.011190559715032578,
                    0.006292164325714111
                ],
                "result_count_noun_chunks": [
                    246000.0,
                    100.0,
                    176000.0
                ],
                "question_answer_similarity": [
                    2.40065147227142,
                    6.823408203199506,
                    -0.2520471587777138
                ],
                "word_count_noun_chunks": [
                    29.0,
                    0.0,
                    2.0
                ],
                "result_count_bing": [
                    913000.0,
                    726000.0,
                    290000.0
                ],
                "word_count_raw": [
                    14.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    167.0,
                    0.0,
                    442.0
                ],
                "answer_relation_to_question": [
                    2.4118064657055487,
                    0.9006662297946701,
                    0.6875273044997816
                ],
                "result_count": [
                    80500.0,
                    7340.0,
                    12600.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which popular root beer brand typically contains caffeine?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "mug"
            ],
            "question": "which popular root beer brand typically contains caffeine?",
            "lines": [
                [
                    0.6864419804614048,
                    0.62004662004662,
                    0.8969331213355272,
                    0.5386628643075225,
                    0.020925751669131156,
                    0.2086858432036097,
                    0.01851925288495856,
                    0.07486865148861646,
                    0.4980549966465459,
                    0.0,
                    0.0,
                    0.3186015074054608,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.08221145721849828,
                    0.06876456876456877,
                    0.03643356108819035,
                    0.0,
                    0.9141013577093695,
                    0.6204173716864072,
                    0.9239798548598168,
                    0.8231173380035026,
                    0.22575452716297786,
                    0.0425531914893617,
                    0.1323529411764706,
                    0.34573102911984954,
                    0.3333333333333333,
                    0.29411764705882354,
                    -1.0
                ],
                [
                    0.2313465623200969,
                    0.3111888111888112,
                    0.06663331757628249,
                    0.4613371356924775,
                    0.0649728906214993,
                    0.17089678510998307,
                    0.05750089225522465,
                    0.1020140105078809,
                    0.2761904761904762,
                    0.9574468085106383,
                    0.8676470588235294,
                    0.3356674634746897,
                    0.6666666666666666,
                    0.7058823529411765,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "barq\u2019s",
                "a&w",
                "mug"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "mug": 0.5192261856426379,
                "a&w": 0.13826803872988083,
                "barq\u2019s": 0.5079997985097009
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.2302105518382254,
                    2.420117203838947,
                    2.3496722443228277
                ],
                "result_count_important_words": [
                    46700.0,
                    2330000.0,
                    145000.0
                ],
                "wikipedia_search": [
                    3.4863849765258212,
                    1.580281690140845,
                    1.9333333333333333
                ],
                "word_count_appended_bing": [
                    0.0,
                    9.0,
                    59.0
                ],
                "answer_relation_to_question_bing": [
                    3.72027972027972,
                    0.4125874125874126,
                    1.867132867132867
                ],
                "cosine_similarity_raw": [
                    0.6453279852867126,
                    0.026213321834802628,
                    0.047941528260707855
                ],
                "result_count_noun_chunks": [
                    171000.0,
                    1880000.0,
                    233000.0
                ],
                "question_answer_similarity": [
                    2.144518733024597,
                    0.0,
                    1.836670383810997
                ],
                "word_count_noun_chunks": [
                    0.0,
                    21.0,
                    42.0
                ],
                "result_count_bing": [
                    370000.0,
                    1100000.0,
                    303000.0
                ],
                "word_count_raw": [
                    0.0,
                    5.0,
                    12.0
                ],
                "result_count": [
                    46700.0,
                    2040000.0,
                    145000.0
                ],
                "answer_relation_to_question": [
                    4.805093863229834,
                    0.5754802005294879,
                    1.6194259362406782
                ],
                "word_count_appended": [
                    0.0,
                    14.0,
                    315.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "When it comes to clothing design, what is a frog?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "cap sleeve"
            ],
            "lines": [
                [
                    0.34558823529411764,
                    0.5,
                    0.7167776986175926,
                    0.19333327828072294,
                    0.15,
                    0.316694954482333,
                    0.19230769230769232,
                    0.13756613756613756,
                    0.8395061728395062,
                    0.24675324675324675,
                    0.3333333333333333,
                    0.17417391288645975,
                    0,
                    0,
                    1.0
                ],
                [
                    0.22990196078431374,
                    0.26785714285714285,
                    0.1204895517456601,
                    0.2942435321215933,
                    0.21428571428571427,
                    0.035256904798642186,
                    0.2980769230769231,
                    0.3386243386243386,
                    0.0,
                    0.0,
                    0.0,
                    0.2468973907259715,
                    0,
                    0,
                    1.0
                ],
                [
                    0.4245098039215686,
                    0.23214285714285715,
                    0.16273274963674733,
                    0.5124231895976837,
                    0.6357142857142857,
                    0.6480481407190248,
                    0.5096153846153846,
                    0.5238095238095238,
                    0.16049382716049382,
                    0.7532467532467533,
                    0.6666666666666666,
                    0.5789286963875688,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "braided fastener": 0.3455028885300952,
                "cap sleeve": 0.4840276565515465,
                "crystal appliqu\u00e9": 0.1704694549183583
            },
            "question": "when it comes to clothing design, what is a frog?",
            "rate_limited": false,
            "answers": [
                "braided fastener",
                "crystal appliqu\u00e9",
                "cap sleeve"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "braided fastener": 0.3158420913899801,
                "cap sleeve": 0.3459376156709961,
                "crystal appliqu\u00e9": 0.14680338812744936
            },
            "integer_answers": {
                "braided fastener": 3,
                "cap sleeve": 9,
                "crystal appliqu\u00e9": 0
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.696695651545839,
                    0.987589562903886,
                    2.3157147855502753
                ],
                "result_count_important_words": [
                    20.0,
                    31.0,
                    53.0
                ],
                "wikipedia_search": [
                    2.5185185185185186,
                    0.0,
                    0.48148148148148145
                ],
                "word_count_appended_bing": [
                    2.0,
                    0.0,
                    4.0
                ],
                "answer_relation_to_question_bing": [
                    2.0,
                    1.0714285714285714,
                    0.9285714285714286
                ],
                "cosine_similarity_raw": [
                    0.14999347925186157,
                    0.02521374076604843,
                    0.034053586423397064
                ],
                "result_count_noun_chunks": [
                    26.0,
                    64.0,
                    99.0
                ],
                "question_answer_similarity": [
                    2.0287215458229184,
                    3.0876122240442783,
                    5.377056524157524
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    8210000.0,
                    914000.0,
                    16800000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    21.0,
                    30.0,
                    89.0
                ],
                "answer_relation_to_question": [
                    1.3823529411764706,
                    0.919607843137255,
                    1.6980392156862745
                ],
                "word_count_appended": [
                    19.0,
                    0.0,
                    58.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "The fictional religion Bokononism was invented by the author of what novel?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "slaughterhouse-five"
            ],
            "question": "the fictional religion bokononism was invented by the author of what novel?",
            "lines": [
                [
                    0.22748768472906403,
                    0.5858585858585859,
                    0.9513608096974995,
                    0.37102420879349646,
                    0.9353078721745908,
                    0.34603564843269824,
                    0.555111821086262,
                    0.8427618218740934,
                    0.5468614718614718,
                    0.8983739837398373,
                    0.9456521739130435,
                    0.5927482096307878,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.17136288998357965,
                    0.2424242424242424,
                    0.023057922814565198,
                    0.25115850222371866,
                    0.034294621979735,
                    0.2698217578365089,
                    0.025559105431309903,
                    0.009283434870902234,
                    0.15357142857142858,
                    0.056910569105691054,
                    0.03260869565217391,
                    0.20050366043882797,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.6011494252873564,
                    0.17171717171717168,
                    0.025581267487935357,
                    0.3778172889827849,
                    0.030397505845674203,
                    0.38414259373079285,
                    0.4193290734824281,
                    0.14795474325500435,
                    0.29956709956709954,
                    0.044715447154471545,
                    0.021739130434782608,
                    0.2067481299303842,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "slaughterhouse-five",
                "infinite jest",
                "animal farm"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "infinite jest": 0.11066660199059868,
                "slaughterhouse-five": 0.9407214593565053,
                "animal farm": 0.17819806499770277
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.5564892577847265,
                    1.203021962632968,
                    1.2404887795823052
                ],
                "result_count_important_words": [
                    1390.0,
                    64.0,
                    1050.0
                ],
                "wikipedia_search": [
                    2.1874458874458873,
                    0.6142857142857143,
                    1.1982683982683981
                ],
                "word_count_appended_bing": [
                    87.0,
                    3.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.7575757575757578,
                    0.7272727272727273,
                    0.5151515151515151
                ],
                "cosine_similarity_raw": [
                    0.4531952738761902,
                    0.010983994230628014,
                    0.012186028063297272
                ],
                "result_count_noun_chunks": [
                    5810.0,
                    64.0,
                    1020.0
                ],
                "question_answer_similarity": [
                    5.173544262186624,
                    3.5021424405276775,
                    5.2682666555047035
                ],
                "word_count_noun_chunks": [
                    8.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    56300.0,
                    43900.0,
                    62500.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    1200.0,
                    44.0,
                    39.0
                ],
                "answer_relation_to_question": [
                    1.13743842364532,
                    0.8568144499178982,
                    3.005747126436782
                ],
                "word_count_appended": [
                    221.0,
                    14.0,
                    11.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Until it was banned, lithium was a key ingredient in which of these brands?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "7up"
            ],
            "lines": [
                [
                    0.06428571428571428,
                    0.14655172413793102,
                    0.07135812956154025,
                    0.21936787856822487,
                    0.5499243570347958,
                    0.26897470039946736,
                    0.21641144177983243,
                    0.585432266848196,
                    0.13513513513513514,
                    0.056962025316455694,
                    0.2391304347826087,
                    0.36500183957063986,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.7579365079365079,
                    0.625,
                    0.7992065896515486,
                    -0.05531387488783091,
                    0.4485627836611195,
                    0.2756324900133156,
                    0.7830106905518637,
                    0.4118447923757658,
                    0.8648648648648649,
                    0.9177215189873418,
                    0.717391304347826,
                    0.5194425694283371,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.1777777777777778,
                    0.22844827586206898,
                    0.12943528078691116,
                    0.8359459963196061,
                    0.0015128593040847202,
                    0.45539280958721706,
                    0.0005778676683039584,
                    0.002722940776038121,
                    0.0,
                    0.02531645569620253,
                    0.043478260869565216,
                    0.11555559100102308,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "cracker jack": 0.2084668319586101,
                "7up": 0.6475214454950472,
                "good and plenty": 0.1440117225463427
            },
            "question": "until it was banned, lithium was a key ingredient in which of these brands?",
            "rate_limited": false,
            "answers": [
                "cracker jack",
                "7up",
                "good and plenty"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "cracker jack": 0.20862312472465147,
                "7up": 0.8232899199413797,
                "good and plenty": 0.13092685556472403
            },
            "integer_answers": {
                "cracker jack": 2,
                "7up": 10,
                "good and plenty": 2
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.8250091978531993,
                    2.5972128471416855,
                    0.5777779550051154
                ],
                "result_count_important_words": [
                    749.0,
                    2710.0,
                    2.0
                ],
                "wikipedia_search": [
                    0.40540540540540543,
                    2.5945945945945947,
                    0.0
                ],
                "word_count_appended_bing": [
                    11.0,
                    33.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.29310344827586204,
                    1.25,
                    0.45689655172413796
                ],
                "cosine_similarity_raw": [
                    0.013775791972875595,
                    0.15428800880908966,
                    0.02498767152428627
                ],
                "result_count_noun_chunks": [
                    1720.0,
                    1210.0,
                    8.0
                ],
                "question_answer_similarity": [
                    4.679841212928295,
                    -1.180027600377798,
                    17.833488434553146
                ],
                "word_count_noun_chunks": [
                    0.0,
                    8.0,
                    0.0
                ],
                "result_count_bing": [
                    20200.0,
                    20700.0,
                    34200.0
                ],
                "word_count_raw": [
                    0.0,
                    9.0,
                    0.0
                ],
                "result_count": [
                    727.0,
                    593.0,
                    2.0
                ],
                "answer_relation_to_question": [
                    0.2571428571428571,
                    3.0317460317460316,
                    0.7111111111111112
                ],
                "word_count_appended": [
                    9.0,
                    145.0,
                    4.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Aside from blood cells, what would you also find inside your blood vessels?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "plasma"
            ],
            "lines": [
                [
                    0.3603028187740992,
                    0.5106736242884251,
                    0.6404770639520677,
                    0.579677638116903,
                    0.5465937874950006,
                    0.3912231559290383,
                    0.5531738348776103,
                    0.5219998393846648,
                    0.08711650922177237,
                    0.371301775147929,
                    0.3142857142857143,
                    0.3899030315667874,
                    0.7083333333333334,
                    1.0,
                    1.0
                ],
                [
                    0.5125182821433321,
                    0.2883459835547122,
                    0.2887399913711652,
                    0.5845872246652679,
                    0.42394347420343953,
                    0.35014005602240894,
                    0.4342414603789241,
                    0.47559985366158347,
                    0.8813045434098065,
                    0.4467455621301775,
                    0.42857142857142855,
                    0.34906546295967444,
                    0.2916666666666667,
                    0.0,
                    1.0
                ],
                [
                    0.12717889908256882,
                    0.20098039215686275,
                    0.07078294467676716,
                    -0.1642648627821709,
                    0.029462738301559793,
                    0.2586367880485528,
                    0.012584704743465635,
                    0.0024003069537517066,
                    0.031578947368421054,
                    0.1819526627218935,
                    0.2571428571428571,
                    0.2610315054735382,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "marrow": 0.41110499926704197,
                "plasma": 0.49821872331238176,
                "plastids": 0.09067627742057628
            },
            "question": "aside from blood cells, what would you also find inside your blood vessels?",
            "rate_limited": false,
            "answers": [
                "plasma",
                "marrow",
                "plastids"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "marrow": 0.4046084166072911,
                "plasma": 0.7308693215874036,
                "plastids": 0.08012620476918583
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.339418189400724,
                    2.0943927777580464,
                    1.5661890328412291
                ],
                "result_count_important_words": [
                    120000.0,
                    94200.0,
                    2730.0
                ],
                "wikipedia_search": [
                    0.4355825461088619,
                    4.406522717049032,
                    0.15789473684210525
                ],
                "word_count_appended_bing": [
                    33.0,
                    45.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    2.0426944971537004,
                    1.1533839342188488,
                    0.803921568627451
                ],
                "cosine_similarity_raw": [
                    0.25211116671562195,
                    0.11365680396556854,
                    0.0278623104095459
                ],
                "result_count_noun_chunks": [
                    5850000.0,
                    5330000.0,
                    26900.0
                ],
                "question_answer_similarity": [
                    3.247341550886631,
                    3.2748449482023716,
                    -0.9202081970870495
                ],
                "word_count_noun_chunks": [
                    17.0,
                    7.0,
                    0.0
                ],
                "word_count_raw": [
                    4.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    4190000.0,
                    3750000.0,
                    2770000.0
                ],
                "word_count_appended": [
                    251.0,
                    302.0,
                    123.0
                ],
                "answer_relation_to_question": [
                    1.4412112750963968,
                    2.0500731285733282,
                    0.5087155963302753
                ],
                "result_count": [
                    410000.0,
                    318000.0,
                    22100.0
                ]
            },
            "integer_answers": {
                "marrow": 5,
                "plasma": 9,
                "plastids": 0
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these Kentucky Derby winners was named for its trainer?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "clyde van dusen"
            ],
            "lines": [
                [
                    0.8251539164582643,
                    0.5916666666666667,
                    0.47604286124139544,
                    -0.07717610215685004,
                    0.009058771542200618,
                    0.36471830310010556,
                    0.3710777626193724,
                    0.008776458440887971,
                    0.7333333333333334,
                    0.5677966101694916,
                    0.39285714285714285,
                    0.31441719635028254,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.03513513513513514,
                    0.125,
                    0.303059475963187,
                    0.6356224678306841,
                    0.8395934600088378,
                    0.5681927248296381,
                    0.6275579809004093,
                    0.8384099122354156,
                    0.0,
                    0.3474576271186441,
                    0.42857142857142855,
                    0.38515423088123035,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.13971094840660062,
                    0.2833333333333333,
                    0.2208976627954176,
                    0.4415536343261659,
                    0.15134776844896156,
                    0.06708897207025626,
                    0.001364256480218281,
                    0.15281362932369644,
                    0.26666666666666666,
                    0.0847457627118644,
                    0.17857142857142858,
                    0.3004285727684871,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "lieut. gibson": 0.1907102196585914,
                "paul jones": 0.4278128702895509,
                "clyde van dusen": 0.3814769100518578
            },
            "question": "which of these kentucky derby winners was named for its trainer?",
            "rate_limited": false,
            "answers": [
                "clyde van dusen",
                "paul jones",
                "lieut. gibson"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "lieut. gibson": 0.12056407770616383,
                "paul jones": 0.20995528245031492,
                "clyde van dusen": 0.4465518914930251
            },
            "integer_answers": {
                "lieut. gibson": 0,
                "paul jones": 7,
                "clyde van dusen": 5
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.5720859817514126,
                    1.9257711544061518,
                    1.5021428638424357
                ],
                "result_count_important_words": [
                    27200.0,
                    46000.0,
                    100.0
                ],
                "wikipedia_search": [
                    2.2,
                    0.0,
                    0.8
                ],
                "answer_relation_to_question": [
                    4.125769582291321,
                    0.17567567567567566,
                    0.6985547420330029
                ],
                "word_count_appended_bing": [
                    11.0,
                    12.0,
                    5.0
                ],
                "answer_relation_to_question_bing": [
                    2.3666666666666667,
                    0.5,
                    1.1333333333333333
                ],
                "cosine_similarity_raw": [
                    0.11097888648509979,
                    0.07065162807703018,
                    0.05149741470813751
                ],
                "result_count_noun_chunks": [
                    85.0,
                    8120.0,
                    1480.0
                ],
                "question_answer_similarity": [
                    -0.440953366458416,
                    3.6316924430429935,
                    2.5228607831522822
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    380000.0,
                    592000.0,
                    69900.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    82.0,
                    7600.0,
                    1370.0
                ],
                "word_count_appended": [
                    67.0,
                    41.0,
                    10.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these Hebrew texts does NOT form a significant part of the Christian Old Testament?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ketuvim"
            ],
            "lines": [
                [
                    0.3532616641503473,
                    0.2681588143657109,
                    0.33283671489356603,
                    0.08249714013652126,
                    0.0466643585138381,
                    0.4003104901308494,
                    0.19097337029783557,
                    0.04374224946769928,
                    0.3461312716575992,
                    0.29268292682926833,
                    0.2815533980582524,
                    0.3288293081500853,
                    0.025000000000000022,
                    0.0,
                    -1.0
                ],
                [
                    0.31528918517958754,
                    0.34038735211149007,
                    0.3801088422406022,
                    0.5,
                    0.49672170168567287,
                    0.32146817476158795,
                    0.4946969504285678,
                    0.49638503474578255,
                    0.44717858129712235,
                    0.3635307781649245,
                    0.3592233009708738,
                    0.33751814742154695,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.33144915067006514,
                    0.39145383352279906,
                    0.28705444286583176,
                    0.4175028598634788,
                    0.45661393980048903,
                    0.27822133510756264,
                    0.3143296792735967,
                    0.45987271578651817,
                    0.2066901470452785,
                    0.3437862950058072,
                    0.3592233009708738,
                    0.3336525444283678,
                    0.475,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "talmud": 0.2635928222370474,
                "ketuvim": 0.1639274215703202,
                "torah": 0.5724797561926324
            },
            "question": "which of these hebrew texts does not form a significant part of the christian old testament?",
            "rate_limited": false,
            "answers": [
                "torah",
                "ketuvim",
                "talmud"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "talmud": 0.2825069450057118,
                "ketuvim": 0.30011092591887933,
                "torah": 0.11353082233265682
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.3963896858988054,
                    2.274745936098343,
                    2.3288643780028515
                ],
                "result_count_important_words": [
                    2430000.0,
                    41700.0,
                    1460000.0
                ],
                "wikipedia_search": [
                    2.154162196793611,
                    0.7394998618402874,
                    4.106337941366101
                ],
                "answer_relation_to_question": [
                    2.0543367018951373,
                    2.5859514074857746,
                    2.359711890619088
                ],
                "answer_relation_to_question_bing": [
                    2.782094227611469,
                    1.9153517746621191,
                    1.3025539977264116
                ],
                "word_count_appended": [
                    357.0,
                    235.0,
                    269.0
                ],
                "cosine_similarity_raw": [
                    0.09564834833145142,
                    0.06859993934631348,
                    0.12184428423643112
                ],
                "result_count_noun_chunks": [
                    3900000.0,
                    30900.0,
                    343000.0
                ],
                "question_answer_similarity": [
                    2.259939356474206,
                    0.0,
                    0.4465563034755178
                ],
                "word_count_noun_chunks": [
                    19.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    8.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    899000.0,
                    1610000.0,
                    2000000.0
                ],
                "result_count": [
                    4190000.0,
                    30300.0,
                    401000.0
                ],
                "word_count_appended_bing": [
                    45.0,
                    29.0,
                    29.0
                ]
            },
            "integer_answers": {
                "talmud": 3,
                "ketuvim": 1,
                "torah": 10
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "How many of the three Baltic countries border Russia?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "three"
            ],
            "lines": [
                [
                    0.6777777777777778,
                    0.3405275779376499,
                    0.45030505841992374,
                    0.35343114781032764,
                    0.3030769230769231,
                    0.45617977528089887,
                    0.8457943925233645,
                    0.668693009118541,
                    0.02857142857142857,
                    0.47003154574132494,
                    0.35911602209944754,
                    0.35390489430746863,
                    0.6831168831168831,
                    0.6229508196721312,
                    5.0
                ],
                [
                    0.18055555555555555,
                    0.3333333333333333,
                    0.39565996603106396,
                    0.3613009743884807,
                    0.46,
                    0.10224719101123596,
                    0.11401869158878504,
                    0.225531914893617,
                    0.2357142857142857,
                    0.231335436382755,
                    0.20994475138121546,
                    0.33386390886245376,
                    0.015584415584415584,
                    0.00819672131147541,
                    5.0
                ],
                [
                    0.14166666666666666,
                    0.32613908872901676,
                    0.15403497554901227,
                    0.28526787780119167,
                    0.23692307692307693,
                    0.44157303370786516,
                    0.04018691588785047,
                    0.10577507598784194,
                    0.7357142857142858,
                    0.29863301787592006,
                    0.430939226519337,
                    0.31223119683007766,
                    0.3012987012987013,
                    0.36885245901639346,
                    5.0
                ]
            ],
            "fraction_answers": {
                "none": 0.29851682846480265,
                "three": 0.4723912325324351,
                "two": 0.2290919390027623
            },
            "question": "how many of the three baltic countries border russia?",
            "rate_limited": false,
            "answers": [
                "three",
                "two",
                "none"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "none": 0.17656135378582166,
                "three": 0.7381635946806018,
                "two": 0.1386403838727408
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 5
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.4156195772298745,
                    1.335455635449815,
                    1.2489247873203106
                ],
                "result_count_important_words": [
                    9050000.0,
                    1220000.0,
                    430000.0
                ],
                "wikipedia_search": [
                    0.05714285714285714,
                    0.4714285714285714,
                    1.4714285714285715
                ],
                "word_count_appended_bing": [
                    65.0,
                    38.0,
                    78.0
                ],
                "answer_relation_to_question_bing": [
                    0.6810551558752997,
                    0.6666666666666666,
                    0.6522781774580335
                ],
                "cosine_similarity_raw": [
                    0.10257619619369507,
                    0.09012844413518906,
                    0.03508803993463516
                ],
                "result_count_noun_chunks": [
                    11000000.0,
                    3710000.0,
                    1740000.0
                ],
                "question_answer_similarity": [
                    3.8972064778208733,
                    3.983985301107168,
                    3.1455852948129177
                ],
                "word_count_noun_chunks": [
                    263.0,
                    6.0,
                    116.0
                ],
                "word_count_raw": [
                    76.0,
                    1.0,
                    45.0
                ],
                "result_count_bing": [
                    812000.0,
                    182000.0,
                    786000.0
                ],
                "word_count_appended": [
                    447.0,
                    220.0,
                    284.0
                ],
                "answer_relation_to_question": [
                    1.3555555555555556,
                    0.3611111111111111,
                    0.2833333333333333
                ],
                "result_count": [
                    1970000.0,
                    2990000.0,
                    1540000.0
                ]
            },
            "integer_answers": {
                "none": 2,
                "three": 10,
                "two": 2
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which NBA franchise has NOT retired any jersey numbers?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "brooklyn nets"
            ],
            "lines": [
                [
                    0.3988002256356852,
                    0.2786220871327254,
                    0.3922072191899955,
                    0.3569612677629822,
                    0.32857142857142857,
                    0.40325077399380804,
                    0.33033749082905356,
                    0.2982791586998088,
                    0.36394707455711417,
                    0.2844036697247706,
                    0.1875,
                    0.32712740673249674,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.40045452573114637,
                    0.28115501519756836,
                    0.3854272766024958,
                    0.3592748944801212,
                    0.30952380952380953,
                    0.4117647058823529,
                    0.3314380044020543,
                    0.3173996175908222,
                    0.3047349146498657,
                    0.33256880733944955,
                    0.359375,
                    0.3376838640189753,
                    0.125,
                    0.5,
                    -1.0
                ],
                [
                    0.20074524863316845,
                    0.4402228976697062,
                    0.22236550420750867,
                    0.2837638377568966,
                    0.3619047619047619,
                    0.18498452012383904,
                    0.33822450476889215,
                    0.38432122370936905,
                    0.33131801079302015,
                    0.3830275229357798,
                    0.453125,
                    0.3351887292485279,
                    0.375,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "dallas mavericks": 0.3205999377973341,
                "brooklyn nets": 0.29285602816716155,
                "los angeles clippers": 0.3865440340355043
            },
            "question": "which nba franchise has not retired any jersey numbers?",
            "rate_limited": false,
            "answers": [
                "brooklyn nets",
                "dallas mavericks",
                "los angeles clippers"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "dallas mavericks": 0.1590588820824112,
                "brooklyn nets": 0.36730754930158915,
                "los angeles clippers": 0.24298712584071994
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7287259326750326,
                    1.6231613598102466,
                    1.648112707514721
                ],
                "result_count_important_words": [
                    925000.0,
                    919000.0,
                    882000.0
                ],
                "wikipedia_search": [
                    1.360529254428858,
                    1.9526508535013432,
                    1.6868198920697985
                ],
                "answer_relation_to_question": [
                    0.8095981949145188,
                    0.7963637941508288,
                    2.3940380109346524
                ],
                "answer_relation_to_question_bing": [
                    1.3282674772036474,
                    1.3130699088145896,
                    0.3586626139817629
                ],
                "word_count_appended": [
                    94.0,
                    73.0,
                    51.0
                ],
                "cosine_similarity_raw": [
                    0.0395403727889061,
                    0.04202738031744957,
                    0.10184143483638763
                ],
                "result_count_noun_chunks": [
                    2110000.0,
                    1910000.0,
                    1210000.0
                ],
                "question_answer_similarity": [
                    2.967781642335467,
                    2.9197782883420587,
                    4.486489097587764
                ],
                "word_count_noun_chunks": [
                    0.0,
                    3.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    2.0
                ],
                "result_count_bing": [
                    1250000.0,
                    1140000.0,
                    4070000.0
                ],
                "result_count": [
                    144000.0,
                    160000.0,
                    116000.0
                ],
                "word_count_appended_bing": [
                    20.0,
                    9.0,
                    3.0
                ]
            },
            "integer_answers": {
                "dallas mavericks": 3,
                "brooklyn nets": 6,
                "los angeles clippers": 5
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Tom from MySpace shares his name with a key character in what film franchise?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "harry potter"
            ],
            "lines": [
                [
                    0.2873179181709988,
                    0.418085051827605,
                    0.16164225277470645,
                    0.2408913770840809,
                    0.5312053358742258,
                    0.336,
                    0.22588099364529174,
                    0.1931407942238267,
                    0.2508146878989451,
                    0.36403508771929827,
                    0.34065934065934067,
                    0.34167784027889414,
                    1.0,
                    0,
                    1.0
                ],
                [
                    0.3120748288046866,
                    0.24866664848154207,
                    0.39441402399603226,
                    0.37857666352058394,
                    0.1924726060028585,
                    0.456,
                    0.3437319468515309,
                    0.38086642599277976,
                    0.2851105287912383,
                    0.18421052631578946,
                    0.37362637362637363,
                    0.32686977602922634,
                    0.0,
                    0,
                    1.0
                ],
                [
                    0.4006072530243146,
                    0.33324829969085284,
                    0.4439437232292613,
                    0.38053195939533513,
                    0.27632205812291566,
                    0.208,
                    0.43038705950317735,
                    0.4259927797833935,
                    0.46407478330981655,
                    0.4517543859649123,
                    0.2857142857142857,
                    0.3314523836918795,
                    0.0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "the godfather": 0.29820156526251096,
                "the matrix": 0.3409253054946265,
                "harry potter": 0.3608731292428626
            },
            "question": "tom from myspace shares his name with a key character in what film franchise?",
            "rate_limited": false,
            "answers": [
                "harry potter",
                "the godfather",
                "the matrix"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "the godfather": 0.10813029274791792,
                "the matrix": 0.29867154909798527,
                "harry potter": 0.5119906729449544
            },
            "integer_answers": {
                "the godfather": 2,
                "the matrix": 7,
                "harry potter": 4
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.391744881952259,
                    2.2880884322045842,
                    2.3201666858431564
                ],
                "result_count_important_words": [
                    782000.0,
                    1190000.0,
                    1490000.0
                ],
                "wikipedia_search": [
                    1.2540734394947255,
                    1.4255526439561916,
                    2.320373916549083
                ],
                "word_count_appended_bing": [
                    31.0,
                    34.0,
                    26.0
                ],
                "answer_relation_to_question_bing": [
                    2.5085103109656304,
                    1.4919998908892527,
                    1.9994897981451174
                ],
                "cosine_similarity_raw": [
                    0.017140304669737816,
                    0.04182307794690132,
                    0.04707513377070427
                ],
                "result_count_noun_chunks": [
                    1070000.0,
                    2110000.0,
                    2360000.0
                ],
                "question_answer_similarity": [
                    5.276467658113688,
                    8.292316418141127,
                    8.335145080462098
                ],
                "word_count_noun_chunks": [
                    3.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    168000.0,
                    228000.0,
                    104000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    2230000.0,
                    808000.0,
                    1160000.0
                ],
                "answer_relation_to_question": [
                    1.7239075090259925,
                    1.8724489728281197,
                    2.4036435181458877
                ],
                "word_count_appended": [
                    83.0,
                    42.0,
                    103.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Before a performance, an orchestra usually tunes to what instrument?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "oboe"
            ],
            "lines": [
                [
                    0.38404422192465026,
                    0.3092074592074592,
                    0.35565325206439746,
                    0.2088085995985237,
                    0.40606060606060607,
                    0.36014344262295084,
                    0.40674466620784583,
                    0.3897849462365591,
                    0.23219795418965544,
                    0.4823386114494519,
                    0.5963302752293578,
                    0.3503595825545981,
                    0.9333333333333333,
                    0.92,
                    1.0
                ],
                [
                    0.28023580694736366,
                    0.41689976689976693,
                    0.3150141673765744,
                    0.6277283574065281,
                    0.27575757575757576,
                    0.32838114754098363,
                    0.19545767377838955,
                    0.27923387096774194,
                    0.41594850516012344,
                    0.10475030450669914,
                    0.11009174311926606,
                    0.3320536385273092,
                    0.0,
                    0.04,
                    1.0
                ],
                [
                    0.33571997112798607,
                    0.2738927738927739,
                    0.32933258055902814,
                    0.16346304299494818,
                    0.3181818181818182,
                    0.3114754098360656,
                    0.3977976600137646,
                    0.33098118279569894,
                    0.3518535406502211,
                    0.41291108404384896,
                    0.29357798165137616,
                    0.3175867789180928,
                    0.06666666666666667,
                    0.04,
                    1.0
                ]
            ],
            "fraction_answers": {
                "french horn": 0.26582518271345157,
                "bassoon": 0.2816743208094492,
                "oboe": 0.45250049647709917
            },
            "question": "before a performance, an orchestra usually tunes to what instrument?",
            "rate_limited": false,
            "answers": [
                "oboe",
                "french horn",
                "bassoon"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "french horn": 0.17907688006701095,
                "bassoon": 0.24094556524703542,
                "oboe": 0.6744027467711207
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7517979127729904,
                    1.660268192636546,
                    1.587933894590464
                ],
                "result_count_important_words": [
                    591000.0,
                    284000.0,
                    578000.0
                ],
                "wikipedia_search": [
                    0.9287918167586218,
                    1.6637940206404938,
                    1.4074141626008845
                ],
                "word_count_appended_bing": [
                    65.0,
                    12.0,
                    32.0
                ],
                "answer_relation_to_question_bing": [
                    1.2368298368298367,
                    1.6675990675990677,
                    1.0955710955710956
                ],
                "cosine_similarity_raw": [
                    0.11880962550640106,
                    0.10523372143507004,
                    0.11001693457365036
                ],
                "result_count_noun_chunks": [
                    1160000.0,
                    831000.0,
                    985000.0
                ],
                "question_answer_similarity": [
                    1.648685345891863,
                    4.956340620294213,
                    1.2906514583155513
                ],
                "word_count_noun_chunks": [
                    14.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    23.0,
                    1.0,
                    1.0
                ],
                "result_count_bing": [
                    703000.0,
                    641000.0,
                    608000.0
                ],
                "word_count_appended": [
                    396.0,
                    86.0,
                    339.0
                ],
                "answer_relation_to_question": [
                    1.536176887698601,
                    1.1209432277894547,
                    1.3428798845119443
                ],
                "result_count": [
                    1340000.0,
                    910000.0,
                    1050000.0
                ]
            },
            "integer_answers": {
                "french horn": 3,
                "bassoon": 0,
                "oboe": 11
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "The \u201cCC:\u201d feature in email stands for what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "carbon copy"
            ],
            "lines": [
                [
                    0.14391459604225562,
                    0.1906204906204906,
                    0.028534451165466528,
                    0.34693977848371177,
                    0.0032126187894421358,
                    0.29902912621359223,
                    0.00015876223190832204,
                    0.33761160714285715,
                    0.2376543209876543,
                    0.1072961373390558,
                    0.08695652173913043,
                    0.17287084100894295,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.34229777314883697,
                    0.4658189033189033,
                    0.013388619966767231,
                    0.33172066386817906,
                    0.22225664581046223,
                    0.34951456310679613,
                    0.007577288341079006,
                    0.5245535714285714,
                    0.024691358024691357,
                    0.18454935622317598,
                    0.08695652173913043,
                    0.2839827141751924,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.5137876308089074,
                    0.34356060606060607,
                    0.9580769288677662,
                    0.3213395576481092,
                    0.7745307354000956,
                    0.35145631067961164,
                    0.9922639494270127,
                    0.13783482142857142,
                    0.7376543209876543,
                    0.7081545064377682,
                    0.8260869565217391,
                    0.5431464448158647,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "carbon copy": 0.6577066263631219,
                "copy chain": 0.13962851798317913,
                "copy contacts": 0.20266485565369893
            },
            "question": "the \u201ccc:\u201d feature in email stands for what?",
            "rate_limited": false,
            "answers": [
                "copy chain",
                "copy contacts",
                "carbon copy"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "carbon copy": 0.7906221929744528,
                "copy chain": 0.09449297153440057,
                "copy contacts": 0.12010909070783748
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.6914833640357718,
                    1.1359308567007695,
                    2.172585779263459
                ],
                "result_count_important_words": [
                    264.0,
                    12600.0,
                    1650000.0
                ],
                "wikipedia_search": [
                    0.7129629629629629,
                    0.07407407407407407,
                    2.212962962962963
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    19.0
                ],
                "answer_relation_to_question_bing": [
                    0.5718614718614718,
                    1.39745670995671,
                    1.0306818181818183
                ],
                "cosine_similarity_raw": [
                    0.018460551276803017,
                    0.008661855943500996,
                    0.6198341846466064
                ],
                "result_count_noun_chunks": [
                    1210000.0,
                    1880000.0,
                    494000.0
                ],
                "question_answer_similarity": [
                    5.126299988478422,
                    4.901425955817103,
                    4.748037189245224
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    27.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    33.0
                ],
                "result_count_bing": [
                    15400000.0,
                    18000000.0,
                    18100000.0
                ],
                "word_count_appended": [
                    25.0,
                    43.0,
                    165.0
                ],
                "answer_relation_to_question": [
                    0.43174378812676684,
                    1.0268933194465109,
                    1.5413628924267222
                ],
                "result_count": [
                    477.0,
                    33000.0,
                    115000.0
                ]
            },
            "integer_answers": {
                "carbon copy": 11,
                "copy chain": 1,
                "copy contacts": 2
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these foods is cultivated in a paddy?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "rice"
            ],
            "lines": [
                [
                    0.6470588235294118,
                    0.0,
                    0.9198372258277894,
                    0.5513478834851779,
                    0.40529184882736236,
                    0.7700857415052398,
                    0.8946813930006701,
                    0.4629239035266585,
                    1.0,
                    0.6532951289398281,
                    0.6945454545454546,
                    0.46776445740156253,
                    0.9299363057324841,
                    1.0,
                    -1.0
                ],
                [
                    0.11764705882352941,
                    0.0,
                    0.021196873105935102,
                    0.44865211651482206,
                    0.5946995972743206,
                    0.0219117180057161,
                    0.10529243604701764,
                    0.5370718880742186,
                    0.0,
                    0.2979942693409742,
                    0.1709090909090909,
                    0.44364632348148075,
                    0.07006369426751592,
                    0.0,
                    -1.0
                ],
                [
                    0.23529411764705882,
                    1.0,
                    0.05896590106627548,
                    0.0,
                    8.553898316959406e-06,
                    0.20800254048904415,
                    2.61709523122645e-05,
                    4.2083991229696226e-06,
                    0.0,
                    0.04871060171919771,
                    0.13454545454545455,
                    0.08858921911695676,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "cake": 0.20207750470318725,
                "rice": 0.6711977261658314,
                "dunkaroos": 0.12672476913098138
            },
            "question": "which of these foods is cultivated in a paddy?",
            "rate_limited": false,
            "answers": [
                "rice",
                "cake",
                "dunkaroos"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "cake": 0.23317989565633634,
                "rice": 0.8232899199413797,
                "dunkaroos": 0.12307128860687358
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.4032933722046876,
                    1.3309389704444423,
                    0.26576765735087027
                ],
                "result_count_important_words": [
                    1470000.0,
                    173000.0,
                    43.0
                ],
                "wikipedia_search": [
                    3.0,
                    0.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    191.0,
                    47.0,
                    37.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "cosine_similarity_raw": [
                    0.5456728339195251,
                    0.012574570253491402,
                    0.03498020023107529
                ],
                "result_count_noun_chunks": [
                    4620000.0,
                    5360000.0,
                    42.0
                ],
                "question_answer_similarity": [
                    2.9255062341690063,
                    2.380592368543148,
                    0.0
                ],
                "word_count_noun_chunks": [
                    146.0,
                    11.0,
                    0.0
                ],
                "word_count_raw": [
                    55.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    4850000.0,
                    138000.0,
                    1310000.0
                ],
                "word_count_appended": [
                    684.0,
                    312.0,
                    51.0
                ],
                "answer_relation_to_question": [
                    1.2941176470588236,
                    0.23529411764705882,
                    0.47058823529411764
                ],
                "result_count": [
                    1990000.0,
                    2920000.0,
                    42.0
                ]
            },
            "integer_answers": {
                "cake": 2,
                "rice": 11,
                "dunkaroos": 1
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these phrases appears in a Shakespeare play?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "at a loss"
            ],
            "lines": [
                [
                    0.8322981366459627,
                    0.7529880478087648,
                    0.5050420399225889,
                    0.38083291225711197,
                    4.190300596927367e-05,
                    0.34015927189988626,
                    0.00011605795755849768,
                    7.962328925659623e-05,
                    0.813953488372093,
                    0.297029702970297,
                    0.3333333333333333,
                    0.33319558162198265,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.08695652173913043,
                    0.15670650730411687,
                    0.3892480458498548,
                    0.2622285306507105,
                    0.9999580969940307,
                    0.4732650739476678,
                    0.9998839420424415,
                    0.9999203767107434,
                    0.13953488372093023,
                    0.6633663366336634,
                    0.3333333333333333,
                    0.6047783541805014,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.08074534161490683,
                    0.09030544488711818,
                    0.10570991422755635,
                    0.35693855709217753,
                    0.0,
                    0.18657565415244595,
                    0.0,
                    0.0,
                    0.046511627906976744,
                    0.039603960396039604,
                    0.3333333333333333,
                    0.062026064197515905,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "at a loss": 0.5090983335922604,
                "in such a pickle": 0.3824225082570671,
                "up a dark creek": 0.10847915815067256
            },
            "question": "which of these phrases appears in a shakespeare play?",
            "rate_limited": false,
            "answers": [
                "in such a pickle",
                "at a loss",
                "up a dark creek"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "at a loss": 0.3959854503061485,
                "in such a pickle": 0.3252645019738909,
                "up a dark creek": 0.14130421857663164
            },
            "integer_answers": {
                "at a loss": 6,
                "in such a pickle": 6,
                "up a dark creek": 0
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3327823264879306,
                    2.4191134167220056,
                    0.24810425679006362
                ],
                "result_count_important_words": [
                    78.0,
                    672000.0,
                    0
                ],
                "wikipedia_search": [
                    0.813953488372093,
                    0.13953488372093023,
                    0.046511627906976744
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    2.2589641434262946,
                    0.4701195219123506,
                    0.27091633466135456
                ],
                "cosine_similarity_raw": [
                    0.11924837529659271,
                    0.09190759062767029,
                    0.024959774687886238
                ],
                "result_count_noun_chunks": [
                    86.0,
                    1080000.0,
                    0
                ],
                "question_answer_similarity": [
                    13.143948335200548,
                    9.050473706331104,
                    12.31926601473242
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    2990000.0,
                    4160000.0,
                    1640000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    44.0,
                    1050000.0,
                    0
                ],
                "answer_relation_to_question": [
                    3.329192546583851,
                    0.34782608695652173,
                    0.32298136645962733
                ],
                "word_count_appended": [
                    30.0,
                    67.0,
                    4.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "The '70s sitcom \u201cThree's Company\u201d was about three people who were what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "roommates"
            ],
            "lines": [
                [
                    0.26666666666666666,
                    0.3333333333333333,
                    0.3514713519660129,
                    0.12018798833433601,
                    0.9998579436801329,
                    0.4312796208530806,
                    0.9081290219028049,
                    0.9992956506427187,
                    0.28205128205128205,
                    0.9514563106796117,
                    0.8809523809523809,
                    0.7928649211502165,
                    1.0,
                    0,
                    1.0
                ],
                [
                    0.43703703703703706,
                    0.0,
                    0.4099379525524295,
                    0.6151645829190068,
                    0.0,
                    0.4834123222748815,
                    2.1119279579134998e-06,
                    0.0,
                    0.3333333333333333,
                    0.019417475728155338,
                    0.047619047619047616,
                    0.06437321271039383,
                    0.0,
                    0,
                    1.0
                ],
                [
                    0.2962962962962963,
                    0.6666666666666666,
                    0.23859069548155762,
                    0.2646474287466572,
                    0.0001420563198671227,
                    0.08530805687203792,
                    0.09186886616923723,
                    0.0007043493572812115,
                    0.3846153846153846,
                    0.02912621359223301,
                    0.07142857142857142,
                    0.1427618661393896,
                    0.0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "trivia show hosts": 0.18540746739248024,
                "roommates": 0.6398112670932753,
                "panda bears": 0.1747812655142446
            },
            "question": "the '70s sitcom \u201cthree's company\u201d was about three people who were what?",
            "rate_limited": false,
            "answers": [
                "roommates",
                "trivia show hosts",
                "panda bears"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "trivia show hosts": 0.2394318473248863,
                "roommates": 0.5112819522001698,
                "panda bears": 0.18469633983261868
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.3785947634506495,
                    0.1931196381311815,
                    0.4282855984181688
                ],
                "result_count_important_words": [
                    1720000.0,
                    4.0,
                    174000.0
                ],
                "wikipedia_search": [
                    0.8461538461538461,
                    1.0,
                    1.1538461538461537
                ],
                "word_count_appended_bing": [
                    37.0,
                    2.0,
                    3.0
                ],
                "answer_relation_to_question_bing": [
                    0.3333333333333333,
                    0.0,
                    0.6666666666666666
                ],
                "cosine_similarity_raw": [
                    0.06192227825522423,
                    0.07222293317317963,
                    0.042034946382045746
                ],
                "result_count_noun_chunks": [
                    90800.0,
                    0,
                    64.0
                ],
                "question_answer_similarity": [
                    2.478737026453018,
                    12.687051760964096,
                    5.458044432569295
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    45500000.0,
                    51000000.0,
                    9000000.0
                ],
                "word_count_appended": [
                    196.0,
                    4.0,
                    6.0
                ],
                "answer_relation_to_question": [
                    0.8,
                    1.3111111111111111,
                    0.8888888888888888
                ],
                "result_count": [
                    183000.0,
                    0,
                    26.0
                ]
            },
            "integer_answers": {
                "trivia show hosts": 4,
                "roommates": 7,
                "panda bears": 2
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "J.K. Rowling\u2019s first book published in England was titled \u201cHarry Potter and the\u201d what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "sorcerer's stone"
            ],
            "lines": [
                [
                    0.20043083462038314,
                    0.28083827941303374,
                    0.2600049163613552,
                    0.3477555735945452,
                    0.621681946003203,
                    0.3533424283765348,
                    0.6368526894609158,
                    0.5366876464136651,
                    0.07086374695863747,
                    0.4824561403508772,
                    0.6785714285714286,
                    0.4671201651467251,
                    0.6451612903225806,
                    0.7142857142857143,
                    1.0
                ],
                [
                    0.7086298498778153,
                    0.7032887047139504,
                    0.6112269453765661,
                    0.31221793421594035,
                    0.3783054438153365,
                    0.3656207366984993,
                    0.363127443384127,
                    0.46329446399812113,
                    0.8777372262773723,
                    0.49122807017543857,
                    0.25,
                    0.47074396160861287,
                    0.3548387096774194,
                    0.2857142857142857,
                    1.0
                ],
                [
                    0.09093931550180151,
                    0.015873015873015872,
                    0.12876813826207878,
                    0.3400264921895144,
                    1.2610181460511217e-05,
                    0.2810368349249659,
                    1.986715495718628e-05,
                    1.7889588213788835e-05,
                    0.05139902676399027,
                    0.02631578947368421,
                    0.07142857142857142,
                    0.06213587324466212,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "magician's stone": 0.07628381604192255,
                "philosopher's stone": 0.4497180571342571,
                "sorcerer's stone": 0.47399812682382036
            },
            "question": "j.k. rowling\u2019s first book published in england was titled \u201charry potter and the\u201d what?",
            "rate_limited": false,
            "answers": [
                "philosopher's stone",
                "sorcerer's stone",
                "magician's stone"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "magician's stone": 0.0396006449091319,
                "philosopher's stone": 0.32470827884231823,
                "sorcerer's stone": 0.892123312056763
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.7369613211738004,
                    3.7659516928689025,
                    0.4970869859572969
                ],
                "result_count_important_words": [
                    577000.0,
                    329000.0,
                    18.0
                ],
                "wikipedia_search": [
                    0.4251824817518248,
                    5.266423357664234,
                    0.3083941605839416
                ],
                "word_count_appended_bing": [
                    19.0,
                    7.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.6850296764782025,
                    4.219732228283703,
                    0.09523809523809523
                ],
                "cosine_similarity_raw": [
                    0.0696406364440918,
                    0.16371318697929382,
                    0.0344897136092186
                ],
                "result_count_noun_chunks": [
                    1170000.0,
                    1010000.0,
                    39.0
                ],
                "question_answer_similarity": [
                    11.789356105029583,
                    10.58458494511433,
                    11.527330418117344
                ],
                "word_count_noun_chunks": [
                    20.0,
                    11.0,
                    0.0
                ],
                "word_count_raw": [
                    10.0,
                    4.0,
                    0.0
                ],
                "result_count_bing": [
                    259000.0,
                    268000.0,
                    206000.0
                ],
                "result_count": [
                    493000.0,
                    300000.0,
                    10.0
                ],
                "answer_relation_to_question": [
                    1.603446676963065,
                    5.669038799022522,
                    0.7275145240144121
                ],
                "word_count_appended": [
                    110.0,
                    112.0,
                    6.0
                ]
            },
            "integer_answers": {
                "magician's stone": 0,
                "philosopher's stone": 7,
                "sorcerer's stone": 7
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "The government vehicle called Marine One is usually what kind of machine?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "helicopter"
            ],
            "lines": [
                [
                    0.31666666666666665,
                    0.3333333333333333,
                    0.2106461397030406,
                    0.3075435627728009,
                    0.07472843868603586,
                    0.3502262443438914,
                    0.2762589928057554,
                    0.24110218140068887,
                    0.3075547640765032,
                    0.24539877300613497,
                    0.2542372881355932,
                    0.328640037496878,
                    0,
                    0.04,
                    1.0
                ],
                [
                    0.2785714285714285,
                    0.26666666666666666,
                    0.6693239726641527,
                    0.4695321523393157,
                    0.7839288051302186,
                    0.3040723981900452,
                    0.42589928057553955,
                    0.505166475315729,
                    0.49782825869782393,
                    0.40245398773006136,
                    0.5,
                    0.3399283533596437,
                    0,
                    0.92,
                    1.0
                ],
                [
                    0.4047619047619048,
                    0.4,
                    0.12002988763280673,
                    0.22292428488788338,
                    0.1413427561837456,
                    0.34570135746606334,
                    0.29784172661870506,
                    0.2537313432835821,
                    0.1946169772256729,
                    0.3521472392638037,
                    0.2457627118644068,
                    0.3314316091434783,
                    0,
                    0.04,
                    1.0
                ]
            ],
            "fraction_answers": {
                "helicopter": 0.48949013686466347,
                "battleship": 0.25771475371785024,
                "limousine": 0.25279510941748634
            },
            "question": "the government vehicle called marine one is usually what kind of machine?",
            "rate_limited": false,
            "answers": [
                "limousine",
                "helicopter",
                "battleship"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "helicopter": 0.7117287636512574,
                "battleship": 0.16405797149988455,
                "limousine": 0.12102636375622179
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.3004802624781457,
                    2.379498473517506,
                    2.3200212640043483
                ],
                "result_count_important_words": [
                    576000.0,
                    888000.0,
                    621000.0
                ],
                "wikipedia_search": [
                    2.1528833485355223,
                    3.4847978108847677,
                    1.3623188405797102
                ],
                "answer_relation_to_question": [
                    1.9,
                    1.6714285714285713,
                    2.428571428571429
                ],
                "result_count": [
                    57100.0,
                    599000.0,
                    108000.0
                ],
                "answer_relation_to_question_bing": [
                    1.6666666666666665,
                    1.3333333333333333,
                    2.0
                ],
                "cosine_similarity_raw": [
                    0.07614126801490784,
                    0.24193738400936127,
                    0.043386638164520264
                ],
                "result_count_noun_chunks": [
                    630000.0,
                    1320000.0,
                    663000.0
                ],
                "question_answer_similarity": [
                    1.7929231487214565,
                    2.737287223339081,
                    1.2996081179007888
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    23.0,
                    1.0
                ],
                "result_count_bing": [
                    387000.0,
                    336000.0,
                    382000.0
                ],
                "word_count_appended": [
                    200.0,
                    328.0,
                    287.0
                ],
                "word_count_appended_bing": [
                    30.0,
                    59.0,
                    29.0
                ]
            },
            "integer_answers": {
                "helicopter": 10,
                "battleship": 2,
                "limousine": 1
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these companies purchased the music startup Napster?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "best buy"
            ],
            "question": "which of these companies purchased the music startup napster?",
            "lines": [
                [
                    0.3935810810810811,
                    0.6666666666666667,
                    0.5816710281436052,
                    0.6544640335897361,
                    0.04036517277802927,
                    0.36760752688172044,
                    0.06236192006426993,
                    0.056054355738898325,
                    0.4452661502966381,
                    0.20038910505836577,
                    0.18181818181818182,
                    0.3278753484583853,
                    0.6666666666666666,
                    0.3333333333333333,
                    -1.0
                ],
                [
                    0.17331081081081082,
                    0.16666666666666669,
                    0.2760441756123103,
                    0.15920971133029388,
                    0.05424777425682813,
                    0.5,
                    0.08405302269532035,
                    0.5435573889832565,
                    0.22778304218853,
                    0.3735408560311284,
                    0.4025974025974026,
                    0.31423428440102513,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.4331081081081081,
                    0.16666666666666669,
                    0.14228479624408452,
                    0.18632625507996994,
                    0.9053870529651425,
                    0.13239247311827956,
                    0.8535850572404097,
                    0.40038825527784516,
                    0.3269508075148319,
                    0.4260700389105058,
                    0.4155844155844156,
                    0.35789036714058947,
                    0.3333333333333333,
                    0.6666666666666666,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "best buy",
                "disney",
                "sony"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sony": 0.1656177400935703,
                "best buy": 0.6157055530869228,
                "disney": 0.049941091711540694
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6393767422919265,
                    1.5711714220051256,
                    1.7894518357029474
                ],
                "result_count_important_words": [
                    62100.0,
                    83700.0,
                    850000.0
                ],
                "wikipedia_search": [
                    1.7810646011865523,
                    0.91113216875412,
                    1.3078032300593276
                ],
                "answer_relation_to_question": [
                    1.5743243243243243,
                    0.6932432432432433,
                    1.7324324324324325
                ],
                "word_count_appended_bing": [
                    14.0,
                    31.0,
                    32.0
                ],
                "answer_relation_to_question_bing": [
                    1.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "cosine_similarity_raw": [
                    0.1567915380001068,
                    0.07440871000289917,
                    0.03835338354110718
                ],
                "result_count_noun_chunks": [
                    231000.0,
                    2240000.0,
                    1650000.0
                ],
                "question_answer_similarity": [
                    6.333364944206551,
                    1.5407007150352001,
                    1.8031123355031013
                ],
                "word_count_noun_chunks": [
                    4.0,
                    0.0,
                    2.0
                ],
                "result_count_bing": [
                    5470000.0,
                    7440000.0,
                    1970000.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    2.0
                ],
                "result_count": [
                    53500.0,
                    71900.0,
                    1200000.0
                ],
                "word_count_appended": [
                    103.0,
                    192.0,
                    219.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which animal is specifically mentioned in the Judeo-Christian Ten Commandments?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "pig"
            ],
            "lines": [
                [
                    0.2619047619047619,
                    0.36,
                    0.3530220464847878,
                    0.41750905911607317,
                    0.31910569105691056,
                    0.32359679266895763,
                    0.3409090909090909,
                    0.3857404021937843,
                    0.10144927536231885,
                    0.3446327683615819,
                    0.2028985507246377,
                    0.3262277248897058,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.08571428571428572,
                    0.48,
                    0.35243456079926094,
                    0.23465756019265496,
                    0.29471544715447157,
                    0.3201603665521191,
                    0.2892561983471074,
                    0.2870201096892139,
                    0.05434782608695652,
                    0.352165725047081,
                    0.6014492753623188,
                    0.34970747304717786,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.6523809523809524,
                    0.16,
                    0.2945433927159513,
                    0.3478333806912719,
                    0.3861788617886179,
                    0.35624284077892326,
                    0.36983471074380164,
                    0.3272394881170018,
                    0.8442028985507246,
                    0.3032015065913371,
                    0.1956521739130435,
                    0.32406480206311633,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "pig": 0.2669282974051865,
                "goat": 0.32581250059533867,
                "ox": 0.40725920199947485
            },
            "question": "which animal is specifically mentioned in the judeo-christian ten commandments?",
            "rate_limited": false,
            "answers": [
                "pig",
                "ox",
                "goat"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "pig": 0.5600560921501991,
                "goat": 0.15071485119430356,
                "ox": 0.5104700271270518
            },
            "integer_answers": {
                "pig": 3,
                "goat": 5,
                "ox": 6
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9573663493382347,
                    2.098244838283067,
                    1.944388812378698
                ],
                "result_count_important_words": [
                    16500.0,
                    14000.0,
                    17900.0
                ],
                "wikipedia_search": [
                    0.30434782608695654,
                    0.16304347826086957,
                    2.532608695652174
                ],
                "word_count_appended_bing": [
                    28.0,
                    83.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    0.36,
                    0.48,
                    0.16
                ],
                "cosine_similarity_raw": [
                    0.028892822563648224,
                    0.028844740241765976,
                    0.02410668134689331
                ],
                "result_count_noun_chunks": [
                    21100.0,
                    15700.0,
                    17900.0
                ],
                "question_answer_similarity": [
                    2.6946067265234888,
                    1.5144817251712084,
                    2.244919354096055
                ],
                "word_count_noun_chunks": [
                    0.0,
                    17.0,
                    0.0
                ],
                "result_count_bing": [
                    56500.0,
                    55900.0,
                    62200.0
                ],
                "word_count_raw": [
                    0.0,
                    10.0,
                    0.0
                ],
                "result_count": [
                    15700.0,
                    14500.0,
                    19000.0
                ],
                "answer_relation_to_question": [
                    0.5238095238095238,
                    0.17142857142857143,
                    1.3047619047619048
                ],
                "word_count_appended": [
                    183.0,
                    187.0,
                    161.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "How do you spell the last name of Duke University\u2019s men's basketball coach?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "krzyzewski"
            ],
            "lines": [
                [
                    0.34173705036432284,
                    0.4572553308280655,
                    0,
                    0,
                    0.0,
                    0.4026079869600652,
                    0.0,
                    0.0,
                    0.0,
                    0.04819277108433735,
                    0.27835051546391754,
                    0.06253498789080215,
                    0.0,
                    0.0,
                    5.0
                ],
                [
                    0.3124648081511719,
                    0.43057286991155325,
                    0,
                    0,
                    1.0,
                    0.32599837000814996,
                    1.0,
                    1.0,
                    1.0,
                    0.9036144578313253,
                    0.44329896907216493,
                    0.8749300242183956,
                    1.0,
                    1.0,
                    5.0
                ],
                [
                    0.3457981414845052,
                    0.11217179926038112,
                    0,
                    0,
                    0.0,
                    0.2713936430317848,
                    0.0,
                    0.0,
                    0.0,
                    0.04819277108433735,
                    0.27835051546391754,
                    0.06253498789080215,
                    0.0,
                    0.0,
                    5.0
                ]
            ],
            "fraction_answers": {
                "khzyrweski": 0.09320348818464402,
                "crzyzewski": 0.13255655354929255,
                "krzyzewski": 0.7742399582660634
            },
            "question": "how do you spell the last name of duke university\u2019s men's basketball coach?",
            "rate_limited": false,
            "answers": [
                "crzyzewski",
                "krzyzewski",
                "khzyrweski"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "khzyrweski": 0.15104784383726919,
                "crzyzewski": 0.15713699989156787,
                "krzyzewski": 0.6580514912745106
            },
            "integer_answers": {
                "khzyrweski": 1,
                "crzyzewski": 2,
                "krzyzewski": 9
            },
            "categorical_data": {
                "question_type": 5
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.37520992734481295,
                    5.249580145310374,
                    0.37520992734481295
                ],
                "result_count_important_words": [
                    0,
                    53100.0,
                    0
                ],
                "wikipedia_search": [
                    0.0,
                    5.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    27.0,
                    43.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    2.743531984968393,
                    2.5834372194693196,
                    0.6730307955622867
                ],
                "cosine_similarity_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    0,
                    16700.0,
                    0
                ],
                "question_answer_similarity": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    133.0,
                    0.0
                ],
                "result_count_bing": [
                    494000.0,
                    400000.0,
                    333000.0
                ],
                "word_count_raw": [
                    0.0,
                    32.0,
                    0.0
                ],
                "result_count": [
                    0,
                    55600.0,
                    0
                ],
                "answer_relation_to_question": [
                    2.050422302185937,
                    1.8747888489070315,
                    2.074788848907031
                ],
                "word_count_appended": [
                    16.0,
                    300.0,
                    16.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these consists of frozen water?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "snowflake"
            ],
            "lines": [
                [
                    0.7222222222222222,
                    0.0,
                    0.7353107130701382,
                    0.1960033110082587,
                    0.11791383219954649,
                    0.36045845272206306,
                    0.26,
                    0.24110671936758893,
                    0.6818181818181819,
                    0.532967032967033,
                    0.6413043478260869,
                    0.38903858605944897,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.05555555555555555,
                    1.0,
                    0.14354983334431318,
                    0.7430981882193818,
                    0.7709750566893424,
                    0.2636103151862464,
                    0.516,
                    0.541501976284585,
                    0.0,
                    0.041758241758241756,
                    0.06521739130434782,
                    0.27362943012434965,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.2222222222222222,
                    0.0,
                    0.1211394535855486,
                    0.06089850077235948,
                    0.1111111111111111,
                    0.37593123209169055,
                    0.224,
                    0.21739130434782608,
                    0.3181818181818182,
                    0.42527472527472526,
                    0.29347826086956524,
                    0.3373319838162014,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "garden rake": 0.3679079990388636,
                "snowflake": 0.4065119499383807,
                "drake": 0.22558005102275572
            },
            "question": "which of these consists of frozen water?",
            "rate_limited": false,
            "answers": [
                "snowflake",
                "garden rake",
                "drake"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "garden rake": 0.08317886723886271,
                "snowflake": 0.7531195666983306,
                "drake": 0.35435141355327204
            },
            "integer_answers": {
                "garden rake": 5,
                "snowflake": 6,
                "drake": 1
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.167115758178347,
                    0.820888290373049,
                    1.011995951448604
                ],
                "result_count_important_words": [
                    650000.0,
                    1290000.0,
                    560000.0
                ],
                "wikipedia_search": [
                    1.3636363636363638,
                    0.0,
                    0.6363636363636364
                ],
                "word_count_appended_bing": [
                    59.0,
                    6.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    1.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.09841964393854141,
                    0.019213814288377762,
                    0.016214236617088318
                ],
                "result_count_noun_chunks": [
                    1220000.0,
                    2740000.0,
                    1100000.0
                ],
                "question_answer_similarity": [
                    0.7907843180000782,
                    2.9980636090040207,
                    0.24569778516888618
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    6290000.0,
                    4600000.0,
                    6560000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    1560000.0,
                    10200000.0,
                    1470000.0
                ],
                "answer_relation_to_question": [
                    1.4444444444444444,
                    0.1111111111111111,
                    0.4444444444444444
                ],
                "word_count_appended": [
                    485.0,
                    38.0,
                    387.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which country did NOT have a native player selected in the first round of the 2016 NBA draft?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "haiti"
            ],
            "lines": [
                [
                    0.36319103313840156,
                    0.3084598018311802,
                    0.27469689704673467,
                    0.4334543957104662,
                    0.3183856502242153,
                    0.33333333333333337,
                    0.43118383060635224,
                    0.39890236857307915,
                    0.3753654432396138,
                    0.31894484412470026,
                    0.31868131868131866,
                    0.33801605371525345,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.2663255360623782,
                    0.35951649316442996,
                    0.39708848637703376,
                    0.38675200249967023,
                    0.33856502242152464,
                    0.33333333333333337,
                    0.3787295476419634,
                    0.4400154053533603,
                    0.2638855120163557,
                    0.34772182254196643,
                    0.3351648351648352,
                    0.33383562402814826,
                    0.5,
                    0,
                    -1.0
                ],
                [
                    0.3704834307992203,
                    0.3320237050043898,
                    0.32821461657623163,
                    0.1797936017898636,
                    0.3430493273542601,
                    0.33333333333333337,
                    0.19008662175168434,
                    0.16108222607356054,
                    0.36074904474403063,
                    0.33333333333333337,
                    0.34615384615384615,
                    0.3281483222565983,
                    0.5,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "bahamas": 0.2798563660607693,
                "brazil": 0.3682382447430228,
                "haiti": 0.35190538919620784
            },
            "question": "which country did not have a native player selected in the first round of the 2016 nba draft?",
            "rate_limited": false,
            "answers": [
                "haiti",
                "bahamas",
                "brazil"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "bahamas": 0.2177624825396658,
                "brazil": 0.205004757039626,
                "haiti": 0.30290258797200675
            },
            "integer_answers": {
                "bahamas": 2,
                "brazil": 4,
                "haiti": 7
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.5917431405559452,
                    2.6586300155496274,
                    2.7496268438944274
                ],
                "result_count_important_words": [
                    71500.0,
                    126000.0,
                    322000.0
                ],
                "wikipedia_search": [
                    1.9941529081661797,
                    3.77783180773831,
                    2.2280152840955103
                ],
                "word_count_appended_bing": [
                    33.0,
                    30.0,
                    28.0
                ],
                "answer_relation_to_question_bing": [
                    2.681562774363477,
                    1.9667690956979804,
                    2.3516681299385427
                ],
                "cosine_similarity_raw": [
                    0.045612871646881104,
                    0.020834553986787796,
                    0.03477814793586731
                ],
                "result_count_noun_chunks": [
                    105000.0,
                    62300.0,
                    352000.0
                ],
                "question_answer_similarity": [
                    0.46972580114379525,
                    0.7993842256255448,
                    2.260242559015751
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    505000.0,
                    505000.0,
                    505000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    151.0,
                    127.0,
                    139.0
                ],
                "answer_relation_to_question": [
                    1.3680896686159842,
                    2.336744639376218,
                    1.2951656920077972
                ],
                "result_count": [
                    81.0,
                    72.0,
                    70.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What word describes joining a cause just to feel good about it?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "slacktivism"
            ],
            "lines": [
                [
                    0.1752057142086846,
                    0.13846153846153847,
                    0.16635322114895737,
                    0.0,
                    0.0005052932549512577,
                    0.2769607843137255,
                    0.0004911591355599214,
                    0.0013767427430144848,
                    0.08739837398373984,
                    0.09349593495934959,
                    0.23728813559322035,
                    0.21209174367080944,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.1744757184832091,
                    0.05384615384615384,
                    0.1531088698477867,
                    1.415706857435261,
                    0.0,
                    0.3602941176470588,
                    0.0,
                    0.0,
                    0.2516759378120097,
                    0.04065040650406504,
                    0.23728813559322035,
                    0.0828787547782125,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.6503185673081062,
                    0.8076923076923077,
                    0.680537909003256,
                    -0.415706857435261,
                    0.9994947067450487,
                    0.3627450980392157,
                    0.99950884086444,
                    0.9986232572569855,
                    0.6609256882042505,
                    0.8658536585365854,
                    0.5254237288135594,
                    0.7050295015509781,
                    0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "slacktivism": 0.6800343389676518,
                "gung-faux": 0.21307115014976746,
                "joinerism": 0.10689451088258085
            },
            "question": "what word describes joining a cause just to feel good about it?",
            "rate_limited": false,
            "answers": [
                "joinerism",
                "gung-faux",
                "slacktivism"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "slacktivism": 0.7610576120966963,
                "gung-faux": 0.1489018555594606,
                "joinerism": 0.11159520436152207
            },
            "integer_answers": {
                "slacktivism": 12,
                "gung-faux": 1,
                "joinerism": 0
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2725504620248567,
                    0.49727252866927496,
                    4.230177009305868
                ],
                "result_count_important_words": [
                    40.0,
                    0,
                    81400.0
                ],
                "wikipedia_search": [
                    0.524390243902439,
                    1.5100556268720582,
                    3.9655541292255028
                ],
                "word_count_appended_bing": [
                    28.0,
                    28.0,
                    62.0
                ],
                "answer_relation_to_question_bing": [
                    0.6923076923076923,
                    0.2692307692307692,
                    4.038461538461538
                ],
                "cosine_similarity_raw": [
                    0.06262539327144623,
                    0.05763942003250122,
                    0.2561955451965332
                ],
                "result_count_noun_chunks": [
                    71.0,
                    0,
                    51500.0
                ],
                "question_answer_similarity": [
                    0.0,
                    4.151298344368115,
                    -1.2189834215678275
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    339000.0,
                    441000.0,
                    444000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    7.0
                ],
                "result_count": [
                    41.0,
                    0,
                    81100.0
                ],
                "answer_relation_to_question": [
                    1.0512342852521077,
                    1.0468543108992547,
                    3.9019114038486373
                ],
                "word_count_appended": [
                    46.0,
                    20.0,
                    426.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "In the musical \u201cHamilton,\u201d who is NOT in \u201cthe room where it happens\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "james monroe"
            ],
            "lines": [
                [
                    0.33585858585858586,
                    0.5,
                    0.31370265844843653,
                    0.3712344343241526,
                    0.2046850365908182,
                    0.24201114042802696,
                    0.4643020769700672,
                    0.3398752751283932,
                    0.23484848484848486,
                    0.3108108108108108,
                    0.38636363636363635,
                    0.3075543309695761,
                    0.0,
                    0,
                    0.0
                ],
                [
                    0.23232323232323232,
                    0.0,
                    0.2810204604732155,
                    0.33953331956766086,
                    0.4996431120628123,
                    0.4397537379067722,
                    0.06857055589492977,
                    0.3235509904622157,
                    0.5,
                    0.38175675675675674,
                    0.34090909090909094,
                    0.36959557520504815,
                    0.5,
                    0,
                    0.0
                ],
                [
                    0.4318181818181818,
                    0.5,
                    0.405276881078348,
                    0.28923224610818654,
                    0.2956718513463696,
                    0.31823512166520085,
                    0.467127367135003,
                    0.3365737344093911,
                    0.26515151515151514,
                    0.30743243243243246,
                    0.2727272727272727,
                    0.3228500938253757,
                    0.5,
                    0,
                    0.0
                ]
            ],
            "fraction_answers": {
                "james monroe": 0.3420527951443485,
                "james madison": 0.2750620465081112,
                "thomas jefferson": 0.3828851583475402
            },
            "question": "in the musical \u201chamilton,\u201d who is not in \u201cthe room where it happens\u201d?",
            "rate_limited": false,
            "answers": [
                "thomas jefferson",
                "james monroe",
                "james madison"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "james monroe": 0.4150240964445132,
                "james madison": 0.18283525257915206,
                "thomas jefferson": 0.12527385925854767
            },
            "integer_answers": {
                "james monroe": 5,
                "james madison": 3,
                "thomas jefferson": 5
            },
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.5395653522433912,
                    1.0432353983596148,
                    1.4171992493969943
                ],
                "result_count_important_words": [
                    935000.0,
                    11300000.0,
                    861000.0
                ],
                "wikipedia_search": [
                    1.0606060606060606,
                    0.0,
                    0.9393939393939393
                ],
                "word_count_appended_bing": [
                    5.0,
                    7.0,
                    10.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    1.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.05483996868133545,
                    0.06446056067943573,
                    0.02788345143198967
                ],
                "result_count_noun_chunks": [
                    873000.0,
                    962000.0,
                    891000.0
                ],
                "question_answer_similarity": [
                    2.805690234526992,
                    3.49643008899875,
                    4.5924469460733235
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    1760000.0,
                    411000.0,
                    1240000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    75300.0,
                    91.0,
                    52100.0
                ],
                "answer_relation_to_question": [
                    0.9848484848484849,
                    1.606060606060606,
                    0.4090909090909091
                ],
                "word_count_appended": [
                    56.0,
                    35.0,
                    57.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What dish is made with ham, poached eggs and Hollandaise sauce?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "eggs benedict"
            ],
            "lines": [
                [
                    0.04946236559139784,
                    0.044642857142857144,
                    0.00633794488508366,
                    0.3073641525418077,
                    0.005493259417916089,
                    0.25613305613305615,
                    0.005378306685162201,
                    0.0021115036342226013,
                    0.0,
                    0.16666666666666666,
                    0.09375,
                    0.28155435120400146,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.8976357871519162,
                    0.8273809523809524,
                    0.986840754993025,
                    0.5406156836881585,
                    0.9902986014867078,
                    0.41995841995842,
                    0.9564138132437761,
                    0.976570430827953,
                    0.8927489177489177,
                    0.7092198581560284,
                    0.8125,
                    0.45628028373204704,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.05290184725668595,
                    0.12797619047619047,
                    0.0068213001218913015,
                    0.15202016377003377,
                    0.004208139095376087,
                    0.3239085239085239,
                    0.03820788007106179,
                    0.021318065537824338,
                    0.10725108225108224,
                    0.12411347517730496,
                    0.09375,
                    0.26216536506395155,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "eggs benedict": 0.8190331073834215,
                "benedict cumberbatch": 0.09390300233785187,
                "pope benedict": 0.08706389027872652
            },
            "question": "what dish is made with ham, poached eggs and hollandaise sauce?",
            "rate_limited": false,
            "answers": [
                "pope benedict",
                "eggs benedict",
                "benedict cumberbatch"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "eggs benedict": 0.7906221929744528,
                "benedict cumberbatch": 0.1294401344098707,
                "pope benedict": 0.10686918892271112
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6893261072240087,
                    2.737681702392282,
                    1.572992190383709
                ],
                "result_count_important_words": [
                    2210.0,
                    393000.0,
                    15700.0
                ],
                "wikipedia_search": [
                    0.0,
                    5.356493506493507,
                    0.6435064935064935
                ],
                "word_count_appended_bing": [
                    3.0,
                    26.0,
                    3.0
                ],
                "answer_relation_to_question_bing": [
                    0.26785714285714285,
                    4.964285714285714,
                    0.7678571428571428
                ],
                "cosine_similarity_raw": [
                    0.005281316116452217,
                    0.8223198652267456,
                    0.0056840889155864716
                ],
                "result_count_noun_chunks": [
                    1040.0,
                    481000.0,
                    10500.0
                ],
                "question_answer_similarity": [
                    4.335982605989557,
                    7.626459304417949,
                    2.144546722236555
                ],
                "word_count_noun_chunks": [
                    0.0,
                    83.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    32.0,
                    0.0
                ],
                "result_count_bing": [
                    616000.0,
                    1010000.0,
                    779000.0
                ],
                "word_count_appended": [
                    47.0,
                    200.0,
                    35.0
                ],
                "answer_relation_to_question": [
                    0.2967741935483871,
                    5.385814722911498,
                    0.31741108354011577
                ],
                "result_count": [
                    2180.0,
                    393000.0,
                    1670.0
                ]
            },
            "integer_answers": {
                "eggs benedict": 14,
                "benedict cumberbatch": 0,
                "pope benedict": 0
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Though perhaps more famous as butter, which of these is a location in Florida?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "tillamook"
            ],
            "lines": [
                [
                    0.1946473617826361,
                    0.12241521918941274,
                    0.073705403597849,
                    -0.2536779518234636,
                    0.9468125235760091,
                    0.4972265806280524,
                    0.5302518788163457,
                    0.08858131487889273,
                    0.19230769230769232,
                    0.6111111111111112,
                    0.5,
                    0.38382895540917816,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.3702263331231168,
                    0.27832919768403636,
                    0.7634720499246703,
                    0.0,
                    0.027536778574122973,
                    0.4972265806280524,
                    0.4697040864255519,
                    0.6899653979238755,
                    0.5,
                    0.3888888888888889,
                    0.5,
                    0.31298282932712884,
                    1.0,
                    0,
                    -1.0
                ],
                [
                    0.4351263050942471,
                    0.5992555831265508,
                    0.16282254647748068,
                    1.2536779518234635,
                    0.025650697849867975,
                    0.005546838743895163,
                    4.403475810239549e-05,
                    0.22145328719723184,
                    0.3076923076923077,
                    0.0,
                    0.0,
                    0.303188215263693,
                    0.0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "tillamook": 0.29901616072874737,
                "kerrygold": 0.44602554942303413,
                "land o\u2019 lakes": 0.2549582898482185
            },
            "question": "though perhaps more famous as butter, which of these is a location in florida?",
            "rate_limited": false,
            "answers": [
                "tillamook",
                "kerrygold",
                "land o\u2019 lakes"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "tillamook": 0.40764433403719735,
                "kerrygold": 0.3599392076674976,
                "land o\u2019 lakes": 0.17470412104409289
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.5353158216367127,
                    1.2519313173085154,
                    1.212752861054772
                ],
                "result_count_important_words": [
                    867000.0,
                    768000.0,
                    72.0
                ],
                "wikipedia_search": [
                    0.38461538461538464,
                    1.0,
                    0.6153846153846154
                ],
                "word_count_appended_bing": [
                    39.0,
                    39.0,
                    0.0
                ],
                "answer_relation_to_question_bing": [
                    0.36724565756823824,
                    0.8349875930521091,
                    1.7977667493796525
                ],
                "cosine_similarity_raw": [
                    0.013325626030564308,
                    0.13803252577781677,
                    0.029437629505991936
                ],
                "result_count_noun_chunks": [
                    128000.0,
                    997000.0,
                    320000.0
                ],
                "question_answer_similarity": [
                    -1.7059812098741531,
                    0.0,
                    8.430969320237637
                ],
                "word_count_noun_chunks": [
                    0.0,
                    7.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    2250000.0,
                    2250000.0,
                    25100.0
                ],
                "word_count_appended": [
                    209.0,
                    133.0,
                    0.0
                ],
                "answer_relation_to_question": [
                    0.7785894471305445,
                    1.4809053324924673,
                    1.7405052203769884
                ],
                "result_count": [
                    2510.0,
                    73.0,
                    68.0
                ]
            },
            "integer_answers": {
                "tillamook": 6,
                "kerrygold": 4,
                "land o\u2019 lakes": 3
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these sharks is NOT a Lamniforme?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "hammerhead shark"
            ],
            "lines": [
                [
                    0.3455703883495146,
                    0.3695652173913043,
                    0.3603311769488582,
                    0.39621993430121516,
                    0.49098135126872516,
                    0.47912176724137934,
                    0.47888829502939606,
                    0.4821342054263566,
                    0.4612794612794613,
                    0.28661616161616166,
                    0.3805970149253731,
                    0.3304070930353294,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.247876213592233,
                    0.32608695652173914,
                    0.17910223178531676,
                    0.1984494618644107,
                    0.03531030265973706,
                    0.10668103448275862,
                    0.10288615713522181,
                    0.07000968992248063,
                    0.14814814814814814,
                    0.3017676767676768,
                    0.26865671641791045,
                    0.33262894379222485,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.4065533980582524,
                    0.30434782608695654,
                    0.46056659126582505,
                    0.40533060383437414,
                    0.4737083460715378,
                    0.4141971982758621,
                    0.4182255478353821,
                    0.44785610465116277,
                    0.39057239057239057,
                    0.4116161616161616,
                    0.35074626865671643,
                    0.33696396317244576,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "goblin shark": 0.18971465553115419,
                "great white shark": 0.6137327444850237,
                "hammerhead shark": 0.1965525999838221
            },
            "question": "which of these sharks is not a lamniforme?",
            "rate_limited": false,
            "answers": [
                "goblin shark",
                "great white shark",
                "hammerhead shark"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "goblin shark": 0.11986154376177419,
                "great white shark": 0.15369216210848952,
                "hammerhead shark": 0.29867154909798527
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.6783716278586824,
                    0.6694842248311006,
                    0.652144147310217
                ],
                "result_count_important_words": [
                    39500.0,
                    743000.0,
                    153000.0
                ],
                "wikipedia_search": [
                    0.07744107744107744,
                    0.7037037037037037,
                    0.21885521885521886
                ],
                "word_count_appended_bing": [
                    16.0,
                    31.0,
                    20.0
                ],
                "answer_relation_to_question_bing": [
                    0.2608695652173913,
                    0.34782608695652173,
                    0.391304347826087
                ],
                "cosine_similarity_raw": [
                    0.09988401085138321,
                    0.22948969900608063,
                    0.02820076048374176
                ],
                "result_count_noun_chunks": [
                    29500.0,
                    710000.0,
                    86100.0
                ],
                "question_answer_similarity": [
                    2.786467866972089,
                    8.096553795039654,
                    2.5418487512506545
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    155000.0,
                    2920000.0,
                    637000.0
                ],
                "word_count_appended": [
                    169.0,
                    157.0,
                    70.0
                ],
                "answer_relation_to_question": [
                    0.6177184466019418,
                    1.008495145631068,
                    0.3737864077669903
                ],
                "result_count": [
                    29500.0,
                    1520000.0,
                    86000.0
                ]
            },
            "integer_answers": {
                "goblin shark": 2,
                "great white shark": 9,
                "hammerhead shark": 1
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which person is most famous for being a children\u2019s book writer?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "dr. seuss"
            ],
            "lines": [
                [
                    0.3275591075591076,
                    0.19458389563652723,
                    0.06795113494960646,
                    0.21003198744933138,
                    0.11862857142857143,
                    0.3474320241691843,
                    0.3140916808149406,
                    0.18459877653832313,
                    0.43396549975696663,
                    0.28493150684931506,
                    0.4166666666666667,
                    0.2881767476202425,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.5948651348651348,
                    0.5449212775528565,
                    0.8841390889721493,
                    0.444350695785928,
                    0.7588571428571429,
                    0.3496978851963746,
                    0.4923599320882852,
                    0.6297229219143576,
                    0.3450155786116677,
                    0.3643835616438356,
                    0.4166666666666667,
                    0.4059557457839256,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.07757575757575758,
                    0.2604948268106163,
                    0.04790977607824424,
                    0.3456173167647406,
                    0.12251428571428571,
                    0.3028700906344411,
                    0.1935483870967742,
                    0.18567830154731918,
                    0.2210189216313656,
                    0.3506849315068493,
                    0.16666666666666666,
                    0.30586750659583195,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "tiger woods": 0.18431762633020662,
                "paris hilton": 0.2277583999599131,
                "dr. seuss": 0.5879239737098804
            },
            "question": "which person is most famous for being a children\u2019s book writer?",
            "rate_limited": false,
            "answers": [
                "paris hilton",
                "dr. seuss",
                "tiger woods"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "tiger woods": 0.15416646071314272,
                "paris hilton": 0.09362944612562255,
                "dr. seuss": 0.693692889839821
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.4408837381012125,
                    2.029778728919628,
                    1.5293375329791599
                ],
                "result_count_important_words": [
                    1850000.0,
                    2900000.0,
                    1140000.0
                ],
                "wikipedia_search": [
                    2.169827498784833,
                    1.7250778930583386,
                    1.105094608156828
                ],
                "word_count_appended_bing": [
                    30.0,
                    30.0,
                    12.0
                ],
                "answer_relation_to_question_bing": [
                    0.9729194781826361,
                    2.7246063877642825,
                    1.3024741340530814
                ],
                "cosine_similarity_raw": [
                    0.016191523522138596,
                    0.21067431569099426,
                    0.011416031047701836
                ],
                "result_count_noun_chunks": [
                    513000.0,
                    1750000.0,
                    516000.0
                ],
                "question_answer_similarity": [
                    2.633950650691986,
                    5.572474071756005,
                    4.334287207573652
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    2.0,
                    0.0
                ],
                "result_count_bing": [
                    46000000.0,
                    46300000.0,
                    40100000.0
                ],
                "word_count_appended": [
                    104.0,
                    133.0,
                    128.0
                ],
                "answer_relation_to_question": [
                    1.637795537795538,
                    2.9743256743256743,
                    0.3878787878787879
                ],
                "result_count": [
                    519000.0,
                    3320000.0,
                    536000.0
                ]
            },
            "integer_answers": {
                "tiger woods": 0,
                "paris hilton": 2,
                "dr. seuss": 12
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which Oscar-winning actress has NOT won the award for playing a real person?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "emma thompson"
            ],
            "lines": [
                [
                    0.34125077547282784,
                    0.3373940497487009,
                    0.3630508052358993,
                    0.2218545548243634,
                    0.33223374175306314,
                    0.4207161125319693,
                    0.33017241379310347,
                    0.3409893992932862,
                    0.1568236451048951,
                    0.28805970149253735,
                    0.3717948717948718,
                    0.3441857087565661,
                    0.5,
                    0,
                    -1.0
                ],
                [
                    0.3624155682640805,
                    0.3554135786693926,
                    0.27741659906780924,
                    0.50371470938339,
                    0.28416588124410935,
                    0.14897698209718668,
                    0.3172413793103448,
                    0.2676678445229682,
                    0.39623397435897434,
                    0.3537313432835821,
                    0.34615384615384615,
                    0.3318666357784548,
                    0.5,
                    0,
                    -1.0
                ],
                [
                    0.29633365626309177,
                    0.3071923715819065,
                    0.3595325956962915,
                    0.27443073579224664,
                    0.3836003770028275,
                    0.430306905370844,
                    0.35258620689655173,
                    0.3913427561837456,
                    0.44694238053613056,
                    0.35820895522388063,
                    0.28205128205128205,
                    0.32394765546497906,
                    0.0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "hilary swank": 0.3528498649132649,
                "emma thompson": 0.3309960338766025,
                "susan sarandon": 0.31615410121013254
            },
            "question": "which oscar-winning actress has not won the award for playing a real person?",
            "rate_limited": false,
            "answers": [
                "emma thompson",
                "susan sarandon",
                "hilary swank"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "hilary swank": 0.0661150920366537,
                "emma thompson": 0.23800605278106207,
                "susan sarandon": 0.18496430264656322
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.493028659894942,
                    2.690133827544723,
                    2.816837512560335
                ],
                "result_count_important_words": [
                    1970000.0,
                    2120000.0,
                    1710000.0
                ],
                "wikipedia_search": [
                    5.490821678321678,
                    1.6602564102564104,
                    0.8489219114219113
                ],
                "answer_relation_to_question": [
                    2.5399875924347555,
                    2.2013509077747124,
                    3.2586614997905317
                ],
                "answer_relation_to_question_bing": [
                    2.6016952040207855,
                    2.313382741289718,
                    3.0849220546894967
                ],
                "word_count_appended": [
                    142.0,
                    98.0,
                    95.0
                ],
                "cosine_similarity_raw": [
                    0.03219468146562576,
                    0.052325986325740814,
                    0.03302175924181938
                ],
                "result_count_noun_chunks": [
                    360000.0,
                    526000.0,
                    246000.0
                ],
                "question_answer_similarity": [
                    2.248649863333412,
                    -0.03003134112805128,
                    1.823600939475
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    1240000.0,
                    5490000.0,
                    1090000.0
                ],
                "result_count": [
                    356000.0,
                    458000.0,
                    247000.0
                ],
                "word_count_appended_bing": [
                    10.0,
                    12.0,
                    17.0
                ]
            },
            "integer_answers": {
                "hilary swank": 5,
                "emma thompson": 3,
                "susan sarandon": 5
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "According to Alexa, which of these is NOT one the top five most popular sports sites?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "yahoo sports"
            ],
            "lines": [
                [
                    0.3687467997951869,
                    0.24922600619195046,
                    0.25912978148387644,
                    0.21925303677174135,
                    0.49990614007166,
                    0.3323952470293934,
                    0.4918394353771504,
                    0.4649870322341608,
                    0.29144171779141104,
                    0.4263005780346821,
                    0.46875,
                    0.3386173191211396,
                    0.375,
                    0,
                    -1.0
                ],
                [
                    0.24646057347670247,
                    0.2786377708978328,
                    0.40437095477272944,
                    0.22396468380442386,
                    0.17761155048439886,
                    0.33364602876798,
                    0.3892319756898495,
                    0.1785846609855502,
                    0.37903374233128834,
                    0.3916184971098266,
                    0.453125,
                    0.31804858789231055,
                    0.125,
                    0,
                    -1.0
                ],
                [
                    0.3847926267281106,
                    0.47213622291021673,
                    0.33649926374339406,
                    0.5567822794238347,
                    0.32248230944394113,
                    0.3339587242026266,
                    0.11892858893300007,
                    0.35642830678028903,
                    0.3295245398773006,
                    0.18208092485549132,
                    0.078125,
                    0.3433340929865498,
                    0.5,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "deadspin": 0.3361426338638838,
                "sports illustrated": 0.4001024575057089,
                "yahoo sports": 0.2637549086304073
            },
            "question": "according to alexa, which of these is not one the top five most popular sports sites?",
            "rate_limited": false,
            "answers": [
                "yahoo sports",
                "sports illustrated",
                "deadspin"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "deadspin": 0.4550972461882418,
                "sports illustrated": 0.21524611520079845,
                "yahoo sports": 0.5241355690191698
            },
            "integer_answers": {
                "deadspin": 3,
                "sports illustrated": 5,
                "yahoo sports": 5
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6138268087886036,
                    1.8195141210768944,
                    1.5666590701345018
                ],
                "result_count_important_words": [
                    66600.0,
                    904000.0,
                    3110000.0
                ],
                "wikipedia_search": [
                    2.0855828220858896,
                    1.2096625766871165,
                    1.7047546012269938
                ],
                "word_count_appended_bing": [
                    2.0,
                    3.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    0.5015479876160991,
                    0.44272445820433437,
                    0.05572755417956656
                ],
                "cosine_similarity_raw": [
                    0.09082356095314026,
                    0.036058299243450165,
                    0.061650291085243225
                ],
                "result_count_noun_chunks": [
                    37800.0,
                    347000.0,
                    155000.0
                ],
                "question_answer_similarity": [
                    9.299221996872802,
                    9.143157435304602,
                    -1.8808075990527868
                ],
                "word_count_noun_chunks": [
                    1.0,
                    3.0,
                    0.0
                ],
                "result_count_bing": [
                    53600000.0,
                    53200000.0,
                    53100000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    92.0,
                    316000.0,
                    174000.0
                ],
                "answer_relation_to_question": [
                    1.0500256016385048,
                    2.02831541218638,
                    0.9216589861751152
                ],
                "word_count_appended": [
                    51.0,
                    75.0,
                    220.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these knots is typically used to add another line to a rope?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "rolling hitch"
            ],
            "lines": [
                [
                    0.6117108255036325,
                    0.3578168743620065,
                    0.4428781242296326,
                    0.009768658477227432,
                    0.7838374917200265,
                    0.6419400855920114,
                    0.7796340493237868,
                    0.7500699692135461,
                    0.5455273736757995,
                    0.6012024048096193,
                    0.46875,
                    0.4470153920819559,
                    0.75,
                    0.0,
                    -1.0
                ],
                [
                    0.2954169573599923,
                    0.43842810170165314,
                    0.5349761125612279,
                    0.9902313415227726,
                    0.193751379995584,
                    0.19400855920114124,
                    0.1994166003712543,
                    0.21130702490904002,
                    0.39576091075341446,
                    0.14829659318637275,
                    0.109375,
                    0.3584511463430306,
                    0.25,
                    1.0,
                    -1.0
                ],
                [
                    0.09287221713637513,
                    0.20375502393634048,
                    0.022145763209139496,
                    0.0,
                    0.02241112828438949,
                    0.16405135520684735,
                    0.020949350304958897,
                    0.038623005877413935,
                    0.058711715570786034,
                    0.250501002004008,
                    0.421875,
                    0.19453346157501353,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "grantchester": 0.10645921593609088,
                "rolling hitch": 0.37995855199324885,
                "bowline": 0.5135822320706602
            },
            "question": "which of these knots is typically used to add another line to a rope?",
            "rate_limited": false,
            "answers": [
                "bowline",
                "rolling hitch",
                "grantchester"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "grantchester": 0.15871385009139574,
                "rolling hitch": 0.6348021194555852,
                "bowline": 0.5731660372494805
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.2350769604097795,
                    1.792255731715153,
                    0.9726673078750676
                ],
                "result_count_important_words": [
                    147000.0,
                    37600.0,
                    3950.0
                ],
                "wikipedia_search": [
                    2.182109494703198,
                    1.5830436430136579,
                    0.23484686228314414
                ],
                "word_count_appended_bing": [
                    30.0,
                    7.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    1.431267497448026,
                    1.7537124068066126,
                    0.8150200957453619
                ],
                "cosine_similarity_raw": [
                    0.1980799287557602,
                    0.23927131295204163,
                    0.009904826991260052
                ],
                "result_count_noun_chunks": [
                    268000.0,
                    75500.0,
                    13800.0
                ],
                "question_answer_similarity": [
                    0.07064885040745139,
                    7.161546908318996,
                    0.0
                ],
                "word_count_noun_chunks": [
                    24.0,
                    8.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    2.0,
                    0.0
                ],
                "result_count_bing": [
                    450000.0,
                    136000.0,
                    115000.0
                ],
                "result_count": [
                    142000.0,
                    35100.0,
                    4060.0
                ],
                "answer_relation_to_question": [
                    3.0585541275181627,
                    1.4770847867999615,
                    0.4643610856818756
                ],
                "word_count_appended": [
                    300.0,
                    74.0,
                    125.0
                ]
            },
            "integer_answers": {
                "grantchester": 0,
                "rolling hitch": 4,
                "bowline": 10
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "The best-selling book \u201cThe Chocolate War\u201d is about what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "high school conformity"
            ],
            "lines": [
                [
                    0.2050387596899225,
                    0.15732368896925858,
                    0.7818881618432012,
                    0.24969023030179077,
                    0.07894736842105263,
                    0.738837134596852,
                    1.1587351247378362e-05,
                    1.1995058036089131e-05,
                    0.061224489795918366,
                    0.3333333333333333,
                    0.16666666666666666,
                    0.26084726459507346,
                    0,
                    0,
                    1.0
                ],
                [
                    0.1482388140894873,
                    0.18762671927228888,
                    0.05388397013968947,
                    0.1743281322598069,
                    0.9210526315789473,
                    0.1429489238676518,
                    0.14638687075854664,
                    0.12434876830745732,
                    0.2571428571428571,
                    0.4666666666666667,
                    0.6666666666666666,
                    0.5732855470875434,
                    0,
                    0,
                    1.0
                ],
                [
                    0.6467224262205902,
                    0.6550495917584526,
                    0.16422786801710929,
                    0.5759816374384024,
                    0.0,
                    0.1182139415354963,
                    0.853601541890206,
                    0.8756392366345066,
                    0.6816326530612244,
                    0.2,
                    0.16666666666666666,
                    0.1658671883173831,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "high school conformity": 0.25281839005186274,
                "the rise of hershey's": 0.42530022929500316,
                "sugar addiction": 0.32188138065313415
            },
            "question": "the best-selling book \u201cthe chocolate war\u201d is about what?",
            "rate_limited": false,
            "answers": [
                "high school conformity",
                "sugar addiction",
                "the rise of hershey's"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "high school conformity": 0.38608857016539566,
                "the rise of hershey's": 0.22551914790227076,
                "sugar addiction": 0.36232833223769456
            },
            "integer_answers": {
                "high school conformity": 2,
                "the rise of hershey's": 6,
                "sugar addiction": 4
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3042363229753673,
                    2.8664277354377172,
                    0.8293359415869155
                ],
                "result_count_important_words": [
                    3.0,
                    37900.0,
                    221000.0
                ],
                "wikipedia_search": [
                    0.30612244897959184,
                    1.2857142857142856,
                    3.4081632653061225
                ],
                "word_count_appended_bing": [
                    2.0,
                    8.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.47197106690777574,
                    0.5628801578168666,
                    1.9651487752753576
                ],
                "cosine_similarity_raw": [
                    0.31859180331230164,
                    0.021955814212560654,
                    0.0669170543551445
                ],
                "result_count_noun_chunks": [
                    3.0,
                    31100.0,
                    219000.0
                ],
                "question_answer_similarity": [
                    9.118160897865891,
                    6.366095930337906,
                    21.03363530896604
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    2300000.0,
                    445000.0,
                    368000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    3.0,
                    35.0,
                    0
                ],
                "answer_relation_to_question": [
                    1.0251937984496124,
                    0.7411940704474365,
                    3.233612131102951
                ],
                "word_count_appended": [
                    5.0,
                    7.0,
                    3.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these acclaimed authors did NOT write a book titled \u201cThe Great American Novel\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "william carlos williams"
            ],
            "question": "which of these acclaimed authors did not write a book titled \u201cthe great american novel\u201d?",
            "lines": [
                [
                    0.3021353807841731,
                    0.3052478270143444,
                    0.3470462634637338,
                    0.2960628941857492,
                    0.3806818181818182,
                    0.2824427480916031,
                    0.41304347826086957,
                    0.32876005776769135,
                    0.3333866805397055,
                    0.4661016949152542,
                    0.39285714285714285,
                    0.37476505909184066,
                    0,
                    0.5,
                    -1.0
                ],
                [
                    0.33665727008655755,
                    0.30575441262731295,
                    0.19951758657575824,
                    0.4531885753222268,
                    0.22159090909090912,
                    0.36259541984732824,
                    0.19182990922121357,
                    0.421910460078399,
                    0.2848027861817898,
                    0.19915254237288138,
                    0.25,
                    0.2165592757036514,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.3612073491292693,
                    0.38899776035834266,
                    0.45343614996050796,
                    0.250748530492024,
                    0.3977272727272727,
                    0.3549618320610687,
                    0.39512661251791686,
                    0.24932948215390965,
                    0.38181053327850484,
                    0.3347457627118644,
                    0.35714285714285715,
                    0.40867566520450793,
                    0,
                    0.5,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "e.l. doctorow",
                "philip roth",
                "william carlos williams"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "philip roth": 0.10396579408566502,
                "william carlos williams": 0.6077323995110641,
                "e.l. doctorow": 0.3500942954648251
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.003759054530549,
                    4.535051588741577,
                    1.4611893567278735
                ],
                "result_count_important_words": [
                    36400.0,
                    129000.0,
                    43900.0
                ],
                "wikipedia_search": [
                    2.6658131113647126,
                    3.4431554210913644,
                    1.891031467543923
                ],
                "word_count_appended_bing": [
                    12.0,
                    28.0,
                    16.0
                ],
                "answer_relation_to_question_bing": [
                    2.3370260758278674,
                    2.330947048472244,
                    1.3320268756998879
                ],
                "cosine_similarity_raw": [
                    0.07458346337080002,
                    0.14652155339717865,
                    0.02270551398396492
                ],
                "result_count_noun_chunks": [
                    166000.0,
                    75700.0,
                    243000.0
                ],
                "question_answer_similarity": [
                    3.31491569429636,
                    0.7609008949948475,
                    4.0514824646525085
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    228000.0,
                    144000.0,
                    152000.0
                ],
                "word_count_raw": [
                    0.0,
                    5.0,
                    0.0
                ],
                "word_count_appended": [
                    8.0,
                    71.0,
                    39.0
                ],
                "answer_relation_to_question": [
                    3.1658339074532296,
                    2.613483678615079,
                    2.2206824139316916
                ],
                "result_count": [
                    42.0,
                    98.0,
                    36.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these celebrities has NOT been a ProActiv spokesperson?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "selena gomez"
            ],
            "lines": [
                [
                    0.3142857142857143,
                    0.2272727272727273,
                    0.10102856448601472,
                    0.334347103790002,
                    0.2655172413793103,
                    0.4873367351715744,
                    0.25,
                    0.2655007949125596,
                    0.4431818181818182,
                    0.30603448275862066,
                    0.4154929577464789,
                    0.2931116930038157,
                    0.06,
                    0.0,
                    -1.0
                ],
                [
                    0.32857142857142857,
                    0.36363636363636365,
                    0.4705751268372039,
                    0.27968802117338293,
                    0.2755968169761273,
                    0.3122747062403446,
                    0.2906862745098039,
                    0.27556968733439324,
                    0.23958333333333331,
                    0.33836206896551724,
                    0.352112676056338,
                    0.34394921442642157,
                    0.46,
                    0.5,
                    -1.0
                ],
                [
                    0.35714285714285715,
                    0.40909090909090906,
                    0.42839630867678136,
                    0.3859648750366151,
                    0.4588859416445623,
                    0.20038855858808108,
                    0.45931372549019606,
                    0.45892951775304713,
                    0.3172348484848485,
                    0.3556034482758621,
                    0.23239436619718312,
                    0.36293909256976287,
                    0.48,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "selena gomez": 0.22767365014989918,
                "katy perry": 0.46241288100162337,
                "lindsay lohan": 0.30991346884847737
            },
            "question": "which of these celebrities has not been a proactiv spokesperson?",
            "rate_limited": false,
            "answers": [
                "katy perry",
                "lindsay lohan",
                "selena gomez"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "selena gomez": 0.45241984334141067,
                "katy perry": 0.01765123390995639,
                "lindsay lohan": 0.3651498854614794
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2413298419771062,
                    0.9363047134414708,
                    0.822365444581423
                ],
                "result_count_important_words": [
                    102000.0,
                    85400.0,
                    16600.0
                ],
                "wikipedia_search": [
                    0.34090909090909094,
                    1.5625,
                    1.0965909090909092
                ],
                "answer_relation_to_question": [
                    0.37142857142857144,
                    0.34285714285714286,
                    0.2857142857142857
                ],
                "answer_relation_to_question_bing": [
                    0.5454545454545454,
                    0.2727272727272727,
                    0.18181818181818182
                ],
                "word_count_appended": [
                    90.0,
                    75.0,
                    67.0
                ],
                "cosine_similarity_raw": [
                    0.4791058599948883,
                    0.035334933549165726,
                    0.08598547428846359
                ],
                "result_count_noun_chunks": [
                    88500.0,
                    84700.0,
                    15500.0
                ],
                "question_answer_similarity": [
                    1.3517169521655887,
                    1.7977315420284867,
                    0.9305192669853568
                ],
                "word_count_noun_chunks": [
                    22.0,
                    2.0,
                    1.0
                ],
                "word_count_raw": [
                    8.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    54100.0,
                    802000.0,
                    1280000.0
                ],
                "result_count": [
                    88400.0,
                    84600.0,
                    15500.0
                ],
                "word_count_appended_bing": [
                    12.0,
                    21.0,
                    38.0
                ]
            },
            "integer_answers": {
                "selena gomez": 2,
                "katy perry": 10,
                "lindsay lohan": 2
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which actor currently stars in a show that is both one word long and pluralized?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "paul giamatti"
            ],
            "lines": [
                [
                    0.45467320261437905,
                    0.21904761904761905,
                    0.3344232318917145,
                    4.467772566598791,
                    0.09566326530612244,
                    0.3754556500607533,
                    0.010942087002935683,
                    0.20272410516312955,
                    0.30431887366818877,
                    0.3728813559322034,
                    0.34285714285714286,
                    0.3081369486488709,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.2644771241830065,
                    0.20476190476190478,
                    0.32898909592631104,
                    -3.6608373768337805,
                    0.09693877551020408,
                    0.16281895504252733,
                    0.010942087002935683,
                    0.4846373139056066,
                    0.33656773211567736,
                    0.3389830508474576,
                    0.18571428571428572,
                    0.2634938804171268,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.28084967320261434,
                    0.5761904761904763,
                    0.33658767218197444,
                    0.19306481023498973,
                    0.8073979591836735,
                    0.4617253948967193,
                    0.9781158259941286,
                    0.31263858093126384,
                    0.359113394216134,
                    0.288135593220339,
                    0.4714285714285714,
                    0.4283691709340023,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "william h. macy": 0.6240746707326542,
                "taraji p. henson": -0.08187609761722807,
                "paul giamatti": 0.457801426884574
            },
            "question": "which actor currently stars in a show that is both one word long and pluralized?",
            "rate_limited": false,
            "answers": [
                "william h. macy",
                "taraji p. henson",
                "paul giamatti"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "william h. macy": 0.18252344438126572,
                "taraji p. henson": 0.1672060788906518,
                "paul giamatti": 0.3069989951990106
            },
            "integer_answers": {
                "william h. macy": 3,
                "taraji p. henson": 1,
                "paul giamatti": 8
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.8488216918932254,
                    1.5809632825027609,
                    2.5702150256040137
                ],
                "result_count_important_words": [
                    82.0,
                    82.0,
                    7330.0
                ],
                "wikipedia_search": [
                    1.217275494672755,
                    1.3462709284627095,
                    1.436453576864536
                ],
                "word_count_appended_bing": [
                    24.0,
                    13.0,
                    33.0
                ],
                "answer_relation_to_question_bing": [
                    0.6571428571428571,
                    0.6142857142857143,
                    1.7285714285714286
                ],
                "cosine_similarity_raw": [
                    0.0314224436879158,
                    0.030911851674318314,
                    0.031625814735889435
                ],
                "result_count_noun_chunks": [
                    64000.0,
                    153000.0,
                    98700.0
                ],
                "question_answer_similarity": [
                    2.2620009689126164,
                    -1.8534555127844214,
                    0.0977473184466362
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    92700.0,
                    40200.0,
                    114000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    75.0,
                    76.0,
                    633.0
                ],
                "answer_relation_to_question": [
                    2.2733660130718953,
                    1.3223856209150326,
                    1.4042483660130718
                ],
                "word_count_appended": [
                    22.0,
                    20.0,
                    17.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "One symptom of argyria is turning roughly the same skin color as which cartoon character?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "garfield"
            ],
            "lines": [
                [
                    0.4483686735662153,
                    0.40217948717948715,
                    0.7601374835528155,
                    0.22209221161234527,
                    0.027777777777777776,
                    0.17083587553386212,
                    0.032809295967190705,
                    0.02903225806451613,
                    0.0347008547008547,
                    0.18309859154929578,
                    0.16666666666666666,
                    0.29962590534408073,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.13021456886689547,
                    0.19833333333333333,
                    0.10904072651314722,
                    0.7749534690347367,
                    0.011904761904761904,
                    0.13361805979255645,
                    0.017088174982911826,
                    0.01129032258064516,
                    0.06256410256410257,
                    0.08450704225352113,
                    0.05555555555555555,
                    0.20969485564807608,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.42141675756688923,
                    0.3994871794871795,
                    0.13082178993403726,
                    0.00295431935291795,
                    0.9603174603174603,
                    0.6955460646735815,
                    0.9501025290498974,
                    0.9596774193548387,
                    0.9027350427350427,
                    0.7323943661971831,
                    0.7777777777777778,
                    0.4906792390078431,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "garfield": 0.6186591621212207,
                "the grinch": 0.14989708108585362,
                "papa smurf": 0.23144375679292564
            },
            "question": "one symptom of argyria is turning roughly the same skin color as which cartoon character?",
            "rate_limited": false,
            "answers": [
                "papa smurf",
                "the grinch",
                "garfield"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "garfield": 0.4111419727148074,
                "the grinch": 0.14426135435173482,
                "papa smurf": 0.38158045794732526
            },
            "integer_answers": {
                "garfield": 8,
                "the grinch": 1,
                "papa smurf": 3
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.397007242752646,
                    1.6775588451846086,
                    3.925433912062745
                ],
                "result_count_important_words": [
                    48.0,
                    25.0,
                    1390.0
                ],
                "wikipedia_search": [
                    0.1735042735042735,
                    0.3128205128205128,
                    4.513675213675214
                ],
                "word_count_appended_bing": [
                    6.0,
                    2.0,
                    28.0
                ],
                "answer_relation_to_question_bing": [
                    2.010897435897436,
                    0.9916666666666667,
                    1.9974358974358974
                ],
                "cosine_similarity_raw": [
                    0.08494608104228973,
                    0.01218540407717228,
                    0.014619458466768265
                ],
                "result_count_noun_chunks": [
                    36.0,
                    14.0,
                    1190.0
                ],
                "question_answer_similarity": [
                    2.1387014188803732,
                    7.4626393773942254,
                    0.02844947576522827
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    5600.0,
                    4380.0,
                    22800.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    35.0,
                    15.0,
                    1210.0
                ],
                "answer_relation_to_question": [
                    2.6902120413972916,
                    0.7812874132013728,
                    2.5285005454013354
                ],
                "word_count_appended": [
                    13.0,
                    6.0,
                    52.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these Summer Olympics events uses the longest playing surface?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "badminton"
            ],
            "question": "which of these summer olympics events uses the longest playing surface?",
            "lines": [
                [
                    0.11805555555555554,
                    0.4455882352941176,
                    0.26059633152753403,
                    0.464416557118236,
                    0.17310087173100872,
                    0.26375321336760926,
                    0.1872085276482345,
                    0.160099076784508,
                    0.4777622377622378,
                    0.3458083832335329,
                    0.3113207547169811,
                    0.34784080148931784,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.5440705128205129,
                    0.3088235294117647,
                    0.4974631662898553,
                    0.3854009773893707,
                    0.5429638854296388,
                    0.45141388174807195,
                    0.5485232067510548,
                    0.5696915109209637,
                    0.3258741258741259,
                    0.36826347305389223,
                    0.41509433962264153,
                    0.3367202242608952,
                    1.0,
                    0,
                    -1.0
                ],
                [
                    0.33787393162393164,
                    0.24558823529411763,
                    0.24194050218261065,
                    0.15018246549239336,
                    0.28393524283935245,
                    0.2848329048843188,
                    0.26426826560071065,
                    0.2702094122945283,
                    0.19636363636363635,
                    0.28592814371257486,
                    0.27358490566037735,
                    0.3154389742497869,
                    0.0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "fencing",
                "badminton",
                "taekwondo"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "fencing": 0.3953169398288925,
                "badminton": 0.44288168632685504,
                "taekwondo": 0.07557485469802164
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.434885610425225,
                    2.3570415698262663,
                    2.208072819748508
                ],
                "result_count_important_words": [
                    84300.0,
                    247000.0,
                    119000.0
                ],
                "wikipedia_search": [
                    2.388811188811189,
                    1.6293706293706294,
                    0.9818181818181817
                ],
                "answer_relation_to_question": [
                    0.7083333333333333,
                    3.264423076923077,
                    2.02724358974359
                ],
                "result_count": [
                    69500.0,
                    218000.0,
                    114000.0
                ],
                "answer_relation_to_question_bing": [
                    1.7823529411764705,
                    1.2352941176470589,
                    0.9823529411764705
                ],
                "cosine_similarity_raw": [
                    0.03996945545077324,
                    0.07629935443401337,
                    0.037108082324266434
                ],
                "result_count_noun_chunks": [
                    71100.0,
                    253000.0,
                    120000.0
                ],
                "question_answer_similarity": [
                    2.037695363163948,
                    1.6910029854625463,
                    0.6589474661741406
                ],
                "word_count_noun_chunks": [
                    0.0,
                    2.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    51300.0,
                    87800.0,
                    55400.0
                ],
                "word_count_appended": [
                    231.0,
                    246.0,
                    191.0
                ],
                "word_count_appended_bing": [
                    33.0,
                    44.0,
                    29.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What was the least popular papal name of the 19th century?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "pius"
            ],
            "question": "what was the least popular papal name of the 19th century?",
            "lines": [
                [
                    0.75,
                    0.5,
                    0.35463770169353925,
                    -3.196354336334176,
                    0.3421955403087479,
                    0.3157894736842105,
                    0.34508627156789196,
                    0.3923941227312014,
                    0.0,
                    0.32588916459884204,
                    0.33793103448275863,
                    0.33850140494503156,
                    0.3333333333333333,
                    0.35714285714285715,
                    1.0
                ],
                [
                    0.0,
                    0.0,
                    0.523084706836448,
                    4.420121932610479,
                    0.31989708404802747,
                    0.3684210526315789,
                    0.31957989497374345,
                    0.21694036300777875,
                    1.0,
                    0.380479735318445,
                    0.41379310344827586,
                    0.3208139231825116,
                    0.5897435897435898,
                    0.5714285714285714,
                    1.0
                ],
                [
                    0.25,
                    0.5,
                    0.12227759147001273,
                    -0.22376759627630285,
                    0.3379073756432247,
                    0.3157894736842105,
                    0.3353338334583646,
                    0.39066551426101986,
                    0.0,
                    0.293631100082713,
                    0.2482758620689655,
                    0.3406846718724568,
                    0.07692307692307693,
                    0.07142857142857142,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "leo",
                "pius",
                "gregory"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "gregory": 0.13516558703642065,
                "pius": 0.5194243991424035,
                "leo": 0.19318446103922637
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3540056197801262,
                    1.2832556927300465,
                    1.3627386874898273
                ],
                "result_count_important_words": [
                    460000.0,
                    426000.0,
                    447000.0
                ],
                "wikipedia_search": [
                    0.0,
                    1.0,
                    0.0
                ],
                "answer_relation_to_question": [
                    0.75,
                    0.0,
                    0.25
                ],
                "word_count_appended_bing": [
                    49.0,
                    60.0,
                    36.0
                ],
                "answer_relation_to_question_bing": [
                    1.0,
                    0.0,
                    1.0
                ],
                "cosine_similarity_raw": [
                    0.27684077620506287,
                    0.40833553671836853,
                    0.09545353800058365
                ],
                "result_count_noun_chunks": [
                    454000.0,
                    251000.0,
                    452000.0
                ],
                "question_answer_similarity": [
                    0.6346324924379587,
                    -0.8776101469993591,
                    0.04442879994167015
                ],
                "word_count_noun_chunks": [
                    13.0,
                    23.0,
                    3.0
                ],
                "result_count_bing": [
                    2820000.0,
                    3290000.0,
                    2820000.0
                ],
                "word_count_raw": [
                    5.0,
                    8.0,
                    1.0
                ],
                "result_count": [
                    399000.0,
                    373000.0,
                    394000.0
                ],
                "word_count_appended": [
                    394.0,
                    460.0,
                    355.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which three-letter-titled movie grossed the most worldwide?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ray"
            ],
            "lines": [
                [
                    0.6172839506172839,
                    0.37777777777777777,
                    0.6907888717823303,
                    0.41395396639383125,
                    0.26153846153846155,
                    0.0818302252114542,
                    0.30458515283842796,
                    0.2509926262053318,
                    0.0,
                    0.3275217932752179,
                    0.4838709677419355,
                    0.34129522445331967,
                    0.2222222222222222,
                    0.0,
                    -1.0
                ],
                [
                    0.25925925925925924,
                    0.49444444444444446,
                    0.11756905893671712,
                    0.17180600473033586,
                    0.21153846153846154,
                    0.43717517578722104,
                    0.27237991266375544,
                    0.21298922291548497,
                    0.0,
                    0.38978829389788294,
                    0.24193548387096775,
                    0.31941230451240143,
                    0.4444444444444444,
                    0.25,
                    -1.0
                ],
                [
                    0.12345679012345678,
                    0.12777777777777777,
                    0.1916420692809526,
                    0.4142400288758329,
                    0.5269230769230769,
                    0.4809945990013248,
                    0.4230349344978166,
                    0.5360181508791833,
                    1.0,
                    0.28268991282689915,
                    0.27419354838709675,
                    0.3392924710342789,
                    0.3333333333333333,
                    0.75,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "big": 0.40000155265167764,
                "saw": 0.320163595537334,
                "ray": 0.2798348518109885
            },
            "question": "which three-letter-titled movie grossed the most worldwide?",
            "rate_limited": false,
            "answers": [
                "saw",
                "ray",
                "big"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "big": 0.10927937721448236,
                "saw": 0.2643585214260746,
                "ray": 0.5450125001213838
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7064761222665983,
                    1.5970615225620073,
                    1.6964623551713944
                ],
                "result_count_important_words": [
                    558000.0,
                    499000.0,
                    775000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_appended_bing": [
                    60.0,
                    30.0,
                    34.0
                ],
                "answer_relation_to_question_bing": [
                    1.1333333333333333,
                    1.4833333333333334,
                    0.3833333333333333
                ],
                "cosine_similarity_raw": [
                    0.160384863615036,
                    0.0272967591881752,
                    0.044494763016700745
                ],
                "result_count_noun_chunks": [
                    885000.0,
                    751000.0,
                    1890000.0
                ],
                "question_answer_similarity": [
                    3.4394874423742294,
                    1.4275128245353699,
                    3.4418642967939377
                ],
                "word_count_noun_chunks": [
                    2.0,
                    4.0,
                    3.0
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    3.0
                ],
                "result_count_bing": [
                    8030000.0,
                    42900000.0,
                    47200000.0
                ],
                "word_count_appended": [
                    263.0,
                    313.0,
                    227.0
                ],
                "answer_relation_to_question": [
                    1.8518518518518519,
                    0.7777777777777777,
                    0.37037037037037035
                ],
                "result_count": [
                    13600000.0,
                    11000000.0,
                    27400000.0
                ]
            },
            "integer_answers": {
                "big": 7,
                "saw": 4,
                "ray": 3
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Basketball is NOT a major theme of which of these 90s movies?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "point break"
            ],
            "lines": [
                [
                    0.13900226757369616,
                    0.22053872053872053,
                    0.16785187087982317,
                    0.1692651722464626,
                    0.49171923411274326,
                    0.3900138696255201,
                    0.4999412584983629,
                    0.49043414275202357,
                    0.28125,
                    0.3850129198966408,
                    0.40476190476190477,
                    0.32821200671079453,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.3734126984126984,
                    0.36868686868686873,
                    0.3665120805623735,
                    0.34904573858107607,
                    0.4808305284978713,
                    0.43869625520110955,
                    0.48595311917373696,
                    0.4841795437821928,
                    0.28125,
                    0.41860465116279066,
                    0.4523809523809524,
                    0.3529066304357538,
                    0.5,
                    0,
                    -1.0
                ],
                [
                    0.4875850340136054,
                    0.4107744107744108,
                    0.4656360485578033,
                    0.48168908917246134,
                    0.027450237389385457,
                    0.1712898751733703,
                    0.014105622327900191,
                    0.025386313465783683,
                    0.4375,
                    0.19638242894056845,
                    0.14285714285714285,
                    0.31888136285345164,
                    0.5,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "white men can't jump": 0.3895379434466627,
                "point break": 0.17654475894193475,
                "eddie": 0.4339172976114026
            },
            "question": "basketball is not a major theme of which of these 90s movies?",
            "rate_limited": false,
            "answers": [
                "white men can't jump",
                "point break",
                "eddie"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "white men can't jump": 0.2255162300549815,
                "point break": 0.4684720412989066,
                "eddie": 0.1812279979729257
            },
            "integer_answers": {
                "white men can't jump": 6,
                "point break": 0,
                "eddie": 7
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7178799328920549,
                    1.4709336956424617,
                    1.8111863714654837
                ],
                "result_count_important_words": [
                    92.0,
                    22000.0,
                    761000.0
                ],
                "wikipedia_search": [
                    1.75,
                    1.75,
                    0.5
                ],
                "word_count_appended_bing": [
                    8.0,
                    4.0,
                    30.0
                ],
                "answer_relation_to_question_bing": [
                    1.6767676767676767,
                    0.7878787878787878,
                    0.5353535353535354
                ],
                "cosine_similarity_raw": [
                    0.12475714832544327,
                    0.050138991326093674,
                    0.012907339259982109
                ],
                "result_count_noun_chunks": [
                    10400.0,
                    17200.0,
                    516000.0
                ],
                "question_answer_similarity": [
                    20.746155932545662,
                    9.468977510929108,
                    1.148596940562129
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    793000.0,
                    442000.0,
                    2370000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    7430.0,
                    17200.0,
                    424000.0
                ],
                "answer_relation_to_question": [
                    3.6099773242630384,
                    1.2658730158730158,
                    0.12414965986394558
                ],
                "word_count_appended": [
                    89.0,
                    63.0,
                    235.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "The creator of Wonder Woman also created an early version of what device?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "lie detector"
            ],
            "lines": [
                [
                    0.2328828828828829,
                    0.36458333333333337,
                    0.040550401711227285,
                    0.31936832344364147,
                    0.32867132867132864,
                    0.1341991341991342,
                    0.9945842842058653,
                    0.9950248756218906,
                    0.03469387755102041,
                    0.1484375,
                    0.09090909090909091,
                    0.2688571933596446,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.5900900900900902,
                    0.5208333333333334,
                    0.0038572961670072367,
                    0.3882621007823312,
                    0.15384615384615385,
                    0.5435305435305435,
                    0.0023161551823972205,
                    0.0021267707265200716,
                    0.3508967223252938,
                    0.265625,
                    0.2727272727272727,
                    0.20040937553488827,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.17702702702702702,
                    0.11458333333333334,
                    0.9555923021217655,
                    0.2923695757740273,
                    0.5174825174825175,
                    0.32227032227032226,
                    0.003099560611737457,
                    0.0028483536515893815,
                    0.6144094001236857,
                    0.5859375,
                    0.6363636363636364,
                    0.5307334311054671,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "lie detector": 0.48233692570465064,
                "hearing aid": 0.23532291530327368,
                "magic marker": 0.28234015899207576
            },
            "question": "the creator of wonder woman also created an early version of what device?",
            "rate_limited": false,
            "answers": [
                "magic marker",
                "hearing aid",
                "lie detector"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "lie detector": 0.7906221929744528,
                "hearing aid": 0.24447813546910271,
                "magic marker": 0.12958394167834492
            },
            "integer_answers": {
                "lie detector": 8,
                "hearing aid": 4,
                "magic marker": 2
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.8820003535175123,
                    1.4028656287442178,
                    3.71513401773827
                ],
                "result_count_important_words": [
                    29200.0,
                    68.0,
                    91.0
                ],
                "wikipedia_search": [
                    0.24285714285714285,
                    2.4562770562770564,
                    4.3008658008658
                ],
                "word_count_appended_bing": [
                    2.0,
                    6.0,
                    14.0
                ],
                "answer_relation_to_question_bing": [
                    1.4583333333333333,
                    2.083333333333333,
                    0.4583333333333333
                ],
                "cosine_similarity_raw": [
                    0.02869383990764618,
                    0.002729458501562476,
                    0.6761859655380249
                ],
                "result_count_noun_chunks": [
                    26200.0,
                    56.0,
                    75.0
                ],
                "question_answer_similarity": [
                    6.546491540968418,
                    7.958693370223045,
                    5.9930644780397415
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    16.0
                ],
                "result_count_bing": [
                    279000.0,
                    1130000.0,
                    670000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    21.0
                ],
                "result_count": [
                    47.0,
                    22.0,
                    74.0
                ],
                "answer_relation_to_question": [
                    1.1644144144144144,
                    2.9504504504504507,
                    0.8851351351351351
                ],
                "word_count_appended": [
                    19.0,
                    34.0,
                    75.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "In the old saying, what keeps the doctor away?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "apple a day"
            ],
            "question": "in the old saying, what keeps the doctor away?",
            "lines": [
                [
                    0.8203015734265735,
                    0.7095959595959597,
                    0.986905644702088,
                    0.3846568860807157,
                    0.15375088086442162,
                    0.304654442877292,
                    0.5205992509363296,
                    0.29968066814050603,
                    0.18817204301075266,
                    0.5663265306122449,
                    0.75,
                    0.5003943549548486,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.10437791375291375,
                    0.10101010101010101,
                    0.005131195502980962,
                    0.24471596589998465,
                    0.00702556108394371,
                    0.3004231311706629,
                    0.0449438202247191,
                    0.024809629083763204,
                    0.10752688172043011,
                    0.2193877551020408,
                    0.10714285714285714,
                    0.23292115081015244,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.07532051282051282,
                    0.18939393939393936,
                    0.007963159794931103,
                    0.3706271480192997,
                    0.8392235580516346,
                    0.39492242595204513,
                    0.4344569288389513,
                    0.6755097027757307,
                    0.7043010752688171,
                    0.21428571428571427,
                    0.14285714285714285,
                    0.26668449423499896,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "apple a day",
                "axe body spray",
                "50 shades of grey"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "apple a day": 1.0262772064601384,
                "axe body spray": 0.0008681112810324458,
                "50 shades of grey": 0.022892494432016932
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.501971774774243,
                    1.1646057540507622,
                    1.3334224711749947
                ],
                "result_count_important_words": [
                    1390000.0,
                    120000.0,
                    1160000.0
                ],
                "wikipedia_search": [
                    0.564516129032258,
                    0.3225806451612903,
                    2.1129032258064515
                ],
                "word_count_appended_bing": [
                    21.0,
                    3.0,
                    4.0
                ],
                "answer_relation_to_question_bing": [
                    2.128787878787879,
                    0.30303030303030304,
                    0.5681818181818181
                ],
                "cosine_similarity_raw": [
                    0.7548282146453857,
                    0.003924560733139515,
                    0.0060905697755515575
                ],
                "result_count_noun_chunks": [
                    1220000.0,
                    101000.0,
                    2750000.0
                ],
                "question_answer_similarity": [
                    11.533798962831497,
                    7.337720591574907,
                    11.113122291862965
                ],
                "word_count_noun_chunks": [
                    9.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    43200000.0,
                    42600000.0,
                    56000000.0
                ],
                "word_count_raw": [
                    61.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    1440000.0,
                    65800.0,
                    7860000.0
                ],
                "answer_relation_to_question": [
                    4.1015078671328675,
                    0.5218895687645687,
                    0.3766025641025641
                ],
                "word_count_appended": [
                    222.0,
                    86.0,
                    84.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Anna Wintour became the editor-in-chief of Vogue in the same year as what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "world series cancelled"
            ],
            "lines": [
                [
                    0.5333333333333333,
                    0.6666666666666667,
                    0.5259247456964948,
                    0.4103360485672182,
                    0.5,
                    0.33406352683461116,
                    0.4666666666666667,
                    0.3333333333333333,
                    0.6111111111111112,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.34583333333333327,
                    0,
                    0,
                    1.0
                ],
                [
                    0.3212121212121212,
                    0.16666666666666669,
                    0.40620283094886256,
                    0.33205012281414287,
                    0.25,
                    0.3231106243154436,
                    0.13333333333333333,
                    0.16666666666666666,
                    0.05555555555555555,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.32708333333333334,
                    0,
                    0,
                    1.0
                ],
                [
                    0.14545454545454545,
                    0.16666666666666669,
                    0.06787242335464266,
                    0.25761382861863896,
                    0.25,
                    0.3428258488499452,
                    0.4,
                    0.5,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.32708333333333334,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "world series cancelled": 0.4494946193507863,
                "41st president elected": 0.26237899345939936,
                "the goonies premiered": 0.28812638718981437
            },
            "question": "anna wintour became the editor-in-chief of vogue in the same year as what?",
            "rate_limited": false,
            "answers": [
                "world series cancelled",
                "41st president elected",
                "the goonies premiered"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "world series cancelled": 0.3915954478367551,
                "41st president elected": 0.15230175741198854,
                "the goonies premiered": 0.11697536857172047
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.383333333333333,
                    1.3083333333333333,
                    1.3083333333333333
                ],
                "result_count_important_words": [
                    7.0,
                    2.0,
                    6.0
                ],
                "wikipedia_search": [
                    1.8333333333333335,
                    0.16666666666666666,
                    1.0
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.6666666666666666,
                    0.16666666666666666,
                    0.16666666666666666
                ],
                "cosine_similarity_raw": [
                    0.08038873970508575,
                    0.0620889849960804,
                    0.010374447330832481
                ],
                "result_count_noun_chunks": [
                    4.0,
                    2.0,
                    6.0
                ],
                "question_answer_similarity": [
                    11.707725159823895,
                    9.474067878676578,
                    7.350248444825411
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    30500.0,
                    29500.0,
                    31300.0
                ],
                "result_count": [
                    4.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question": [
                    1.6,
                    0.9636363636363636,
                    0.43636363636363634
                ],
                "word_count_appended": [
                    5.0,
                    5.0,
                    5.0
                ]
            },
            "integer_answers": {
                "world series cancelled": 10,
                "41st president elected": 0,
                "the goonies premiered": 2
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these best-selling authors uses his/her given last name?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "j.d. robb"
            ],
            "lines": [
                [
                    0.377009100837277,
                    0.34841269841269845,
                    0.23115252245263493,
                    0.1085123983363008,
                    0.19486404833836857,
                    0.34330708661417325,
                    0.09015256588072122,
                    0.20891434262948208,
                    0.265625,
                    0.2288135593220339,
                    0.2222222222222222,
                    0.3394473166589414,
                    0.25,
                    0,
                    -1.0
                ],
                [
                    0.41143162237592523,
                    0.37857142857142856,
                    0.2644053368671957,
                    -0.062449521878158214,
                    0.2084592145015106,
                    0.3401574803149606,
                    0.09847434119278779,
                    0.20094621513944222,
                    0.5587121212121212,
                    0.4152542372881356,
                    0.5833333333333334,
                    0.31162624545662854,
                    0.75,
                    0,
                    -1.0
                ],
                [
                    0.2115592767867977,
                    0.273015873015873,
                    0.5044421406801693,
                    0.9539371235418574,
                    0.5966767371601208,
                    0.3165354330708661,
                    0.811373092926491,
                    0.5901394422310757,
                    0.17566287878787878,
                    0.3559322033898305,
                    0.19444444444444445,
                    0.3489264378844301,
                    0.0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "r.l. stine": 0.3429940041827163,
                "e.l. james": 0.4102034679938334,
                "j.d. robb": 0.24680252782345027
            },
            "question": "which of these best-selling authors uses his/her given last name?",
            "rate_limited": false,
            "answers": [
                "j.d. robb",
                "r.l. stine",
                "e.l. james"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "r.l. stine": 0.30254693022365486,
                "e.l. james": 0.24753409188915657,
                "j.d. robb": 0.48602254485623625
            },
            "integer_answers": {
                "r.l. stine": 6,
                "e.l. james": 6,
                "j.d. robb": 1
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.697236583294707,
                    1.5581312272831427,
                    1.7446321894221506
                ],
                "result_count_important_words": [
                    130000.0,
                    142000.0,
                    1170000.0
                ],
                "wikipedia_search": [
                    1.0625,
                    2.234848484848485,
                    0.7026515151515151
                ],
                "word_count_appended_bing": [
                    8.0,
                    21.0,
                    7.0
                ],
                "answer_relation_to_question_bing": [
                    1.0452380952380953,
                    1.1357142857142857,
                    0.819047619047619
                ],
                "cosine_similarity_raw": [
                    0.05698461830615997,
                    0.06518223136663437,
                    0.12435703724622726
                ],
                "result_count_noun_chunks": [
                    83900.0,
                    80700.0,
                    237000.0
                ],
                "question_answer_similarity": [
                    0.731094989401754,
                    -0.4207494556903839,
                    6.427087244577706
                ],
                "word_count_noun_chunks": [
                    1.0,
                    3.0,
                    0.0
                ],
                "result_count_bing": [
                    2180000.0,
                    2160000.0,
                    2010000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    12900.0,
                    13800.0,
                    39500.0
                ],
                "answer_relation_to_question": [
                    1.885045504186385,
                    2.0571581118796263,
                    1.0577963839339886
                ],
                "word_count_appended": [
                    27.0,
                    49.0,
                    42.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "The scientist who famously separated visible light into seven colors is also famous for his work on what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "gravity"
            ],
            "lines": [
                [
                    0.5511363636363636,
                    0.29440993788819875,
                    0.24383712893126194,
                    0.23885856220271673,
                    0.9198045415349239,
                    0.690155713908683,
                    0.9199500802282047,
                    0.9424133358590642,
                    0.7547619047619049,
                    0.915929203539823,
                    0.84375,
                    0.421840127164112,
                    1.0,
                    0.5,
                    1.0
                ],
                [
                    0.38838383838383833,
                    0.5063664596273292,
                    0.39902901501101684,
                    0.27211160095612996,
                    0.0635958608795631,
                    0.032726313011348644,
                    0.0631128543412373,
                    0.04271642356506914,
                    0.1785714285714286,
                    0.05309734513274336,
                    0.09375,
                    0.3317361992956773,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.060479797979797974,
                    0.19922360248447202,
                    0.3571338560577212,
                    0.48902983684115325,
                    0.01659959758551308,
                    0.2771179730799683,
                    0.01693706543055803,
                    0.01487024057586664,
                    0.06666666666666668,
                    0.030973451327433628,
                    0.0625,
                    0.24642367354021066,
                    0.0,
                    0.5,
                    1.0
                ]
            ],
            "fraction_answers": {
                "atomic nuclei": 0.17322838134109872,
                "gravity": 0.659774778546804,
                "steel production": 0.16699684011209726
            },
            "question": "the scientist who famously separated visible light into seven colors is also famous for his work on what?",
            "rate_limited": false,
            "answers": [
                "gravity",
                "atomic nuclei",
                "steel production"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "atomic nuclei": 0.27038105749792696,
                "gravity": 0.4783149177593956,
                "steel production": 0.2705163003427973
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.796561144477008,
                    2.985625793661096,
                    2.217813061861896
                ],
                "result_count_important_words": [
                    25800.0,
                    1770.0,
                    475.0
                ],
                "wikipedia_search": [
                    3.7738095238095237,
                    0.8928571428571428,
                    0.3333333333333333
                ],
                "word_count_appended_bing": [
                    27.0,
                    3.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.177639751552795,
                    2.0254658385093167,
                    0.7968944099378881
                ],
                "cosine_similarity_raw": [
                    0.05208047106862068,
                    0.08522745966911316,
                    0.07627919316291809
                ],
                "result_count_noun_chunks": [
                    1990000.0,
                    90200.0,
                    31400.0
                ],
                "question_answer_similarity": [
                    3.8784078508615494,
                    4.418345985701308,
                    7.940503120422363
                ],
                "word_count_noun_chunks": [
                    3.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    523000.0,
                    24800.0,
                    210000.0
                ],
                "word_count_appended": [
                    207.0,
                    12.0,
                    7.0
                ],
                "answer_relation_to_question": [
                    3.3068181818181817,
                    2.33030303030303,
                    0.36287878787878786
                ],
                "result_count": [
                    25600.0,
                    1770.0,
                    462.0
                ]
            },
            "integer_answers": {
                "atomic nuclei": 2,
                "gravity": 11,
                "steel production": 1
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What was the first popular home video game?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "pong"
            ],
            "lines": [
                [
                    0.4077353098048687,
                    0.21890547263681592,
                    0.09464189878389216,
                    0.15572708926335596,
                    0.013999321423829826,
                    0.4220985691573927,
                    0.01008233910267182,
                    0.013207329056123924,
                    0.09539914521327161,
                    0.15081521739130435,
                    0.13402061855670103,
                    0.3080632980973424,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.35019724461351126,
                    0.22307069304639593,
                    0.8519627478644958,
                    0.10277820441426974,
                    0.9647582941185148,
                    0.15580286168521462,
                    0.974626113258276,
                    0.9667071267556788,
                    0.7075650244795225,
                    0.7065217391304348,
                    0.7010309278350515,
                    0.43682004443533035,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.24206744558162002,
                    0.5580238343167881,
                    0.053395353351612086,
                    0.7414947063223744,
                    0.02124238445765537,
                    0.4220985691573927,
                    0.01529154763905226,
                    0.020085544188197215,
                    0.19703583030720578,
                    0.14266304347826086,
                    0.16494845360824742,
                    0.2551166574673273,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "tekken 2": 0.14462111489196933,
                "pong": 0.652988644402621,
                "half-life 3": 0.2023902407054095
            },
            "question": "what was the first popular home video game?",
            "rate_limited": false,
            "answers": [
                "tekken 2",
                "pong",
                "half-life 3"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "tekken 2": 0.09131179726462739,
                "pong": 0.6753844124645527,
                "half-life 3": 0.08928949855617384
            },
            "integer_answers": {
                "tekken 2": 2,
                "pong": 10,
                "half-life 3": 2
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2322531923893694,
                    1.7472801777413212,
                    1.020466629869309
                ],
                "result_count_important_words": [
                    120000.0,
                    11600000.0,
                    182000.0
                ],
                "wikipedia_search": [
                    0.38159658085308645,
                    2.83026009791809,
                    0.7881433212288231
                ],
                "word_count_appended_bing": [
                    13.0,
                    68.0,
                    16.0
                ],
                "answer_relation_to_question_bing": [
                    0.8756218905472637,
                    0.8922827721855837,
                    2.2320953372671526
                ],
                "cosine_similarity_raw": [
                    0.028733020648360252,
                    0.25865355134010315,
                    0.016210682690143585
                ],
                "result_count_noun_chunks": [
                    91400.0,
                    6690000.0,
                    139000.0
                ],
                "question_answer_similarity": [
                    2.11887746816501,
                    1.3984363451600075,
                    10.089037388563156
                ],
                "word_count_noun_chunks": [
                    0.0,
                    31.0,
                    0.0
                ],
                "result_count_bing": [
                    53100000.0,
                    19600000.0,
                    53100000.0
                ],
                "word_count_raw": [
                    0.0,
                    11.0,
                    0.0
                ],
                "result_count": [
                    94900.0,
                    6540000.0,
                    144000.0
                ],
                "answer_relation_to_question": [
                    1.6309412392194749,
                    1.400788978454045,
                    0.9682697823264801
                ],
                "word_count_appended": [
                    111.0,
                    520.0,
                    105.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these verbs has two meanings that are opposites of each other?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "cleave"
            ],
            "lines": [
                [
                    0.0,
                    0.0,
                    0.17216661284850598,
                    0.5529125994186738,
                    0.15710872162485065,
                    0.21979586992641822,
                    0.3143100511073254,
                    0.31761006289308175,
                    0,
                    0.3244005641748942,
                    0.3103448275862069,
                    0.332812558993568,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.16666666666666669,
                    0.0,
                    0.5365683158519028,
                    0.2563376711823779,
                    0.37873357228195936,
                    0.6812247804414906,
                    0.206984667802385,
                    0.029350104821802937,
                    0,
                    0.2651622002820874,
                    0.26436781609195403,
                    0.2966708070454468,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.8333333333333334,
                    1.0,
                    0.2912650712995912,
                    0.19074972939894838,
                    0.46415770609318996,
                    0.09897934963209115,
                    0.4787052810902896,
                    0.6530398322851153,
                    0,
                    0.4104372355430183,
                    0.42528735632183906,
                    0.37051663396098516,
                    1.0,
                    1.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "cleave": 0.5551131945352616,
                "jut": 0.23708204634369795,
                "branch": 0.20780475912104038
            },
            "question": "which of these verbs has two meanings that are opposites of each other?",
            "rate_limited": false,
            "answers": [
                "branch",
                "jut",
                "cleave"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "cleave": 0.5708356297307872,
                "jut": 0.18529520967241758,
                "branch": 0.04863383037840421
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.998437676980704,
                    0.8900124211363404,
                    1.1115499018829555
                ],
                "result_count_important_words": [
                    369000.0,
                    243000.0,
                    562000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    27.0,
                    23.0,
                    37.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "cosine_similarity_raw": [
                    0.03137444704771042,
                    0.09778048098087311,
                    0.053078122437000275
                ],
                "result_count_noun_chunks": [
                    3030000.0,
                    280000.0,
                    6230000.0
                ],
                "question_answer_similarity": [
                    3.058261123485863,
                    1.4178507328033447,
                    1.0550717823207378
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    2.0
                ],
                "result_count_bing": [
                    9260000.0,
                    28700000.0,
                    4170000.0
                ],
                "result_count": [
                    2630000.0,
                    6340000.0,
                    7770000.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    0.3333333333333333,
                    1.6666666666666665
                ],
                "word_count_appended": [
                    230.0,
                    188.0,
                    291.0
                ]
            },
            "integer_answers": {
                "cleave": 10,
                "jut": 2,
                "branch": 1
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these is NOT a real animal?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "jackalope"
            ],
            "lines": [
                [
                    0.4005456349206349,
                    0.32333333333333336,
                    0.4032920644909005,
                    0.21366376270780446,
                    0.2712252160650737,
                    0.3372434017595308,
                    0.32577903682719545,
                    0.32521489971346706,
                    0.16379310344827586,
                    0.3179692718770875,
                    0.33088235294117646,
                    0.3518515402595749,
                    0.5,
                    0,
                    -1.0
                ],
                [
                    0.2757936507936508,
                    0.30666666666666664,
                    0.19572242734848366,
                    0.2863362372921955,
                    0.2534316217590239,
                    0.33431085043988273,
                    0.2577903682719547,
                    0.2578796561604585,
                    0.39655172413793105,
                    0.28891115564462255,
                    0.3088235294117647,
                    0.32711347988612904,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.3236607142857143,
                    0.37,
                    0.4009855081606158,
                    0.5,
                    0.4753431621759024,
                    0.32844574780058655,
                    0.4164305949008499,
                    0.4169054441260745,
                    0.4396551724137931,
                    0.3931195724782899,
                    0.3602941176470588,
                    0.3210349798542961,
                    0.5,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "liger": 0.4631797895672672,
                "wholphin": 0.1929423055625875,
                "jackalope": 0.3438779048701454
            },
            "question": "which of these is not a real animal?",
            "rate_limited": false,
            "answers": [
                "jackalope",
                "liger",
                "wholphin"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "liger": 0.07687381715380737,
                "wholphin": 0.2097234632991426,
                "jackalope": 0.35467681700370246
            },
            "integer_answers": {
                "liger": 9,
                "wholphin": 2,
                "jackalope": 1
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.5925938389617005,
                    0.6915460804554839,
                    0.7158600805828157
                ],
                "result_count_important_words": [
                    2460000.0,
                    3420000.0,
                    1180000.0
                ],
                "wikipedia_search": [
                    1.3448275862068966,
                    0.41379310344827586,
                    0.2413793103448276
                ],
                "answer_relation_to_question": [
                    0.3978174603174603,
                    0.8968253968253967,
                    0.7053571428571428
                ],
                "word_count_appended_bing": [
                    92.0,
                    104.0,
                    76.0
                ],
                "answer_relation_to_question_bing": [
                    0.7066666666666667,
                    0.7733333333333333,
                    0.52
                ],
                "cosine_similarity_raw": [
                    0.03330664709210396,
                    0.10479456186294556,
                    0.034101035445928574
                ],
                "result_count_noun_chunks": [
                    2440000.0,
                    3380000.0,
                    1160000.0
                ],
                "question_answer_similarity": [
                    -0.7042543757706881,
                    -0.5255137849599123,
                    0.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    6.0,
                    0.0
                ],
                "result_count_bing": [
                    111000000.0,
                    113000000.0,
                    117000000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    1800000.0,
                    1940000.0,
                    194000.0
                ],
                "word_count_appended": [
                    545.0,
                    632.0,
                    320.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Most humans have how many kidneys?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ten"
            ],
            "lines": [
                [
                    0.0,
                    0,
                    0.4362222221676091,
                    0.3905897319789175,
                    0.578163219924394,
                    0.37823834196891193,
                    0.09838895281933256,
                    0.5803571428571429,
                    0.0,
                    0.31019978969505785,
                    0.3723404255319149,
                    0.397393108722381,
                    0.2484076433121019,
                    0.2611464968152866,
                    5.0
                ],
                [
                    1.0,
                    0,
                    0.33473757294366,
                    0.25290540524516186,
                    0.014157586539174264,
                    0.17789291882556132,
                    0.1766398158803222,
                    0.014136904761904762,
                    0.0,
                    0.25236593059936907,
                    0.14893617021276595,
                    0.3014365343855173,
                    0.0,
                    0.0,
                    5.0
                ],
                [
                    0.0,
                    0,
                    0.22904020488873084,
                    0.3565048627759206,
                    0.40767919353643167,
                    0.4438687392055268,
                    0.7249712313003452,
                    0.4055059523809524,
                    1.0,
                    0.4374342797055731,
                    0.4787234042553192,
                    0.3011703568921017,
                    0.7515923566878981,
                    0.7388535031847133,
                    5.0
                ]
            ],
            "fraction_answers": {
                "zero": 0.20563144918411053,
                "two": 0.3116497750610039,
                "ten": 0.48271877575488564
            },
            "question": "most humans have how many kidneys?",
            "rate_limited": false,
            "answers": [
                "two",
                "zero",
                "ten"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "zero": 0.33154400644733284,
                "two": 0.3602427367210622,
                "ten": 0.5459680941164622
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 5
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.794786217444762,
                    0.6028730687710346,
                    0.6023407137842034
                ],
                "result_count_important_words": [
                    17100000.0,
                    30700000.0,
                    126000000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_appended_bing": [
                    70.0,
                    28.0,
                    90.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.11902302503585815,
                    0.09133298695087433,
                    0.06249351054430008
                ],
                "result_count_noun_chunks": [
                    156000000.0,
                    3800000.0,
                    109000000.0
                ],
                "question_answer_similarity": [
                    3.0198150128126144,
                    1.9553190395236015,
                    2.7562904208898544
                ],
                "word_count_noun_chunks": [
                    39.0,
                    0.0,
                    118.0
                ],
                "word_count_raw": [
                    41.0,
                    0.0,
                    116.0
                ],
                "result_count_bing": [
                    219000000.0,
                    103000000.0,
                    257000000.0
                ],
                "word_count_appended": [
                    295.0,
                    240.0,
                    416.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count": [
                    156000000.0,
                    3820000.0,
                    110000000.0
                ]
            },
            "integer_answers": {
                "zero": 1,
                "two": 5,
                "ten": 7
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which Hawaiian island has active volcanoes?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "oahu"
            ],
            "lines": [
                [
                    0.6810360047564584,
                    0.6415289941613588,
                    0.7765701742964894,
                    0.6555825533334825,
                    0.3683431952662722,
                    0.2801807617817947,
                    0.38033261026753434,
                    0.36787564766839376,
                    0.0,
                    0.17365269461077845,
                    0.1574074074074074,
                    0.3631099860108257,
                    0.0392156862745098,
                    0.046511627906976744,
                    -1.0
                ],
                [
                    0.15686021549298332,
                    0.1659617170912951,
                    0.12652543822207332,
                    0.17846040819448183,
                    0.34245562130177515,
                    0.3615235635894125,
                    0.33550253073029646,
                    0.3427091043671355,
                    0.3795045045045045,
                    0.45748502994011975,
                    0.39814814814814814,
                    0.3271644206519448,
                    0.45098039215686275,
                    0.5581395348837209,
                    -1.0
                ],
                [
                    0.16210377975055823,
                    0.19250928874734607,
                    0.09690438748143729,
                    0.1659570384720357,
                    0.28920118343195267,
                    0.35829567462879275,
                    0.2841648590021692,
                    0.28941524796447077,
                    0.6204954954954954,
                    0.3688622754491018,
                    0.4444444444444444,
                    0.3097255933372295,
                    0.5098039215686274,
                    0.3953488372093023,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "maui": 0.32724433066248243,
                "oahu": 0.320516573355926,
                "big island": 0.35223909598159153
            },
            "question": "which hawaiian island has active volcanoes?",
            "rate_limited": false,
            "answers": [
                "big island",
                "maui",
                "oahu"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "maui": 0.3527811042295974,
                "oahu": 0.47104156438740113,
                "big island": 0.24307718127639405
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.452439944043303,
                    1.3086576826077791,
                    1.238902373348918
                ],
                "result_count_important_words": [
                    526000.0,
                    464000.0,
                    393000.0
                ],
                "wikipedia_search": [
                    0.0,
                    1.1385135135135136,
                    1.8614864864864864
                ],
                "word_count_appended_bing": [
                    17.0,
                    43.0,
                    48.0
                ],
                "answer_relation_to_question_bing": [
                    2.566115976645435,
                    0.6638468683651804,
                    0.7700371549893843
                ],
                "cosine_similarity_raw": [
                    0.342020720243454,
                    0.05572493374347687,
                    0.042679090052843094
                ],
                "result_count_noun_chunks": [
                    497000.0,
                    463000.0,
                    391000.0
                ],
                "question_answer_similarity": [
                    4.776122942566872,
                    1.3001396171748638,
                    1.2090486772358418
                ],
                "word_count_noun_chunks": [
                    2.0,
                    23.0,
                    26.0
                ],
                "word_count_raw": [
                    2.0,
                    24.0,
                    17.0
                ],
                "result_count_bing": [
                    868000.0,
                    1120000.0,
                    1110000.0
                ],
                "word_count_appended": [
                    145.0,
                    382.0,
                    308.0
                ],
                "answer_relation_to_question": [
                    2.7241440190258337,
                    0.6274408619719333,
                    0.6484151190022329
                ],
                "result_count": [
                    498000.0,
                    463000.0,
                    391000.0
                ]
            },
            "integer_answers": {
                "maui": 3,
                "oahu": 3,
                "big island": 8
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these rappers is NOT a founding member of the Wu-Tang Clan?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "redman"
            ],
            "question": "which of these rappers is not a founding member of the wu-tang clan?",
            "lines": [
                [
                    0.38187494331744637,
                    0.3261331294337539,
                    0.24362487934053834,
                    0.5751777517825865,
                    0.3959421454399357,
                    0.3054129724685021,
                    0.4426086956521739,
                    0.4769326683291771,
                    0.26666666666666666,
                    0.24751655629139074,
                    0.30434782608695654,
                    0.32297591000319503,
                    0.20370370370370372,
                    0.25,
                    -1.0
                ],
                [
                    0.2011674075590006,
                    0.25398666156917715,
                    0.3399884552368968,
                    -0.18703140123935358,
                    0.1946564885496183,
                    0.3331777881474568,
                    0.33999999999999997,
                    0.43231207695048096,
                    0.24047619047619045,
                    0.4503311258278146,
                    0.42934782608695654,
                    0.32984755585356984,
                    0.40740740740740744,
                    0.25,
                    -1.0
                ],
                [
                    0.416957649123553,
                    0.41988020899706896,
                    0.41638666542256486,
                    0.6118536494567671,
                    0.40940136601044597,
                    0.36140923938404107,
                    0.2173913043478261,
                    0.09075525472034202,
                    0.4928571428571429,
                    0.30215231788079466,
                    0.2663043478260869,
                    0.3471765341432351,
                    0.3888888888888889,
                    0.5,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "raekwon",
                "inspectah deck",
                "redman"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "inspectah deck": 0.10069821503806574,
                "redman": 0.6288970855911641,
                "raekwon": 0.12859219695863644
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.12428907996166,
                    2.0418293297571615,
                    1.8338815902811783
                ],
                "result_count_important_words": [
                    66000.0,
                    184000.0,
                    325000.0
                ],
                "wikipedia_search": [
                    2.333333333333333,
                    2.595238095238095,
                    0.07142857142857142
                ],
                "word_count_appended_bing": [
                    36.0,
                    13.0,
                    43.0
                ],
                "answer_relation_to_question_bing": [
                    1.7386687056624612,
                    2.4601333843082283,
                    0.8011979100293105
                ],
                "cosine_similarity_raw": [
                    0.13003471493721008,
                    0.08115863800048828,
                    0.04240909218788147
                ],
                "result_count_noun_chunks": [
                    51800.0,
                    152000.0,
                    919000.0
                ],
                "question_answer_similarity": [
                    -0.15664296224713326,
                    1.4315223759040236,
                    -0.23306213039904833
                ],
                "word_count_noun_chunks": [
                    16.0,
                    5.0,
                    6.0
                ],
                "result_count_bing": [
                    83400.0,
                    71500.0,
                    59400.0
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    0.0
                ],
                "result_count": [
                    51800.0,
                    152000.0,
                    45100.0
                ],
                "answer_relation_to_question": [
                    1.4175006801906433,
                    3.5859911092919927,
                    0.9965082105173644
                ],
                "word_count_appended": [
                    305.0,
                    60.0,
                    239.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Pixies, Bon Iver, Iron & Wine and Bauhaus were all once signed to which record label?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "4ad"
            ],
            "lines": [
                [
                    0.3439990455738487,
                    0.177826699798015,
                    0.255165740002254,
                    -0.0,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.0023180343069077423,
                    0.15730337078651685,
                    0.3812222222222222,
                    0.024390243902439025,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.47980493915533284,
                    0.4787952104857811,
                    0.6605919952654253,
                    0.5718448795319769,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.8966156699119147,
                    0.29555446995603324,
                    0.6154444444444445,
                    0.9512195121951219,
                    0.3333333333333333,
                    0.3333333333333333,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.1761960152708184,
                    0.34337808971620387,
                    0.08424226473232063,
                    0.42815512046802306,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.10106629578117757,
                    0.5471421592574499,
                    0.0033333333333333335,
                    0.024390243902439025,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "4ad": 0.5916574610199545,
                "geffen": 0.21723120398536425,
                "subpop": 0.19111133499468116
            },
            "question": "pixies, bon iver, iron & wine and bauhaus were all once signed to which record label?",
            "rate_limited": false,
            "answers": [
                "subpop",
                "4ad",
                "geffen"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "4ad": 0.8232899199413797,
                "geffen": 0.04198881172203027,
                "subpop": 0.06852950850800085
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.6666666666666665,
                    2.6666666666666665,
                    2.6666666666666665
                ],
                "result_count_important_words": [
                    25.0,
                    9670.0,
                    1090.0
                ],
                "wikipedia_search": [
                    2.287333333333333,
                    3.6926666666666668,
                    0.02
                ],
                "word_count_appended_bing": [
                    8.0,
                    8.0,
                    8.0
                ],
                "answer_relation_to_question_bing": [
                    1.244786898586105,
                    3.351566473400468,
                    2.403646628013427
                ],
                "cosine_similarity_raw": [
                    0.06281088292598724,
                    0.16260947287082672,
                    0.02073683962225914
                ],
                "result_count_noun_chunks": [
                    32200.0,
                    60500.0,
                    112000.0
                ],
                "question_answer_similarity": [
                    0.0,
                    -1.0108989626169205,
                    -0.7568863211199641
                ],
                "word_count_noun_chunks": [
                    0.0,
                    74.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    11.0,
                    0.0
                ],
                "result_count_bing": [
                    1290000.0,
                    1290000.0,
                    1290000.0
                ],
                "word_count_appended": [
                    1.0,
                    39.0,
                    1.0
                ],
                "answer_relation_to_question": [
                    2.7519923645907896,
                    3.8384395132426627,
                    1.4095681221665473
                ],
                "result_count": [
                    228000.0,
                    228000.0,
                    228000.0
                ]
            },
            "integer_answers": {
                "4ad": 8,
                "geffen": 1,
                "subpop": 4
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these trees produces fruit called apples?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "jujube"
            ],
            "question": "which of these trees produces fruit called apples?",
            "lines": [
                [
                    0.4642857142857143,
                    0.5235294117647059,
                    0.4364692066531791,
                    0.5188824881727048,
                    0.5159450350012963,
                    0.36784511784511786,
                    0.43868131868131865,
                    0.4496179136307091,
                    0.5921911421911423,
                    0.4913733609385783,
                    0.4711111111111111,
                    0.3326003362035227,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.46031746031746035,
                    0.39803921568627454,
                    0.4502634893669596,
                    0.05833749424906839,
                    0.08996629504796474,
                    0.3501683501683502,
                    0.0734065934065934,
                    0.07232983827972277,
                    0.29924242424242425,
                    0.34644582470669427,
                    0.3422222222222222,
                    0.3862973706191178,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.07539682539682539,
                    0.0784313725490196,
                    0.11326730397986132,
                    0.42278001757822686,
                    0.39408866995073893,
                    0.281986531986532,
                    0.4879120879120879,
                    0.47805224808956814,
                    0.10856643356643358,
                    0.1621808143547274,
                    0.18666666666666668,
                    0.28110229317735946,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "cashew",
                "jujube",
                "hickory"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "jujube": 0.6859596720245226,
                "cashew": 0.6224228230869974,
                "hickory": 0.011769723262626092
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6630016810176134,
                    1.9314868530955889,
                    1.4055114658867973
                ],
                "result_count_important_words": [
                    998000.0,
                    167000.0,
                    1110000.0
                ],
                "wikipedia_search": [
                    2.960955710955711,
                    1.496212121212121,
                    0.5428321678321678
                ],
                "word_count_appended_bing": [
                    106.0,
                    77.0,
                    42.0
                ],
                "answer_relation_to_question_bing": [
                    2.6176470588235294,
                    1.9901960784313726,
                    0.39215686274509803
                ],
                "cosine_similarity_raw": [
                    0.06580217182636261,
                    0.06788180023431778,
                    0.017076198011636734
                ],
                "result_count_noun_chunks": [
                    506000.0,
                    81400.0,
                    538000.0
                ],
                "question_answer_similarity": [
                    1.474129954352975,
                    0.16573511285241693,
                    1.2011056495830417
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    4370000.0,
                    4160000.0,
                    3350000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    199000.0,
                    34700.0,
                    152000.0
                ],
                "answer_relation_to_question": [
                    2.321428571428571,
                    2.3015873015873014,
                    0.3769841269841269
                ],
                "word_count_appended": [
                    712.0,
                    502.0,
                    235.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these would an oologist study?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ostrich egg"
            ],
            "lines": [
                [
                    0.0,
                    0,
                    0.036031999610958945,
                    0.3367300813824786,
                    0.056074766355140186,
                    0.08895265423242468,
                    0.49590340664079346,
                    0.05420560747663551,
                    0.0,
                    0.15463917525773196,
                    0.52,
                    0.21302180685358255,
                    0,
                    0,
                    -1.0
                ],
                [
                    1.0,
                    0,
                    0.34105873400227965,
                    0.44847547374738744,
                    0.049399198931909215,
                    0.5117168818747011,
                    0.016817593790426907,
                    0.048598130841121495,
                    0.08333333333333333,
                    0.21649484536082475,
                    0.4,
                    0.2995638629283489,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.0,
                    0,
                    0.6229092663867615,
                    0.21479444487013397,
                    0.8945260347129506,
                    0.39933046389287424,
                    0.4872789995687796,
                    0.897196261682243,
                    0.9166666666666666,
                    0.6288659793814433,
                    0.08,
                    0.48741433021806857,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "ice cave": 0.17777813616452234,
                "human liver": 0.3104961868009393,
                "ostrich egg": 0.5117256770345383
            },
            "question": "which of these would an oologist study?",
            "rate_limited": false,
            "answers": [
                "ice cave",
                "human liver",
                "ostrich egg"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "ice cave": 0.12420513924514831,
                "human liver": 0.32505035480367345,
                "ostrich egg": 0.5813210385815812
            },
            "integer_answers": {
                "ice cave": 2,
                "human liver": 3,
                "ostrich egg": 6
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.4260436137071651,
                    0.5991277258566978,
                    0.9748286604361371
                ],
                "result_count_important_words": [
                    2300.0,
                    78.0,
                    2260.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.08333333333333333,
                    0.9166666666666666
                ],
                "word_count_appended_bing": [
                    13.0,
                    10.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.008457301184535027,
                    0.08005207777023315,
                    0.1462070196866989
                ],
                "result_count_noun_chunks": [
                    87.0,
                    78.0,
                    1440.0
                ],
                "question_answer_similarity": [
                    3.237850099802017,
                    4.3123452216386795,
                    2.0653700195252895
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    186000.0,
                    1070000.0,
                    835000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    84.0,
                    74.0,
                    1340.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_appended": [
                    15.0,
                    21.0,
                    61.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What book that heavily influenced \u201cThe Matrix\u201d makes a cameo in the movie?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "neuromancer"
            ],
            "lines": [
                [
                    0.5754636950851462,
                    0.6901960784313727,
                    0.5171811896341882,
                    -0.1829135133320409,
                    0.6088549979310752,
                    0.2687425624752082,
                    0.48159927305770106,
                    0.5227830127791403,
                    0.8431818181818183,
                    0.9556962025316456,
                    0.9090909090909091,
                    0.3485568346912764,
                    0,
                    1.0,
                    1.0
                ],
                [
                    0.1507528643806246,
                    0.07973856209150328,
                    0.3403488317968572,
                    0.20680081164226882,
                    0.37772654726015253,
                    0.05097183657278857,
                    0.48159927305770106,
                    0.4375887440299471,
                    0.030303030303030304,
                    0.0,
                    0.0,
                    0.3636825069497388,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.2737834405342292,
                    0.23006535947712417,
                    0.14246997856895455,
                    0.976112701689772,
                    0.013418454808772242,
                    0.6802856009520032,
                    0.03680145388459791,
                    0.03962824319091261,
                    0.1265151515151515,
                    0.04430379746835443,
                    0.09090909090909091,
                    0.2877606583589848,
                    0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "simulacra & simulation": 0.19380869292958558,
                "neuromancer": 0.5798794661967261,
                "gravity's rainbow": 0.22631184087368825
            },
            "question": "what book that heavily influenced \u201cthe matrix\u201d makes a cameo in the movie?",
            "rate_limited": false,
            "answers": [
                "neuromancer",
                "simulacra & simulation",
                "gravity's rainbow"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "simulacra & simulation": 0.25589760633838304,
                "neuromancer": 0.6522136761017001,
                "gravity's rainbow": 0.16845805914882833
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.439897842838935,
                    2.5457775486481715,
                    2.0143246085128936
                ],
                "result_count_important_words": [
                    1060.0,
                    1060.0,
                    81.0
                ],
                "wikipedia_search": [
                    5.0590909090909095,
                    0.18181818181818182,
                    0.759090909090909
                ],
                "word_count_appended_bing": [
                    60.0,
                    0.0,
                    6.0
                ],
                "answer_relation_to_question_bing": [
                    2.070588235294118,
                    0.23921568627450981,
                    0.6901960784313725
                ],
                "cosine_similarity_raw": [
                    0.11165349930524826,
                    0.07347741723060608,
                    0.030757637694478035
                ],
                "result_count_noun_chunks": [
                    40500.0,
                    33900.0,
                    3070.0
                ],
                "question_answer_similarity": [
                    -2.136249199975282,
                    2.4152292544022202,
                    11.400032398290932
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    4.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    27100.0,
                    5140.0,
                    68600.0
                ],
                "word_count_appended": [
                    302.0,
                    0.0,
                    14.0
                ],
                "answer_relation_to_question": [
                    2.877318475425731,
                    0.753764321903123,
                    1.368917202671146
                ],
                "result_count": [
                    10300.0,
                    6390.0,
                    227.0
                ]
            },
            "integer_answers": {
                "simulacra & simulation": 1,
                "neuromancer": 10,
                "gravity's rainbow": 2
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Occasionally found in print in the 1800s, what word was brought into widespread use by the telephone?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "hello"
            ],
            "question": "occasionally found in print in the 1800s, what word was brought into widespread use by the telephone?",
            "lines": [
                [
                    0.5228234315712875,
                    0.45314833250373315,
                    0.5056415291025421,
                    0.32684363107756037,
                    0.8924395946999221,
                    0.3333333333333333,
                    0.5310457516339869,
                    0.9052404285358359,
                    0.6659901118917513,
                    0.3657587548638132,
                    0.4479166666666667,
                    0.32536556461510846,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.23126461874317794,
                    0.10701841712294674,
                    0.1046217997178061,
                    0.3705126301314678,
                    0.05465705378020265,
                    0.3333333333333333,
                    0.04411764705882353,
                    0.047919608005979566,
                    0.16716107207910486,
                    0.32879377431906615,
                    0.28125,
                    0.3423647476532051,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.2459119496855346,
                    0.43983325037332005,
                    0.38973667117965183,
                    0.30264373879097184,
                    0.05290335151987529,
                    0.3333333333333333,
                    0.42483660130718953,
                    0.04683996345818454,
                    0.1668488160291439,
                    0.30544747081712065,
                    0.2708333333333333,
                    0.33226968773168636,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "hello",
                "chat",
                "goodbye"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "chat": 0.2520051827360793,
                "hello": 0.6810812866504973,
                "goodbye": 0.24386601953074666
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.9282900815359763,
                    3.081282728878846,
                    2.9904271895851773
                ],
                "result_count_important_words": [
                    1950000.0,
                    162000.0,
                    1560000.0
                ],
                "wikipedia_search": [
                    3.3299505594587564,
                    0.8358053603955242,
                    0.8342440801457195
                ],
                "word_count_appended_bing": [
                    43.0,
                    27.0,
                    26.0
                ],
                "answer_relation_to_question_bing": [
                    1.8125933300149326,
                    0.42807366849178696,
                    1.7593330014932802
                ],
                "cosine_similarity_raw": [
                    0.1212891936302185,
                    0.025095829740166664,
                    0.09348687529563904
                ],
                "result_count_noun_chunks": [
                    1090000.0,
                    57700.0,
                    56400.0
                ],
                "question_answer_similarity": [
                    3.479793258011341,
                    3.9447222761809826,
                    3.222145214676857
                ],
                "word_count_noun_chunks": [
                    12.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    1600000.0,
                    1600000.0,
                    1600000.0
                ],
                "word_count_raw": [
                    7.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    916000.0,
                    56100.0,
                    54300.0
                ],
                "answer_relation_to_question": [
                    2.6141171578564375,
                    1.1563230937158897,
                    1.229559748427673
                ],
                "word_count_appended": [
                    188.0,
                    169.0,
                    157.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these states does NOT touch the Mason-Dixon Line?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "west virginia"
            ],
            "lines": [
                [
                    0.417973674453096,
                    0.3055419405681774,
                    0.325086984399501,
                    0.1962000328379232,
                    0.3821039903264813,
                    0.2534562211981567,
                    0.36495271867612294,
                    0.3509719222462203,
                    0.3337639783124365,
                    0.4400998336106489,
                    0.45108695652173914,
                    0.340623102847155,
                    0.4107142857142857,
                    0.4166666666666667,
                    -1.0
                ],
                [
                    0.16905821282906935,
                    0.3687879617883366,
                    0.24179677976234842,
                    0.42186295554480047,
                    0.3778718258766627,
                    0.37327188940092165,
                    0.33008274231678486,
                    0.37365010799136067,
                    0.3209575849994352,
                    0.2512479201331115,
                    0.23369565217391303,
                    0.3214928209828667,
                    0.125,
                    0.08333333333333331,
                    -1.0
                ],
                [
                    0.4129681127178346,
                    0.325670097643486,
                    0.43311623583815057,
                    0.3819370116172764,
                    0.2400241837968561,
                    0.37327188940092165,
                    0.3049645390070922,
                    0.275377969762419,
                    0.3452784366881283,
                    0.3086522462562396,
                    0.31521739130434784,
                    0.3378840761699783,
                    0.4642857142857143,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "tennessee": 0.2830502993587936,
                "delaware": 0.42969860183815084,
                "west virginia": 0.28725109880305555
            },
            "question": "which of these states does not touch the mason-dixon line?",
            "rate_limited": false,
            "answers": [
                "west virginia",
                "delaware",
                "tennessee"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "tennessee": 0.30093639514780524,
                "delaware": 0.1519780347309273,
                "west virginia": 0.4578245978444114
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.5937689715284498,
                    1.785071790171333,
                    1.621159238300217
                ],
                "result_count_important_words": [
                    914000.0,
                    1150000.0,
                    1320000.0
                ],
                "wikipedia_search": [
                    0.6649440867502541,
                    0.7161696600022591,
                    0.6188862532474868
                ],
                "word_count_appended_bing": [
                    9.0,
                    49.0,
                    34.0
                ],
                "answer_relation_to_question_bing": [
                    1.1667483565909353,
                    0.7872722292699805,
                    1.0459794141390844
                ],
                "cosine_similarity_raw": [
                    0.0861324816942215,
                    0.1271471083164215,
                    0.032935597002506256
                ],
                "result_count_noun_chunks": [
                    138000.0,
                    117000.0,
                    208000.0
                ],
                "question_answer_similarity": [
                    5.432002059184015,
                    1.3971054386347532,
                    2.110989023465663
                ],
                "word_count_noun_chunks": [
                    5.0,
                    21.0,
                    2.0
                ],
                "word_count_raw": [
                    1.0,
                    5.0,
                    0.0
                ],
                "result_count_bing": [
                    642000.0,
                    330000.0,
                    330000.0
                ],
                "word_count_appended": [
                    72.0,
                    299.0,
                    230.0
                ],
                "answer_relation_to_question": [
                    0.6562106043752318,
                    2.647534297367445,
                    0.696255098257323
                ],
                "result_count": [
                    58500.0,
                    60600.0,
                    129000.0
                ]
            },
            "integer_answers": {
                "tennessee": 3,
                "delaware": 8,
                "west virginia": 3
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these startups is based in Germany?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "soundcloud"
            ],
            "question": "which of these startups is based in germany?",
            "lines": [
                [
                    0.39999999999999997,
                    0.0,
                    0.2760065819526924,
                    0.6365932703184416,
                    0.2801608579088472,
                    0.2866857551896922,
                    0.1958266452648475,
                    0.8086680761099366,
                    0.0,
                    0.2642169728783902,
                    0.3163265306122449,
                    0.28844902578660536,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.5666666666666667,
                    0.875,
                    0.5317116605201683,
                    0.11564919109210263,
                    0.5730563002680965,
                    0.2945597709377237,
                    0.29373996789727125,
                    0.0755813953488372,
                    0.42857142857142855,
                    0.3910761154855643,
                    0.35714285714285715,
                    0.4024824778821785,
                    0,
                    1.0,
                    -1.0
                ],
                [
                    0.03333333333333333,
                    0.125,
                    0.1922817575271393,
                    0.24775753858945573,
                    0.1467828418230563,
                    0.41875447387258413,
                    0.5104333868378812,
                    0.11575052854122622,
                    0.5714285714285714,
                    0.3447069116360455,
                    0.32653061224489793,
                    0.3090684963312162,
                    0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "zola",
                "soundcloud",
                "roku"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "zola": 0.045149096837227835,
                "roku": 0.133754006498315,
                "soundcloud": 0.6441961983902615
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.865347077359816,
                    1.2074474336465353,
                    0.9272054889936485
                ],
                "result_count_important_words": [
                    122000.0,
                    183000.0,
                    318000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.42857142857142855,
                    0.5714285714285714
                ],
                "answer_relation_to_question": [
                    1.2,
                    1.7,
                    0.1
                ],
                "word_count_appended_bing": [
                    31.0,
                    35.0,
                    32.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    1.75,
                    0.25
                ],
                "cosine_similarity_raw": [
                    0.04429653659462929,
                    0.08533486723899841,
                    0.030859466642141342
                ],
                "result_count_noun_chunks": [
                    1530000.0,
                    143000.0,
                    219000.0
                ],
                "question_answer_similarity": [
                    -1.1563779823482037,
                    -0.21007790137082338,
                    -0.45005402341485023
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    8010000.0,
                    8230000.0,
                    11700000.0
                ],
                "word_count_raw": [
                    0.0,
                    2.0,
                    0.0
                ],
                "result_count": [
                    418000.0,
                    855000.0,
                    219000.0
                ],
                "word_count_appended": [
                    302.0,
                    447.0,
                    394.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "One of Tupac Shakur\u2019s biggest posthumous hits samples what singer?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "bruce hornsby"
            ],
            "lines": [
                [
                    0.46698005698005696,
                    0.5317460317460317,
                    0.26459023970296947,
                    0.45633501858049685,
                    0.025604551920341393,
                    0.6239799121155053,
                    0.05934959349593496,
                    0.1415929203539823,
                    0.36768018018018017,
                    0.18811881188118812,
                    0.11764705882352941,
                    0.22859529643311194,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.12703703703703703,
                    0.4047619047619048,
                    0.19540915839806128,
                    0.36276600365520895,
                    0.23826458036984352,
                    0.07470182046453233,
                    0.532520325203252,
                    0.5162241887905604,
                    0.4671546546546546,
                    0.12871287128712872,
                    0.11764705882352941,
                    0.31580469351932094,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.405982905982906,
                    0.06349206349206349,
                    0.5400006018989693,
                    0.18089897776429417,
                    0.7361308677098151,
                    0.3013182674199623,
                    0.408130081300813,
                    0.3421828908554572,
                    0.16516516516516516,
                    0.6831683168316832,
                    0.7647058823529411,
                    0.4556000100475671,
                    0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "john mellencamp": 0.2677695613050026,
                "bruce hornsby": 0.46513661775551046,
                "christopher cross": 0.26709382093948686
            },
            "question": "one of tupac shakur\u2019s biggest posthumous hits samples what singer?",
            "rate_limited": false,
            "answers": [
                "christopher cross",
                "john mellencamp",
                "bruce hornsby"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "john mellencamp": 0.1287238274202652,
                "bruce hornsby": 0.6197955968384589,
                "christopher cross": 0.18242109037754156
            },
            "integer_answers": {
                "john mellencamp": 3,
                "bruce hornsby": 6,
                "christopher cross": 4
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6001670750317836,
                    2.2106328546352465,
                    3.1892000703329697
                ],
                "result_count_important_words": [
                    73.0,
                    655.0,
                    502.0
                ],
                "wikipedia_search": [
                    1.1030405405405406,
                    1.4014639639639639,
                    0.4954954954954955
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    13.0
                ],
                "answer_relation_to_question_bing": [
                    1.5952380952380953,
                    1.2142857142857144,
                    0.19047619047619047
                ],
                "cosine_similarity_raw": [
                    0.04487049952149391,
                    0.03313843533396721,
                    0.09157592803239822
                ],
                "result_count_noun_chunks": [
                    9600.0,
                    35000.0,
                    23200.0
                ],
                "question_answer_similarity": [
                    3.2179248952306807,
                    2.5581068880856037,
                    1.275640265084803
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    99400.0,
                    11900.0,
                    48000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count": [
                    72.0,
                    670.0,
                    2070.0
                ],
                "answer_relation_to_question": [
                    2.334900284900285,
                    0.6351851851851852,
                    2.02991452991453
                ],
                "word_count_appended": [
                    19.0,
                    13.0,
                    69.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "To help first create Maps, Google acquired what company?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "waze"
            ],
            "lines": [
                [
                    0.26692973058689823,
                    0.5939431913116123,
                    0.10454451558148638,
                    0.0041149995010338536,
                    0.08806203671246463,
                    0.33896260554885405,
                    0.3701828766689869,
                    0.5285001476691589,
                    0.10114341399718757,
                    0.3416149068322981,
                    0.4148148148148148,
                    0.32122514180397893,
                    0.0,
                    0.03125,
                    1.0
                ],
                [
                    0.4924483111712738,
                    0.1919172932330827,
                    0.4769973710494258,
                    1.0679859411524217,
                    0.005668459256346024,
                    0.3220747889022919,
                    0.022337530848573057,
                    0.0020673682247058273,
                    0.7860637062877629,
                    0.06388642413487133,
                    0.014814814814814815,
                    0.3238938901683654,
                    0.0,
                    0.09375,
                    1.0
                ],
                [
                    0.2406219582418281,
                    0.21413951545530494,
                    0.4184581133690878,
                    -0.0721009406534556,
                    0.9062695040311893,
                    0.33896260554885405,
                    0.60747959248244,
                    0.46943248410613525,
                    0.11279287971504952,
                    0.5944986690328306,
                    0.5703703703703704,
                    0.3548809680276557,
                    1.0,
                    0.875,
                    1.0
                ]
            ],
            "fraction_answers": {
                "mapquest": 0.25037774150205533,
                "waze": 0.4736289799805207,
                "where 2 technologies": 0.2759932785174239
            },
            "question": "to help first create maps, google acquired what company?",
            "rate_limited": false,
            "answers": [
                "mapquest",
                "where 2 technologies",
                "waze"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "mapquest": 0.2920060920920267,
                "waze": 0.3772338305751047,
                "where 2 technologies": 0.11348671013722832
            },
            "integer_answers": {
                "mapquest": 3,
                "waze": 7,
                "where 2 technologies": 4
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9273508508238735,
                    1.9433633410101925,
                    2.129285808165934
                ],
                "result_count_important_words": [
                    117000.0,
                    7060.0,
                    192000.0
                ],
                "wikipedia_search": [
                    0.6068604839831254,
                    4.716382237726577,
                    0.6767572782902971
                ],
                "word_count_appended_bing": [
                    56.0,
                    2.0,
                    77.0
                ],
                "answer_relation_to_question_bing": [
                    3.563659147869674,
                    1.1515037593984963,
                    1.2848370927318296
                ],
                "cosine_similarity_raw": [
                    0.06674104928970337,
                    0.3045143485069275,
                    0.26714298129081726
                ],
                "result_count_noun_chunks": [
                    1700000.0,
                    6650.0,
                    1510000.0
                ],
                "question_answer_similarity": [
                    0.036335155775304884,
                    9.430240642279387,
                    -0.6366462279111147
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    4.0
                ],
                "result_count_bing": [
                    56200000.0,
                    53400000.0,
                    56200000.0
                ],
                "word_count_raw": [
                    1.0,
                    3.0,
                    28.0
                ],
                "result_count": [
                    103000.0,
                    6630.0,
                    1060000.0
                ],
                "answer_relation_to_question": [
                    1.6015783835213893,
                    2.9546898670276427,
                    1.4437317494509687
                ],
                "word_count_appended": [
                    385.0,
                    72.0,
                    670.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which alien race was created by sci-fi legend Douglas Adams?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "vogons"
            ],
            "question": "which alien race was created by sci-fi legend douglas adams?",
            "lines": [
                [
                    0.49087238196663113,
                    0.6135590599876314,
                    0.8320740479556858,
                    0,
                    0.6362922230950511,
                    0.3333333333333333,
                    0.634020618556701,
                    0.9597314642996443,
                    0.2188917575062153,
                    0.6935483870967742,
                    0.38636363636363635,
                    0.6158683207294112,
                    0,
                    1.0,
                    -1.0
                ],
                [
                    0.2992944621938232,
                    0.09891774891774892,
                    0.04133570878941859,
                    0,
                    0.3587326525268395,
                    0.3333333333333333,
                    0.3556701030927835,
                    0.0320304466533502,
                    0.6602859055268694,
                    0.1774193548387097,
                    0.3068181818181818,
                    0.2579118382643037,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.20983315583954562,
                    0.2875231910946197,
                    0.12659024325489554,
                    0,
                    0.004975124378109453,
                    0.3333333333333333,
                    0.010309278350515464,
                    0.008238089047005567,
                    0.12082233696691527,
                    0.12903225806451613,
                    0.3068181818181818,
                    0.12621984100628514,
                    0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "vogons",
                "snarfs",
                "ovions"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "snarfs": 0.06584909848149813,
                "vogons": 1.004722996006491,
                "ovions": 0.016605709195842627
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    4.926946565835289,
                    2.063294706114429,
                    1.0097587280502809
                ],
                "result_count_important_words": [
                    2460.0,
                    1380.0,
                    40.0
                ],
                "wikipedia_search": [
                    1.5322423025435072,
                    4.622001338688086,
                    0.8457563587684069
                ],
                "word_count_appended_bing": [
                    34.0,
                    27.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    4.29491341991342,
                    0.6924242424242424,
                    2.012662337662338
                ],
                "cosine_similarity_raw": [
                    0.17997527122497559,
                    0.008940797299146652,
                    0.027381112799048424
                ],
                "result_count_noun_chunks": [
                    81200.0,
                    2710.0,
                    697.0
                ],
                "question_answer_similarity": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    217000.0,
                    217000.0,
                    217000.0
                ],
                "word_count_raw": [
                    3.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    2430.0,
                    1370.0,
                    19.0
                ],
                "answer_relation_to_question": [
                    3.926979055733049,
                    2.3943556975505857,
                    1.678665246716365
                ],
                "word_count_appended": [
                    215.0,
                    55.0,
                    40.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "The lyrics to \u201cThe Star-Spangled Banner\u201d were written during what conflict?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "the war of 1812"
            ],
            "lines": [
                [
                    0.11505602240896359,
                    0.28287461773700306,
                    0.09189460833883341,
                    0.3798194555148806,
                    0.4361111111111111,
                    0.3679694947569113,
                    0.45979381443298967,
                    0.5508241758241759,
                    0.19607344632768361,
                    0.3048128342245989,
                    0.38461538461538464,
                    0.32653634792600433,
                    0.1,
                    0.0,
                    1.0
                ],
                [
                    0.12686274509803921,
                    0.16258919469928645,
                    0.226469555201115,
                    0.17684281224940673,
                    0.3194444444444444,
                    0.4747378455672069,
                    0.32577319587628867,
                    0.3159340659340659,
                    0.0927401129943503,
                    0.21390374331550802,
                    0.15384615384615385,
                    0.30603759179765394,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.7580812324929972,
                    0.5545361875637105,
                    0.6816358364600515,
                    0.4433377322357126,
                    0.24444444444444444,
                    0.1572926596758818,
                    0.21443298969072164,
                    0.13324175824175824,
                    0.7111864406779661,
                    0.48128342245989303,
                    0.46153846153846156,
                    0.3674260602763417,
                    0.9,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "american revolution": 0.20679867578739428,
                "the war of 1812": 0.5077455161255672,
                "the civil war": 0.2854558080870386
            },
            "question": "the lyrics to \u201cthe star-spangled banner\u201d were written during what conflict?",
            "rate_limited": false,
            "answers": [
                "the civil war",
                "american revolution",
                "the war of 1812"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "american revolution": 0.07585346914536653,
                "the war of 1812": 0.776432132067992,
                "the civil war": 0.060160287364955066
            },
            "integer_answers": {
                "american revolution": 1,
                "the war of 1812": 10,
                "the civil war": 3
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9592180875560261,
                    1.8362255507859235,
                    2.2045563616580504
                ],
                "result_count_important_words": [
                    223000.0,
                    158000.0,
                    104000.0
                ],
                "wikipedia_search": [
                    1.1764406779661016,
                    0.5564406779661017,
                    4.267118644067796
                ],
                "word_count_appended_bing": [
                    5.0,
                    2.0,
                    6.0
                ],
                "answer_relation_to_question_bing": [
                    0.8486238532110092,
                    0.48776758409785936,
                    1.6636085626911314
                ],
                "cosine_similarity_raw": [
                    0.03359289467334747,
                    0.08278796821832657,
                    0.24917806684970856
                ],
                "result_count_noun_chunks": [
                    802000.0,
                    460000.0,
                    194000.0
                ],
                "question_answer_similarity": [
                    14.88093376904726,
                    6.928518638014793,
                    17.369514212419745
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    18.0
                ],
                "result_count_bing": [
                    386000.0,
                    498000.0,
                    165000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    8.0
                ],
                "result_count": [
                    157000.0,
                    115000.0,
                    88000.0
                ],
                "answer_relation_to_question": [
                    0.575280112044818,
                    0.634313725490196,
                    3.790406162464986
                ],
                "word_count_appended": [
                    57.0,
                    40.0,
                    90.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Where do marsupials keep their undeveloped young?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "in their pouches"
            ],
            "lines": [
                [
                    0.984375,
                    0.9696969696969697,
                    0.9653586890029502,
                    0.4179999846109188,
                    0.00032759069642422157,
                    0.7814761215629522,
                    0.0004294784738424394,
                    0.197179294244757,
                    0,
                    0.08163265306122448,
                    0.275,
                    0.26855823992560385,
                    0,
                    0,
                    3.0
                ],
                [
                    0.015625,
                    0.0,
                    0.018044688514824006,
                    0.48067960832710066,
                    0.00010079713736129893,
                    0.022534628902212115,
                    0.00016250536848092304,
                    0.00021222820319885782,
                    0,
                    0.030612244897959183,
                    0.05,
                    0.10265070217179645,
                    0,
                    0,
                    3.0
                ],
                [
                    0.0,
                    0.030303030303030304,
                    0.016596622482225848,
                    0.10132040706198056,
                    0.9995716121662145,
                    0.19598924953483565,
                    0.9994080161576766,
                    0.8026084775520441,
                    0,
                    0.8877551020408163,
                    0.675,
                    0.6287910579025997,
                    0,
                    0,
                    3.0
                ]
            ],
            "fraction_answers": {
                "in their pouches": 0.44927582011596756,
                "underwater": 0.48521305229103856,
                "in a paper bag": 0.06551112759299396
            },
            "question": "where do marsupials keep their undeveloped young?",
            "rate_limited": false,
            "answers": [
                "in their pouches",
                "in a paper bag",
                "underwater"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "in their pouches": 0.3811610440003659,
                "underwater": 0.3781749382740332,
                "in a paper bag": 0.14130421857663164
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 3
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.8056747197768115,
                    0.30795210651538935,
                    1.8863731737077991
                ],
                "result_count_important_words": [
                    37.0,
                    14.0,
                    86100.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    11.0,
                    2.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    2.909090909090909,
                    0.0,
                    0.09090909090909091
                ],
                "cosine_similarity_raw": [
                    0.5574354529380798,
                    0.010419701226055622,
                    0.009583531878888607
                ],
                "result_count_noun_chunks": [
                    51100.0,
                    55.0,
                    208000.0
                ],
                "question_answer_similarity": [
                    6.696236303076148,
                    7.700345363467932,
                    1.6231229975819588
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    378000.0,
                    10900.0,
                    94800.0
                ],
                "word_count_appended": [
                    16.0,
                    6.0,
                    174.0
                ],
                "answer_relation_to_question": [
                    1.96875,
                    0.03125,
                    0.0
                ],
                "result_count": [
                    39.0,
                    12.0,
                    119000.0
                ]
            },
            "integer_answers": {
                "in their pouches": 4,
                "underwater": 6,
                "in a paper bag": 1
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Mount Rushmore was named after a person with what profession?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "lawyer"
            ],
            "lines": [
                [
                    0.24763071895424835,
                    0.09375,
                    0.19266510696689385,
                    0.15028102021251624,
                    0.17818453492715727,
                    0.28401360544217685,
                    0.024766715779134217,
                    0.051794638800545204,
                    0.265625,
                    0.26957637997432604,
                    0.3333333333333333,
                    0.30312260311393113,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.3715686274509804,
                    0.3269230769230769,
                    0.66460097872643,
                    0.4813700883683983,
                    0.3772880089652596,
                    0.20578231292517007,
                    0.5472532098505578,
                    0.558836892321672,
                    0.6437190594059405,
                    0.337612323491656,
                    0.34408602150537637,
                    0.3756742722977804,
                    0.0,
                    1.0,
                    1.0
                ],
                [
                    0.3808006535947712,
                    0.5793269230769231,
                    0.14273391430667615,
                    0.36834889141908544,
                    0.4445274561075831,
                    0.5102040816326531,
                    0.427980074370308,
                    0.3893684688777828,
                    0.0906559405940594,
                    0.392811296534018,
                    0.3225806451612903,
                    0.32120312458828854,
                    1.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "architect": 0.3836101050188171,
                "prospector": 0.17105311839316162,
                "lawyer": 0.44533677658802134
            },
            "question": "mount rushmore was named after a person with what profession?",
            "rate_limited": false,
            "answers": [
                "prospector",
                "lawyer",
                "architect"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "architect": 0.23131395501586682,
                "prospector": 0.17624398372632563,
                "lawyer": 0.5991751911620458
            },
            "integer_answers": {
                "architect": 6,
                "prospector": 0,
                "lawyer": 8
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.5156130155696557,
                    1.878371361488902,
                    1.6060156229414426
                ],
                "result_count_important_words": [
                    706.0,
                    15600.0,
                    12200.0
                ],
                "wikipedia_search": [
                    1.0625,
                    2.574876237623762,
                    0.3626237623762376
                ],
                "answer_relation_to_question": [
                    0.9905228758169934,
                    1.4862745098039216,
                    1.5232026143790849
                ],
                "word_count_appended_bing": [
                    31.0,
                    32.0,
                    30.0
                ],
                "answer_relation_to_question_bing": [
                    0.1875,
                    0.6538461538461539,
                    1.1586538461538463
                ],
                "cosine_similarity_raw": [
                    0.04947086423635483,
                    0.1706504374742508,
                    0.03664996847510338
                ],
                "result_count_noun_chunks": [
                    1140000.0,
                    12300000.0,
                    8570000.0
                ],
                "question_answer_similarity": [
                    0.9013332910835743,
                    2.887090368196368,
                    2.209228537976742
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    3.0
                ],
                "result_count_bing": [
                    835000.0,
                    605000.0,
                    1500000.0
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_appended": [
                    210.0,
                    263.0,
                    306.0
                ],
                "result_count": [
                    47700.0,
                    101000.0,
                    119000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which verb describes the sound minerals make when they are heated?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "decrepitate"
            ],
            "lines": [
                [
                    0.8714574898785425,
                    1.0,
                    0.7203094327513677,
                    0,
                    1.0,
                    0.3355119825708061,
                    1.0,
                    1.0,
                    1.0,
                    0.7217391304347827,
                    0.5137614678899083,
                    0.7354746488654065,
                    1.0,
                    0,
                    2.0
                ],
                [
                    0.025809716599190284,
                    0.0,
                    0.07812464411946665,
                    0,
                    0.0,
                    0.3311546840958606,
                    0.0,
                    0.0,
                    0.0,
                    0.1391304347826087,
                    0.24770642201834864,
                    0.13226267556729676,
                    0.0,
                    0,
                    2.0
                ],
                [
                    0.10273279352226722,
                    0.0,
                    0.20156592312916558,
                    0,
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.0,
                    0.0,
                    0.1391304347826087,
                    0.23853211009174313,
                    0.13226267556729676,
                    0.0,
                    0,
                    2.0
                ]
            ],
            "fraction_answers": {
                "frangelle": 0.07951571476523096,
                "decrepitate": 0.8248545126992345,
                "recleft": 0.09562977253553456
            },
            "question": "which verb describes the sound minerals make when they are heated?",
            "rate_limited": false,
            "answers": [
                "decrepitate",
                "frangelle",
                "recleft"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "frangelle": 0.1308211994649323,
                "decrepitate": 0.6775951740413777,
                "recleft": 0.1308211994649323
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 2
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.677373244327032,
                    0.6613133778364838,
                    0.6613133778364838
                ],
                "result_count_important_words": [
                    4240.0,
                    0,
                    0
                ],
                "wikipedia_search": [
                    2.0,
                    0.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    56.0,
                    27.0,
                    26.0
                ],
                "answer_relation_to_question_bing": [
                    4.0,
                    0.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.15951946377754211,
                    0.01730145514011383,
                    0.04463871568441391
                ],
                "result_count_noun_chunks": [
                    4440.0,
                    0,
                    0
                ],
                "question_answer_similarity": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_noun_chunks": [
                    27.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    154000.0,
                    152000.0,
                    153000.0
                ],
                "word_count_appended": [
                    83.0,
                    16.0,
                    16.0
                ],
                "answer_relation_to_question": [
                    3.48582995951417,
                    0.10323886639676114,
                    0.4109311740890689
                ],
                "result_count": [
                    4220.0,
                    0,
                    0
                ]
            },
            "integer_answers": {
                "frangelle": 0,
                "decrepitate": 12,
                "recleft": 0
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What was the name of the dog on the television show \u201c7th Heaven\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "happy"
            ],
            "lines": [
                [
                    0.5,
                    0.3333333333333333,
                    0.07470969559012156,
                    0.16834532708018576,
                    0.2257495590828924,
                    0.12229004420122079,
                    0.0609189826044771,
                    0.19791425260718423,
                    0.0,
                    0.3328550932568149,
                    0.24647887323943662,
                    0.2995759358810546,
                    0.016666666666666666,
                    0.0,
                    1.0
                ],
                [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.11318735665200726,
                    0.1812326207636048,
                    0.27336860670194,
                    0.21048200378867607,
                    0.7207706701781135,
                    0.0813441483198146,
                    1.0,
                    0.25538020086083213,
                    0.3591549295774648,
                    0.27483445352285435,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.16666666666666666,
                    0.3333333333333333,
                    0.8121029477578712,
                    0.6504220521562094,
                    0.5008818342151675,
                    0.6672279520101031,
                    0.21831034721740938,
                    0.7207415990730012,
                    0.0,
                    0.4117647058823529,
                    0.39436619718309857,
                    0.4255896105960912,
                    0.9833333333333333,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "comet": 0.29545868978799816,
                "eddie": 0.18420269739595627,
                "happy": 0.5203386128160457
            },
            "question": "what was the name of the dog on the television show \u201c7th heaven\u201d?",
            "rate_limited": false,
            "answers": [
                "eddie",
                "comet",
                "happy"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "comet": 0.1499273880156206,
                "eddie": 0.23541647334745136,
                "happy": 0.7745553692078216
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.198303743524218,
                    1.0993378140914172,
                    1.7023584423843645
                ],
                "result_count_important_words": [
                    87900.0,
                    1040000.0,
                    315000.0
                ],
                "wikipedia_search": [
                    0.0,
                    2.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    35.0,
                    51.0,
                    56.0
                ],
                "answer_relation_to_question_bing": [
                    1.0,
                    1.0,
                    1.0
                ],
                "cosine_similarity_raw": [
                    0.032556284219026566,
                    0.04932371526956558,
                    0.3538905382156372
                ],
                "result_count_noun_chunks": [
                    85400.0,
                    35100.0,
                    311000.0
                ],
                "question_answer_similarity": [
                    1.3901408510282636,
                    1.4965599225834012,
                    5.370973348617554
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    59.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    44.0
                ],
                "result_count_bing": [
                    581000.0,
                    1000000.0,
                    3170000.0
                ],
                "word_count_appended": [
                    232.0,
                    178.0,
                    287.0
                ],
                "answer_relation_to_question": [
                    1.5,
                    1.0,
                    0.5
                ],
                "result_count": [
                    128000.0,
                    155000.0,
                    284000.0
                ]
            },
            "integer_answers": {
                "comet": 2,
                "eddie": 2,
                "happy": 10
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which two countries have almost perfectly identical flags?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "indonesia / monaco"
            ],
            "lines": [
                [
                    0.07814207650273224,
                    0.22727272727272727,
                    0.18648432704347195,
                    0.16941492720506374,
                    0.29508196721311475,
                    0.11865631208235507,
                    0.4010207801676996,
                    0.2857142857142857,
                    0.00315955766192733,
                    0.17647058823529413,
                    0.3333333333333333,
                    0.30020775351038576,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.15100182149362476,
                    0.045454545454545456,
                    0.2097497344410357,
                    0.48353237953530304,
                    0.39344262295081966,
                    0.6429474444645115,
                    0.5723660226029894,
                    0.38095238095238093,
                    0.00315955766192733,
                    0.17647058823529413,
                    0.3333333333333333,
                    0.32209508703240725,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.770856102003643,
                    0.7272727272727273,
                    0.6037659385154923,
                    0.3470526932596332,
                    0.3114754098360656,
                    0.23839624345313346,
                    0.026613197229310975,
                    0.3333333333333333,
                    0.9936808846761452,
                    0.6470588235294118,
                    0.3333333333333333,
                    0.3776971594572071,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "luxembourg / russia": 0.30954212651318097,
                "indonesia / monaco": 0.4758779871582864,
                "armenia / romania": 0.21457988632853262
            },
            "question": "which two countries have almost perfectly identical flags?",
            "rate_limited": false,
            "answers": [
                "armenia / romania",
                "luxembourg / russia",
                "indonesia / monaco"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "luxembourg / russia": 0.058267561365857194,
                "indonesia / monaco": 0.8770063378678692,
                "armenia / romania": 0.08858325755959118
            },
            "integer_answers": {
                "luxembourg / russia": 5,
                "indonesia / monaco": 6,
                "armenia / romania": 1
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2008310140415428,
                    1.2883803481296288,
                    1.5107886378288282
                ],
                "result_count_important_words": [
                    1100.0,
                    1570.0,
                    73.0
                ],
                "wikipedia_search": [
                    0.009478672985781991,
                    0.009478672985781991,
                    2.9810426540284363
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.45454545454545453,
                    0.09090909090909091,
                    1.4545454545454546
                ],
                "cosine_similarity_raw": [
                    0.03942738473415375,
                    0.044346265494823456,
                    0.12765100598335266
                ],
                "result_count_noun_chunks": [
                    18.0,
                    24.0,
                    21.0
                ],
                "question_answer_similarity": [
                    0.5158121178392321,
                    1.4721953067928553,
                    1.0566600456368178
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    65700.0,
                    356000.0,
                    132000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    3.0,
                    3.0,
                    11.0
                ],
                "answer_relation_to_question": [
                    0.31256830601092894,
                    0.604007285974499,
                    3.083424408014572
                ],
                "result_count": [
                    18.0,
                    24.0,
                    19.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these is a common piece of playground equipment?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "monkey bars"
            ],
            "question": "which of these is a common piece of playground equipment?",
            "lines": [
                [
                    0.5604166666666667,
                    0.9444444444444445,
                    0.8496701089524507,
                    0.3869770916555688,
                    0.9998843109061761,
                    0.36726384364820847,
                    0.9999273980107055,
                    0.9998276159282882,
                    0.7568027210884354,
                    0.9272727272727272,
                    0.6923076923076923,
                    0.8782603746153956,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.1527777777777778,
                    0.05555555555555555,
                    0.07326876212841349,
                    0.3883870709267776,
                    9.089857371874329e-05,
                    0.31921824104234525,
                    5.2801446759641216e-05,
                    0.00014480262023789002,
                    0.21258503401360543,
                    0.03636363636363636,
                    0.15384615384615385,
                    0.06483672745964238,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.28680555555555554,
                    0.0,
                    0.07706112891913582,
                    0.22463583741765356,
                    2.4790520105111805e-05,
                    0.3135179153094462,
                    1.9800542534865456e-05,
                    2.7581451473883813e-05,
                    0.030612244897959183,
                    0.03636363636363636,
                    0.15384615384615385,
                    0.05690289792496202,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "monkey bars",
                "mosquito box",
                "mongoose pit"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "monkey bars": 0.9876379308623414,
                "mosquito box": 0.0098752922135764,
                "mongoose pit": 0.09697848126691634
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.5130414984615825,
                    0.2593469098385695,
                    0.22761159169984807
                ],
                "result_count_important_words": [
                    303000.0,
                    16.0,
                    6.0
                ],
                "wikipedia_search": [
                    3.0272108843537415,
                    0.8503401360544217,
                    0.12244897959183673
                ],
                "word_count_appended_bing": [
                    9.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    2.8333333333333335,
                    0.16666666666666666,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.14071935415267944,
                    0.012134512886404991,
                    0.012762591242790222
                ],
                "result_count_noun_chunks": [
                    145000.0,
                    21.0,
                    4.0
                ],
                "question_answer_similarity": [
                    4.983603470027447,
                    5.001761592924595,
                    2.8929256097762845
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    451000.0,
                    392000.0,
                    385000.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    121000.0,
                    11.0,
                    3.0
                ],
                "answer_relation_to_question": [
                    2.2416666666666667,
                    0.6111111111111112,
                    1.1472222222222221
                ],
                "word_count_appended": [
                    102.0,
                    4.0,
                    4.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "In the original NES \u201cMike Tyson\u2019s Punch-Out!!\u201d, who does Little Mac face right before the final opponent?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "super macho man"
            ],
            "lines": [
                [
                    0.36523377012360814,
                    0.19088635276285318,
                    0.4252266697142876,
                    0.393206579596363,
                    0.42105263157894735,
                    0.2639821029082774,
                    0.4408893709327549,
                    0.11270491803278689,
                    0.0104775828460039,
                    0.3431372549019608,
                    0.17647058823529413,
                    0.323683314704996,
                    0.52,
                    0.2,
                    0.0
                ],
                [
                    0.4810620695910211,
                    0.4170044286942126,
                    0.45769247077795405,
                    0.3606642667217029,
                    0.5252416756176155,
                    0.2639821029082774,
                    0.05368763557483731,
                    0.44774590163934425,
                    0.91973858256753,
                    0.3431372549019608,
                    0.4411764705882353,
                    0.34637095390165856,
                    0.28,
                    0.6,
                    0.0
                ],
                [
                    0.1537041602853708,
                    0.3921092185429342,
                    0.11708085950775837,
                    0.2461291536819341,
                    0.05370569280343716,
                    0.4720357941834452,
                    0.5054229934924078,
                    0.4395491803278688,
                    0.06978383458646617,
                    0.3137254901960784,
                    0.38235294117647056,
                    0.3299457313933454,
                    0.2,
                    0.2,
                    0.0
                ]
            ],
            "fraction_answers": {
                "super macho man": 0.4241074152488821,
                "mr. dream": 0.29906793830986667,
                "mr. sandman": 0.2768246464412512
            },
            "question": "in the original nes \u201cmike tyson\u2019s punch-out!!\u201d, who does little mac face right before the final opponent?",
            "rate_limited": false,
            "answers": [
                "mr. dream",
                "super macho man",
                "mr. sandman"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "super macho man": 0.6374772501228256,
                "mr. dream": 0.26532228872964864,
                "mr. sandman": 0.09989428190398059
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.5605164617549567,
                    3.810080492918245,
                    3.6294030453267996
                ],
                "result_count_important_words": [
                    813.0,
                    99.0,
                    932.0
                ],
                "wikipedia_search": [
                    0.09429824561403508,
                    8.277647243107769,
                    0.6280545112781954
                ],
                "word_count_appended_bing": [
                    6.0,
                    15.0,
                    13.0
                ],
                "answer_relation_to_question_bing": [
                    1.9088635276285317,
                    4.170044286942126,
                    3.921092185429342
                ],
                "cosine_similarity_raw": [
                    0.1723729521036148,
                    0.1855335235595703,
                    0.04746074229478836
                ],
                "result_count_noun_chunks": [
                    2200.0,
                    8740.0,
                    8580.0
                ],
                "question_answer_similarity": [
                    20.357109935488552,
                    18.672327749431133,
                    12.74261037283577
                ],
                "word_count_noun_chunks": [
                    13.0,
                    7.0,
                    5.0
                ],
                "word_count_raw": [
                    1.0,
                    3.0,
                    1.0
                ],
                "result_count_bing": [
                    118000.0,
                    118000.0,
                    211000.0
                ],
                "word_count_appended": [
                    35.0,
                    35.0,
                    32.0
                ],
                "answer_relation_to_question": [
                    4.017571471359689,
                    5.291682765501232,
                    1.6907457631390788
                ],
                "result_count": [
                    784.0,
                    978.0,
                    100.0
                ]
            },
            "integer_answers": {
                "super macho man": 9,
                "mr. dream": 3,
                "mr. sandman": 2
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Who is credited with coining the phrase: \u201cFor whom the bell tolls\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ernest hemingway"
            ],
            "question": "who is credited with coining the phrase: \u201cfor whom the bell tolls\u201d?",
            "lines": [
                [
                    0.2361111111111111,
                    0.297008547008547,
                    0.348547209051515,
                    0.5476221663723629,
                    0.2631578947368421,
                    0.2134453781512605,
                    0.32637571157495254,
                    0.13237063778580024,
                    0.3137254901960784,
                    0.33766233766233766,
                    0.525,
                    0.2866409099675185,
                    0.43636363636363634,
                    0.6428571428571429,
                    0.0
                ],
                [
                    0.375,
                    0.1111111111111111,
                    0.09373102742012508,
                    0.6136205203971605,
                    0.2736842105263158,
                    0.4789915966386555,
                    0.269449715370019,
                    0.4428399518652226,
                    0.6752450980392157,
                    0.18181818181818182,
                    0.175,
                    0.2676354703056908,
                    0.0,
                    0.0,
                    0.0
                ],
                [
                    0.38888888888888884,
                    0.5918803418803419,
                    0.5577217635283599,
                    -0.16124268676952347,
                    0.4631578947368421,
                    0.30756302521008405,
                    0.40417457305502846,
                    0.42478941034897716,
                    0.011029411764705883,
                    0.4805194805194805,
                    0.3,
                    0.44572361972679075,
                    0.5636363636363636,
                    0.35714285714285715,
                    0.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "john donne",
                "william shakespeare",
                "ernest hemingway"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "ernest hemingway": 0.634467197486036,
                "john donne": 0.09476596517458105,
                "william shakespeare": 0.15140567690642276
            },
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.4332045498375925,
                    1.3381773515284539,
                    2.228618098633954
                ],
                "result_count_important_words": [
                    3440.0,
                    2840.0,
                    4260.0
                ],
                "wikipedia_search": [
                    1.2549019607843137,
                    2.700980392156863,
                    0.04411764705882353
                ],
                "word_count_appended_bing": [
                    21.0,
                    7.0,
                    12.0
                ],
                "answer_relation_to_question_bing": [
                    0.891025641025641,
                    0.3333333333333333,
                    1.7756410256410255
                ],
                "cosine_similarity_raw": [
                    0.16659016907215118,
                    0.04479929059743881,
                    0.26656636595726013
                ],
                "result_count_noun_chunks": [
                    110000.0,
                    368000.0,
                    353000.0
                ],
                "question_answer_similarity": [
                    1.90947618894279,
                    2.1396025301655754,
                    -0.5622290147002786
                ],
                "word_count_noun_chunks": [
                    24.0,
                    0.0,
                    31.0
                ],
                "word_count_raw": [
                    9.0,
                    0.0,
                    5.0
                ],
                "result_count_bing": [
                    38100.0,
                    85500.0,
                    54900.0
                ],
                "word_count_appended": [
                    26.0,
                    14.0,
                    37.0
                ],
                "answer_relation_to_question": [
                    0.9444444444444444,
                    1.5,
                    1.5555555555555554
                ],
                "result_count": [
                    25.0,
                    26.0,
                    44.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these do NOT have flippers?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "new yorkers"
            ],
            "lines": [
                [
                    0.5,
                    0,
                    0.3730384513498896,
                    0.299061207536534,
                    0.48055471811878203,
                    0.4080970909764684,
                    0.2815487571701721,
                    0.2815487571701721,
                    0.5,
                    0.44422572178477693,
                    0.4166666666666667,
                    0.3737151248164464,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.5,
                    0,
                    0.18828251621425135,
                    0.3917375172628791,
                    0.03572505275851673,
                    0.1507318880859737,
                    0.3212237093690249,
                    0.3212237093690249,
                    0.4,
                    0.11811023622047245,
                    0.1893939393939394,
                    0.3303964757709251,
                    0.5,
                    0.0,
                    -1.0
                ],
                [
                    0.0,
                    0,
                    0.43867903243585904,
                    0.30920127520058693,
                    0.48372022912270124,
                    0.4411710209375579,
                    0.3972275334608031,
                    0.3972275334608031,
                    0.09999999999999998,
                    0.4376640419947507,
                    0.3939393939393939,
                    0.29588839941262846,
                    0.0,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "new yorkers": 0.1756220776015526,
                "pinball machines": 0.3546586984669101,
                "dolphins": 0.46971922393153737
            },
            "question": "which of these do not have flippers?",
            "rate_limited": false,
            "answers": [
                "new yorkers",
                "dolphins",
                "pinball machines"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "new yorkers": 0.493594252616099,
                "pinball machines": 0.3092522856084977,
                "dolphins": 0.22859587671768583
            },
            "integer_answers": {
                "new yorkers": 3,
                "pinball machines": 4,
                "dolphins": 6
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.2525697503671072,
                    0.3392070484581498,
                    0.40822320117474303
                ],
                "result_count_important_words": [
                    457000.0,
                    374000.0,
                    215000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.2,
                    0.8
                ],
                "word_count_appended_bing": [
                    11.0,
                    41.0,
                    14.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.05516479164361954,
                    0.13544124364852905,
                    0.026643959805369377
                ],
                "result_count_noun_chunks": [
                    457000.0,
                    374000.0,
                    215000.0
                ],
                "question_answer_similarity": [
                    2.8958178497850895,
                    1.5602185428142548,
                    2.7496848478913307
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    4.0
                ],
                "result_count_bing": [
                    9920000.0,
                    37700000.0,
                    6350000.0
                ],
                "word_count_raw": [
                    0.0,
                    6.0,
                    0.0
                ],
                "result_count": [
                    129000.0,
                    3080000.0,
                    108000.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_appended": [
                    85.0,
                    582.0,
                    95.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "In \u201cPeanuts,\u201d what breed of dog is Snoopy?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "beagle"
            ],
            "lines": [
                [
                    0.31523330090105334,
                    0.2967171717171717,
                    0.14459802928745488,
                    0.5113000362162664,
                    0.11110211414141605,
                    0.5833053972510895,
                    0.154627539503386,
                    2.9027593393319743e-05,
                    0.3698524365133837,
                    0.09806157354618016,
                    0.07291666666666667,
                    0.32725024730659974,
                    0.0,
                    0.017543859649122806,
                    1.0
                ],
                [
                    0.0844748085790431,
                    0.2142255892255892,
                    0.16419063488624788,
                    0.21743359478926708,
                    8.09727272556083e-05,
                    0.19678176332551123,
                    0.2832957110609481,
                    0.6634878489901656,
                    0.31762611530542206,
                    0.1949828962371722,
                    0.3125,
                    0.26221871571512967,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.6002918905199035,
                    0.4890572390572391,
                    0.6912113358262972,
                    0.27126636899446643,
                    0.8888169131313284,
                    0.21991283942339926,
                    0.5620767494356659,
                    0.3364831234164411,
                    0.3125214481811942,
                    0.7069555302166477,
                    0.6145833333333334,
                    0.4105310369782706,
                    1.0,
                    0.9824561403508771,
                    1.0
                ]
            ],
            "fraction_answers": {
                "beagle": 0.5775831392046474,
                "pitbull": 0.20794990363155372,
                "border collie": 0.21446695716379893
            },
            "question": "in \u201cpeanuts,\u201d what breed of dog is snoopy?",
            "rate_limited": false,
            "answers": [
                "border collie",
                "pitbull",
                "beagle"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "beagle": 0.7906221929744528,
                "pitbull": 0.08503370439464925,
                "border collie": 0.12693445336246878
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.309000989226399,
                    1.0488748628605187,
                    1.6421241479130824
                ],
                "result_count_important_words": [
                    137000.0,
                    251000.0,
                    498000.0
                ],
                "wikipedia_search": [
                    0.7397048730267674,
                    0.6352522306108441,
                    0.6250428963623884
                ],
                "word_count_appended_bing": [
                    7.0,
                    30.0,
                    59.0
                ],
                "answer_relation_to_question_bing": [
                    0.5934343434343434,
                    0.4284511784511784,
                    0.9781144781144782
                ],
                "cosine_similarity_raw": [
                    0.06967154145240784,
                    0.07911182940006256,
                    0.33304575085639954
                ],
                "result_count_noun_chunks": [
                    49.0,
                    1120000.0,
                    568000.0
                ],
                "question_answer_similarity": [
                    3.995528713800013,
                    1.6991240167990327,
                    2.1197975545364898
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    56.0
                ],
                "result_count_bing": [
                    1740000.0,
                    587000.0,
                    656000.0
                ],
                "word_count_appended": [
                    86.0,
                    171.0,
                    620.0
                ],
                "answer_relation_to_question": [
                    0.9456999027031601,
                    0.2534244257371293,
                    1.8008756715597105
                ],
                "result_count": [
                    118000.0,
                    86.0,
                    944000.0
                ]
            },
            "integer_answers": {
                "beagle": 10,
                "pitbull": 1,
                "border collie": 3
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of the following is a dice-based game?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "farkle"
            ],
            "lines": [
                [
                    0.3883090142527342,
                    0.46782178217821785,
                    0.20038423710254116,
                    -0.0,
                    0.9976120153333752,
                    0.03831319105168424,
                    0.9976089056388656,
                    0.9972516686297606,
                    0.0,
                    0.21513513513513513,
                    0.2191780821917808,
                    0.35061828379368815,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.05788297723781594,
                    0.0,
                    0.02319628529757626,
                    -13.448183182848236,
                    0.0011782819078740652,
                    0.48084340447415785,
                    0.0011084543395987396,
                    0.001287789556340793,
                    0.0,
                    0.2972972972972973,
                    0.19863013698630136,
                    0.30327997294484654,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.5538080085094498,
                    0.5321782178217821,
                    0.7764194775998826,
                    14.448183182848236,
                    0.001209702758750707,
                    0.48084340447415785,
                    0.0012826400215356843,
                    0.0014605418138987044,
                    1.0,
                    0.4875675675675676,
                    0.5821917808219178,
                    0.3461017432614654,
                    1.0,
                    1.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "sparkle": -0.8631056130576024,
                "quirkle": 0.348016593950556,
                "farkle": 1.5150890191070463
            },
            "question": "which of the following is a dice-based game?",
            "rate_limited": false,
            "answers": [
                "quirkle",
                "sparkle",
                "farkle"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sparkle": 0.1260103156444987,
                "quirkle": 0.2911571601508031,
                "farkle": 0.776432132067992
            },
            "integer_answers": {
                "sparkle": 2,
                "quirkle": 4,
                "farkle": 8
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.4024731351747524,
                    1.213119891779386,
                    1.3844069730458615
                ],
                "result_count_important_words": [
                    63000.0,
                    70.0,
                    81.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    4.0
                ],
                "word_count_appended_bing": [
                    32.0,
                    29.0,
                    85.0
                ],
                "answer_relation_to_question_bing": [
                    1.4034653465346536,
                    0.0,
                    1.5965346534653464
                ],
                "cosine_similarity_raw": [
                    0.11361358314752579,
                    0.013151798397302628,
                    0.44021326303482056
                ],
                "result_count_noun_chunks": [
                    63500.0,
                    82.0,
                    93.0
                ],
                "question_answer_similarity": [
                    0.0,
                    1.0122998552396894,
                    -1.087573952972889
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    4.0
                ],
                "result_count_bing": [
                    149000.0,
                    1870000.0,
                    1870000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    3.0
                ],
                "result_count": [
                    63500.0,
                    75.0,
                    77.0
                ],
                "answer_relation_to_question": [
                    1.1649270427582026,
                    0.17364893171344783,
                    1.6614240255283494
                ],
                "word_count_appended": [
                    199.0,
                    275.0,
                    451.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these music artists has NOT performed on the Super Bowl halftime show?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "van halen"
            ],
            "question": "which of these music artists has not performed on the super bowl halftime show?",
            "lines": [
                [
                    0.431924416579589,
                    0.3594006659267481,
                    0.45772050453351626,
                    0.4061425356000589,
                    0.3513777392059015,
                    0.32830076968620486,
                    0.4504310344827586,
                    0.3514422034265886,
                    0.31074767887051946,
                    0.21462264150943394,
                    0.23333333333333334,
                    0.3589691654493696,
                    0.25,
                    0.5,
                    -1.0
                ],
                [
                    0.15288915803806086,
                    0.2037513873473918,
                    0.1153347918670879,
                    0.32154712799963453,
                    0.20167064439140814,
                    0.47957371225577267,
                    0.125,
                    0.20180004337453916,
                    0.32876856953255545,
                    0.38443396226415094,
                    0.33333333333333337,
                    0.316269071796686,
                    0.25,
                    0.0,
                    -1.0
                ],
                [
                    0.4151864253823501,
                    0.43684794672586014,
                    0.42694470359939585,
                    0.27231033640030655,
                    0.4469516164026904,
                    0.19212551805802247,
                    0.4245689655172414,
                    0.44675775319887223,
                    0.3604837515969252,
                    0.40094339622641506,
                    0.43333333333333335,
                    0.32476176275394447,
                    0.5,
                    0.5,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "van halen",
                "britney spears",
                "james brown"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "van halen": 0.4296775980841843,
                "james brown": 0.3372387342412851,
                "britney spears": 0.07208183906511842
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6923700146075655,
                    2.204771138439768,
                    2.1028588469526666
                ],
                "result_count_important_words": [
                    57500.0,
                    435000.0,
                    87500.0
                ],
                "wikipedia_search": [
                    1.8925232112948058,
                    1.7123143046744458,
                    1.3951624840307484
                ],
                "word_count_appended_bing": [
                    16.0,
                    10.0,
                    4.0
                ],
                "answer_relation_to_question_bing": [
                    1.4059933407325194,
                    2.9624861265260822,
                    0.6315205327413984
                ],
                "cosine_similarity_raw": [
                    0.010404810309410095,
                    0.09466452896595001,
                    0.01797860860824585
                ],
                "result_count_noun_chunks": [
                    137000.0,
                    275000.0,
                    49100.0
                ],
                "question_answer_similarity": [
                    2.064023678191006,
                    3.9243650529533625,
                    5.007133529055864
                ],
                "word_count_noun_chunks": [
                    1.0,
                    1.0,
                    0.0
                ],
                "result_count_bing": [
                    4060000.0,
                    483000.0,
                    7280000.0
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count": [
                    137000.0,
                    275000.0,
                    48900.0
                ],
                "answer_relation_to_question": [
                    0.8169070010449321,
                    4.16533010354327,
                    1.0177628954117983
                ],
                "word_count_appended": [
                    121.0,
                    49.0,
                    42.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Featuring 20 scoops of ice cream, the Vermonster is found on what chain's menu?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ben & jerry's"
            ],
            "lines": [
                [
                    0.6422193351193357,
                    0.5105135090786799,
                    0.3198228465418106,
                    0.10819974874763372,
                    0.0219435736677116,
                    0.3333333333333333,
                    0.0461361014994233,
                    0.043115438108484005,
                    0.15921843066037847,
                    0.71,
                    0.7894736842105263,
                    0.2760707064998635,
                    0.03636363636363636,
                    0.08333333333333333,
                    1.0
                ],
                [
                    0.23272491558093172,
                    0.3584671682986541,
                    0.3070715402804091,
                    0.31262316818317987,
                    0.019704433497536946,
                    0.3333333333333333,
                    0.049596309111880045,
                    0.03894297635605007,
                    0.36529698691253404,
                    0.29,
                    0.10526315789473684,
                    0.23575192402134781,
                    0.03636363636363636,
                    0.08333333333333333,
                    1.0
                ],
                [
                    0.12505574929973262,
                    0.13101932262266608,
                    0.3731056131777803,
                    0.5791770830691864,
                    0.9583519928347515,
                    0.3333333333333333,
                    0.9042675893886967,
                    0.9179415855354659,
                    0.47548458242708747,
                    0.0,
                    0.10526315789473684,
                    0.4881773694787887,
                    0.9272727272727272,
                    0.8333333333333334,
                    1.0
                ]
            ],
            "fraction_answers": {
                "ben & jerry's": 0.5108416742620204,
                "dairy queen": 0.19774806308339737,
                "baskin-robbins": 0.2914102626545821
            },
            "question": "featuring 20 scoops of ice cream, the vermonster is found on what chain's menu?",
            "rate_limited": false,
            "answers": [
                "baskin-robbins",
                "dairy queen",
                "ben & jerry's"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "ben & jerry's": 0.6023840401923439,
                "dairy queen": 0.2975229937551846,
                "baskin-robbins": 0.41540902135596824
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.484636358498771,
                    2.1217673161921304,
                    4.393596325309098
                ],
                "result_count_important_words": [
                    40.0,
                    43.0,
                    784.0
                ],
                "wikipedia_search": [
                    1.2737474452830277,
                    2.9223758953002723,
                    3.8038766594166997
                ],
                "word_count_appended_bing": [
                    45.0,
                    6.0,
                    6.0
                ],
                "answer_relation_to_question_bing": [
                    3.573594563550759,
                    2.5092701780905786,
                    0.9171352583586626
                ],
                "cosine_similarity_raw": [
                    0.13548241555690765,
                    0.13008074462413788,
                    0.1580539047718048
                ],
                "result_count_noun_chunks": [
                    62.0,
                    56.0,
                    1320.0
                ],
                "question_answer_similarity": [
                    2.0412506726570427,
                    5.897816397249699,
                    10.926509757060558
                ],
                "word_count_noun_chunks": [
                    2.0,
                    2.0,
                    51.0
                ],
                "word_count_raw": [
                    2.0,
                    2.0,
                    20.0
                ],
                "result_count_bing": [
                    1710000.0,
                    1710000.0,
                    1710000.0
                ],
                "word_count_appended": [
                    71.0,
                    29.0,
                    0.0
                ],
                "answer_relation_to_question": [
                    5.137754680954686,
                    1.8617993246474538,
                    1.000445994397861
                ],
                "result_count": [
                    49.0,
                    44.0,
                    2140.0
                ]
            },
            "integer_answers": {
                "ben & jerry's": 9,
                "dairy queen": 0,
                "baskin-robbins": 5
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "The U.S. has never had a Miss America from what state?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "new mexico"
            ],
            "lines": [
                [
                    0.3564887609614261,
                    0.3236425339366516,
                    0.427871200064868,
                    0.24551597015623117,
                    0.2903225806451613,
                    0.42162733356815785,
                    0.3066581306017926,
                    0.42324641939037827,
                    0.31891853664788095,
                    0.42290748898678415,
                    0.4383561643835616,
                    0.33705449973807206,
                    0.5,
                    0.5,
                    1.0
                ],
                [
                    0.32403503305579895,
                    0.37569489334195216,
                    0.21838977672980747,
                    0.3169507826414436,
                    0.40880893300248144,
                    0.33022190912293065,
                    0.3969270166453265,
                    0.176827029012119,
                    0.29487962728531547,
                    0.37298091042584436,
                    0.4041095890410959,
                    0.3270636424162821,
                    0.01315789473684209,
                    0.0,
                    1.0
                ],
                [
                    0.319476205982775,
                    0.30066257272139624,
                    0.3537390232053246,
                    0.4375332472023252,
                    0.30086848635235736,
                    0.2481507573089116,
                    0.29641485275288093,
                    0.3999265515975028,
                    0.3862018360668036,
                    0.2041116005873715,
                    0.15753424657534248,
                    0.33588185784564584,
                    0.4868421052631579,
                    0.5,
                    1.0
                ]
            ],
            "fraction_answers": {
                "nebraska": 0.324665236648315,
                "new mexico": 0.24105576870271928,
                "north dakota": 0.4342789946489658
            },
            "question": "the u.s. has never had a miss america from what state?",
            "rate_limited": false,
            "answers": [
                "new mexico",
                "north dakota",
                "nebraska"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "nebraska": 0.12325300933012143,
                "new mexico": 0.6963123554551894,
                "north dakota": 0.6320329807090421
            },
            "integer_answers": {
                "nebraska": 6,
                "new mexico": 2,
                "north dakota": 6
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3035640020954236,
                    1.3834908606697431,
                    1.3129451372348333
                ],
                "result_count_important_words": [
                    3020000.0,
                    1610000.0,
                    3180000.0
                ],
                "wikipedia_search": [
                    1.0864887801127145,
                    1.230722236288107,
                    0.6827889835991785
                ],
                "word_count_appended_bing": [
                    9.0,
                    14.0,
                    50.0
                ],
                "answer_relation_to_question_bing": [
                    1.4108597285067872,
                    0.9944408532643826,
                    1.59469941822883
                ],
                "cosine_similarity_raw": [
                    0.023262985050678253,
                    0.09082494676113129,
                    0.047172099351882935
                ],
                "result_count_noun_chunks": [
                    836000.0,
                    3520000.0,
                    1090000.0
                ],
                "question_answer_similarity": [
                    7.735453888773918,
                    5.564077168703079,
                    1.8987780339084566
                ],
                "word_count_noun_chunks": [
                    0.0,
                    37.0,
                    1.0
                ],
                "result_count_bing": [
                    44500000.0,
                    96400000.0,
                    143000000.0
                ],
                "word_count_raw": [
                    0.0,
                    2.0,
                    0.0
                ],
                "result_count": [
                    3380000.0,
                    1470000.0,
                    3210000.0
                ],
                "answer_relation_to_question": [
                    1.1480899123085913,
                    1.4077197355536086,
                    1.4441903521378001
                ],
                "word_count_appended": [
                    105.0,
                    173.0,
                    403.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these do adverbs NOT typically modify?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "pronoun"
            ],
            "lines": [
                [
                    0.5,
                    0.5,
                    0.48416897113382734,
                    0.32544746535615543,
                    0.3462745098039216,
                    0.23618482672494534,
                    0.3669527896995708,
                    0.336950623982637,
                    0.5000000000000001,
                    0.4189663823381836,
                    0.4078341013824885,
                    0.3593101923047063,
                    0.4985687022900763,
                    0.5,
                    -1.0
                ],
                [
                    0.11712826726674747,
                    0.09090909090909088,
                    0.13496327981950984,
                    0.3791113996727453,
                    0.34980392156862744,
                    0.4163284420855448,
                    0.3812589413447782,
                    0.3304395008138904,
                    0.17996632996632997,
                    0.21700953336678375,
                    0.21198156682027652,
                    0.3141530817227256,
                    0.036736641221374045,
                    0.07482993197278914,
                    -1.0
                ],
                [
                    0.38287173273325253,
                    0.4090909090909091,
                    0.38086774904666276,
                    0.29544113497109925,
                    0.303921568627451,
                    0.34748673118950985,
                    0.25178826895565093,
                    0.3326098752034726,
                    0.32003367003367006,
                    0.36402408429503263,
                    0.38018433179723504,
                    0.3265367259725681,
                    0.46469465648854963,
                    0.4251700680272109,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "adjective": 0.2878969276525322,
                "adverb": 0.5379114387783981,
                "pronoun": 0.1741916335690697
            },
            "question": "which of these do adverbs not typically modify?",
            "rate_limited": false,
            "answers": [
                "pronoun",
                "adverb",
                "adjective"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "adjective": 0.7465153967762261,
                "adverb": 0.015878103149579853,
                "pronoun": 0.9016824609325857
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.8441388461717625,
                    1.1150815096636464,
                    1.0407796441645911
                ],
                "result_count_important_words": [
                    186000.0,
                    166000.0,
                    347000.0
                ],
                "wikipedia_search": [
                    0.0,
                    1.9202020202020202,
                    1.0797979797979798
                ],
                "word_count_appended_bing": [
                    40.0,
                    125.0,
                    52.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    2.4545454545454546,
                    0.5454545454545454
                ],
                "cosine_similarity_raw": [
                    0.031964004039764404,
                    0.7370358109474182,
                    0.24053671956062317
                ],
                "result_count_noun_chunks": [
                    601000.0,
                    625000.0,
                    617000.0
                ],
                "question_answer_similarity": [
                    1.4027007594704628,
                    0.9714584313333035,
                    1.643831044435501
                ],
                "word_count_noun_chunks": [
                    3.0,
                    971.0,
                    74.0
                ],
                "word_count_raw": [
                    0.0,
                    250.0,
                    44.0
                ],
                "result_count_bing": [
                    16900000.0,
                    5360000.0,
                    9770000.0
                ],
                "result_count": [
                    784000.0,
                    766000.0,
                    1000000.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    2.2972303963995153,
                    0.7027696036004847
                ],
                "word_count_appended": [
                    323.0,
                    1128.0,
                    542.0
                ]
            },
            "integer_answers": {
                "adjective": 3,
                "adverb": 10,
                "pronoun": 1
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "In the game of Candy Land, which player goes first?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "the youngest"
            ],
            "question": "in the game of candy land, which player goes first?",
            "lines": [
                [
                    0.39657874465428855,
                    0.3333333333333333,
                    0.37037853666394666,
                    0.2030613014024972,
                    0.20333839150227617,
                    0.2686733556298774,
                    0.002801432255431254,
                    0.1927877947295423,
                    0.0875,
                    0.8441558441558441,
                    0.25,
                    0.4819555789033698,
                    0,
                    1.0,
                    -1.0
                ],
                [
                    0.20207951496799215,
                    0.041666666666666664,
                    0.19066187250622446,
                    0.18875789490779968,
                    0.0,
                    0.12374581939799331,
                    0.0,
                    0.0,
                    0.0125,
                    0.03896103896103896,
                    0.5,
                    0.02643207210504061,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.40134174037771936,
                    0.625,
                    0.4389595908298289,
                    0.6081808036897031,
                    0.7966616084977238,
                    0.6075808249721293,
                    0.9971985677445687,
                    0.8072122052704577,
                    0.9,
                    0.11688311688311688,
                    0.25,
                    0.4916123489915895,
                    0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "the youngest",
                "blue token holder",
                "first to draw a red card"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "first to draw a red card": 0.4065383465436521,
                "blue token holder": 0.10853619714556147,
                "the youngest": 0.6819164149858246
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.409777894516849,
                    0.13216036052520305,
                    2.4580617449579476
                ],
                "result_count_important_words": [
                    60400.0,
                    0,
                    21500000.0
                ],
                "wikipedia_search": [
                    0.35,
                    0.05,
                    3.6
                ],
                "word_count_appended_bing": [
                    2.0,
                    4.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.0,
                    0.125,
                    1.875
                ],
                "cosine_similarity_raw": [
                    0.14485405385494232,
                    0.07456734776496887,
                    0.1716759204864502
                ],
                "result_count_noun_chunks": [
                    1390000.0,
                    0,
                    5820000.0
                ],
                "question_answer_similarity": [
                    8.35710547119379,
                    7.768440492451191,
                    25.030033230781555
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    2410000.0,
                    1110000.0,
                    5450000.0
                ],
                "result_count": [
                    1340000.0,
                    0,
                    5250000.0
                ],
                "answer_relation_to_question": [
                    1.5863149786171542,
                    0.8083180598719686,
                    1.6053669615108774
                ],
                "word_count_appended": [
                    65.0,
                    3.0,
                    9.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    }
}