{
    "Which of these actresses is NOT mentioned in Madonna\u2019s song \u201cVogue\u201d?": {
        "raw_data": {
            "best_answer_by_ml": [
                "audrey hepburn"
            ],
            "negative_question": true,
            "fraction_answers": {
                "jean harlow": 0.20334308035044288,
                "audrey hepburn": 0.2447914348797792,
                "rita hayworth": 0.5518654847697779
            },
            "rate_limited": false,
            "integer_answers": {
                "jean harlow": 2,
                "audrey hepburn": 2,
                "rita hayworth": 2
            },
            "data": {
                "wikipedia_search": [
                    0.24945295404814005,
                    2.532199966335634,
                    1.2183470796162261
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_relation_to_question": [
                    1.247615991197506,
                    1.6646799926645883,
                    1.0877040161379057
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_appended": [
                    26.0,
                    14.0,
                    26.0
                ],
                "result_count": [
                    61.0,
                    28.0,
                    46.0
                ]
            },
            "ml_answers": {
                "jean harlow": 0.30619748317698575,
                "audrey hepburn": 0.2616867615999172,
                "rita hayworth": 0.5029543303809376
            }
        },
        "lines": [
            [
                0,
                0.06236323851203501,
                0.0,
                0.3119039977993765,
                0.0,
                0.45185185185185184,
                0.3939393939393939
            ],
            [
                1,
                0.6330499915839085,
                0.0,
                0.4161699981661471,
                0.0,
                0.2074074074074074,
                0.21212121212121213
            ],
            [
                0,
                0.30458676990405653,
                1.0,
                0.2719260040344764,
                1.0,
                0.34074074074074073,
                0.3939393939393939
            ]
        ]
    },
    "Where in the home does the Maillard reaction typically occur?": {
        "raw_data": {
            "best_answer_by_ml": [
                "kitchen"
            ],
            "negative_question": false,
            "fraction_answers": {
                "bathroom": 0.09537159681680592,
                "kitchen": 0.7316458434690412,
                "bedroom": 0.17298255971415288
            },
            "rate_limited": false,
            "integer_answers": {
                "bathroom": 0,
                "kitchen": 6,
                "bedroom": 0
            },
            "data": {
                "wikipedia_search": [
                    2.859835438193647,
                    0.5027267508610792,
                    0.6374378109452736
                ],
                "word_count_entities": [
                    4.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.3045454545454547,
                    1.0324675324675325,
                    0.662987012987013
                ],
                "word_count_raw": [
                    29.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    289.0,
                    130.0,
                    98.0
                ],
                "result_count": [
                    18500.0,
                    8600.0,
                    63.0
                ]
            },
            "ml_answers": {
                "bathroom": 0.20931099396584285,
                "kitchen": 0.5123212089064794,
                "bedroom": 0.29743905136413207
            }
        },
        "lines": [
            [
                1,
                0.7149588595484118,
                1.0,
                0.4348484848484848,
                1.0,
                0.5589941972920697,
                0.6810735191252807
            ],
            [
                0,
                0.1256816877152698,
                0.0,
                0.3441558441558441,
                0.0,
                0.2514506769825919,
                0.3166071494312116
            ],
            [
                0,
                0.1593594527363184,
                0.0,
                0.22099567099567097,
                0.0,
                0.1895551257253385,
                0.0023193314435077128
            ]
        ]
    },
    "The man famously known as the Science Guy holds a patent for which of these items?": {
        "raw_data": {
            "best_answer_by_ml": [
                "mechanical pencil"
            ],
            "negative_question": false,
            "fraction_answers": {
                "pulse rate monitor": 0.20617999472156243,
                "mechanical pencil": 0.6799824819803648,
                "ballet shoe": 0.11383752329807272
            },
            "rate_limited": false,
            "integer_answers": {
                "pulse rate monitor": 1,
                "mechanical pencil": 3,
                "ballet shoe": 0
            },
            "data": {
                "wikipedia_search": [
                    1.1911111111111112,
                    3.568888888888889,
                    0.24
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    2.216666666666667,
                    2.0210144927536233,
                    0.7623188405797101
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    10.0,
                    44.0,
                    18.0
                ],
                "result_count": [
                    63.0,
                    14600.0,
                    72.0
                ]
            },
            "ml_answers": {
                "pulse rate monitor": 0.26636501882429947,
                "mechanical pencil": 0.2680793195506166,
                "ballet shoe": 0.24193020907293974
            }
        },
        "lines": [
            [
                0,
                0.23822222222222225,
                0,
                0.4433333333333333,
                0,
                0.1388888888888889,
                0.004275534441805225
            ],
            [
                0,
                0.7137777777777778,
                0,
                0.4042028985507246,
                0,
                0.6111111111111112,
                0.9908381404818459
            ],
            [
                1,
                0.048,
                0,
                0.152463768115942,
                0,
                0.25,
                0.0048863250763488296
            ]
        ]
    },
    "What advertising mascot wears epaulettes?": {
        "raw_data": {
            "best_answer_by_ml": [
                "mr. peanut"
            ],
            "negative_question": false,
            "fraction_answers": {
                "sun-maid raisin girl": 0.1929825646810023,
                "cap'n crunch": 0.41906923798968887,
                "mr. peanut": 0.38794819732930885
            },
            "rate_limited": false,
            "integer_answers": {
                "sun-maid raisin girl": 0,
                "cap'n crunch": 2,
                "mr. peanut": 2
            },
            "data": {
                "wikipedia_search": [
                    0.25,
                    0.6538461538461539,
                    2.0961538461538463
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.9302704463994786,
                    1.555229716520039,
                    1.514499837080482
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    13.0,
                    21.0,
                    4.0
                ],
                "result_count": [
                    9.0,
                    31.0,
                    39.0
                ]
            },
            "ml_answers": {
                "sun-maid raisin girl": 0.2832631507752357,
                "cap'n crunch": 0.2881144844315946,
                "mr. peanut": 0.29732573426454856
            }
        },
        "lines": [
            [
                0,
                0.08333333333333333,
                0,
                0.23256761159986966,
                0,
                0.34210526315789475,
                0.11392405063291139
            ],
            [
                0,
                0.21794871794871795,
                0,
                0.38880742913000976,
                0,
                0.5526315789473685,
                0.3924050632911392
            ],
            [
                1,
                0.6987179487179488,
                0,
                0.3786249592701205,
                0,
                0.10526315789473684,
                0.4936708860759494
            ]
        ]
    },
    "Which color is NOT represented in the original electronic Simon game?": {
        "raw_data": {
            "best_answer_by_ml": [
                "green"
            ],
            "negative_question": true,
            "fraction_answers": {
                "blue": 0.4400983755311633,
                "orange": 0.18605816961516472,
                "green": 0.373843454853672
            },
            "rate_limited": false,
            "integer_answers": {
                "blue": 4,
                "orange": 1,
                "green": 1
            },
            "data": {
                "wikipedia_search": [
                    1.5954504013175037,
                    0.6676804503438043,
                    2.736869148338692
                ],
                "word_count_entities": [
                    6.0,
                    0.0,
                    6.0
                ],
                "word_relation_to_question": [
                    3.1501270210907935,
                    1.2262748418852047,
                    1.6235981370240018
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    1.0
                ],
                "word_count_appended": [
                    227.0,
                    183.0,
                    199.0
                ],
                "result_count": [
                    909000.0,
                    1690000.0,
                    937000.0
                ]
            },
            "ml_answers": {
                "blue": 0.3796423607623041,
                "orange": 0.34600693372887925,
                "green": 0.2714074088133256
            }
        },
        "lines": [
            [
                0,
                0.3190900802635007,
                0.5,
                0.525021170181799,
                0.6666666666666666,
                0.2570701357466063,
                0.3727422003284072
            ],
            [
                1,
                0.13353609006876085,
                0.0,
                0.2043791403142008,
                0.0,
                0.47794117647058826,
                0.30049261083743845
            ],
            [
                0,
                0.5473738296677384,
                0.5,
                0.2705996895040003,
                0.3333333333333333,
                0.26498868778280543,
                0.32676518883415434
            ]
        ]
    },
    "Which of these songs was written by the man nicknamed \u201cSlowhand\u201d?": {
        "raw_data": {
            "best_answer_by_ml": [
                "lay down sally"
            ],
            "negative_question": false,
            "fraction_answers": {
                "lay down sally": 0.5754430456283801,
                "lover lay down": 0.09561815489998408,
                "lay lady lay": 0.3289387994716359
            },
            "rate_limited": false,
            "integer_answers": {
                "lay down sally": 2,
                "lover lay down": 0,
                "lay lady lay": 2
            },
            "data": {
                "wikipedia_search": [
                    0.6867965367965367,
                    2.0627705627705626,
                    0.25043290043290045
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.5516339869281046,
                    1.6359477124183006,
                    0.8124183006535948
                ],
                "word_count_raw": [
                    5.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    50.0,
                    16.0,
                    7.0
                ],
                "result_count": [
                    0,
                    0,
                    0
                ]
            },
            "ml_answers": {
                "lay down sally": 0.40321137606635743,
                "lover lay down": 0.27921317847639643,
                "lay lady lay": 0.28718724762403164
            }
        },
        "lines": [
            [
                1,
                0.2289321789321789,
                0,
                0.38790849673202615,
                1.0,
                0,
                0.684931506849315
            ],
            [
                0,
                0.6875901875901875,
                0,
                0.40898692810457515,
                0.0,
                0,
                0.2191780821917808
            ],
            [
                0,
                0.08347763347763348,
                0,
                0.2031045751633987,
                0.0,
                0,
                0.0958904109589041
            ]
        ]
    },
    "Which of these quantities is the largest?": {
        "raw_data": {
            "best_answer_by_ml": [
                "dozen"
            ],
            "negative_question": false,
            "fraction_answers": {
                "two half-dozens": 0.17071230235292797,
                "baker's dozen": 0.0044462326944398585,
                "dozen": 0.8248414649526321
            },
            "rate_limited": false,
            "integer_answers": {
                "two half-dozens": 0,
                "baker's dozen": 0,
                "dozen": 3
            },
            "data": {
                "wikipedia_search": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    402.0,
                    5.0,
                    5.0
                ],
                "result_count": [
                    14200000.0,
                    14.0,
                    17100.0
                ]
            },
            "ml_answers": {
                "two half-dozens": 0.32985248556570446,
                "baker's dozen": 0.23989384238959452,
                "dozen": 0.35122893170921893
            }
        },
        "lines": [
            [
                0,
                0,
                0,
                0.5,
                0,
                0.9757281553398058,
                0.9987962395180907
            ],
            [
                0,
                0,
                0,
                0.5,
                0,
                0.012135922330097087,
                9.847286868488218e-07
            ],
            [
                1,
                0,
                0,
                0.0,
                0,
                0.012135922330097087,
                0.0012027757532224895
            ]
        ]
    },
    "Which of these substances expands when it freezes?": {
        "raw_data": {
            "best_answer_by_ml": [
                "carbon dioxide"
            ],
            "negative_question": false,
            "fraction_answers": {
                "sodium chloride": 0.20191684011810832,
                "carbon dioxide": 0.5856332102825921,
                "dihydrogen monoxide": 0.21244994959929958
            },
            "rate_limited": false,
            "integer_answers": {
                "sodium chloride": 0,
                "carbon dioxide": 3,
                "dihydrogen monoxide": 2
            },
            "data": {
                "wikipedia_search": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_entities": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.3333333333333333,
                    0.0,
                    0.6666666666666666
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    88.0,
                    94.0,
                    101.0
                ],
                "result_count": [
                    152000.0,
                    248000.0,
                    16100.0
                ]
            },
            "ml_answers": {
                "sodium chloride": 0.3291973342234458,
                "carbon dioxide": 0.4034400042820544,
                "dihydrogen monoxide": 0.3482815600239995
            }
        },
        "lines": [
            [
                0,
                0.0,
                0.0,
                0.3333333333333333,
                0,
                0.31095406360424027,
                0.365296803652968
            ],
            [
                0,
                1.0,
                1.0,
                0.0,
                0,
                0.3321554770318021,
                0.5960105743811583
            ],
            [
                1,
                0.0,
                0.0,
                0.6666666666666666,
                0,
                0.3568904593639576,
                0.038692621965873586
            ]
        ]
    },
    "Which U.S. president's wife was NOT born in North America?": {
        "raw_data": {
            "best_answer_by_ml": [
                "rutherford b. hayes"
            ],
            "negative_question": true,
            "fraction_answers": {
                "martin van buren": 0.2344626805257496,
                "john quincy adams": 0.560693838836186,
                "rutherford b. hayes": 0.20484348063806443
            },
            "rate_limited": false,
            "integer_answers": {
                "martin van buren": 2,
                "john quincy adams": 4,
                "rutherford b. hayes": 0
            },
            "data": {
                "wikipedia_search": [
                    1.773564818443563,
                    1.4449655907573167,
                    2.7814695907991203
                ],
                "word_count_entities": [
                    3.0,
                    1.0,
                    0.0
                ],
                "word_relation_to_question": [
                    2.863890962237807,
                    2.2616991763918914,
                    1.874409861370302
                ],
                "word_count_raw": [
                    7.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    133.0,
                    110.0,
                    149.0
                ],
                "result_count": [
                    128000.0,
                    30200.0,
                    66300.0
                ]
            },
            "ml_answers": {
                "martin van buren": 0.3061413867494274,
                "john quincy adams": 0.5009341801169119,
                "rutherford b. hayes": 0.2841822324070629
            }
        },
        "lines": [
            [
                1,
                0.2955941364072605,
                0.75,
                0.40912728031968665,
                1.0,
                0.5701559020044543,
                0.3392857142857143
            ],
            [
                0,
                0.2408275984595528,
                0.25,
                0.32309988234169873,
                0.0,
                0.13452115812917595,
                0.28061224489795916
            ],
            [
                0,
                0.46357826513318673,
                0.0,
                0.2677728373386145,
                0.0,
                0.2953229398663697,
                0.38010204081632654
            ]
        ]
    },
    "Who was the president of the Screen Actors Guild before its merger with AFTRA?": {
        "raw_data": {
            "best_answer_by_ml": [
                "ken howard"
            ],
            "negative_question": false,
            "fraction_answers": {
                "gabrielle carteris": 0.2768857678554103,
                "ken howard": 0.5171573772940935,
                "melissa gilbert": 0.20595685485049617
            },
            "rate_limited": false,
            "integer_answers": {
                "gabrielle carteris": 0,
                "ken howard": 5,
                "melissa gilbert": 1
            },
            "data": {
                "wikipedia_search": [
                    2.274146341463415,
                    1.5565040650406503,
                    3.1693495934959346
                ],
                "word_count_entities": [
                    12.0,
                    3.0,
                    45.0
                ],
                "word_relation_to_question": [
                    2.802103949060245,
                    0.7444886136785112,
                    3.453407437261244
                ],
                "word_count_raw": [
                    8.0,
                    1.0,
                    22.0
                ],
                "word_count_appended": [
                    61.0,
                    66.0,
                    95.0
                ],
                "result_count": [
                    37.0,
                    96.0,
                    49.0
                ]
            },
            "ml_answers": {
                "gabrielle carteris": 0.23396746657733952,
                "ken howard": 0.4185713630270666,
                "melissa gilbert": 0.3052463489398373
            }
        },
        "lines": [
            [
                0,
                0.3248780487804878,
                0.2,
                0.40030056415146353,
                0.25806451612903225,
                0.2747747747747748,
                0.2032967032967033
            ],
            [
                0,
                0.22235772357723577,
                0.05,
                0.10635551623978731,
                0.03225806451612903,
                0.2972972972972973,
                0.5274725274725275
            ],
            [
                1,
                0.45276422764227636,
                0.75,
                0.4933439196087491,
                0.7096774193548387,
                0.42792792792792794,
                0.2692307692307692
            ]
        ]
    },
    "What generation of the iPod was the first to offer video?": {
        "raw_data": {
            "best_answer_by_ml": [
                "u2 special edition"
            ],
            "negative_question": false,
            "fraction_answers": {
                "third generation": 0.194493860057362,
                "u2 special edition": 0.6562148666401659,
                "fifth generation": 0.14929127330247208
            },
            "rate_limited": false,
            "integer_answers": {
                "third generation": 0,
                "u2 special edition": 4,
                "fifth generation": 0
            },
            "data": {
                "wikipedia_search": [
                    1.0,
                    3.0,
                    0.0
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.255696410186788,
                    2.08616173958298,
                    1.6581418502302325
                ],
                "word_count_raw": [
                    0.0,
                    2.0,
                    0.0
                ],
                "word_count_appended": [
                    49.0,
                    81.0,
                    47.0
                ],
                "result_count": [
                    0,
                    0,
                    0
                ]
            },
            "ml_answers": {
                "third generation": 0.21418391130128908,
                "u2 special edition": 0.3932606366414801,
                "fifth generation": 0.22992688769920006
            }
        },
        "lines": [
            [
                0,
                0.25,
                0,
                0.2511392820373576,
                0.0,
                0.2768361581920904,
                0
            ],
            [
                0,
                0.75,
                0,
                0.41723234791659597,
                1.0,
                0.4576271186440678,
                0
            ],
            [
                1,
                0.0,
                0,
                0.3316283700460465,
                0.0,
                0.2655367231638418,
                0
            ]
        ]
    },
    "Lonnie Lynn's only Academy Award win was in what category?": {
        "raw_data": {
            "best_answer_by_ml": [
                "best original song"
            ],
            "negative_question": false,
            "fraction_answers": {
                "best adapted screenplay": 0.22672679018108524,
                "best original song": 0.5888093892914322,
                "best cinematography": 0.18446382052748253
            },
            "rate_limited": false,
            "integer_answers": {
                "best adapted screenplay": 1,
                "best original song": 5,
                "best cinematography": 0
            },
            "data": {
                "wikipedia_search": [
                    0.9987955435109906,
                    1.0477696046801737,
                    1.9534348518088356
                ],
                "word_count_entities": [
                    1.0,
                    0.0,
                    27.0
                ],
                "word_relation_to_question": [
                    1.4201432492200767,
                    1.1329737294159459,
                    1.4468830213639774
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    25.0
                ],
                "word_count_appended": [
                    51.0,
                    36.0,
                    85.0
                ],
                "result_count": [
                    13100.0,
                    10900.0,
                    6940.0
                ]
            },
            "ml_answers": {
                "best adapted screenplay": 0.29726963783699023,
                "best original song": 0.5029543303809376,
                "best cinematography": 0.3055377672814248
            }
        },
        "lines": [
            [
                0,
                0.24969888587774766,
                0.03571428571428571,
                0.3550358123050192,
                0.0,
                0.29651162790697677,
                0.4234001292824822
            ],
            [
                0,
                0.2619424011700434,
                0.0,
                0.28324343235398647,
                0.0,
                0.20930232558139536,
                0.35229476405946997
            ],
            [
                1,
                0.4883587129522089,
                0.9642857142857143,
                0.36172075534099435,
                1.0,
                0.4941860465116279,
                0.22430510665804784
            ]
        ]
    },
    "Which of these phrases, written backwards, is a hip hop group?": {
        "raw_data": {
            "best_answer_by_ml": [
                "drummers ear"
            ],
            "negative_question": false,
            "fraction_answers": {
                "beat chefs": 0.17590472958351852,
                "drummers ear": 0.24968119175938042,
                "blues rhythm": 0.5744140786571011
            },
            "rate_limited": false,
            "integer_answers": {
                "beat chefs": 0,
                "drummers ear": 2,
                "blues rhythm": 2
            },
            "data": {
                "wikipedia_search": [
                    0.14285714285714285,
                    0.0,
                    3.857142857142857
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    2.8470420527441194,
                    1.731503920171062,
                    0.4214540270848183
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    11.0,
                    10.0,
                    7.0
                ],
                "result_count": [
                    34.0,
                    8.0,
                    45600.0
                ]
            },
            "ml_answers": {
                "beat chefs": 0.34361982283889536,
                "drummers ear": 0.3694409882700053,
                "blues rhythm": 0.2762664951847521
            }
        },
        "lines": [
            [
                1,
                0.03571428571428571,
                0,
                0.5694084105488239,
                0,
                0.0007449279172691819,
                0.39285714285714285
            ],
            [
                0,
                0.0,
                0,
                0.3463007840342124,
                0,
                0.0001752771570045134,
                0.35714285714285715
            ],
            [
                0,
                0.9642857142857143,
                0,
                0.08429080541696365,
                0,
                0.9990797949257263,
                0.25
            ]
        ]
    },
    "What makeup item often contains dried cochineal bugs as an ingredient?": {
        "raw_data": {
            "best_answer_by_ml": [
                "lipstick"
            ],
            "negative_question": false,
            "fraction_answers": {
                "lipstick": 0.6335347000063087,
                "mascara": 0.1753105632204022,
                "eyeliner": 0.191154736773289
            },
            "rate_limited": false,
            "integer_answers": {
                "lipstick": 4,
                "mascara": 1,
                "eyeliner": 1
            },
            "data": {
                "wikipedia_search": [
                    1.3928571428571428,
                    0.5119047619047619,
                    6.095238095238096
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    50.0
                ],
                "word_relation_to_question": [
                    2.213193486349764,
                    1.0249278816004013,
                    1.761878632049835
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    38.0
                ],
                "word_count_appended": [
                    186.0,
                    149.0,
                    379.0
                ],
                "result_count": [
                    8770.0,
                    39400.0,
                    10700.0
                ]
            },
            "ml_answers": {
                "lipstick": 0.5123212089064794,
                "mascara": 0.31931825962347504,
                "eyeliner": 0.3130844177175331
            }
        },
        "lines": [
            [
                0,
                0.17410714285714285,
                0.0,
                0.4426386972699528,
                0.02564102564102564,
                0.2605042016806723,
                0.14897231187361984
            ],
            [
                0,
                0.06398809523809523,
                0.0,
                0.20498557632008024,
                0.0,
                0.20868347338935575,
                0.6692712756922031
            ],
            [
                1,
                0.761904761904762,
                1.0,
                0.352375726409967,
                0.9743589743589743,
                0.530812324929972,
                0.181756412434177
            ]
        ]
    },
    "What term describes a person from the state between New York and Rhode Island?": {
        "raw_data": {
            "best_answer_by_ml": [
                "nutmegger"
            ],
            "negative_question": false,
            "fraction_answers": {
                "hoosier": 0.4354888157949032,
                "nutmegger": 0.21283024696227051,
                "cheesehead": 0.3516809372428263
            },
            "rate_limited": false,
            "integer_answers": {
                "hoosier": 2,
                "nutmegger": 1,
                "cheesehead": 1
            },
            "data": {
                "wikipedia_search": [
                    1.4167446393762182,
                    0.32966861598440544,
                    4.253586744639376
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    2.16079224398751,
                    3.1728268730373705,
                    1.6663808829751199
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    129.0,
                    143.0,
                    182.0
                ],
                "result_count": [
                    69200.0,
                    3370.0,
                    47200.0
                ]
            },
            "ml_answers": {
                "hoosier": 0.3040439792866246,
                "nutmegger": 0.3443745855316527,
                "cheesehead": 0.286864368196669
            }
        },
        "lines": [
            [
                0,
                0.23612410656270302,
                0,
                0.30868460628392996,
                0,
                0.5777740669616765,
                0.2841409691629956
            ],
            [
                1,
                0.05494476933073424,
                0,
                0.4532609818624814,
                0,
                0.028137263087584536,
                0.31497797356828194
            ],
            [
                0,
                0.7089311241065627,
                0,
                0.23805441185358853,
                0,
                0.39408866995073893,
                0.4008810572687225
            ]
        ]
    },
    "What Japanese word means \u201cempty orchestra\u201d?": {
        "raw_data": {
            "best_answer_by_ml": [
                "karaoke"
            ],
            "negative_question": false,
            "fraction_answers": {
                "sake": 0.16552143558671617,
                "anime": 0.18856940216155801,
                "karaoke": 0.6459091622517258
            },
            "rate_limited": false,
            "integer_answers": {
                "sake": 1,
                "anime": 1,
                "karaoke": 3
            },
            "data": {
                "wikipedia_search": [
                    0.7159090909090909,
                    2.526515151515152,
                    1.7575757575757576
                ],
                "word_count_entities": [
                    0.0,
                    5.0,
                    273.0
                ],
                "word_relation_to_question": [
                    1.974143955276031,
                    0.7658979734451432,
                    1.259958071278826
                ],
                "word_count_raw": [
                    0.0,
                    4.0,
                    296.0
                ],
                "word_count_appended": [
                    176.0,
                    198.0,
                    548.0
                ],
                "result_count": [
                    0,
                    0,
                    0
                ]
            },
            "ml_answers": {
                "sake": 0.3112729585156925,
                "anime": 0.21439335636548013,
                "karaoke": 0.5123212089064794
            }
        },
        "lines": [
            [
                0,
                0.1431818181818182,
                0.0,
                0.49353598881900773,
                0.0,
                0.19088937093275488,
                0
            ],
            [
                0,
                0.5053030303030304,
                0.017985611510791366,
                0.1914744933612858,
                0.013333333333333334,
                0.21475054229934923,
                0
            ],
            [
                1,
                0.3515151515151515,
                0.9820143884892086,
                0.3149895178197065,
                0.9866666666666667,
                0.5943600867678959,
                0
            ]
        ]
    },
    "Which of these astronomical objects orbits the Earth?": {
        "raw_data": {
            "best_answer_by_ml": [
                "moon"
            ],
            "negative_question": false,
            "fraction_answers": {
                "sun": 0.3867341217447222,
                "milky way": 0.19681942253277565,
                "moon": 0.41644645572250205
            },
            "rate_limited": false,
            "integer_answers": {
                "sun": 3,
                "milky way": 0,
                "moon": 3
            },
            "data": {
                "wikipedia_search": [
                    1.6205680284627655,
                    1.3107287449392713,
                    1.0687032265979635
                ],
                "word_count_entities": [
                    60.0,
                    8.0,
                    61.0
                ],
                "word_relation_to_question": [
                    1.7166666666666666,
                    1.638095238095238,
                    0.6452380952380952
                ],
                "word_count_raw": [
                    55.0,
                    7.0,
                    97.0
                ],
                "word_count_appended": [
                    449.0,
                    85.0,
                    369.0
                ],
                "result_count": [
                    813000.0,
                    556000.0,
                    914000.0
                ]
            },
            "ml_answers": {
                "sun": 0.3391007846776226,
                "milky way": 0.24770243285802473,
                "moon": 0.3545848374425595
            }
        },
        "lines": [
            [
                1,
                0.40514200711569137,
                0.46511627906976744,
                0.4291666666666667,
                0.34591194968553457,
                0.35611038107752957,
                0.49723145071982283
            ],
            [
                0,
                0.3276821862348178,
                0.06201550387596899,
                0.40952380952380957,
                0.0440251572327044,
                0.24353920280332894,
                0.09413067552602436
            ],
            [
                0,
                0.26717580664949087,
                0.4728682170542636,
                0.16130952380952382,
                0.610062893081761,
                0.40035041611914146,
                0.40863787375415284
            ]
        ]
    },
    "Made famous in a documentary, where is the \u201cGrey Gardens\u201d home located?": {
        "raw_data": {
            "best_answer_by_ml": [
                "east hampton, ny"
            ],
            "negative_question": false,
            "fraction_answers": {
                "east hampton, ny": 0.6972107258264977,
                "savannah, ga": 0.14238108554383724,
                "cape cod, ma": 0.16040818862966516
            },
            "rate_limited": false,
            "integer_answers": {
                "east hampton, ny": 6,
                "savannah, ga": 0,
                "cape cod, ma": 0
            },
            "data": {
                "wikipedia_search": [
                    1.726190476190476,
                    1.9249999999999998,
                    1.3488095238095237
                ],
                "word_count_entities": [
                    0.0,
                    2.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.174944071588367,
                    3.2565995525727067,
                    0.5684563758389262
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_appended": [
                    4.0,
                    16.0,
                    5.0
                ],
                "result_count": [
                    32.0,
                    73.0,
                    39.0
                ]
            },
            "ml_answers": {
                "east hampton, ny": 0.5123212089064794,
                "savannah, ga": 0.29786235741493083,
                "cape cod, ma": 0.2616867615999172
            }
        },
        "lines": [
            [
                0,
                0.34523809523809523,
                0.0,
                0.2349888143176734,
                0.0,
                0.16,
                0.2222222222222222
            ],
            [
                1,
                0.38499999999999995,
                1.0,
                0.6513199105145413,
                1.0,
                0.64,
                0.5069444444444444
            ],
            [
                0,
                0.26976190476190476,
                0.0,
                0.11369127516778524,
                0.0,
                0.2,
                0.2708333333333333
            ]
        ]
    },
    "How does the second verse of \u201cThe Star-Spangled Banner\u201d begin?": {
        "raw_data": {
            "best_answer_by_ml": [
                "on the shore"
            ],
            "negative_question": false,
            "fraction_answers": {
                "on the shore": 0.5777341492514685,
                "travels far": 0.15794838755550694,
                "where the foe": 0.26431746319302457
            },
            "rate_limited": false,
            "integer_answers": {
                "on the shore": 5,
                "travels far": 1,
                "where the foe": 0
            },
            "data": {
                "wikipedia_search": [
                    3.2222222222222223,
                    0.3611111111111111,
                    1.4166666666666665
                ],
                "word_count_entities": [
                    0.0,
                    5.0,
                    2.0
                ],
                "word_relation_to_question": [
                    1.141830065359477,
                    2.2951213818860876,
                    1.563048552754435
                ],
                "word_count_raw": [
                    0.0,
                    2.0,
                    0.0
                ],
                "word_count_appended": [
                    6.0,
                    38.0,
                    38.0
                ],
                "result_count": [
                    44.0,
                    19500.0,
                    6200.0
                ]
            },
            "ml_answers": {
                "on the shore": 0.5107980474518787,
                "travels far": 0.21599901240454542,
                "where the foe": 0.26315056053667407
            }
        },
        "lines": [
            [
                0,
                0.6444444444444445,
                0.0,
                0.2283660130718954,
                0.0,
                0.07317073170731707,
                0.001709136109384711
            ],
            [
                1,
                0.07222222222222222,
                0.7142857142857143,
                0.45902427637721754,
                1.0,
                0.4634146341463415,
                0.7574580484773151
            ],
            [
                0,
                0.2833333333333333,
                0.2857142857142857,
                0.31260971055088704,
                0.0,
                0.4634146341463415,
                0.24083281541330018
            ]
        ]
    },
    "Which of these was a name the ancient Greeks gave the planet Venus?": {
        "raw_data": {
            "best_answer_by_ml": [
                "phosphorus"
            ],
            "negative_question": false,
            "fraction_answers": {
                "antimony": 0.30745758241788307,
                "flourine": 0.05057515758856306,
                "phosphorus": 0.6419672599935539
            },
            "rate_limited": false,
            "integer_answers": {
                "antimony": 1,
                "flourine": 0,
                "phosphorus": 5
            },
            "data": {
                "wikipedia_search": [
                    0.42857142857142855,
                    1.9047619047619047,
                    0.6666666666666666
                ],
                "word_count_entities": [
                    0.0,
                    5.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.0,
                    0.0,
                    3.0
                ],
                "word_count_raw": [
                    0.0,
                    7.0,
                    0.0
                ],
                "word_count_appended": [
                    91.0,
                    358.0,
                    171.0
                ],
                "result_count": [
                    1690.0,
                    78200.0,
                    42400.0
                ]
            },
            "ml_answers": {
                "antimony": 0.3566497799030562,
                "flourine": 0.2589713020172907,
                "phosphorus": 0.5123212089064794
            }
        },
        "lines": [
            [
                0,
                0.14285714285714288,
                0.0,
                0.0,
                0.0,
                0.14677419354838708,
                0.013819609125848393
            ],
            [
                1,
                0.634920634920635,
                1.0,
                0.0,
                1.0,
                0.5774193548387097,
                0.6394635702019789
            ],
            [
                0,
                0.22222222222222224,
                0.0,
                1.0,
                0.0,
                0.27580645161290324,
                0.3467168206721727
            ]
        ]
    },
    "Which of these figure skating jumps was invented the most recently?": {
        "raw_data": {
            "best_answer_by_ml": [
                "lutz"
            ],
            "negative_question": false,
            "fraction_answers": {
                "salchow": 0.4699635202798625,
                "lutz": 0.19720178540395286,
                "axel": 0.33283469431618473
            },
            "rate_limited": false,
            "integer_answers": {
                "salchow": 2,
                "lutz": 0,
                "axel": 4
            },
            "data": {
                "wikipedia_search": [
                    0.05811277330264672,
                    1.8964326812428078,
                    0.045454545454545456
                ],
                "word_count_entities": [
                    16.0,
                    20.0,
                    34.0
                ],
                "word_relation_to_question": [
                    0.4335637480798771,
                    3.9781105990783407,
                    0.5883256528417818
                ],
                "word_count_raw": [
                    10.0,
                    8.0,
                    28.0
                ],
                "word_count_appended": [
                    177.0,
                    174.0,
                    230.0
                ],
                "result_count": [
                    64.0,
                    64.0,
                    74.0
                ]
            },
            "ml_answers": {
                "salchow": 0.3129724230988068,
                "lutz": 0.3496481567996166,
                "axel": 0.34007838664426526
            }
        },
        "lines": [
            [
                1,
                0.02905638665132336,
                0.22857142857142856,
                0.08671274961597542,
                0.21739130434782608,
                0.3046471600688468,
                0.31683168316831684
            ],
            [
                0,
                0.9482163406214039,
                0.2857142857142857,
                0.7956221198156681,
                0.17391304347826086,
                0.29948364888123924,
                0.31683168316831684
            ],
            [
                0,
                0.022727272727272728,
                0.4857142857142857,
                0.11766513056835637,
                0.6086956521739131,
                0.3958691910499139,
                0.36633663366336633
            ]
        ]
    },
    "Romaine, Iceberg and Butterhead are all varieties of what?": {
        "raw_data": {
            "best_answer_by_ml": [
                "lettuce"
            ],
            "negative_question": false,
            "fraction_answers": {
                "lettuce": 0.9933789522302829,
                "race cars": 0.00437219384648511,
                "disney dwarfs": 0.0022488539232318806
            },
            "rate_limited": false,
            "integer_answers": {
                "lettuce": 6,
                "race cars": 0,
                "disney dwarfs": 0
            },
            "data": {
                "wikipedia_search": [
                    3.9583333333333335,
                    0.0,
                    0.041666666666666664
                ],
                "word_count_entities": [
                    684.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    4.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    666.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    881.0,
                    11.0,
                    14.0
                ],
                "result_count": [
                    38400.0,
                    52.0,
                    14.0
                ]
            },
            "ml_answers": {
                "lettuce": 0.5315519547669878,
                "race cars": 0.23989384238959452,
                "disney dwarfs": 0.23989384238959452
            }
        },
        "lines": [
            [
                1,
                0.9895833333333334,
                1.0,
                1.0,
                1.0,
                0.9724061810154525,
                0.9982841990329122
            ],
            [
                0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.012141280353200883,
                0.001351843186190402
            ],
            [
                0,
                0.010416666666666666,
                0.0,
                0.0,
                0.0,
                0.01545253863134658,
                0.0003639577808974159
            ]
        ]
    },
    "Rejected in the late 1700s, what was the name of the proposed 14th U.S. colony?": {
        "raw_data": {
            "best_answer_by_ml": [
                "vandalia"
            ],
            "negative_question": false,
            "fraction_answers": {
                "roanoke": 0.21091415444496012,
                "new albion": 0.17420096867964166,
                "vandalia": 0.6148848768753983
            },
            "rate_limited": false,
            "integer_answers": {
                "roanoke": 1,
                "new albion": 1,
                "vandalia": 3
            },
            "data": {
                "wikipedia_search": [
                    0.015873015873015872,
                    0.6071428571428572,
                    1.376984126984127
                ],
                "word_count_entities": [
                    0.0,
                    10.0,
                    1.0
                ],
                "word_relation_to_question": [
                    2.2433064565765988,
                    2.467594017357051,
                    0.2890995260663507
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_appended": [
                    177.0,
                    109.0,
                    10.0
                ],
                "result_count": [
                    0,
                    0,
                    0
                ]
            },
            "ml_answers": {
                "roanoke": 0.3239522738049262,
                "new albion": 0.23989384238959452,
                "vandalia": 0.4915673015913703
            }
        },
        "lines": [
            [
                0,
                0.007936507936507936,
                0.0,
                0.4486612913153197,
                0.0,
                0.597972972972973,
                0
            ],
            [
                1,
                0.3035714285714286,
                0.9090909090909091,
                0.4935188034714101,
                1.0,
                0.36824324324324326,
                0
            ],
            [
                0,
                0.6884920634920635,
                0.09090909090909091,
                0.05781990521327013,
                0.0,
                0.033783783783783786,
                0
            ]
        ]
    },
    "Which of these are you most likely to find in a toolbox?": {
        "raw_data": {
            "best_answer_by_ml": [
                "hammer"
            ],
            "negative_question": false,
            "fraction_answers": {
                "mc hammer": 0.25933278191342707,
                "hammer": 0.620323269920044,
                "hammerhead shark": 0.12034394816652882
            },
            "rate_limited": false,
            "integer_answers": {
                "mc hammer": 1,
                "hammer": 3,
                "hammerhead shark": 1
            },
            "data": {
                "wikipedia_search": [
                    0.1111111111111111,
                    1.7777777777777777,
                    0.1111111111111111
                ],
                "word_count_entities": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.5625,
                    1.0,
                    1.4375
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    346.0,
                    30.0,
                    27.0
                ],
                "result_count": [
                    0,
                    0,
                    0
                ]
            },
            "ml_answers": {
                "mc hammer": 0.28097903241165123,
                "hammer": 0.5221850762414461,
                "hammerhead shark": 0.3373884471713129
            }
        },
        "lines": [
            [
                1,
                0.05555555555555555,
                1.0,
                0.1875,
                1.0,
                0,
                0.858560794044665
            ],
            [
                0,
                0.8888888888888888,
                0.0,
                0.3333333333333333,
                0.0,
                0,
                0.07444168734491315
            ],
            [
                0,
                0.05555555555555555,
                0.0,
                0.4791666666666667,
                0.0,
                0,
                0.06699751861042183
            ]
        ]
    },
    "Which of these is usually found on the ocean floor?": {
        "raw_data": {
            "best_answer_by_ml": [
                "sea cucumber"
            ],
            "negative_question": false,
            "fraction_answers": {
                "sweet potato": 0.3914802252537421,
                "cherry tomato": 0.06785779441368897,
                "sea cucumber": 0.540661980332569
            },
            "rate_limited": false,
            "integer_answers": {
                "sweet potato": 1,
                "cherry tomato": 0,
                "sea cucumber": 3
            },
            "data": {
                "wikipedia_search": [
                    1.1243589743589744,
                    0.14166666666666666,
                    2.7339743589743586
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.28431372549019607,
                    0.25,
                    3.4656862745098036
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    133.0,
                    42.0,
                    175.0
                ],
                "result_count": [
                    497000.0,
                    31900.0,
                    67200.0
                ]
            },
            "ml_answers": {
                "sweet potato": 0.30081599610428833,
                "cherry tomato": 0.23947130236001732,
                "sea cucumber": 0.3531172370984082
            }
        },
        "lines": [
            [
                0,
                0.28108974358974365,
                0,
                0.07107843137254903,
                0,
                0.38,
                0.8337527260526757
            ],
            [
                0,
                0.03541666666666667,
                0,
                0.06250000000000001,
                0,
                0.12,
                0.053514510988089244
            ],
            [
                1,
                0.6834935897435898,
                0,
                0.866421568627451,
                0,
                0.5,
                0.11273276295923502
            ]
        ]
    },
    "The '90s band The Lightning Seeds took their name from which song?": {
        "raw_data": {
            "best_answer_by_ml": [
                "raspberry beret"
            ],
            "negative_question": false,
            "fraction_answers": {
                "when doves cry": 0.21431609953334788,
                "raspberry beret": 0.4729384422894902,
                "purple rain": 0.3127454581771619
            },
            "rate_limited": false,
            "integer_answers": {
                "when doves cry": 0,
                "raspberry beret": 2,
                "purple rain": 4
            },
            "data": {
                "wikipedia_search": [
                    0.964624464273168,
                    1.6883299374692615,
                    1.3470455982575706
                ],
                "word_count_entities": [
                    2.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.4650515176830966,
                    1.5325814536340852,
                    1.0023670286828181
                ],
                "word_count_raw": [
                    3.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    25.0,
                    26.0,
                    21.0
                ],
                "result_count": [
                    9530.0,
                    58900.0,
                    32700.0
                ]
            },
            "ml_answers": {
                "when doves cry": 0.2779365228566735,
                "raspberry beret": 0.5029543303809376,
                "purple rain": 0.3375712372110017
            }
        },
        "lines": [
            [
                1,
                0.241156116068292,
                1.0,
                0.1550171725610322,
                1.0,
                0.3472222222222222,
                0.09423514288539504
            ],
            [
                0,
                0.4220824843673154,
                0.0,
                0.510860484544695,
                0.0,
                0.3611111111111111,
                0.5824186690398497
            ],
            [
                0,
                0.33676139956439266,
                0.0,
                0.33412234289427273,
                0.0,
                0.2916666666666667,
                0.3233461880747553
            ]
        ]
    },
    "Which Las Vegas hotel features a replica of the Rialto Bridge?": {
        "raw_data": {
            "best_answer_by_ml": [
                "the venetian"
            ],
            "negative_question": false,
            "fraction_answers": {
                "caesars palace": 0.2114402026927739,
                "luxor": 0.1833447924526115,
                "the venetian": 0.6052150048546145
            },
            "rate_limited": false,
            "integer_answers": {
                "caesars palace": 2,
                "luxor": 1,
                "the venetian": 3
            },
            "data": {
                "wikipedia_search": [
                    1.1891355572035387,
                    0.19458070544265357,
                    4.6162837373538075
                ],
                "word_count_entities": [
                    2.0,
                    63.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.5695398843766446,
                    1.4645870396822627,
                    1.9658730759410927
                ],
                "word_count_raw": [
                    1.0,
                    74.0,
                    0.0
                ],
                "word_count_appended": [
                    168.0,
                    126.0,
                    31.0
                ],
                "result_count": [
                    3500.0,
                    125000.0,
                    1390.0
                ]
            },
            "ml_answers": {
                "caesars palace": 0.20239003499174044,
                "luxor": 0.26088750005115374,
                "the venetian": 0.5221850762414461
            }
        },
        "lines": [
            [
                0,
                0.1981892595339231,
                0.03076923076923077,
                0.3139079768753289,
                0.013333333333333334,
                0.5169230769230769,
                0.02694587728077604
            ],
            [
                1,
                0.0324301175737756,
                0.9692307692307692,
                0.29291740793645255,
                0.9866666666666667,
                0.38769230769230767,
                0.9623527600277157
            ],
            [
                0,
                0.7693806228923012,
                0.0,
                0.39317461518821856,
                0.0,
                0.09538461538461539,
                0.010701362691508199
            ]
        ]
    },
    "What TV series derived from a nearly 20-year-old Michael Crichton screenplay?": {
        "raw_data": {
            "best_answer_by_ml": [
                "er"
            ],
            "negative_question": false,
            "fraction_answers": {
                "numb3rs": 0.108290427232507,
                "the expanse": 0.1421276067471535,
                "er": 0.7495819660203394
            },
            "rate_limited": false,
            "integer_answers": {
                "numb3rs": 0,
                "the expanse": 0,
                "er": 6
            },
            "data": {
                "wikipedia_search": [
                    0.09090909090909091,
                    1.5118082343700572,
                    3.397282674720852
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    1082.0
                ],
                "word_relation_to_question": [
                    2.794889803333558,
                    1.1755898729698766,
                    3.0295203236965653
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1156.0
                ],
                "word_count_appended": [
                    26.0,
                    113.0,
                    1437.0
                ],
                "result_count": [
                    138000.0,
                    35500.0,
                    156000.0
                ]
            },
            "ml_answers": {
                "numb3rs": 0.28287062945238856,
                "the expanse": 0.3290158334114941,
                "er": 0.5123212089064794
            }
        },
        "lines": [
            [
                0,
                0.01818181818181818,
                0.0,
                0.39926997190479396,
                0.0,
                0.01649746192893401,
                0.4188163884673748
            ],
            [
                0,
                0.30236164687401146,
                0.0,
                0.16794141042426808,
                0.0,
                0.0717005076142132,
                0.10773899848254932
            ],
            [
                1,
                0.6794565349441705,
                1.0,
                0.4327886176709379,
                1.0,
                0.9118020304568528,
                0.47344461305007585
            ]
        ]
    },
    "Mardi Gras is celebrated right before what other observance?": {
        "raw_data": {
            "best_answer_by_ml": [
                "lent"
            ],
            "negative_question": false,
            "fraction_answers": {
                "kwanzaa": 0.2293879573192313,
                "ramadan": 0.14269219860302082,
                "lent": 0.6279198440777479
            },
            "rate_limited": false,
            "integer_answers": {
                "kwanzaa": 1,
                "ramadan": 0,
                "lent": 5
            },
            "data": {
                "wikipedia_search": [
                    1.1778021978021977,
                    0.8624175824175825,
                    0.9597802197802197
                ],
                "word_count_entities": [
                    62.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    2.543563579277865,
                    1.5710361067503924,
                    0.8854003139717426
                ],
                "word_count_raw": [
                    68.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    358.0,
                    208.0,
                    192.0
                ],
                "result_count": [
                    32600.0,
                    41400.0,
                    8760.0
                ]
            },
            "ml_answers": {
                "kwanzaa": 0.3061413867494274,
                "ramadan": 0.2735195667435238,
                "lent": 0.5029543303809376
            }
        },
        "lines": [
            [
                1,
                0.3926007326007326,
                1.0,
                0.508712715855573,
                1.0,
                0.47229551451187335,
                0.39391010149830835
            ],
            [
                0,
                0.2874725274725275,
                0.0,
                0.3142072213500785,
                0.0,
                0.27440633245382584,
                0.500241662638956
            ],
            [
                0,
                0.31992673992673987,
                0.0,
                0.17708006279434851,
                0.0,
                0.2532981530343008,
                0.10584823586273562
            ]
        ]
    },
    "Which of these is classified as a neurological condition or disorder?": {
        "raw_data": {
            "best_answer_by_ml": [
                "multiple sclerosis"
            ],
            "negative_question": false,
            "fraction_answers": {
                "halitosis": 0.16296009381279478,
                "cystic fibrosis": 0.1254386982943078,
                "multiple sclerosis": 0.7116012078928974
            },
            "rate_limited": false,
            "integer_answers": {
                "halitosis": 1,
                "cystic fibrosis": 0,
                "multiple sclerosis": 5
            },
            "data": {
                "wikipedia_search": [
                    0.8295739348370927,
                    0.6328320802005012,
                    1.537593984962406
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    4.0
                ],
                "word_relation_to_question": [
                    0.5344687152906331,
                    0.6235468345057387,
                    1.8419844502036284
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    10.0
                ],
                "word_count_appended": [
                    296.0,
                    140.0,
                    147.0
                ],
                "result_count": [
                    40000.0,
                    244000.0,
                    2320000.0
                ]
            },
            "ml_answers": {
                "halitosis": 0.28997993450656795,
                "cystic fibrosis": 0.29321358321140656,
                "multiple sclerosis": 0.5221850762414461
            }
        },
        "lines": [
            [
                0,
                0.27652464494569756,
                0.0,
                0.17815623843021103,
                0.0,
                0.5077186963979416,
                0.015360983102918587
            ],
            [
                0,
                0.2109440267335004,
                0.0,
                0.2078489448352462,
                0.0,
                0.24013722126929674,
                0.09370199692780339
            ],
            [
                1,
                0.5125313283208021,
                1.0,
                0.6139948167345428,
                1.0,
                0.2521440823327616,
                0.890937019969278
            ]
        ]
    },
    "Which of these Uranus moons is NOT named after a Shakespearean character?": {
        "raw_data": {
            "best_answer_by_ml": [
                "trinculo"
            ],
            "negative_question": true,
            "fraction_answers": {
                "oberon": 0.3686674217102324,
                "trinculo": 0.2008991267270611,
                "umbriel": 0.4304334515627065
            },
            "rate_limited": false,
            "integer_answers": {
                "oberon": 3,
                "trinculo": 0,
                "umbriel": 2
            },
            "data": {
                "wikipedia_search": [
                    0.2747040072859745,
                    2.7148451730418945,
                    2.010450819672131
                ],
                "word_count_entities": [
                    34.0,
                    13.0,
                    2.0
                ],
                "word_relation_to_question": [
                    0.7197792918704868,
                    2.786518751455858,
                    1.4937019566736547
                ],
                "word_count_raw": [
                    25.0,
                    20.0,
                    0.0
                ],
                "word_count_appended": [
                    269.0,
                    233.0,
                    179.0
                ],
                "result_count": [
                    0,
                    0,
                    0
                ]
            },
            "ml_answers": {
                "oberon": 0.3546021798075912,
                "trinculo": 0.21418391130128908,
                "umbriel": 0.3501295990105652
            }
        },
        "lines": [
            [
                0,
                0.0549408014571949,
                0.6938775510204082,
                0.14395585837409736,
                0.5555555555555556,
                0.39500734214390604,
                0
            ],
            [
                1,
                0.5429690346083789,
                0.2653061224489796,
                0.5573037502911716,
                0.4444444444444444,
                0.342143906020558,
                0
            ],
            [
                0,
                0.4020901639344262,
                0.04081632653061224,
                0.29874039133473096,
                0.0,
                0.26284875183553597,
                0
            ]
        ]
    },
    "Which of these celebrities is known for having aviophobia?": {
        "raw_data": {
            "best_answer_by_ml": [
                "john madden"
            ],
            "negative_question": false,
            "fraction_answers": {
                "angelina jolie": 0.3754906540029491,
                "john travolta": 0.17440268019776217,
                "john madden": 0.45010666579928876
            },
            "rate_limited": false,
            "integer_answers": {
                "angelina jolie": 2,
                "john travolta": 0,
                "john madden": 3
            },
            "data": {
                "wikipedia_search": [
                    2.5034722222222223,
                    0.22271825396825395,
                    0.2738095238095238
                ],
                "word_count_entities": [
                    0.0,
                    3.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.6964285714285714,
                    1.5803571428571428,
                    0.7232142857142857
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    18.0,
                    21.0,
                    11.0
                ],
                "result_count": [
                    55.0,
                    28.0,
                    39.0
                ]
            },
            "ml_answers": {
                "angelina jolie": 0.31972409367888693,
                "john travolta": 0.30205293552097223,
                "john madden": 0.41808518025783903
            }
        },
        "lines": [
            [
                0,
                0.8344907407407408,
                0.0,
                0.23214285714285712,
                0,
                0.45081967213114754,
                0.36
            ],
            [
                1,
                0.07423941798941798,
                1.0,
                0.5267857142857143,
                0,
                0.22950819672131148,
                0.42
            ],
            [
                0,
                0.09126984126984126,
                0.0,
                0.24107142857142858,
                0,
                0.319672131147541,
                0.22
            ]
        ]
    },
    "Who wrote a #1 hit song for the Monkees?": {
        "raw_data": {
            "best_answer_by_ml": [
                "neil diamond"
            ],
            "negative_question": false,
            "fraction_answers": {
                "neil diamond": 0.3757678234704354,
                "james taylor": 0.3435505490034226,
                "jackson browne": 0.28068162752614195
            },
            "rate_limited": false,
            "integer_answers": {
                "neil diamond": 1,
                "james taylor": 2,
                "jackson browne": 0
            },
            "data": {
                "wikipedia_search": [
                    2.4829541569351923,
                    1.6037097536800289,
                    0.9133360893847786
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.0021856112171925,
                    2.3646891314775473,
                    1.6331252573052601
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    38300000.0,
                    38300000.0,
                    38200000.0
                ]
            },
            "ml_answers": {
                "neil diamond": 0.34808545339564345,
                "james taylor": 0.32158836794401885,
                "jackson browne": 0.3222642879645335
            }
        },
        "lines": [
            [
                0,
                0.49659083138703847,
                0,
                0.20043712224343851,
                0,
                0,
                0.3336236933797909
            ],
            [
                1,
                0.32074195073600575,
                0,
                0.47293782629550946,
                0,
                0,
                0.3336236933797909
            ],
            [
                0,
                0.18266721787695572,
                0,
                0.32662505146105203,
                0,
                0,
                0.3327526132404181
            ]
        ]
    },
    "Which of these modes of transportation has only one wheel?": {
        "raw_data": {
            "best_answer_by_ml": [
                "bus"
            ],
            "negative_question": false,
            "fraction_answers": {
                "unicycle": 0.3370151753704771,
                "bus": 0.610584536518457,
                "monster truck": 0.052400288111065826
            },
            "rate_limited": false,
            "integer_answers": {
                "unicycle": 1,
                "bus": 4,
                "monster truck": 0
            },
            "data": {
                "wikipedia_search": [
                    0.19148936170212766,
                    1.0851063829787233,
                    0.723404255319149
                ],
                "word_count_entities": [
                    0.0,
                    24.0,
                    3.0
                ],
                "word_relation_to_question": [
                    0.21568627450980393,
                    1.8053221288515404,
                    0.9789915966386554
                ],
                "word_count_raw": [
                    0.0,
                    35.0,
                    21.0
                ],
                "word_count_appended": [
                    82.0,
                    343.0,
                    444.0
                ],
                "result_count": [
                    0,
                    0,
                    0
                ]
            },
            "ml_answers": {
                "unicycle": 0.23449525634818646,
                "bus": 0.41400197697177055,
                "monster truck": 0.22131431533958254
            }
        },
        "lines": [
            [
                0,
                0.09574468085106383,
                0.0,
                0.07189542483660132,
                0.0,
                0.09436133486766399,
                0
            ],
            [
                0,
                0.5425531914893617,
                0.8888888888888888,
                0.6017740429505135,
                0.625,
                0.3947065592635213,
                0
            ],
            [
                1,
                0.3617021276595745,
                0.1111111111111111,
                0.32633053221288516,
                0.375,
                0.5109321058688148,
                0
            ]
        ]
    },
    "What is Telluride, Colorado named after?": {
        "raw_data": {
            "best_answer_by_ml": [
                "an element"
            ],
            "negative_question": false,
            "fraction_answers": {
                "a european city": 0.07934972285629743,
                "a governor": 0.6315745069099972,
                "an element": 0.2890757702337055
            },
            "rate_limited": false,
            "integer_answers": {
                "a european city": 0,
                "a governor": 2,
                "an element": 2
            },
            "data": {
                "wikipedia_search": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.2564102564102564,
                    0.29914529914529914,
                    0.4444444444444444
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    67.0,
                    49.0,
                    12.0
                ],
                "result_count": [
                    98.0,
                    20900.0,
                    30.0
                ]
            },
            "ml_answers": {
                "a european city": 0.20931099396584285,
                "a governor": 0.29622798250293203,
                "an element": 0.36193925452221676
            }
        },
        "lines": [
            [
                1,
                0.0,
                0,
                0.6282051282051282,
                0,
                0.5234375,
                0.004660452729693742
            ],
            [
                0,
                1.0,
                0,
                0.14957264957264957,
                0,
                0.3828125,
                0.9939128780673387
            ],
            [
                0,
                0.0,
                0,
                0.2222222222222222,
                0,
                0.09375,
                0.001426669202967472
            ]
        ]
    },
    "Which of these is NOT a machine used for printing?": {
        "raw_data": {
            "best_answer_by_ml": [
                "hydraulophone"
            ],
            "negative_question": true,
            "fraction_answers": {
                "spirit duplicator": 0.4383154407459505,
                "hydraulophone": 0.22145115838467147,
                "hectograph": 0.340233400869378
            },
            "rate_limited": false,
            "integer_answers": {
                "spirit duplicator": 3,
                "hydraulophone": 0,
                "hectograph": 1
            },
            "data": {
                "wikipedia_search": [
                    0.6138613861386139,
                    0.998493327593629,
                    1.387645286267757
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.42105263157894735,
                    0.6721804511278195,
                    1.906766917293233
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    199.0,
                    269.0,
                    109.0
                ],
                "result_count": [
                    11600.0,
                    20000.0,
                    27600.0
                ]
            },
            "ml_answers": {
                "spirit duplicator": 0.3439405022458909,
                "hydraulophone": 0.29863348412404556,
                "hectograph": 0.31465012114147306
            }
        },
        "lines": [
            [
                1,
                0.20462046204620463,
                0,
                0.14035087719298245,
                0,
                0.34488734835355284,
                0.19594594594594594
            ],
            [
                0,
                0.33283110919787634,
                0,
                0.22406015037593985,
                0,
                0.4662045060658579,
                0.33783783783783783
            ],
            [
                0,
                0.462548428755919,
                0,
                0.6355889724310777,
                0,
                0.18890814558058924,
                0.46621621621621623
            ]
        ]
    },
    "Which of these sharks is NOT a Lamniforme?": {
        "raw_data": {
            "best_answer_by_ml": [
                "hammerhead shark"
            ],
            "negative_question": true,
            "fraction_answers": {
                "goblin shark": 0.23154463964786431,
                "great white shark": 0.5701847230629978,
                "hammerhead shark": 0.19827063728913785
            },
            "rate_limited": false,
            "integer_answers": {
                "goblin shark": 1,
                "great white shark": 3,
                "hammerhead shark": 0
            },
            "data": {
                "wikipedia_search": [
                    0.27882055231598635,
                    1.3521619540103422,
                    0.3690174936736715
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.9940217391304347,
                    1.313586956521739,
                    0.6923913043478261
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    156.0,
                    146.0,
                    85.0
                ],
                "result_count": [
                    22600.0,
                    341000.0,
                    68300.0
                ]
            },
            "ml_answers": {
                "goblin shark": 0.3290158334114941,
                "great white shark": 0.34354206266413584,
                "hammerhead shark": 0.2851403301773476
            }
        },
        "lines": [
            [
                0,
                0.13941027615799317,
                0,
                0.3313405797101449,
                0,
                0.05232692752952072,
                0.40310077519379844
            ],
            [
                0,
                0.6760809770051711,
                0,
                0.4378623188405797,
                0,
                0.7895346144940959,
                0.3772609819121447
            ],
            [
                1,
                0.18450874683683574,
                0,
                0.23079710144927537,
                0,
                0.15813845797638343,
                0.21963824289405684
            ]
        ]
    },
    "Who is NOT considered an official member of the Eagles?": {
        "raw_data": {
            "best_answer_by_ml": [
                "j.d. souther"
            ],
            "negative_question": true,
            "fraction_answers": {
                "j.d. souther": 0.14831689055400746,
                "randy meisner": 0.45496024750101266,
                "bernie leadon": 0.39672286194497985
            },
            "rate_limited": false,
            "integer_answers": {
                "j.d. souther": 0,
                "randy meisner": 3,
                "bernie leadon": 3
            },
            "data": {
                "wikipedia_search": [
                    1.143929912390488,
                    2.6726145777459918,
                    1.1834555098635198
                ],
                "word_count_entities": [
                    0.0,
                    3.0,
                    2.0
                ],
                "word_relation_to_question": [
                    2.0384312955104664,
                    1.8601007184540448,
                    2.1014679860354883
                ],
                "word_count_raw": [
                    0.0,
                    5.0,
                    6.0
                ],
                "word_count_appended": [
                    13.0,
                    54.0,
                    48.0
                ],
                "result_count": [
                    15.0,
                    26.0,
                    31.0
                ]
            },
            "ml_answers": {
                "j.d. souther": 0.26239950536163925,
                "randy meisner": 0.29739417434232246,
                "bernie leadon": 0.2836680397663313
            }
        },
        "lines": [
            [
                1,
                0.22878598247809762,
                0.0,
                0.3397385492517444,
                0.0,
                0.20833333333333334,
                0.11304347826086956
            ],
            [
                0,
                0.5345229155491984,
                0.6,
                0.3100167864090075,
                0.45454545454545453,
                0.3611111111111111,
                0.46956521739130436
            ],
            [
                0,
                0.23669110197270396,
                0.4,
                0.35024466433924806,
                0.5454545454545454,
                0.4305555555555556,
                0.41739130434782606
            ]
        ]
    },
    "What SportsCenter anchor shares their last name with Linus & Lucy from \u201cPeanuts\u201d?": {
        "raw_data": {
            "best_answer_by_ml": [
                "scott van pelt"
            ],
            "negative_question": false,
            "fraction_answers": {
                "jemele hill": 0.19882228198862675,
                "kenny mayne": 0.27642500479294296,
                "scott van pelt": 0.5247527132184303
            },
            "rate_limited": false,
            "integer_answers": {
                "jemele hill": 1,
                "kenny mayne": 0,
                "scott van pelt": 3
            },
            "data": {
                "wikipedia_search": [
                    1.5666666666666667,
                    0.41666666666666663,
                    1.0166666666666666
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.092267622929139,
                    1.8404878559393287,
                    1.0672445211315322
                ],
                "word_count_raw": [
                    0.0,
                    3.0,
                    1.0
                ],
                "word_count_appended": [
                    0.0,
                    3.0,
                    1.0
                ],
                "result_count": [
                    0,
                    0,
                    0
                ]
            },
            "ml_answers": {
                "jemele hill": 0.22789052101585483,
                "kenny mayne": 0.21418391130128908,
                "scott van pelt": 0.3793316682881308
            }
        },
        "lines": [
            [
                0,
                0.5222222222222223,
                0,
                0.27306690573228476,
                0.0,
                0.0,
                0
            ],
            [
                1,
                0.13888888888888887,
                0,
                0.4601219639848322,
                0.75,
                0.75,
                0
            ],
            [
                0,
                0.33888888888888885,
                0,
                0.26681113028288306,
                0.25,
                0.25,
                0
            ]
        ]
    },
    "Which of these countries is closest to the International Date Line?": {
        "raw_data": {
            "best_answer_by_ml": [
                "japan"
            ],
            "negative_question": false,
            "fraction_answers": {
                "brazil": 0.22260772677964835,
                "japan": 0.3909082798598416,
                "spain": 0.38648399336051
            },
            "rate_limited": false,
            "integer_answers": {
                "brazil": 2,
                "japan": 3,
                "spain": 1
            },
            "data": {
                "wikipedia_search": [
                    1.2894692345310046,
                    1.5109666693815988,
                    1.1995640960873966
                ],
                "word_count_entities": [
                    1.0,
                    0.0,
                    1.0
                ],
                "word_relation_to_question": [
                    1.1888797186562934,
                    1.3864080343414513,
                    1.4247122470022555
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    1.0
                ],
                "word_count_appended": [
                    302.0,
                    351.0,
                    332.0
                ],
                "result_count": [
                    14800000.0,
                    9000000.0,
                    11500000.0
                ]
            },
            "ml_answers": {
                "brazil": 0.25129117353639385,
                "japan": 0.32346509370848175,
                "spain": 0.2844473804612991
            }
        },
        "lines": [
            [
                1,
                0.32236730863275115,
                0.5,
                0.29721992966407335,
                0.5,
                0.3065989847715736,
                0.4192634560906516
            ],
            [
                0,
                0.3777416673453997,
                0.0,
                0.34660200858536283,
                0.0,
                0.35634517766497464,
                0.254957507082153
            ],
            [
                0,
                0.29989102402184914,
                0.5,
                0.3561780617505639,
                0.5,
                0.33705583756345175,
                0.32577903682719545
            ]
        ]
    },
    "Which Oscar-winning actress has NOT won the award for playing a real person?": {
        "raw_data": {
            "best_answer_by_ml": [
                "hilary swank"
            ],
            "negative_question": true,
            "fraction_answers": {
                "hilary swank": 0.2841606493352366,
                "emma thompson": 0.27516416925954856,
                "susan sarandon": 0.44067518140521494
            },
            "rate_limited": false,
            "integer_answers": {
                "hilary swank": 2,
                "emma thompson": 2,
                "susan sarandon": 2
            },
            "data": {
                "wikipedia_search": [
                    6.122587262494878,
                    1.881392310563643,
                    0.9960204269414795
                ],
                "word_count_entities": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_relation_to_question": [
                    2.2266253678682344,
                    3.064769135019611,
                    3.708605497112155
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    1.0
                ],
                "word_count_appended": [
                    101.0,
                    71.0,
                    87.0
                ],
                "result_count": [
                    77.0,
                    74.0,
                    80.0
                ]
            },
            "ml_answers": {
                "hilary swank": 0.3087224987302469,
                "emma thompson": 0.3241564349619091,
                "susan sarandon": 0.38783555194416053
            }
        },
        "lines": [
            [
                1,
                0.680287473610542,
                0.0,
                0.24740281865202604,
                0.0,
                0.3333333333333333,
                0.38996138996138996
            ],
            [
                0,
                0.209043590062627,
                1.0,
                0.3405299038910679,
                0.5,
                0.3203463203463203,
                0.27413127413127414
            ],
            [
                0,
                0.11066893632683106,
                0.0,
                0.4120672774569061,
                0.5,
                0.3463203463203463,
                0.3359073359073359
            ]
        ]
    },
    "Which player won Rookie of the Year in their sport most recently?": {
        "raw_data": {
            "best_answer_by_ml": [
                "mike trout"
            ],
            "negative_question": false,
            "fraction_answers": {
                "mike trout": 0.4886791348247741,
                "von miller": 0.1732047992816855,
                "blake griffin": 0.33811606589354043
            },
            "rate_limited": false,
            "integer_answers": {
                "mike trout": 3,
                "von miller": 1,
                "blake griffin": 2
            },
            "data": {
                "wikipedia_search": [
                    1.9608352630556578,
                    0.8094145209687973,
                    2.229750215975545
                ],
                "word_count_entities": [
                    2.0,
                    0.0,
                    2.0
                ],
                "word_relation_to_question": [
                    2.8884023931665004,
                    1.0983338739582957,
                    2.0132637328752034
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    47.0,
                    75.0,
                    72.0
                ],
                "result_count": [
                    37.0,
                    36.0,
                    44.0
                ]
            },
            "ml_answers": {
                "mike trout": 0.4276949236301649,
                "von miller": 0.37002608970733586,
                "blake griffin": 0.28569045966037643
            }
        },
        "lines": [
            [
                1,
                0.39216705261113155,
                0.5,
                0.4814003988610834,
                1.0,
                0.3162393162393162,
                0.2422680412371134
            ],
            [
                0,
                0.16188290419375945,
                0.0,
                0.18305564565971597,
                0.0,
                0.3076923076923077,
                0.3865979381443299
            ],
            [
                0,
                0.445950043195109,
                0.5,
                0.3355439554792006,
                0.0,
                0.37606837606837606,
                0.3711340206185567
            ]
        ]
    },
    "In computing, what unit is half a byte?": {
        "raw_data": {
            "best_answer_by_ml": [
                "nibble"
            ],
            "negative_question": false,
            "fraction_answers": {
                "demibyte": 0.045146343701257,
                "octet": 0.2517091815996222,
                "nibble": 0.7031444746991209
            },
            "rate_limited": false,
            "integer_answers": {
                "demibyte": 0,
                "octet": 2,
                "nibble": 4
            },
            "data": {
                "wikipedia_search": [
                    4.0,
                    0.0,
                    0.0
                ],
                "word_count_entities": [
                    164.0,
                    0.0,
                    28.0
                ],
                "word_relation_to_question": [
                    1.2884559884559883,
                    1.0095238095238095,
                    1.702020202020202
                ],
                "word_count_raw": [
                    161.0,
                    0.0,
                    22.0
                ],
                "word_count_appended": [
                    576.0,
                    16.0,
                    273.0
                ],
                "result_count": [
                    64300.0,
                    0,
                    65100.0
                ]
            },
            "ml_answers": {
                "demibyte": 0.22789052101585483,
                "octet": 0.3312533609964167,
                "nibble": 0.5009341801169119
            }
        },
        "lines": [
            [
                1,
                1.0,
                0.8541666666666666,
                0.32211399711399713,
                0.8797814207650273,
                0.6658959537572254,
                0.49690880989180836
            ],
            [
                0,
                0.0,
                0.0,
                0.25238095238095243,
                0.0,
                0.018497109826589597,
                0.0
            ],
            [
                0,
                0.0,
                0.14583333333333334,
                0.42550505050505055,
                0.12021857923497267,
                0.315606936416185,
                0.5030911901081917
            ]
        ]
    },
    "What book that heavily influenced \u201cThe Matrix\u201d makes a cameo in the movie?": {
        "raw_data": {
            "best_answer_by_ml": [
                "neuromancer"
            ],
            "negative_question": false,
            "fraction_answers": {
                "simulacra & simulation": 0.05444164087240934,
                "neuromancer": 0.6830276505128405,
                "gravity's rainbow": 0.2625307086147502
            },
            "rate_limited": false,
            "integer_answers": {
                "simulacra & simulation": 0,
                "neuromancer": 4,
                "gravity's rainbow": 1
            },
            "data": {
                "wikipedia_search": [
                    5.659090909090909,
                    0.5818181818181818,
                    0.759090909090909
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    3.698562762066848,
                    1.0951661785825628,
                    1.2062710593505888
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    137.0,
                    0.0,
                    2.0
                ],
                "result_count": [
                    22.0,
                    31.0,
                    4670.0
                ]
            },
            "ml_answers": {
                "simulacra & simulation": 0.23989384238959452,
                "neuromancer": 0.44004944024354964,
                "gravity's rainbow": 0.33166394476754507
            }
        },
        "lines": [
            [
                0,
                0.8084415584415584,
                0,
                0.6164271270111414,
                1.0,
                0.9856115107913669,
                0.004658056320135507
            ],
            [
                1,
                0.08311688311688312,
                0,
                0.18252769643042716,
                0.0,
                0.0,
                0.006563624814736396
            ],
            [
                0,
                0.10844155844155844,
                0,
                0.2010451765584315,
                0.0,
                0.014388489208633094,
                0.9887783188651281
            ]
        ]
    },
    "Which of these things was created by a person who chose to remain anonymous?": {
        "raw_data": {
            "best_answer_by_ml": [
                "bitcoin"
            ],
            "negative_question": false,
            "fraction_answers": {
                "hoverboards": 0.11533501836876744,
                "fidget spinners": 0.07039501136346331,
                "bitcoin": 0.8142699702677693
            },
            "rate_limited": false,
            "integer_answers": {
                "hoverboards": 0,
                "fidget spinners": 0,
                "bitcoin": 5
            },
            "data": {
                "wikipedia_search": [
                    3.577777777777778,
                    0.3666666666666667,
                    0.05555555555555555
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    2.033333333333333,
                    1.0,
                    0.9666666666666667
                ],
                "word_count_raw": [
                    21.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    760.0,
                    198.0,
                    57.0
                ],
                "result_count": [
                    281000.0,
                    12200.0,
                    12300.0
                ]
            },
            "ml_answers": {
                "hoverboards": 0.23048668140180895,
                "fidget spinners": 0.24906620845182095,
                "bitcoin": 0.44340299837836944
            }
        },
        "lines": [
            [
                1,
                0.8944444444444445,
                0,
                0.5083333333333333,
                1.0,
                0.7487684729064039,
                0.9198036006546645
            ],
            [
                0,
                0.09166666666666667,
                0,
                0.25,
                0.0,
                0.19507389162561575,
                0.03993453355155483
            ],
            [
                0,
                0.013888888888888888,
                0,
                0.24166666666666667,
                0.0,
                0.0561576354679803,
                0.04026186579378069
            ]
        ]
    },
    "In which town were a President, Governor, Senator, NFL owner and late night host all born?": {
        "raw_data": {
            "best_answer_by_ml": [
                "muncie, in"
            ],
            "negative_question": false,
            "fraction_answers": {
                "muncie, in": 0.3856801723658216,
                "hope, ar": 0.2071271523147308,
                "brookline, ma": 0.4071926753194476
            },
            "rate_limited": false,
            "integer_answers": {
                "muncie, in": 3,
                "hope, ar": 0,
                "brookline, ma": 1
            },
            "data": {
                "wikipedia_search": [
                    3.062874322581769,
                    1.4110131650025266,
                    3.5261125124157036
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.6234682549848425,
                    3.317078842552776,
                    4.059452902462381
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    5.0,
                    5.0,
                    9.0
                ],
                "result_count": [
                    20600.0,
                    524.0,
                    4550.0
                ]
            },
            "ml_answers": {
                "muncie, in": 0.32401403486950564,
                "hope, ar": 0.23862536506592158,
                "brookline, ma": 0.3119517774264935
            }
        },
        "lines": [
            [
                1,
                0.38285929032272115,
                0,
                0.1803853616649825,
                0,
                0.2631578947368421,
                0.8023681545532445
            ],
            [
                0,
                0.17637664562531583,
                0,
                0.36856431583919735,
                0,
                0.2631578947368421,
                0.020409753057567967
            ],
            [
                0,
                0.44076406405196294,
                0,
                0.45105032249582006,
                0,
                0.47368421052631576,
                0.1772220923891875
            ]
        ]
    },
    "What gargantuan fruit is the subject of a Roald Dahl children's book?": {
        "raw_data": {
            "best_answer_by_ml": [
                "peach"
            ],
            "negative_question": false,
            "fraction_answers": {
                "loquat": 0.15956492794989183,
                "dragonfruit": 0.13380011743981482,
                "peach": 0.7066349546102934
            },
            "rate_limited": false,
            "integer_answers": {
                "loquat": 0,
                "dragonfruit": 0,
                "peach": 6
            },
            "data": {
                "wikipedia_search": [
                    1.7165602811446603,
                    1.335544717779531,
                    1.9478950010758087
                ],
                "word_count_entities": [
                    0.0,
                    1.0,
                    13.0
                ],
                "word_relation_to_question": [
                    1.2766097890059784,
                    1.825165166213041,
                    1.8982250447809803
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    9.0
                ],
                "word_count_appended": [
                    58.0,
                    43.0,
                    185.0
                ],
                "result_count": [
                    15.0,
                    38.0,
                    10900.0
                ]
            },
            "ml_answers": {
                "loquat": 0.20239003499174044,
                "dragonfruit": 0.20239003499174044,
                "peach": 0.5201649259774205
            }
        },
        "lines": [
            [
                0,
                0.34331205622893207,
                0.0,
                0.2553219578011957,
                0.0,
                0.20279720279720279,
                0.001369487811558477
            ],
            [
                0,
                0.2671089435559062,
                0.07142857142857142,
                0.3650330332426082,
                0.1,
                0.15034965034965034,
                0.003469369122614809
            ],
            [
                1,
                0.38957900021516173,
                0.9285714285714286,
                0.3796450089561961,
                0.9,
                0.6468531468531469,
                0.9951611430658267
            ]
        ]
    },
    "Which game is an example of combinatorics?": {
        "raw_data": {
            "best_answer_by_ml": [
                "risk"
            ],
            "negative_question": false,
            "fraction_answers": {
                "sudoku": 0.3874809386581647,
                "risk": 0.4305777348784171,
                "crossword puzzles": 0.18194132646341818
            },
            "rate_limited": false,
            "integer_answers": {
                "sudoku": 3,
                "risk": 1,
                "crossword puzzles": 0
            },
            "data": {
                "wikipedia_search": [
                    0.2903225806451613,
                    0.7088285229202038,
                    1.000848896434635
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.6909090909090909,
                    0.5318181818181817,
                    0.7772727272727272
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    310.0,
                    79.0,
                    618.0
                ],
                "result_count": [
                    478000.0,
                    15000.0,
                    24400.0
                ]
            },
            "ml_answers": {
                "sudoku": 0.23924269364493622,
                "risk": 0.28993494368346895,
                "crossword puzzles": 0.23911468201500813
            }
        },
        "lines": [
            [
                0,
                0.14516129032258066,
                0,
                0.34545454545454546,
                0,
                0.30784508440913605,
                0.9238500193274063
            ],
            [
                0,
                0.3544142614601019,
                0,
                0.2659090909090909,
                0,
                0.07845084409136048,
                0.028991109393119444
            ],
            [
                1,
                0.5004244482173175,
                0,
                0.3886363636363636,
                0,
                0.6137040714995035,
                0.047158871279474296
            ]
        ]
    },
    "Queen Victoria is credited with starting what fashion trend?": {
        "raw_data": {
            "best_answer_by_ml": [
                "white wedding dress"
            ],
            "negative_question": false,
            "fraction_answers": {
                "white wedding dress": 0.5359156797326399,
                "mini dress": 0.2713436716031735,
                "little black dress": 0.19274064866418666
            },
            "rate_limited": false,
            "integer_answers": {
                "white wedding dress": 3,
                "mini dress": 2,
                "little black dress": 0
            },
            "data": {
                "wikipedia_search": [
                    2.3746031746031746,
                    0.4761904761904762,
                    1.1492063492063491
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.016694306246545,
                    1.6927584300718628,
                    3.2905472636815922
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    5.0
                ],
                "word_count_appended": [
                    19.0,
                    29.0,
                    44.0
                ],
                "result_count": [
                    36.0,
                    23.0,
                    34.0
                ]
            },
            "ml_answers": {
                "white wedding dress": 0.44521525789087896,
                "mini dress": 0.3154209969002212,
                "little black dress": 0.2855175494759675
            }
        },
        "lines": [
            [
                0,
                0.5936507936507937,
                0,
                0.16944905104109084,
                0.0,
                0.20652173913043478,
                0.3870967741935484
            ],
            [
                0,
                0.11904761904761905,
                0,
                0.2821264050119771,
                0.0,
                0.31521739130434784,
                0.24731182795698925
            ],
            [
                1,
                0.2873015873015873,
                0,
                0.5484245439469321,
                1.0,
                0.4782608695652174,
                0.3655913978494624
            ]
        ]
    },
    "How do you let someone on Tinder know you're interested?": {
        "raw_data": {
            "best_answer_by_ml": [
                "swipe right"
            ],
            "negative_question": false,
            "fraction_answers": {
                "draw circle around face": 0.07454379706226998,
                "shake phone": 0.06545240381709307,
                "swipe right": 0.8600037991206371
            },
            "rate_limited": false,
            "integer_answers": {
                "draw circle around face": 0,
                "shake phone": 0,
                "swipe right": 6
            },
            "data": {
                "wikipedia_search": [
                    5.184975369458128,
                    0.6682969739619986,
                    0.14672765657987333
                ],
                "word_count_entities": [
                    10.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.7531501831501832,
                    1.2223076923076923,
                    1.0245421245421245
                ],
                "word_count_raw": [
                    8.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    85.0,
                    3.0,
                    11.0
                ],
                "result_count": [
                    49300.0,
                    0,
                    50.0
                ]
            },
            "ml_answers": {
                "draw circle around face": 0.22789052101585483,
                "shake phone": 0.20931099396584285,
                "swipe right": 0.5315519547669878
            }
        },
        "lines": [
            [
                1,
                0.8641625615763546,
                1.0,
                0.4382875457875458,
                1.0,
                0.8585858585858586,
                0.9989868287740629
            ],
            [
                0,
                0.11138282899366643,
                0.0,
                0.3055769230769231,
                0.0,
                0.030303030303030304,
                0.0
            ],
            [
                0,
                0.024454609429978888,
                0.0,
                0.2561355311355311,
                0.0,
                0.1111111111111111,
                0.0010131712259371835
            ]
        ]
    },
    "According to the old saying, what kind of animal can NOT change its spots?": {
        "raw_data": {
            "best_answer_by_ml": [
                "tiger"
            ],
            "negative_question": true,
            "fraction_answers": {
                "tiger": 0.2695441353648273,
                "leopard": 0.5787434113923161,
                "zebra": 0.1517124532428566
            },
            "rate_limited": false,
            "integer_answers": {
                "tiger": 1,
                "leopard": 5,
                "zebra": 0
            },
            "data": {
                "wikipedia_search": [
                    0.2531335805092544,
                    2.1861611233383105,
                    4.5607052961524355
                ],
                "word_count_entities": [
                    10.0,
                    134.0,
                    9.0
                ],
                "word_relation_to_question": [
                    1.4706692406692405,
                    4.90663173377459,
                    1.6226990255561682
                ],
                "word_count_raw": [
                    0.0,
                    82.0,
                    5.0
                ],
                "word_count_appended": [
                    331.0,
                    404.0,
                    339.0
                ],
                "result_count": [
                    89.0,
                    99.0,
                    93.0
                ]
            },
            "ml_answers": {
                "tiger": 0.3352921117712341,
                "leopard": 0.4915673015913703,
                "zebra": 0.3632745442603753
            }
        },
        "lines": [
            [
                0,
                0.03616194007275063,
                0.06535947712418301,
                0.1838336550836551,
                0.0,
                0.3167259786476868,
                0.30819366852886404
            ],
            [
                1,
                0.3123087319054729,
                0.8758169934640523,
                0.6133289667218239,
                0.9425287356321839,
                0.35231316725978645,
                0.3761638733705773
            ],
            [
                0,
                0.6515293280217765,
                0.058823529411764705,
                0.20283737819452108,
                0.05747126436781609,
                0.3309608540925267,
                0.31564245810055863
            ]
        ]
    },
    "Who defeated Napoleon at the Battle of Waterloo?": {
        "raw_data": {
            "best_answer_by_ml": [
                "the duke of wellington"
            ],
            "negative_question": false,
            "fraction_answers": {
                "jack skellington": 0.0590203008951514,
                "the duke of wellington": 0.8278191419138556,
                "beef wellington": 0.11316055719099283
            },
            "rate_limited": false,
            "integer_answers": {
                "jack skellington": 0,
                "the duke of wellington": 5,
                "beef wellington": 1
            },
            "data": {
                "wikipedia_search": [
                    0.336046511627907,
                    4.617441860465116,
                    0.046511627906976744
                ],
                "word_count_entities": [
                    0.0,
                    15.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.24390243902439024,
                    4.166777129727819,
                    0.5893204312477908
                ],
                "word_count_raw": [
                    0.0,
                    13.0,
                    0.0
                ],
                "word_count_appended": [
                    31.0,
                    96.0,
                    107.0
                ],
                "result_count": [
                    11400.0,
                    86300.0,
                    10200.0
                ]
            },
            "ml_answers": {
                "jack skellington": 0.24700726396562575,
                "the duke of wellington": 0.5123212089064794,
                "beef wellington": 0.32411643136935003
            }
        },
        "lines": [
            [
                0,
                0.0672093023255814,
                0.0,
                0.04878048780487804,
                0.0,
                0.1056533827618165,
                0.13247863247863248
            ],
            [
                1,
                0.9234883720930233,
                1.0,
                0.8333554259455637,
                1.0,
                0.7998146431881371,
                0.41025641025641024
            ],
            [
                0,
                0.009302325581395349,
                0.0,
                0.11786408624955813,
                0.0,
                0.09453197405004633,
                0.45726495726495725
            ]
        ]
    },
    "Which of these is NOT a geometric shape?": {
        "raw_data": {
            "best_answer_by_ml": [
                "tarragon"
            ],
            "negative_question": true,
            "fraction_answers": {
                "hexagon": 0.5787273168604538,
                "tarragon": 0.09280398545607542,
                "octagon": 0.3284686976834708
            },
            "rate_limited": false,
            "integer_answers": {
                "hexagon": 5,
                "tarragon": 0,
                "octagon": 1
            },
            "data": {
                "wikipedia_search": [
                    0.6972602739726027,
                    0.3963470319634703,
                    0.9063926940639269
                ],
                "word_count_entities": [
                    2.0,
                    0.0,
                    3.0
                ],
                "word_relation_to_question": [
                    1.4343434343434343,
                    0.3181818181818182,
                    1.2474747474747474
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    3.0
                ],
                "word_count_appended": [
                    322.0,
                    214.0,
                    406.0
                ],
                "result_count": [
                    1270000.0,
                    212000.0,
                    6860000.0
                ]
            },
            "ml_answers": {
                "hexagon": 0.2975076198068822,
                "tarragon": 0.2647596207003535,
                "octagon": 0.3501295990105652
            }
        },
        "lines": [
            [
                0,
                0.3486301369863014,
                0.4,
                0.4781144781144781,
                0.25,
                0.34182590233545646,
                0.15224166866458883
            ],
            [
                1,
                0.1981735159817352,
                0.0,
                0.10606060606060606,
                0.0,
                0.22717622080679406,
                0.02541356988731719
            ],
            [
                0,
                0.4531963470319635,
                0.6,
                0.4158249158249158,
                0.75,
                0.4309978768577495,
                0.822344761448094
            ]
        ]
    },
    "What tech mogul became a billionaire the youngest?": {
        "raw_data": {
            "best_answer_by_ml": [
                "mark zuckerberg"
            ],
            "negative_question": false,
            "fraction_answers": {
                "mark zuckerberg": 0.5787120980517657,
                "evan spiegel": 0.26851683696364753,
                "larry page": 0.15277106498458679
            },
            "rate_limited": false,
            "integer_answers": {
                "mark zuckerberg": 4,
                "evan spiegel": 2,
                "larry page": 0
            },
            "data": {
                "wikipedia_search": [
                    1.3324829931972788,
                    1.6530612244897958,
                    1.0144557823129252
                ],
                "word_count_entities": [
                    0.0,
                    9.0,
                    0.0
                ],
                "word_relation_to_question": [
                    3.058129370629371,
                    0.7792103729603731,
                    1.1626602564102564
                ],
                "word_count_raw": [
                    7.0,
                    17.0,
                    2.0
                ],
                "word_count_appended": [
                    86.0,
                    86.0,
                    49.0
                ],
                "result_count": [
                    2840.0,
                    306000.0,
                    46900.0
                ]
            },
            "ml_answers": {
                "mark zuckerberg": 0.4349103447890911,
                "evan spiegel": 0.33945954260019096,
                "larry page": 0.26126686507059965
            }
        },
        "lines": [
            [
                0,
                0.3331207482993197,
                0.0,
                0.6116258741258742,
                0.2692307692307692,
                0.3891402714932127,
                0.007983358632709282
            ],
            [
                1,
                0.41326530612244894,
                1.0,
                0.1558420745920746,
                0.6538461538461539,
                0.3891402714932127,
                0.8601787822567043
            ],
            [
                0,
                0.2536139455782313,
                0.0,
                0.2325320512820513,
                0.07692307692307693,
                0.22171945701357465,
                0.13183785911058638
            ]
        ]
    },
    "Which of these is NOT a marsupial?": {
        "raw_data": {
            "best_answer_by_ml": [
                "quintana roo"
            ],
            "negative_question": true,
            "fraction_answers": {
                "cuscus": 0.16375306020324565,
                "quintana roo": 0.1095495811130411,
                "wombat": 0.7266973586837132
            },
            "rate_limited": false,
            "integer_answers": {
                "cuscus": 0,
                "quintana roo": 0,
                "wombat": 6
            },
            "data": {
                "wikipedia_search": [
                    0.5319148936170213,
                    0.029787234042553193,
                    1.4382978723404256
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    16.0
                ],
                "word_relation_to_question": [
                    0.475,
                    0.6225490196078431,
                    0.9024509803921569
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    6.0
                ],
                "word_count_appended": [
                    60.0,
                    469.0,
                    543.0
                ],
                "result_count": [
                    34000.0,
                    26400.0,
                    287000.0
                ]
            },
            "ml_answers": {
                "cuscus": 0.3104363063614821,
                "quintana roo": 0.24794258478192838,
                "wombat": 0.5123212089064794
            }
        },
        "lines": [
            [
                1,
                0.26595744680851063,
                0.0,
                0.2375,
                0.0,
                0.09786989061600461,
                0.055970149253731345
            ],
            [
                0,
                0.014893617021276596,
                0.0,
                0.3112745098039216,
                0.14285714285714285,
                0.07599309153713299,
                0.4375
            ],
            [
                0,
                0.7191489361702128,
                1.0,
                0.45122549019607844,
                0.8571428571428571,
                0.8261370178468624,
                0.5065298507462687
            ]
        ]
    },
    "What is the total cost of all the vowels on \u201cWheel of Fortune\u201d?": {
        "raw_data": {
            "best_answer_by_ml": [
                "$2,500"
            ],
            "negative_question": false,
            "fraction_answers": {
                "$2,500": 0.5956542303714668,
                "$1,250": 0.1421048462398137,
                "$2,900": 0.26224092338871946
            },
            "rate_limited": false,
            "integer_answers": {
                "$2,500": 4,
                "$1,250": 0,
                "$2,900": 0
            },
            "data": {
                "wikipedia_search": [
                    0.0,
                    1.0,
                    1.0
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.8642156862745098,
                    2.5512745098039216,
                    1.5845098039215686
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    15.0,
                    17.0,
                    9.0
                ],
                "result_count": [
                    90.0,
                    2900.0,
                    38.0
                ]
            },
            "ml_answers": {
                "$2,500": 0.33298322360964533,
                "$1,250": 0.2895384793379233,
                "$2,900": 0.24490300069807097
            }
        },
        "lines": [
            [
                1,
                0.0,
                0,
                0.17284313725490197,
                0,
                0.36585365853658536,
                0.029722589167767502
            ],
            [
                0,
                0.5,
                0,
                0.5102549019607843,
                0,
                0.4146341463414634,
                0.9577278731836195
            ],
            [
                0,
                0.5,
                0,
                0.3169019607843137,
                0,
                0.21951219512195122,
                0.012549537648612946
            ]
        ]
    },
    "Which brand mascot was NOT a real person?": {
        "raw_data": {
            "best_answer_by_ml": [
                "sara lee"
            ],
            "negative_question": true,
            "fraction_answers": {
                "betty crocker": 0.4252099168001962,
                "sara lee": 0.12608120063590209,
                "little debbie": 0.4487088825639017
            },
            "rate_limited": false,
            "integer_answers": {
                "betty crocker": 3,
                "sara lee": 0,
                "little debbie": 3
            },
            "data": {
                "wikipedia_search": [
                    1.878128128128128,
                    0.9923673673673674,
                    2.1295045045045047
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_relation_to_question": [
                    1.5201608554996404,
                    1.175638030194105,
                    2.3042011143062546
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    120.0,
                    69.0,
                    101.0
                ],
                "result_count": [
                    160000.0,
                    22700.0,
                    84500.0
                ]
            },
            "ml_answers": {
                "betty crocker": 0.41054582778669424,
                "sara lee": 0.2794304775593598,
                "little debbie": 0.39365410017167446
            }
        },
        "lines": [
            [
                0,
                0.37562562562562557,
                0.0,
                0.3040321710999281,
                1.0,
                0.41379310344827586,
                0.5988023952095808
            ],
            [
                0,
                0.1984734734734735,
                0.0,
                0.235127606038821,
                0.0,
                0.23793103448275862,
                0.08495508982035928
            ],
            [
                1,
                0.42590090090090094,
                1.0,
                0.4608402228612509,
                0.0,
                0.3482758620689655,
                0.3162425149700599
            ]
        ]
    },
    "What iconic painting once hung in Napoleon's bedroom?": {
        "raw_data": {
            "best_answer_by_ml": [
                "mona lisa"
            ],
            "negative_question": false,
            "fraction_answers": {
                "the birth of venus": 0.1739500402318189,
                "mona lisa": 0.6035384021313326,
                "the starry night": 0.2225115576368486
            },
            "rate_limited": false,
            "integer_answers": {
                "the birth of venus": 1,
                "mona lisa": 4,
                "the starry night": 1
            },
            "data": {
                "wikipedia_search": [
                    2.7593782117163412,
                    0.7642600205549845,
                    0.4763617677286742
                ],
                "word_count_entities": [
                    0.0,
                    22.0,
                    1.0
                ],
                "word_relation_to_question": [
                    2.089193534738089,
                    0.8732757891173732,
                    3.0375306761445375
                ],
                "word_count_raw": [
                    0.0,
                    41.0,
                    0.0
                ],
                "word_count_appended": [
                    45.0,
                    222.0,
                    65.0
                ],
                "result_count": [
                    43100.0,
                    176000.0,
                    47800.0
                ]
            },
            "ml_answers": {
                "the birth of venus": 0.3188089201213009,
                "mona lisa": 0.5123212089064794,
                "the starry night": 0.2616867615999172
            }
        },
        "lines": [
            [
                0,
                0.6898445529290853,
                0.0,
                0.3481989224563482,
                0.0,
                0.1355421686746988,
                0.16148370176095916
            ],
            [
                1,
                0.19106500513874614,
                0.9565217391304348,
                0.14554596485289553,
                1.0,
                0.6686746987951807,
                0.6594230048707381
            ],
            [
                0,
                0.11909044193216856,
                0.043478260869565216,
                0.5062551126907563,
                0.0,
                0.19578313253012047,
                0.17909329336830274
            ]
        ]
    },
    "What is the correct pronunciation of the performer who sings \u201cSmooth Operator\u201d?": {
        "raw_data": {
            "best_answer_by_ml": [
                "shah-day"
            ],
            "negative_question": false,
            "fraction_answers": {
                "say-dee": 0.3701319095477387,
                "sayd": 0.041823701842546065,
                "shah-day": 0.5880443886097153
            },
            "rate_limited": false,
            "integer_answers": {
                "say-dee": 1,
                "sayd": 0,
                "shah-day": 3
            },
            "data": {
                "wikipedia_search": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    2.333333333333333,
                    0.16666666666666666,
                    1.5
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    153.0,
                    25.0,
                    21.0
                ],
                "result_count": [
                    0,
                    0,
                    0
                ]
            },
            "ml_answers": {
                "say-dee": 0.20239003499174044,
                "sayd": 0.22131431533958254,
                "shah-day": 0.4488714576673582
            }
        },
        "lines": [
            [
                1,
                0.0,
                0,
                0.5833333333333334,
                1.0,
                0.7688442211055276,
                0
            ],
            [
                0,
                0.0,
                0,
                0.04166666666666667,
                0.0,
                0.12562814070351758,
                0
            ],
            [
                0,
                1.0,
                0,
                0.37500000000000006,
                0.0,
                0.10552763819095477,
                0
            ]
        ]
    },
    "In which version of \u201cDragnet\u201d is the line \u201cJust the facts, ma\u2019am\u201d first said?": {
        "raw_data": {
            "best_answer_by_ml": [
                "'50s tv show"
            ],
            "negative_question": false,
            "fraction_answers": {
                "'80s movie": 0.32606047995561105,
                "'50s movie": 0.4007374462477458,
                "'50s tv show": 0.2732020737966431
            },
            "rate_limited": false,
            "integer_answers": {
                "'80s movie": 0,
                "'50s movie": 1,
                "'50s tv show": 2
            },
            "data": {
                "wikipedia_search": [
                    0.056179775280898875,
                    1.7348314606741573,
                    1.208988764044944
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    2.805277777777778,
                    1.7436111111111112,
                    1.4511111111111112
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    2.0,
                    2.0,
                    2.0
                ],
                "result_count": [
                    0,
                    0,
                    0
                ]
            },
            "ml_answers": {
                "'80s movie": 0.30968271843117734,
                "'50s movie": 0.29428185547539226,
                "'50s tv show": 0.34808545339564345
            }
        },
        "lines": [
            [
                0,
                0.018726591760299626,
                0,
                0.46754629629629624,
                0,
                0,
                0.3333333333333333
            ],
            [
                0,
                0.5782771535580524,
                0,
                0.29060185185185183,
                0,
                0,
                0.3333333333333333
            ],
            [
                1,
                0.40299625468164796,
                0,
                0.24185185185185185,
                0,
                0,
                0.3333333333333333
            ]
        ]
    },
    "What topic would a herpetologist study?": {
        "raw_data": {
            "best_answer_by_ml": [
                "crocodile teeth"
            ],
            "negative_question": false,
            "fraction_answers": {
                "venereal disease": 0.5780051119628776,
                "mushroom farming": 0.11478568092861423,
                "crocodile teeth": 0.30720920710850813
            },
            "rate_limited": false,
            "integer_answers": {
                "venereal disease": 3,
                "mushroom farming": 0,
                "crocodile teeth": 1
            },
            "data": {
                "wikipedia_search": [
                    1.8952380952380952,
                    0.2857142857142857,
                    0.819047619047619
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.7359903381642513,
                    0.23792270531400966,
                    2.026086956521739
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    35.0,
                    19.0,
                    20.0
                ],
                "result_count": [
                    8120.0,
                    235.0,
                    86.0
                ]
            },
            "ml_answers": {
                "venereal disease": 0.2930432726722662,
                "mushroom farming": 0.26008719609337455,
                "crocodile teeth": 0.3161458758511387
            }
        },
        "lines": [
            [
                0,
                0.6317460317460317,
                0,
                0.2453301127214171,
                0,
                0.47297297297297297,
                0.9619713304110887
            ],
            [
                0,
                0.09523809523809523,
                0,
                0.07930756843800323,
                0,
                0.25675675675675674,
                0.027840303281601708
            ],
            [
                1,
                0.273015873015873,
                0,
                0.6753623188405796,
                0,
                0.2702702702702703,
                0.01018836630730956
            ]
        ]
    },
    "Which writer has stated that his/her trademark series of books would never be adapted for film?": {
        "raw_data": {
            "best_answer_by_ml": [
                "jeff kinney"
            ],
            "negative_question": false,
            "fraction_answers": {
                "jeff kinney": 0.4392588515502246,
                "james patterson": 0.2606789432525787,
                "sue grafton": 0.3000622051971966
            },
            "rate_limited": false,
            "integer_answers": {
                "jeff kinney": 2,
                "james patterson": 2,
                "sue grafton": 1
            },
            "data": {
                "wikipedia_search": [
                    3.6736054052657416,
                    2.3263945947342584,
                    0.0
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_relation_to_question": [
                    1.8427523456935222,
                    2.458835405820082,
                    1.6984122484863957
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    60.0,
                    49.0,
                    48.0
                ],
                "result_count": [
                    94.0,
                    20000.0,
                    31100.0
                ]
            },
            "ml_answers": {
                "jeff kinney": 0.38604983760522776,
                "james patterson": 0.22083756184996922,
                "sue grafton": 0.31459334479604456
            }
        },
        "lines": [
            [
                0,
                0.6122675675442902,
                0.0,
                0.3071253909489204,
                0,
                0.3821656050955414,
                0.001836152674141501
            ],
            [
                1,
                0.3877324324557097,
                0.0,
                0.4098059009700137,
                0,
                0.31210191082802546,
                0.39067078173223424
            ],
            [
                0,
                0.0,
                1.0,
                0.28306870808106593,
                0,
                0.3057324840764331,
                0.6074930655936243
            ]
        ]
    },
    "If you tunneled through the center of the earth from Honolulu, what country would you end up in?": {
        "raw_data": {
            "best_answer_by_ml": [
                "mongolia"
            ],
            "negative_question": false,
            "fraction_answers": {
                "norway": 0.38950845790289046,
                "botswana": 0.286440133012564,
                "mongolia": 0.32405140908454555
            },
            "rate_limited": false,
            "integer_answers": {
                "norway": 3,
                "botswana": 1,
                "mongolia": 0
            },
            "data": {
                "wikipedia_search": [
                    0.7785730737018426,
                    2.1913452680067,
                    1.0300816582914574
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.4984398741735654,
                    1.0528511680616326,
                    1.448708957764802
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    93.0,
                    104.0,
                    90.0
                ],
                "result_count": [
                    12800.0,
                    19500.0,
                    18400.0
                ]
            },
            "ml_answers": {
                "norway": 0.29739417434232246,
                "botswana": 0.27664580056353033,
                "mongolia": 0.3166157646900897
            }
        },
        "lines": [
            [
                1,
                0.19464326842546065,
                0,
                0.37460996854339135,
                0,
                0.3240418118466899,
                0.252465483234714
            ],
            [
                0,
                0.547836317001675,
                0,
                0.26321279201540815,
                0,
                0.3623693379790941,
                0.38461538461538464
            ],
            [
                0,
                0.25752041457286434,
                0,
                0.3621772394412005,
                0,
                0.313588850174216,
                0.3629191321499014
            ]
        ]
    },
    "Which of these two U.S. cities are in the same time zone?": {
        "raw_data": {
            "best_answer_by_ml": [
                "bismarck / cheyenne"
            ],
            "negative_question": false,
            "fraction_answers": {
                "el paso / pierre": 0.17735601776981458,
                "pensacola / sioux falls": 0.22001638765521675,
                "bismarck / cheyenne": 0.6026275945749686
            },
            "rate_limited": false,
            "integer_answers": {
                "el paso / pierre": 1,
                "pensacola / sioux falls": 0,
                "bismarck / cheyenne": 3
            },
            "data": {
                "wikipedia_search": [
                    0.043478260869565216,
                    2.3014492753623186,
                    0.6550724637681159
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.7797252698242797,
                    2.391624297564892,
                    0.8286504326108286
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    3.0,
                    3.0,
                    3.0
                ],
                "result_count": [
                    11.0,
                    47.0,
                    8.0
                ]
            },
            "ml_answers": {
                "el paso / pierre": 0.3127030534205965,
                "pensacola / sioux falls": 0.2958143138425335,
                "bismarck / cheyenne": 0.34365328002001955
            }
        },
        "lines": [
            [
                0,
                0.014492753623188406,
                0,
                0.19493131745606992,
                0,
                0.3333333333333333,
                0.16666666666666666
            ],
            [
                0,
                0.7671497584541062,
                0,
                0.597906074391223,
                0,
                0.3333333333333333,
                0.7121212121212122
            ],
            [
                1,
                0.2183574879227053,
                0,
                0.20716260815270715,
                0,
                0.3333333333333333,
                0.12121212121212122
            ]
        ]
    },
    "Until it was banned, lithium was a key ingredient in which of these brands?": {
        "raw_data": {
            "best_answer_by_ml": [
                "7up"
            ],
            "negative_question": false,
            "fraction_answers": {
                "cracker jack": 0.08459787220455882,
                "7up": 0.8951290161818296,
                "good and plenty": 0.020273111613611592
            },
            "rate_limited": false,
            "integer_answers": {
                "cracker jack": 0,
                "7up": 6,
                "good and plenty": 0
            },
            "data": {
                "wikipedia_search": [
                    0.40540540540540543,
                    2.5945945945945947,
                    0.0
                ],
                "word_count_entities": [
                    0.0,
                    5.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.4361003861003861,
                    3.1588803088803092,
                    0.40501930501930505
                ],
                "word_count_raw": [
                    0.0,
                    9.0,
                    0.0
                ],
                "word_count_appended": [
                    9.0,
                    184.0,
                    4.0
                ],
                "result_count": [
                    8240.0,
                    29600.0,
                    3.0
                ]
            },
            "ml_answers": {
                "cracker jack": 0.29758130834521007,
                "7up": 0.5315519547669878,
                "good and plenty": 0.23989384238959452
            }
        },
        "lines": [
            [
                0,
                0.13513513513513514,
                0.0,
                0.10902509652509652,
                0.0,
                0.04568527918781726,
                0.21774172237930398
            ],
            [
                1,
                0.8648648648648649,
                1.0,
                0.7897200772200773,
                1.0,
                0.934010152284264,
                0.7821790027217715
            ],
            [
                0,
                0.0,
                0.0,
                0.10125482625482626,
                0.0,
                0.02030456852791878,
                7.927489892450387e-05
            ]
        ]
    },
    "Aside from blood cells, what would you also find inside your blood vessels?": {
        "raw_data": {
            "best_answer_by_ml": [
                "marrow"
            ],
            "negative_question": false,
            "fraction_answers": {
                "marrow": 0.4888237940889546,
                "plasma": 0.450459997513528,
                "plastids": 0.06071620839751743
            },
            "rate_limited": false,
            "integer_answers": {
                "marrow": 3,
                "plasma": 3,
                "plastids": 0
            },
            "data": {
                "wikipedia_search": [
                    0.4355825461088619,
                    4.406522717049032,
                    0.15789473684210525
                ],
                "word_count_entities": [
                    15.0,
                    10.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.5970406049107475,
                    2.7462943680356373,
                    0.6566650270536153
                ],
                "word_count_raw": [
                    14.0,
                    4.0,
                    0.0
                ],
                "word_count_appended": [
                    251.0,
                    314.0,
                    117.0
                ],
                "result_count": [
                    417000.0,
                    318000.0,
                    22600.0
                ]
            },
            "ml_answers": {
                "marrow": 0.3712993192204382,
                "plasma": 0.3158217679842297,
                "plastids": 0.23947130236001732
            }
        },
        "lines": [
            [
                1,
                0.08711650922177237,
                0.6,
                0.31940812098214943,
                0.7777777777777778,
                0.3680351906158358,
                0.5504223864836325
            ],
            [
                0,
                0.8813045434098065,
                0.4,
                0.5492588736071273,
                0.2222222222222222,
                0.4604105571847507,
                0.4197465681098205
            ],
            [
                0,
                0.031578947368421054,
                0.0,
                0.13133300541072304,
                0.0,
                0.17155425219941348,
                0.02983104540654699
            ]
        ]
    },
    "Which of these Kentucky Derby winners was named for its trainer?": {
        "raw_data": {
            "best_answer_by_ml": [
                "clyde van dusen"
            ],
            "negative_question": false,
            "fraction_answers": {
                "lieut. gibson": 0.08104430539351078,
                "paul jones": 0.17327936443014447,
                "clyde van dusen": 0.7456763301763447
            },
            "rate_limited": false,
            "integer_answers": {
                "lieut. gibson": 0,
                "paul jones": 0,
                "clyde van dusen": 5
            },
            "data": {
                "wikipedia_search": [
                    2.2,
                    0.0,
                    0.8
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    4.403426084618745,
                    0.15203145478374835,
                    0.44454246059750646
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    62.0,
                    36.0,
                    5.0
                ],
                "result_count": [
                    23700.0,
                    22500.0,
                    51.0
                ]
            },
            "ml_answers": {
                "lieut. gibson": 0.23989384238959452,
                "paul jones": 0.3455868528921485,
                "clyde van dusen": 0.45458213641642065
            }
        },
        "lines": [
            [
                1,
                0.7333333333333334,
                0,
                0.8806852169237491,
                1.0,
                0.6019417475728155,
                0.5124213530518259
            ],
            [
                0,
                0.0,
                0,
                0.03040629095674967,
                0.0,
                0.34951456310679613,
                0.48647596808717647
            ],
            [
                0,
                0.26666666666666666,
                0,
                0.08890849211950129,
                0.0,
                0.04854368932038835,
                0.0011026788609976
            ]
        ]
    },
    "How many of the three Baltic countries border Russia?": {
        "raw_data": {
            "best_answer_by_ml": [
                "three"
            ],
            "negative_question": false,
            "fraction_answers": {
                "none": 0.46611882658670384,
                "three": 0.3790446838380062,
                "two": 0.15483648957528995
            },
            "rate_limited": false,
            "integer_answers": {
                "none": 2,
                "three": 3,
                "two": 1
            },
            "data": {
                "wikipedia_search": [
                    0.7093167701863354,
                    0.5149068322981366,
                    1.775776397515528
                ],
                "word_count_entities": [
                    266.0,
                    1.0,
                    118.0
                ],
                "word_relation_to_question": [
                    0.6644760861628332,
                    1.5282949981745162,
                    0.8072289156626506
                ],
                "word_count_raw": [
                    251.0,
                    10.0,
                    113.0
                ],
                "word_count_appended": [
                    397.0,
                    191.0,
                    286.0
                ],
                "result_count": [
                    86.0,
                    97.0,
                    1200000.0
                ]
            },
            "ml_answers": {
                "none": 0.28340582964012107,
                "three": 0.37424050470116205,
                "two": 0.3170612771987553
            }
        },
        "lines": [
            [
                1,
                0.23643892339544514,
                0.6909090909090909,
                0.2214920287209444,
                0.6711229946524064,
                0.45423340961098396,
                7.165573916644378e-05
            ],
            [
                0,
                0.17163561076604553,
                0.0025974025974025974,
                0.509431666058172,
                0.026737967914438502,
                0.21853546910755148,
                8.082100812959357e-05
            ],
            [
                0,
                0.5919254658385094,
                0.3064935064935065,
                0.26907630522088355,
                0.30213903743315507,
                0.32723112128146453,
                0.999847523252704
            ]
        ]
    },
    "Which of these actors was a high school cheerleader?": {
        "raw_data": {
            "best_answer_by_ml": [
                "michael douglas"
            ],
            "negative_question": false,
            "fraction_answers": {
                "john travolta": 0.3025406255894061,
                "george clooney": 0.2351993659669413,
                "michael douglas": 0.46226000844365267
            },
            "rate_limited": false,
            "integer_answers": {
                "john travolta": 3,
                "george clooney": 0,
                "michael douglas": 2
            },
            "data": {
                "wikipedia_search": [
                    1.0944444444444446,
                    0.31666666666666665,
                    1.588888888888889
                ],
                "word_count_entities": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.6615384615384616,
                    1.1205128205128205,
                    1.2179487179487178
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    41.0,
                    44.0,
                    51.0
                ],
                "result_count": [
                    166000.0,
                    292000.0,
                    116000.0
                ]
            },
            "ml_answers": {
                "john travolta": 0.24062111712601966,
                "george clooney": 0.28680827176911067,
                "michael douglas": 0.39952910067975217
            }
        },
        "lines": [
            [
                0,
                0.36481481481481487,
                0.0,
                0.22051282051282053,
                0,
                0.3014705882352941,
                0.289198606271777
            ],
            [
                1,
                0.10555555555555556,
                1.0,
                0.3735042735042735,
                0,
                0.3235294117647059,
                0.5087108013937283
            ],
            [
                0,
                0.5296296296296297,
                0.0,
                0.40598290598290593,
                0,
                0.375,
                0.20209059233449478
            ]
        ]
    },
    "J.K. Rowling\u2019s first book published in England was titled \u201cHarry Potter and the\u201d what?": {
        "raw_data": {
            "best_answer_by_ml": [
                "sorcerer's stone"
            ],
            "negative_question": false,
            "fraction_answers": {
                "magician's stone": 0.09169062271759376,
                "philosopher's stone": 0.1926753287910765,
                "sorcerer's stone": 0.7156340484913298
            },
            "rate_limited": false,
            "integer_answers": {
                "magician's stone": 0,
                "philosopher's stone": 1,
                "sorcerer's stone": 3
            },
            "data": {
                "wikipedia_search": [
                    0.5151515151515151,
                    5.462121212121212,
                    0.022727272727272728
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.6898389418143167,
                    5.18415935328387,
                    1.1260017049018136
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    4.0,
                    3.0,
                    2.0
                ],
                "result_count": [
                    201000.0,
                    6690000.0,
                    15.0
                ]
            },
            "ml_answers": {
                "magician's stone": 0.22710263402264538,
                "philosopher's stone": 0.30319617383614056,
                "sorcerer's stone": 0.34365328002001955
            }
        },
        "lines": [
            [
                1,
                0.08585858585858587,
                0,
                0.2112298677267896,
                0,
                0.4444444444444444,
                0.029168417134485994
            ],
            [
                0,
                0.9103535353535355,
                0,
                0.6480199191604837,
                0,
                0.3333333333333333,
                0.9708294061179666
            ],
            [
                0,
                0.0037878787878787884,
                0,
                0.1407502131127267,
                0,
                0.2222222222222222,
                2.176747547349701e-06
            ]
        ]
    },
    "How do you spell the last name of Duke University\u2019s men's basketball coach?": {
        "raw_data": {
            "best_answer_by_ml": [
                "krzyzewski"
            ],
            "negative_question": false,
            "fraction_answers": {
                "khzyrweski": 0.06557083211607752,
                "crzyzewski": 0.06712884275780609,
                "krzyzewski": 0.8673003251261164
            },
            "rate_limited": false,
            "integer_answers": {
                "khzyrweski": 0,
                "crzyzewski": 1,
                "krzyzewski": 5
            },
            "data": {
                "wikipedia_search": [
                    0.0,
                    6.0,
                    0.0
                ],
                "word_count_entities": [
                    0.0,
                    127.0,
                    0.0
                ],
                "word_relation_to_question": [
                    2.410416666666667,
                    2.1791666666666663,
                    2.410416666666667
                ],
                "word_count_raw": [
                    0.0,
                    33.0,
                    0.0
                ],
                "word_count_appended": [
                    19.0,
                    291.0,
                    16.0
                ],
                "result_count": [
                    3.0,
                    20600.0,
                    0
                ]
            },
            "ml_answers": {
                "khzyrweski": 0.24906620845182095,
                "crzyzewski": 0.25789411121874123,
                "krzyzewski": 0.5315519547669878
            }
        },
        "lines": [
            [
                0,
                0.0,
                0.0,
                0.34434523809523815,
                0.0,
                0.0001456098626413629,
                0.05828220858895705
            ],
            [
                1,
                1.0,
                1.0,
                0.31130952380952376,
                1.0,
                0.9998543901373587,
                0.8926380368098159
            ],
            [
                0,
                0.0,
                0.0,
                0.34434523809523815,
                0.0,
                0.0,
                0.049079754601226995
            ]
        ]
    },
    "Which of these consists of frozen water?": {
        "raw_data": {
            "best_answer_by_ml": [
                "snowflake"
            ],
            "negative_question": false,
            "fraction_answers": {
                "garden rake": 0.032655296359551104,
                "snowflake": 0.5967152799281716,
                "drake": 0.3706294237122772
            },
            "rate_limited": false,
            "integer_answers": {
                "garden rake": 0,
                "snowflake": 4,
                "drake": 0
            },
            "data": {
                "wikipedia_search": [
                    1.3636363636363638,
                    0.0,
                    0.6363636363636364
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.2857142857142856,
                    0.14285714285714285,
                    0.5714285714285714
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    456.0,
                    44.0,
                    404.0
                ],
                "result_count": [
                    5620000.0,
                    106000.0,
                    4350000.0
                ]
            },
            "ml_answers": {
                "garden rake": 0.23989384238959452,
                "snowflake": 0.3466409181078626,
                "drake": 0.30619748317698575
            }
        },
        "lines": [
            [
                1,
                0.6818181818181819,
                0,
                0.6428571428571429,
                0,
                0.504424778761062,
                0.5577610162763001
            ],
            [
                0,
                0.0,
                0,
                0.07142857142857144,
                0,
                0.048672566371681415,
                0.010520047637951568
            ],
            [
                0,
                0.3181818181818182,
                0,
                0.28571428571428575,
                0,
                0.4469026548672566,
                0.4317189360857483
            ]
        ]
    },
    "What word describes joining a cause just to feel good about it?": {
        "raw_data": {
            "best_answer_by_ml": [
                "slacktivism"
            ],
            "negative_question": false,
            "fraction_answers": {
                "slacktivism": 0.8380302943742988,
                "gung-faux": 0.07178809086969813,
                "joinerism": 0.09018161475600305
            },
            "rate_limited": false,
            "integer_answers": {
                "slacktivism": 5,
                "gung-faux": 0,
                "joinerism": 0
            },
            "data": {
                "wikipedia_search": [
                    0.524390243902439,
                    1.8671984840149152,
                    4.608411272082646
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.9908974358974358,
                    0.33565018315018313,
                    4.673452380952381
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    7.0
                ],
                "word_count_appended": [
                    40.0,
                    20.0,
                    392.0
                ],
                "result_count": [
                    39.0,
                    0,
                    12600.0
                ]
            },
            "ml_answers": {
                "slacktivism": 0.44340299837836944,
                "gung-faux": 0.23989384238959452,
                "joinerism": 0.20931099396584285
            }
        },
        "lines": [
            [
                0,
                0.07491289198606273,
                0,
                0.2844139194139194,
                0.0,
                0.08849557522123894,
                0.0030856871587942084
            ],
            [
                0,
                0.2667426405735593,
                0,
                0.047950026164311876,
                0.0,
                0.04424778761061947,
                0.0
            ],
            [
                1,
                0.6583444674403779,
                0,
                0.6676360544217687,
                1.0,
                0.8672566371681416,
                0.9969143128412058
            ]
        ]
    },
    "One of Apple\u2019s biggest flops was a product named after a man who did what?": {
        "raw_data": {
            "best_answer_by_ml": [
                "invented the transistor"
            ],
            "negative_question": false,
            "fraction_answers": {
                "invented the transistor": 0.47480272119352,
                "developed calculus": 0.20322270514174245,
                "discovered saturn": 0.32197457366473753
            },
            "rate_limited": false,
            "integer_answers": {
                "invented the transistor": 4,
                "developed calculus": 0,
                "discovered saturn": 0
            },
            "data": {
                "wikipedia_search": [
                    1.2596783867927224,
                    1.7077499452553004,
                    1.032571667951977
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.603049699791044,
                    2.431789527512542,
                    0.9651607726964142
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    8.0,
                    11.0,
                    3.0
                ],
                "result_count": [
                    41.0,
                    69.0,
                    32.0
                ]
            },
            "ml_answers": {
                "invented the transistor": 0.37588733282179454,
                "developed calculus": 0.2750499707226464,
                "discovered saturn": 0.30816380664347254
            }
        },
        "lines": [
            [
                0,
                0.3149195966981806,
                0,
                0.3206099399582088,
                0,
                0.2887323943661972,
                0.36363636363636365
            ],
            [
                0,
                0.4269374863138251,
                0,
                0.48635790550250835,
                0,
                0.4859154929577465,
                0.5
            ],
            [
                1,
                0.25814291698799424,
                0,
                0.19303215453928285,
                0,
                0.22535211267605634,
                0.13636363636363635
            ]
        ]
    },
    "What dish is made with ham, poached eggs and Hollandaise sauce?": {
        "raw_data": {
            "best_answer_by_ml": [
                "eggs benedict"
            ],
            "negative_question": false,
            "fraction_answers": {
                "eggs benedict": 0.8792533213420349,
                "benedict cumberbatch": 0.08421016726820274,
                "pope benedict": 0.036536511389762334
            },
            "rate_limited": false,
            "integer_answers": {
                "eggs benedict": 6,
                "benedict cumberbatch": 0,
                "pope benedict": 0
            },
            "data": {
                "wikipedia_search": [
                    0.0,
                    5.499350649350649,
                    1.5006493506493506
                ],
                "word_count_entities": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.5189964157706093,
                    5.790322580645161,
                    0.6906810035842295
                ],
                "word_count_raw": [
                    0.0,
                    145.0,
                    0.0
                ],
                "word_count_appended": [
                    43.0,
                    210.0,
                    46.0
                ],
                "result_count": [
                    537.0,
                    408000.0,
                    16300.0
                ]
            },
            "ml_answers": {
                "eggs benedict": 0.5315519547669878,
                "benedict cumberbatch": 0.2550202619473189,
                "pope benedict": 0.22131431533958254
            }
        },
        "lines": [
            [
                0,
                0.0,
                0.0,
                0.07414234511008704,
                0.0,
                0.14381270903010032,
                0.0012640141983866753
            ],
            [
                1,
                0.7856215213358071,
                1.0,
                0.8271889400921658,
                1.0,
                0.7023411371237458,
                0.9603683295004908
            ],
            [
                0,
                0.21437847866419293,
                0.0,
                0.09866871479774707,
                0.0,
                0.15384615384615385,
                0.03836765630112255
            ]
        ]
    },
    "Which verb describes the sound minerals make when they are heated?": {
        "raw_data": {
            "best_answer_by_ml": [
                "decrepitate"
            ],
            "negative_question": false,
            "fraction_answers": {
                "frangelle": 0.026276789785681716,
                "decrepitate": 0.9411964204286367,
                "recleft": 0.032526789785681715
            },
            "rate_limited": false,
            "integer_answers": {
                "frangelle": 0,
                "decrepitate": 5,
                "recleft": 0
            },
            "data": {
                "wikipedia_search": [
                    2.0,
                    0.0,
                    0.0
                ],
                "word_count_entities": [
                    7.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    3.8161764705882355,
                    0.029411764705882353,
                    0.15441176470588236
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    97.0,
                    16.0,
                    16.0
                ],
                "result_count": [
                    4470.0,
                    0,
                    0
                ]
            },
            "ml_answers": {
                "frangelle": 0.22131431533958254,
                "decrepitate": 0.42340284570691367,
                "recleft": 0.22131431533958254
            }
        },
        "lines": [
            [
                1,
                1.0,
                1.0,
                0.9540441176470589,
                0,
                0.751937984496124,
                1.0
            ],
            [
                0,
                0.0,
                0.0,
                0.007352941176470588,
                0,
                0.12403100775193798,
                0.0
            ],
            [
                0,
                0.0,
                0.0,
                0.03860294117647059,
                0,
                0.12403100775193798,
                0.0
            ]
        ]
    },
    "Talking is discouraged on what Amtrak car?": {
        "raw_data": {
            "best_answer_by_ml": [
                "quiet car"
            ],
            "negative_question": false,
            "fraction_answers": {
                "sports argument car": 0.10099955857002503,
                "quiet car": 0.8410325328384696,
                "meet & greet car": 0.05796790859150549
            },
            "rate_limited": false,
            "integer_answers": {
                "sports argument car": 0,
                "quiet car": 5,
                "meet & greet car": 0
            },
            "data": {
                "wikipedia_search": [
                    0.47368421052631576,
                    0.3157894736842105,
                    1.2105263157894737
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.39178515007898895,
                    0.09913112164296997,
                    2.509083728278041
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    3.0
                ],
                "word_count_appended": [
                    11.0,
                    0.0,
                    100.0
                ],
                "result_count": [
                    14.0,
                    36.0,
                    314.0
                ]
            },
            "ml_answers": {
                "sports argument car": 0.2550202619473189,
                "quiet car": 0.44340299837836944,
                "meet & greet car": 0.2850867906729111
            }
        },
        "lines": [
            [
                0,
                0.23684210526315788,
                0,
                0.13059505002632965,
                0.0,
                0.0990990990990991,
                0.038461538461538464
            ],
            [
                0,
                0.15789473684210525,
                0,
                0.03304370721432332,
                0.0,
                0.0,
                0.0989010989010989
            ],
            [
                1,
                0.6052631578947368,
                0,
                0.836361242759347,
                1.0,
                0.9009009009009009,
                0.8626373626373627
            ]
        ]
    },
    "The creator of Wonder Woman also created an early version of what device?": {
        "raw_data": {
            "best_answer_by_ml": [
                "lie detector"
            ],
            "negative_question": false,
            "fraction_answers": {
                "lie detector": 0.6688728275837312,
                "hearing aid": 0.22026697220019562,
                "magic marker": 0.11086020021607317
            },
            "rate_limited": false,
            "integer_answers": {
                "lie detector": 5,
                "hearing aid": 1,
                "magic marker": 0
            },
            "data": {
                "wikipedia_search": [
                    0.25396825396825395,
                    2.3853820598006643,
                    4.360649686231082
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    13.0
                ],
                "word_relation_to_question": [
                    0.7604166666666666,
                    2.28125,
                    1.9583333333333333
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    24.0
                ],
                "word_count_appended": [
                    15.0,
                    42.0,
                    62.0
                ],
                "result_count": [
                    47.0,
                    23.0,
                    64.0
                ]
            },
            "ml_answers": {
                "lie detector": 0.5029543303809376,
                "hearing aid": 0.3501295990105652,
                "magic marker": 0.35234680825751
            }
        },
        "lines": [
            [
                0,
                0.036281179138321996,
                0.0,
                0.15208333333333332,
                0.0,
                0.12605042016806722,
                0.35074626865671643
            ],
            [
                0,
                0.34076886568580916,
                0.0,
                0.45625,
                0.0,
                0.35294117647058826,
                0.17164179104477612
            ],
            [
                1,
                0.6229499551758689,
                1.0,
                0.39166666666666666,
                1.0,
                0.5210084033613446,
                0.47761194029850745
            ]
        ]
    },
    "Which of these verbs has two meanings that are opposites of each other?": {
        "raw_data": {
            "best_answer_by_ml": [
                "cleave"
            ],
            "negative_question": false,
            "fraction_answers": {
                "cleave": 0.6804442450824031,
                "jut": 0.19664739500265815,
                "branch": 0.12290835991493886
            },
            "rate_limited": false,
            "integer_answers": {
                "cleave": 4,
                "jut": 0,
                "branch": 0
            },
            "data": {
                "wikipedia_search": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    7.0
                ],
                "word_relation_to_question": [
                    0.0,
                    0.3333333333333333,
                    1.6666666666666665
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    239.0,
                    194.0,
                    287.0
                ],
                "result_count": [
                    2670000.0,
                    5860000.0,
                    8190000.0
                ]
            },
            "ml_answers": {
                "cleave": 0.4222393765222859,
                "jut": 0.35813512694057287,
                "branch": 0.29907666595983784
            }
        },
        "lines": [
            [
                0,
                0,
                0.0,
                0.0,
                0,
                0.33194444444444443,
                0.159688995215311
            ],
            [
                0,
                0,
                0.0,
                0.16666666666666669,
                0,
                0.26944444444444443,
                0.35047846889952156
            ],
            [
                1,
                0,
                1.0,
                0.8333333333333334,
                0,
                0.39861111111111114,
                0.48983253588516745
            ]
        ]
    },
    "Which of these is NOT a real animal?": {
        "raw_data": {
            "best_answer_by_ml": [
                "wholphin"
            ],
            "negative_question": true,
            "fraction_answers": {
                "liger": 0.6178042558417797,
                "wholphin": 0.11705491964676469,
                "jackalope": 0.26514082451145565
            },
            "rate_limited": false,
            "integer_answers": {
                "liger": 5,
                "wholphin": 0,
                "jackalope": 1
            },
            "data": {
                "wikipedia_search": [
                    1.9609413302827259,
                    0.7123713025004086,
                    0.32668736721686553
                ],
                "word_count_entities": [
                    0.0,
                    8.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.6189354005167959,
                    1.365705426356589,
                    1.015359173126615
                ],
                "word_count_raw": [
                    0.0,
                    5.0,
                    0.0
                ],
                "word_count_appended": [
                    524.0,
                    620.0,
                    306.0
                ],
                "result_count": [
                    2060000.0,
                    3270000.0,
                    245000.0
                ]
            },
            "ml_answers": {
                "liger": 0.5029543303809376,
                "wholphin": 0.2499866810590823,
                "jackalope": 0.3459775902352217
            }
        },
        "lines": [
            [
                1,
                0.6536471100942419,
                0.0,
                0.20631180017226528,
                0.0,
                0.36137931034482756,
                0.3695067264573991
            ],
            [
                0,
                0.23745710083346952,
                1.0,
                0.455235142118863,
                1.0,
                0.42758620689655175,
                0.5865470852017938
            ],
            [
                0,
                0.10889578907228852,
                0.0,
                0.33845305770887163,
                0.0,
                0.2110344827586207,
                0.04394618834080718
            ]
        ]
    },
    "Pixies, Bon Iver, Iron & Wine and Bauhaus were all once signed to which record label?": {
        "raw_data": {
            "best_answer_by_ml": [
                "4ad"
            ],
            "negative_question": false,
            "fraction_answers": {
                "4ad": 0.7187085424532867,
                "geffen": 0.1225179214207251,
                "subpop": 0.15877353612598813
            },
            "rate_limited": false,
            "integer_answers": {
                "4ad": 5,
                "geffen": 0,
                "subpop": 1
            },
            "data": {
                "wikipedia_search": [
                    2.0054018445322797,
                    4.091304347826087,
                    0.9032938076416337
                ],
                "word_count_entities": [
                    0.0,
                    141.0,
                    1.0
                ],
                "word_relation_to_question": [
                    2.39539715069043,
                    4.813393126962657,
                    1.7912097223469146
                ],
                "word_count_raw": [
                    0.0,
                    12.0,
                    0.0
                ],
                "word_count_appended": [
                    1.0,
                    13.0,
                    1.0
                ],
                "result_count": [
                    31.0,
                    31.0,
                    31.0
                ]
            },
            "ml_answers": {
                "4ad": 0.5221850762414461,
                "geffen": 0.29976535391250525,
                "subpop": 0.25395470723645064
            }
        },
        "lines": [
            [
                0,
                0.28648597779032564,
                0.0,
                0.2661552389656033,
                0.0,
                0.3333333333333333,
                0.06666666666666667
            ],
            [
                1,
                0.584472049689441,
                0.9929577464788732,
                0.5348214585514062,
                1.0,
                0.3333333333333333,
                0.8666666666666667
            ],
            [
                0,
                0.1290419725202334,
                0.007042253521126761,
                0.19902330248299047,
                0.0,
                0.3333333333333333,
                0.06666666666666667
            ]
        ]
    },
    "Which of these would an oologist study?": {
        "raw_data": {
            "best_answer_by_ml": [
                "human liver"
            ],
            "negative_question": false,
            "fraction_answers": {
                "ice cave": 0.0408344733242134,
                "human liver": 0.3368160054719562,
                "ostrich egg": 0.6223495212038304
            },
            "rate_limited": false,
            "integer_answers": {
                "ice cave": 0,
                "human liver": 1,
                "ostrich egg": 3
            },
            "data": {
                "wikipedia_search": [
                    0.0,
                    0.08333333333333333,
                    0.9166666666666666
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.0,
                    0.9333333333333333,
                    0.06666666666666667
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    10.0,
                    20.0,
                    56.0
                ],
                "result_count": [
                    24.0,
                    50.0,
                    436.0
                ]
            },
            "ml_answers": {
                "ice cave": 0.23947130236001732,
                "human liver": 0.32459723880436375,
                "ostrich egg": 0.3098856770011492
            }
        },
        "lines": [
            [
                0,
                0.0,
                0,
                0.0,
                0,
                0.11627906976744186,
                0.047058823529411764
            ],
            [
                0,
                0.08333333333333333,
                0,
                0.9333333333333333,
                0,
                0.23255813953488372,
                0.09803921568627451
            ],
            [
                1,
                0.9166666666666666,
                0,
                0.06666666666666667,
                0,
                0.6511627906976745,
                0.8549019607843137
            ]
        ]
    },
    "Who holds the record as the youngest solo artist with a Billboard #1 hit?": {
        "raw_data": {
            "best_answer_by_ml": [
                "michael jackson"
            ],
            "negative_question": false,
            "fraction_answers": {
                "justin bieber": 0.3956474061263049,
                "stevie wonder": 0.30438567077307993,
                "michael jackson": 0.2999669231006152
            },
            "rate_limited": false,
            "integer_answers": {
                "justin bieber": 4,
                "stevie wonder": 0,
                "michael jackson": 2
            },
            "data": {
                "wikipedia_search": [
                    2.8547565138991158,
                    4.75777751511861,
                    1.387465970982275
                ],
                "word_count_entities": [
                    10.0,
                    6.0,
                    7.0
                ],
                "word_relation_to_question": [
                    2.3085866121426464,
                    3.4156535514294135,
                    2.27575983642794
                ],
                "word_count_raw": [
                    12.0,
                    3.0,
                    9.0
                ],
                "word_count_appended": [
                    12.0,
                    3.0,
                    9.0
                ],
                "result_count": [
                    647000.0,
                    647000.0,
                    647000.0
                ]
            },
            "ml_answers": {
                "justin bieber": 0.286864368196669,
                "stevie wonder": 0.2857882432318079,
                "michael jackson": 0.290891667018397
            }
        },
        "lines": [
            [
                0,
                0.31719516821101285,
                0.43478260869565216,
                0.2885733265178308,
                0.5,
                0.5,
                0.3333333333333333
            ],
            [
                0,
                0.5286419461242899,
                0.2608695652173913,
                0.4269566939286767,
                0.125,
                0.125,
                0.3333333333333333
            ],
            [
                1,
                0.1541628856646972,
                0.30434782608695654,
                0.2844699795534925,
                0.375,
                0.375,
                0.3333333333333333
            ]
        ]
    },
    "Which of these states does NOT touch the Mason-Dixon Line?": {
        "raw_data": {
            "best_answer_by_ml": [
                "west virginia"
            ],
            "negative_question": true,
            "fraction_answers": {
                "tennessee": 0.2280003982912578,
                "delaware": 0.5313961333575683,
                "west virginia": 0.24060346835117388
            },
            "rate_limited": false,
            "integer_answers": {
                "tennessee": 0,
                "delaware": 4,
                "west virginia": 1
            },
            "data": {
                "wikipedia_search": [
                    1.0734902364916081,
                    0.9728810078526103,
                    0.9536287556557816
                ],
                "word_count_entities": [
                    8.0,
                    18.0,
                    3.0
                ],
                "word_relation_to_question": [
                    1.0215802101895635,
                    2.7942759123882754,
                    1.184143877422161
                ],
                "word_count_raw": [
                    3.0,
                    7.0,
                    1.0
                ],
                "word_count_appended": [
                    55.0,
                    308.0,
                    233.0
                ],
                "result_count": [
                    0,
                    0,
                    0
                ]
            },
            "ml_answers": {
                "tennessee": 0.23692976605918165,
                "delaware": 0.3531172370984082,
                "west virginia": 0.21439335636548013
            }
        },
        "lines": [
            [
                0,
                0.357830078830536,
                0.27586206896551724,
                0.2043160420379127,
                0.2727272727272727,
                0.09228187919463088,
                0
            ],
            [
                0,
                0.32429366928420345,
                0.6206896551724138,
                0.5588551824776551,
                0.6363636363636364,
                0.5167785234899329,
                0
            ],
            [
                1,
                0.31787625188526053,
                0.10344827586206896,
                0.2368287754844322,
                0.09090909090909091,
                0.39093959731543626,
                0
            ]
        ]
    },
    "To help first create Maps, Google acquired what company?": {
        "raw_data": {
            "best_answer_by_ml": [
                "waze"
            ],
            "negative_question": false,
            "fraction_answers": {
                "mapquest": 0.185034167742401,
                "waze": 0.5317471998743106,
                "where 2 technologies": 0.28321863238328837
            },
            "rate_limited": false,
            "integer_answers": {
                "mapquest": 0,
                "waze": 4,
                "where 2 technologies": 2
            },
            "data": {
                "wikipedia_search": [
                    0.6120399142457966,
                    4.705283458592282,
                    0.6826766271619212
                ],
                "word_count_entities": [
                    0.0,
                    1.0,
                    28.0
                ],
                "word_relation_to_question": [
                    1.035155827584482,
                    3.4748857306285057,
                    1.489958441787012
                ],
                "word_count_raw": [
                    10.0,
                    5.0,
                    39.0
                ],
                "word_count_appended": [
                    398.0,
                    78.0,
                    674.0
                ],
                "result_count": [
                    101000.0,
                    46800.0,
                    184000.0
                ]
            },
            "ml_answers": {
                "mapquest": 0.3524616949059466,
                "waze": 0.42504647745412427,
                "where 2 technologies": 0.32549693856000345
            }
        },
        "lines": [
            [
                0,
                0.10200665237429944,
                0.0,
                0.17252597126408034,
                0.18518518518518517,
                0.34608695652173915,
                0.30440024110910185
            ],
            [
                1,
                0.7842139097653803,
                0.034482758620689655,
                0.5791476217714177,
                0.09259259259259259,
                0.06782608695652174,
                0.1410488245931284
            ],
            [
                0,
                0.1137794378603202,
                0.9655172413793104,
                0.248326406964502,
                0.7222222222222222,
                0.5860869565217391,
                0.5545509342977697
            ]
        ]
    },
    "The lyrics to \u201cThe Star-Spangled Banner\u201d were written during what conflict?": {
        "raw_data": {
            "best_answer_by_ml": [
                "the war of 1812"
            ],
            "negative_question": false,
            "fraction_answers": {
                "american revolution": 0.14044932563805604,
                "the war of 1812": 0.6650228631314222,
                "the civil war": 0.19452781123052168
            },
            "rate_limited": false,
            "integer_answers": {
                "american revolution": 0,
                "the war of 1812": 5,
                "the civil war": 1
            },
            "data": {
                "wikipedia_search": [
                    1.1929824561403508,
                    0.5679824561403508,
                    4.239035087719298
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    7.0
                ],
                "word_relation_to_question": [
                    0.6719858156028369,
                    0.50177304964539,
                    3.826241134751773
                ],
                "word_count_raw": [
                    3.0,
                    2.0,
                    21.0
                ],
                "word_count_appended": [
                    55.0,
                    44.0,
                    83.0
                ],
                "result_count": [
                    22400.0,
                    17700.0,
                    13700.0
                ]
            },
            "ml_answers": {
                "american revolution": 0.3231532046054523,
                "the war of 1812": 0.4334145410017103,
                "the civil war": 0.3299047500524129
            }
        },
        "lines": [
            [
                0,
                0.19883040935672514,
                0.0,
                0.13439716312056738,
                0.11538461538461539,
                0.3021978021978022,
                0.4163568773234201
            ],
            [
                0,
                0.09466374269005846,
                0.0,
                0.100354609929078,
                0.07692307692307693,
                0.24175824175824176,
                0.32899628252788105
            ],
            [
                1,
                0.7065058479532164,
                1.0,
                0.7652482269503545,
                0.8076923076923077,
                0.45604395604395603,
                0.25464684014869887
            ]
        ]
    },
    "Catching a catfish with your bare hands is called what?": {
        "raw_data": {
            "best_answer_by_ml": [
                "noodling"
            ],
            "negative_question": false,
            "fraction_answers": {
                "noodling": 0.8019763947593991,
                "whiskering": 0.024975939013279134,
                "strumming": 0.17304766622732173
            },
            "rate_limited": false,
            "integer_answers": {
                "noodling": 5,
                "whiskering": 0,
                "strumming": 1
            },
            "data": {
                "wikipedia_search": [
                    0.3333333333333333,
                    0.3333333333333333,
                    4.333333333333333
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    127.0
                ],
                "word_relation_to_question": [
                    0.31666666666666665,
                    0.12121212121212122,
                    4.5621212121212125
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    201.0
                ],
                "word_count_appended": [
                    116.0,
                    34.0,
                    441.0
                ],
                "result_count": [
                    20100.0,
                    40.0,
                    8090.0
                ]
            },
            "ml_answers": {
                "noodling": 0.5123212089064794,
                "whiskering": 0.23989384238959452,
                "strumming": 0.2994580302567745
            }
        },
        "lines": [
            [
                0,
                0.06666666666666667,
                0.0,
                0.06333333333333332,
                0.0,
                0.19627749576988154,
                0.7120085015940489
            ],
            [
                0,
                0.06666666666666667,
                0.0,
                0.024242424242424242,
                0.0,
                0.05752961082910321,
                0.0014169323414806943
            ],
            [
                1,
                0.8666666666666666,
                1.0,
                0.9124242424242425,
                1.0,
                0.7461928934010152,
                0.2865745660644704
            ]
        ]
    },
    "Which of these do NOT have flippers?": {
        "raw_data": {
            "best_answer_by_ml": [
                "new yorkers"
            ],
            "negative_question": true,
            "fraction_answers": {
                "new yorkers": 0.08517123882348432,
                "pinball machines": 0.33495672744216204,
                "dolphins": 0.5798720337343536
            },
            "rate_limited": false,
            "integer_answers": {
                "new yorkers": 0,
                "pinball machines": 3,
                "dolphins": 3
            },
            "data": {
                "wikipedia_search": [
                    0.19183168316831684,
                    0.7829207920792078,
                    1.0252475247524753
                ],
                "word_count_entities": [
                    0.0,
                    2.0,
                    4.0
                ],
                "word_relation_to_question": [
                    0.3170731707317073,
                    0.34146341463414637,
                    1.3414634146341464
                ],
                "word_count_raw": [
                    0.0,
                    8.0,
                    0.0
                ],
                "word_count_appended": [
                    75.0,
                    574.0,
                    71.0
                ],
                "result_count": [
                    106000.0,
                    547000.0,
                    42500.0
                ]
            },
            "ml_answers": {
                "new yorkers": 0.29262816875595676,
                "pinball machines": 0.31188796114719847,
                "dolphins": 0.43044863956167584
            }
        },
        "lines": [
            [
                1,
                0.09591584158415842,
                0.0,
                0.15853658536585366,
                0.0,
                0.10416666666666667,
                0.15240833932422718
            ],
            [
                0,
                0.3914603960396039,
                0.3333333333333333,
                0.17073170731707318,
                1.0,
                0.7972222222222223,
                0.7864845434938893
            ],
            [
                0,
                0.5126237623762376,
                0.6666666666666666,
                0.6707317073170732,
                0.0,
                0.09861111111111111,
                0.06110711718188354
            ]
        ]
    },
    "The '70s sitcom \u201cThree's Company\u201d was about three people who were what?": {
        "raw_data": {
            "best_answer_by_ml": [
                "roommates"
            ],
            "negative_question": false,
            "fraction_answers": {
                "roommates": 0.7288265425594193,
                "trivia show hosts": 0.146469884757556,
                "panda bears": 0.12470357268302475
            },
            "rate_limited": false,
            "integer_answers": {
                "roommates": 3,
                "trivia show hosts": 1,
                "panda bears": 1
            },
            "data": {
                "wikipedia_search": [
                    1.3461538461538463,
                    0.16666666666666666,
                    1.4871794871794872
                ],
                "word_count_entities": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.9277777777777778,
                    2.6523809523809523,
                    0.4198412698412698
                ],
                "word_count_raw": [
                    3.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    211.0,
                    3.0,
                    5.0
                ],
                "result_count": [
                    0,
                    0,
                    0
                ]
            },
            "ml_answers": {
                "roommates": 0.5221850762414461,
                "trivia show hosts": 0.32985248556570446,
                "panda bears": 0.2735997889973309
            }
        },
        "lines": [
            [
                1,
                0.44871794871794873,
                1.0,
                0.23194444444444448,
                1.0,
                0,
                0.9634703196347032
            ],
            [
                0,
                0.05555555555555555,
                0.0,
                0.6630952380952382,
                0.0,
                0,
                0.0136986301369863
            ],
            [
                0,
                0.49572649572649574,
                0.0,
                0.10496031746031746,
                0.0,
                0,
                0.0228310502283105
            ]
        ]
    },
    "What was the first popular home video game?": {
        "raw_data": {
            "best_answer_by_ml": [
                "pong"
            ],
            "negative_question": false,
            "fraction_answers": {
                "tekken 2": 0.11693791941127953,
                "pong": 0.7190048079544535,
                "half-life 3": 0.16405727263426692
            },
            "rate_limited": false,
            "integer_answers": {
                "tekken 2": 0,
                "pong": 6,
                "half-life 3": 0
            },
            "data": {
                "wikipedia_search": [
                    0.38159658085308645,
                    2.83026009791809,
                    1.788143321228823
                ],
                "word_count_entities": [
                    0.0,
                    35.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.6449468426778762,
                    1.6940814113487455,
                    1.6609717459733782
                ],
                "word_count_raw": [
                    0.0,
                    28.0,
                    0.0
                ],
                "word_count_appended": [
                    114.0,
                    475.0,
                    103.0
                ],
                "result_count": [
                    140000.0,
                    769000.0,
                    155000.0
                ]
            },
            "ml_answers": {
                "tekken 2": 0.24685054577433765,
                "pong": 0.5123212089064794,
                "half-life 3": 0.2616867615999172
            }
        },
        "lines": [
            [
                0,
                0.0763193161706173,
                0.0,
                0.32898936853557526,
                0.0,
                0.16473988439306358,
                0.13157894736842105
            ],
            [
                1,
                0.5660520195836181,
                1.0,
                0.3388162822697491,
                1.0,
                0.6864161849710982,
                0.7227443609022557
            ],
            [
                0,
                0.35762866424576456,
                0.0,
                0.3321943491946756,
                0.0,
                0.14884393063583815,
                0.14567669172932332
            ]
        ]
    },
    "Featuring 20 scoops of ice cream, the Vermonster is found on what chain's menu?": {
        "raw_data": {
            "best_answer_by_ml": [
                "baskin-robbins"
            ],
            "negative_question": false,
            "fraction_answers": {
                "ben & jerry's": 0.39343823619206425,
                "dairy queen": 0.22522316574585077,
                "baskin-robbins": 0.38133859806208503
            },
            "rate_limited": false,
            "integer_answers": {
                "ben & jerry's": 2,
                "dairy queen": 0,
                "baskin-robbins": 2
            },
            "data": {
                "wikipedia_search": [
                    1.2737474452830277,
                    2.9223758953002723,
                    3.8038766594166997
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    4.854670057600247,
                    2.081315515398991,
                    1.064014427000762
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    69.0,
                    24.0,
                    0.0
                ],
                "result_count": [
                    43.0,
                    43.0,
                    2390.0
                ]
            },
            "ml_answers": {
                "ben & jerry's": 0.26259078193113855,
                "dairy queen": 0.28718724762403164,
                "baskin-robbins": 0.3447825871574297
            }
        },
        "lines": [
            [
                0,
                0.15921843066037847,
                0,
                0.6068337572000309,
                0,
                0.017366720516962843,
                0.7419354838709677
            ],
            [
                0,
                0.36529698691253404,
                0,
                0.2601644394248739,
                0,
                0.017366720516962843,
                0.25806451612903225
            ],
            [
                1,
                0.47548458242708747,
                0,
                0.13300180337509526,
                0,
                0.9652665589660743,
                0.0
            ]
        ]
    },
    "The U.S. has never had a Miss America from what state?": {
        "raw_data": {
            "best_answer_by_ml": [
                "north dakota"
            ],
            "negative_question": false,
            "fraction_answers": {
                "nebraska": 0.2762448452399865,
                "new mexico": 0.19192662736172467,
                "north dakota": 0.5318285273982888
            },
            "rate_limited": false,
            "integer_answers": {
                "nebraska": 1,
                "new mexico": 1,
                "north dakota": 3
            },
            "data": {
                "wikipedia_search": [
                    1.0836012452882555,
                    1.232206198436106,
                    0.6841925562756388
                ],
                "word_count_entities": [
                    0.0,
                    35.0,
                    1.0
                ],
                "word_relation_to_question": [
                    1.5639143730886849,
                    1.3639143730886851,
                    1.07217125382263
                ],
                "word_count_raw": [
                    1.0,
                    11.0,
                    4.0
                ],
                "word_count_appended": [
                    79.0,
                    135.0,
                    331.0
                ],
                "result_count": [
                    0,
                    0,
                    0
                ]
            },
            "ml_answers": {
                "nebraska": 0.24331727377199505,
                "new mexico": 0.20239003499174044,
                "north dakota": 0.3856275334907294
            }
        },
        "lines": [
            [
                1,
                0.36120041509608514,
                0.0,
                0.3909785932721712,
                0.0625,
                0.14495412844036698,
                0
            ],
            [
                0,
                0.4107353994787019,
                0.9722222222222222,
                0.3409785932721713,
                0.6875,
                0.24770642201834864,
                0
            ],
            [
                0,
                0.2280641854252129,
                0.027777777777777776,
                0.2680428134556575,
                0.25,
                0.6073394495412844,
                0
            ]
        ]
    }
}