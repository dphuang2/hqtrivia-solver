{
    "Which of these versions of the Old Testament typically contains the most books?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.2849344978165939,
                    0.3167517006802721,
                    0.29469901168014373,
                    0.2826025048456836,
                    0.5554285714285714,
                    0.7294117647058823,
                    0.6865671641791045,
                    0.13188195452346396,
                    -1
                ],
                [
                    0.18013100436681223,
                    0.1985544217687075,
                    0.5058400718778078,
                    0.226874977812804,
                    0.38285714285714284,
                    0.25882352941176473,
                    0.30597014925373134,
                    0.41601354620222547,
                    -1
                ],
                [
                    0.5349344978165939,
                    0.4846938775510204,
                    0.19946091644204852,
                    0.4905225173415125,
                    0.061714285714285715,
                    0.011764705882352941,
                    0.007462686567164179,
                    0.4521044992743106,
                    -1
                ]
            ],
            "fraction_answers": {
                "catholic": 0.41028464623246447,
                "protestant": 0.30938310544387454,
                "eastern orthodox": 0.2803322483236611
            },
            "question": "which of these versions of the old testament typically contains the most books?",
            "rate_limited": false,
            "answers": [
                "catholic",
                "protestant",
                "eastern orthodox"
            ],
            "ml_answers": {
                "catholic": 0.39323085488599535,
                "protestant": 0.1443840619642946,
                "eastern orthodox": 0.4028308274653409
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "catholic": 3,
                "protestant": 1,
                "eastern orthodox": 4
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    1490000.0,
                    934000.0,
                    2280000.0
                ],
                "wikipedia_search": [
                    1.413012524228418,
                    1.13437488906402,
                    2.4526125867075623
                ],
                "result_count_noun_chunks": [
                    3280000.0,
                    5630000.0,
                    2220000.0
                ],
                "word_relation_to_question": [
                    0.6594097726173198,
                    2.0800677310111273,
                    2.260522496371553
                ],
                "word_count_noun_chunks": [
                    124.0,
                    44.0,
                    2.0
                ],
                "word_count_raw": [
                    92.0,
                    41.0,
                    1.0
                ],
                "result_count": [
                    2610000.0,
                    1650000.0,
                    4900000.0
                ],
                "word_count_appended": [
                    486.0,
                    335.0,
                    54.0
                ]
            },
            "z-best_answer_by_ml": [
                "eastern orthodox"
            ]
        },
        "lines": [
            [
                0,
                0.2849344978165939,
                0.3167517006802721,
                0.29469901168014373,
                0.2826025048456836,
                0.5554285714285714,
                0.7294117647058823,
                0.6865671641791045,
                0.13188195452346396,
                -1
            ],
            [
                0,
                0.18013100436681223,
                0.1985544217687075,
                0.5058400718778078,
                0.226874977812804,
                0.38285714285714284,
                0.25882352941176473,
                0.30597014925373134,
                0.41601354620222547,
                -1
            ],
            [
                1,
                0.5349344978165939,
                0.4846938775510204,
                0.19946091644204852,
                0.4905225173415125,
                0.061714285714285715,
                0.011764705882352941,
                0.007462686567164179,
                0.4521044992743106,
                -1
            ]
        ]
    },
    "The material that forms images in an Etch A Sketch is also the main component in which item?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.9994921934823436,
                    0.9999763598969291,
                    0.9999525202400461,
                    0.5069444444444444,
                    0.3,
                    0,
                    0,
                    0.36843033509700174,
                    -1
                ],
                [
                    0.00024181262745540572,
                    1.0165244320465237e-05,
                    2.373987997692196e-05,
                    0.23333333333333334,
                    0.15,
                    0,
                    0,
                    0.46031746031746024,
                    -1
                ],
                [
                    0.0002659938902009463,
                    1.3474858750384152e-05,
                    2.373987997692196e-05,
                    0.25972222222222224,
                    0.55,
                    0,
                    0,
                    0.1712522045855379,
                    -1
                ]
            ],
            "fraction_answers": {
                "zinc supplement tablets": 0.6957993088601274,
                "soda cans": 0.1635462725727814,
                "u.s. nickels": 0.14065441856709107
            },
            "question": "the material that forms images in an etch a sketch is also the main component in which item?",
            "rate_limited": false,
            "answers": [
                "zinc supplement tablets",
                "u.s. nickels",
                "soda cans"
            ],
            "ml_answers": {
                "zinc supplement tablets": 0.23870396025239765,
                "soda cans": 0.18074862240989148,
                "u.s. nickels": 0.04064325980107343
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "zinc supplement tablets": 4,
                "soda cans": 1,
                "u.s. nickels": 1
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    4230000.0,
                    43.0,
                    57.0
                ],
                "wikipedia_search": [
                    3.548611111111111,
                    1.6333333333333333,
                    1.8180555555555555
                ],
                "result_count_noun_chunks": [
                    1390000.0,
                    33.0,
                    33.0
                ],
                "word_relation_to_question": [
                    2.210582010582011,
                    2.761904761904762,
                    1.0275132275132275
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    6.0,
                    3.0,
                    11.0
                ],
                "result_count": [
                    124000.0,
                    30.0,
                    33.0
                ]
            },
            "z-best_answer_by_ml": [
                "zinc supplement tablets"
            ]
        },
        "lines": [
            [
                0,
                0.9994921934823436,
                0.9999763598969291,
                0.9999525202400461,
                0.5069444444444444,
                0.3,
                0,
                0,
                0.36843033509700174,
                -1
            ],
            [
                0,
                0.00024181262745540572,
                1.0165244320465237e-05,
                2.373987997692196e-05,
                0.23333333333333334,
                0.15,
                0,
                0,
                0.46031746031746024,
                -1
            ],
            [
                1,
                0.0002659938902009463,
                1.3474858750384152e-05,
                2.373987997692196e-05,
                0.25972222222222224,
                0.55,
                0,
                0,
                0.1712522045855379,
                -1
            ]
        ]
    },
    "Which of these actresses is NOT mentioned in Madonna\u2019s song \u201cVogue\u201d?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.03118048753239344,
                    0.3946932006633499,
                    0.4305776364599894,
                    0.5000000000000001,
                    0.3253968253968254,
                    0.5,
                    0.25,
                    0.33725490196078434,
                    -1
                ],
                [
                    0.49997439779967373,
                    0.33913764510779437,
                    0.10784313725490197,
                    0.14871794871794872,
                    0.376984126984127,
                    0.5,
                    0.5,
                    0.3130718954248366,
                    -1
                ],
                [
                    0.4688451146679328,
                    0.26616915422885573,
                    0.4615792262851086,
                    0.3512820512820513,
                    0.2976190476190476,
                    0.0,
                    0.25,
                    0.34967320261437906,
                    -1
                ]
            ],
            "fraction_answers": {
                "jean harlow": 0.3077242369966644,
                "audrey hepburn": 0.3035677121776794,
                "rita hayworth": 0.3887080508256562
            },
            "question": "which of these actresses is not mentioned in madonna\u2019s song \u201cvogue\u201d?",
            "rate_limited": false,
            "answers": [
                "jean harlow",
                "audrey hepburn",
                "rita hayworth"
            ],
            "ml_answers": {
                "jean harlow": 0.19578697844574275,
                "audrey hepburn": 0.5251117194835642,
                "rita hayworth": 0.44041373810585654
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "jean harlow": 2,
                "audrey hepburn": 3,
                "rita hayworth": 3
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    127000.0,
                    194000.0,
                    282000.0
                ],
                "wikipedia_search": [
                    0.0,
                    2.1076923076923078,
                    0.8923076923076922
                ],
                "result_count_noun_chunks": [
                    262000.0,
                    1480000.0,
                    145000.0
                ],
                "word_relation_to_question": [
                    0.9764705882352942,
                    1.1215686274509804,
                    0.9019607843137256
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    1.0
                ],
                "word_count_appended": [
                    44.0,
                    31.0,
                    51.0
                ],
                "result_count": [
                    1410000.0,
                    77.0,
                    93700.0
                ]
            },
            "z-best_answer_by_ml": [
                "audrey hepburn"
            ]
        },
        "lines": [
            [
                0,
                0.03118048753239344,
                0.3946932006633499,
                0.4305776364599894,
                0.5000000000000001,
                0.3253968253968254,
                0.5,
                0.25,
                0.33725490196078434,
                -1
            ],
            [
                1,
                0.49997439779967373,
                0.33913764510779437,
                0.10784313725490197,
                0.14871794871794872,
                0.376984126984127,
                0.5,
                0.5,
                0.3130718954248366,
                -1
            ],
            [
                0,
                0.4688451146679328,
                0.26616915422885573,
                0.4615792262851086,
                0.3512820512820513,
                0.2976190476190476,
                0.0,
                0.25,
                0.34967320261437906,
                -1
            ]
        ]
    },
    "Wrestling legend Ric Flair entered the ring to the same music used in what classic film?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.20833333333333334,
                    0.21022727272727273,
                    0.19205298013245034,
                    0.732760827637417,
                    0.15625,
                    0,
                    0,
                    0.24259039203721522,
                    1
                ],
                [
                    0.3472222222222222,
                    0.32954545454545453,
                    0.4370860927152318,
                    0.0445646010268299,
                    0.34375,
                    0,
                    0,
                    0.5456166015815718,
                    1
                ],
                [
                    0.4444444444444444,
                    0.4602272727272727,
                    0.3708609271523179,
                    0.22267457133575316,
                    0.5,
                    0,
                    0,
                    0.21179300638121293,
                    1
                ]
            ],
            "fraction_answers": {
                "star wars: episode iv": 0.2903691343112815,
                "back to the future": 0.36833337034016683,
                "2001: a space odyssey": 0.3412974953485517
            },
            "question": "wrestling legend ric flair entered the ring to the same music used in what classic film?",
            "rate_limited": false,
            "answers": [
                "star wars: episode iv",
                "2001: a space odyssey",
                "back to the future"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "star wars: episode iv": 1,
                "back to the future": 3,
                "2001: a space odyssey": 2
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    37.0,
                    58.0,
                    81.0
                ],
                "wikipedia_search": [
                    5.129325793461918,
                    0.31195220718780925,
                    1.5587219993502717
                ],
                "result_count_noun_chunks": [
                    29.0,
                    66.0,
                    56.0
                ],
                "word_relation_to_question": [
                    1.4555423522232913,
                    3.2736996094894306,
                    1.2707580382872776
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    30.0,
                    50.0,
                    64.0
                ],
                "word_count_appended": [
                    5.0,
                    11.0,
                    16.0
                ]
            },
            "z-best_answer_by_ml": [
                "star wars: episode iv"
            ],
            "ml_answers": {
                "star wars: episode iv": 0.30741590639612193,
                "back to the future": 0.2292586981197429,
                "2001: a space odyssey": 0.21812123933519256
            }
        },
        "lines": [
            [
                0,
                0.20833333333333334,
                0.21022727272727273,
                0.19205298013245034,
                0.732760827637417,
                0.15625,
                0,
                0,
                0.24259039203721522,
                1
            ],
            [
                1,
                0.3472222222222222,
                0.32954545454545453,
                0.4370860927152318,
                0.0445646010268299,
                0.34375,
                0,
                0,
                0.5456166015815718,
                1
            ],
            [
                0,
                0.4444444444444444,
                0.4602272727272727,
                0.3708609271523179,
                0.22267457133575316,
                0.5,
                0,
                0,
                0.21179300638121293,
                1
            ]
        ]
    },
    "The man famously known as the Science Guy holds a patent for which of these items?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.12,
                    0.07058823529411765,
                    0.006802721088435374,
                    0.23822222222222225,
                    0.15217391304347827,
                    0,
                    0,
                    0.4001040466557708,
                    -1
                ],
                [
                    0.54,
                    0.6705882352941176,
                    0.974025974025974,
                    0.7137777777777778,
                    0.5869565217391305,
                    0,
                    0,
                    0.21576284507318988,
                    -1
                ],
                [
                    0.34,
                    0.25882352941176473,
                    0.0191713048855906,
                    0.048,
                    0.2608695652173913,
                    0,
                    0,
                    0.3841331082710393,
                    -1
                ]
            ],
            "fraction_answers": {
                "pulse rate monitor": 0.1646485230506707,
                "mechanical pencil": 0.6168518923183649,
                "ballet shoe": 0.21849958463096433
            },
            "question": "the man famously known as the science guy holds a patent for which of these items?",
            "rate_limited": false,
            "answers": [
                "pulse rate monitor",
                "mechanical pencil",
                "ballet shoe"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "pulse rate monitor": 0.048288390870521614,
                "mechanical pencil": 0.39131407062737483,
                "ballet shoe": 0.5034619246085046
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    6.0,
                    57.0,
                    22.0
                ],
                "wikipedia_search": [
                    1.1911111111111112,
                    3.568888888888889,
                    0.24
                ],
                "result_count_noun_chunks": [
                    22.0,
                    3150.0,
                    62.0
                ],
                "word_relation_to_question": [
                    2.400624279934625,
                    1.2945770704391393,
                    2.304798649626236
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    12.0,
                    54.0,
                    34.0
                ],
                "word_count_appended": [
                    7.0,
                    27.0,
                    12.0
                ]
            },
            "z-best_answer_by_ml": [
                "ballet shoe"
            ],
            "integer_answers": {
                "pulse rate monitor": 1,
                "mechanical pencil": 5,
                "ballet shoe": 0
            }
        },
        "lines": [
            [
                0,
                0.12,
                0.07058823529411765,
                0.006802721088435374,
                0.23822222222222225,
                0.15217391304347827,
                0,
                0,
                0.4001040466557708,
                -1
            ],
            [
                0,
                0.54,
                0.6705882352941176,
                0.974025974025974,
                0.7137777777777778,
                0.5869565217391305,
                0,
                0,
                0.21576284507318988,
                -1
            ],
            [
                1,
                0.34,
                0.25882352941176473,
                0.0191713048855906,
                0.048,
                0.2608695652173913,
                0,
                0,
                0.3841331082710393,
                -1
            ]
        ]
    },
    "What advertising mascot wears epaulettes?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.0891089108910891,
                    0.08653846153846154,
                    0.08333333333333333,
                    0.08333333333333333,
                    0.2765957446808511,
                    0,
                    0,
                    0.15944272445820432,
                    1
                ],
                [
                    0.5742574257425742,
                    0.5865384615384616,
                    0.6203703703703703,
                    0.21794871794871795,
                    0.6382978723404256,
                    0,
                    0,
                    0.4613003095975232,
                    1
                ],
                [
                    0.33663366336633666,
                    0.3269230769230769,
                    0.2962962962962963,
                    0.6987179487179488,
                    0.0851063829787234,
                    0,
                    0,
                    0.37925696594427244,
                    1
                ]
            ],
            "fraction_answers": {
                "sun-maid raisin girl": 0.12972541803921214,
                "cap'n crunch": 0.3538223890377758,
                "mr. peanut": 0.5164521929230121
            },
            "question": "what advertising mascot wears epaulettes?",
            "rate_limited": false,
            "answers": [
                "sun-maid raisin girl",
                "mr. peanut",
                "cap'n crunch"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "sun-maid raisin girl": 0.04459066907345321,
                "cap'n crunch": 0.4532571970888039,
                "mr. peanut": 0.17891933209693114
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    9.0,
                    61.0,
                    34.0
                ],
                "wikipedia_search": [
                    0.25,
                    0.6538461538461539,
                    2.0961538461538463
                ],
                "result_count_noun_chunks": [
                    9.0,
                    67.0,
                    32.0
                ],
                "word_relation_to_question": [
                    0.478328173374613,
                    1.3839009287925697,
                    1.1377708978328174
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    9.0,
                    58.0,
                    34.0
                ],
                "word_count_appended": [
                    13.0,
                    30.0,
                    4.0
                ]
            },
            "z-best_answer_by_ml": [
                "cap'n crunch"
            ],
            "integer_answers": {
                "sun-maid raisin girl": 0,
                "cap'n crunch": 1,
                "mr. peanut": 5
            }
        },
        "lines": [
            [
                0,
                0.0891089108910891,
                0.08653846153846154,
                0.08333333333333333,
                0.08333333333333333,
                0.2765957446808511,
                0,
                0,
                0.15944272445820432,
                1
            ],
            [
                0,
                0.5742574257425742,
                0.5865384615384616,
                0.6203703703703703,
                0.21794871794871795,
                0.6382978723404256,
                0,
                0,
                0.4613003095975232,
                1
            ],
            [
                1,
                0.33663366336633666,
                0.3269230769230769,
                0.2962962962962963,
                0.6987179487179488,
                0.0851063829787234,
                0,
                0,
                0.37925696594427244,
                1
            ]
        ]
    },
    "Which alum from \u201cThe Hills\u201d founded a wildly popular millennial skincare line?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.10843373493975904,
                    0.2064516129032258,
                    0.20121951219512196,
                    0.3932291666666667,
                    0.175,
                    0.5,
                    0,
                    0.44800338550338553,
                    -1
                ],
                [
                    0.6506024096385542,
                    0.5096774193548387,
                    0.5182926829268293,
                    0.5166495901639344,
                    0.575,
                    0.0,
                    0,
                    0.255515318015318,
                    -1
                ],
                [
                    0.24096385542168675,
                    0.2838709677419355,
                    0.2804878048780488,
                    0.0901212431693989,
                    0.25,
                    0.5,
                    0,
                    0.29648129648129645,
                    -1
                ]
            ],
            "fraction_answers": {
                "emily weiss": 0.290333916029737,
                "whitney port": 0.2774178810989095,
                "lauren conrad": 0.4322482028713535
            },
            "question": "which alum from \u201cthe hills\u201d founded a wildly popular millennial skincare line?",
            "rate_limited": false,
            "answers": [
                "emily weiss",
                "lauren conrad",
                "whitney port"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "emily weiss": 2,
                "whitney port": 0,
                "lauren conrad": 5
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    32.0,
                    79.0,
                    44.0
                ],
                "wikipedia_search": [
                    1.5729166666666667,
                    2.0665983606557377,
                    0.3604849726775956
                ],
                "result_count_noun_chunks": [
                    33.0,
                    85.0,
                    46.0
                ],
                "word_relation_to_question": [
                    2.688020313020313,
                    1.533091908091908,
                    1.7788877788877788
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    7.0,
                    23.0,
                    10.0
                ],
                "result_count": [
                    9.0,
                    54.0,
                    20.0
                ]
            },
            "z-best_answer_by_ml": [
                "emily weiss"
            ],
            "ml_answers": {
                "emily weiss": 0.39141709473421193,
                "whitney port": 0.33245924055353093,
                "lauren conrad": 0.3883947940426572
            }
        },
        "lines": [
            [
                1,
                0.10843373493975904,
                0.2064516129032258,
                0.20121951219512196,
                0.3932291666666667,
                0.175,
                0.5,
                0,
                0.44800338550338553,
                -1
            ],
            [
                0,
                0.6506024096385542,
                0.5096774193548387,
                0.5182926829268293,
                0.5166495901639344,
                0.575,
                0.0,
                0,
                0.255515318015318,
                -1
            ],
            [
                0,
                0.24096385542168675,
                0.2838709677419355,
                0.2804878048780488,
                0.0901212431693989,
                0.25,
                0.5,
                0,
                0.29648129648129645,
                -1
            ]
        ]
    },
    "Which color is NOT represented in the original electronic Simon game?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.3383404864091559,
                    0.3373925501432665,
                    0.36498054474708175,
                    0.49856505102040816,
                    0.31152204836415365,
                    0.2272727272727273,
                    0.2,
                    0.2920212765957447,
                    -1
                ],
                [
                    0.32260371959942774,
                    0.3230659025787966,
                    0.2544747081712062,
                    0.39096513605442174,
                    0.3506401137980085,
                    0.5,
                    0.5,
                    0.32127659574468087,
                    -1
                ],
                [
                    0.3390557939914163,
                    0.339541547277937,
                    0.38054474708171204,
                    0.11046981292517005,
                    0.33783783783783783,
                    0.2727272727272727,
                    0.3,
                    0.38670212765957446,
                    -1
                ]
            ],
            "fraction_answers": {
                "blue": 0.3574763288618655,
                "orange": 0.2592434560133646,
                "green": 0.3832802151247699
            },
            "question": "which color is not represented in the original electronic simon game?",
            "rate_limited": false,
            "answers": [
                "blue",
                "orange",
                "green"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "blue": 4,
                "orange": 3,
                "green": 1
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    2270000.0,
                    2470000.0,
                    2240000.0
                ],
                "wikipedia_search": [
                    0.011479591836734694,
                    0.8722789115646259,
                    3.1162414965986396
                ],
                "result_count_noun_chunks": [
                    3470000.0,
                    6310000.0,
                    3070000.0
                ],
                "word_relation_to_question": [
                    2.079787234042553,
                    1.7872340425531914,
                    1.1329787234042552
                ],
                "word_count_noun_chunks": [
                    6.0,
                    0.0,
                    5.0
                ],
                "word_count_raw": [
                    3.0,
                    0.0,
                    2.0
                ],
                "result_count": [
                    2260000.0,
                    2480000.0,
                    2250000.0
                ],
                "word_count_appended": [
                    265.0,
                    210.0,
                    228.0
                ]
            },
            "z-best_answer_by_ml": [
                "orange"
            ],
            "ml_answers": {
                "blue": 0.11122357611663379,
                "orange": 0.6797868204095359,
                "green": 0.06850887784913658
            }
        },
        "lines": [
            [
                0,
                0.3383404864091559,
                0.3373925501432665,
                0.36498054474708175,
                0.49856505102040816,
                0.31152204836415365,
                0.2272727272727273,
                0.2,
                0.2920212765957447,
                -1
            ],
            [
                1,
                0.32260371959942774,
                0.3230659025787966,
                0.2544747081712062,
                0.39096513605442174,
                0.3506401137980085,
                0.5,
                0.5,
                0.32127659574468087,
                -1
            ],
            [
                0,
                0.3390557939914163,
                0.339541547277937,
                0.38054474708171204,
                0.11046981292517005,
                0.33783783783783783,
                0.2727272727272727,
                0.3,
                0.38670212765957446,
                -1
            ]
        ]
    },
    "What car brand is sung about by Will Smith, Charli XCX and Janis Joplin?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.09561231172233137,
                    0.11982039769082746,
                    0.4124004550625711,
                    0.0045045045045045045,
                    0.28444444444444444,
                    0,
                    0.0,
                    0.5792837172147517,
                    1
                ],
                [
                    0.49181401440733463,
                    0.11302116741500962,
                    0.36120591581342437,
                    0.5995995995995996,
                    0.3422222222222222,
                    0,
                    1.0,
                    0.2953268539475436,
                    1
                ],
                [
                    0.412573673870334,
                    0.7671584348941629,
                    0.22639362912400454,
                    0.3958958958958959,
                    0.37333333333333335,
                    0,
                    0.0,
                    0.1253894288377047,
                    1
                ]
            ],
            "fraction_answers": {
                "porsche": 0.3286777708507765,
                "rolls-royce": 0.21372369009134723,
                "mercedes-benz": 0.45759853905787634
            },
            "question": "what car brand is sung about by will smith, charli xcx and janis joplin?",
            "rate_limited": false,
            "answers": [
                "rolls-royce",
                "mercedes-benz",
                "porsche"
            ],
            "ml_answers": {
                "porsche": 0.46686418988347245,
                "rolls-royce": 0.10534751049819024,
                "mercedes-benz": 0.617169986315936
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "porsche": 2,
                "rolls-royce": 2,
                "mercedes-benz": 3
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    9340.0,
                    8810.0,
                    59800.0
                ],
                "wikipedia_search": [
                    0.018018018018018018,
                    2.3983983983983985,
                    1.5835835835835836
                ],
                "result_count_noun_chunks": [
                    14500.0,
                    12700.0,
                    7960.0
                ],
                "word_relation_to_question": [
                    2.8964185860737586,
                    1.476634269737718,
                    0.6269471441885235
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count": [
                    14600.0,
                    75100.0,
                    63000.0
                ],
                "word_count_appended": [
                    64.0,
                    77.0,
                    84.0
                ]
            },
            "z-best_answer_by_ml": [
                "mercedes-benz"
            ]
        },
        "lines": [
            [
                0,
                0.09561231172233137,
                0.11982039769082746,
                0.4124004550625711,
                0.0045045045045045045,
                0.28444444444444444,
                0,
                0.0,
                0.5792837172147517,
                1
            ],
            [
                0,
                0.49181401440733463,
                0.11302116741500962,
                0.36120591581342437,
                0.5995995995995996,
                0.3422222222222222,
                0,
                1.0,
                0.2953268539475436,
                1
            ],
            [
                1,
                0.412573673870334,
                0.7671584348941629,
                0.22639362912400454,
                0.3958958958958959,
                0.37333333333333335,
                0,
                0.0,
                0.1253894288377047,
                1
            ]
        ]
    },
    "Which person is most likely to use a Reuleaux triangle at work?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.09009009009009009,
                    0.0989010989010989,
                    0.13432835820895522,
                    0.02611754966887417,
                    0.1111111111111111,
                    0,
                    0,
                    0.3925083936255734,
                    -1
                ],
                [
                    0.4594594594594595,
                    0.21978021978021978,
                    0.27611940298507465,
                    0.15378565970453387,
                    0.1111111111111111,
                    0,
                    0,
                    0.14005532699673712,
                    -1
                ],
                [
                    0.45045045045045046,
                    0.6813186813186813,
                    0.5895522388059702,
                    0.820096790626592,
                    0.7777777777777778,
                    0,
                    0,
                    0.4674362793776895,
                    -1
                ]
            ],
            "fraction_answers": {
                "banksy": 0.6311053697261936,
                "adam levine": 0.22671853000618933,
                "greta gerwig": 0.14217610026761715
            },
            "question": "which person is most likely to use a reuleaux triangle at work?",
            "rate_limited": false,
            "answers": [
                "greta gerwig",
                "adam levine",
                "banksy"
            ],
            "integer_answers": {
                "banksy": 5,
                "adam levine": 1,
                "greta gerwig": 0
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "banksy": 0.49697035198607864,
                "adam levine": 0.3544633677300479,
                "greta gerwig": 0.05540668399080747
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    9.0,
                    20.0,
                    62.0
                ],
                "wikipedia_search": [
                    0.07835264900662252,
                    0.46135697911360163,
                    2.460290371879776
                ],
                "result_count_noun_chunks": [
                    18.0,
                    37.0,
                    79.0
                ],
                "word_relation_to_question": [
                    1.5700335745022935,
                    0.5602213079869485,
                    1.869745117510758
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    4.0,
                    4.0,
                    28.0
                ],
                "result_count": [
                    10.0,
                    51.0,
                    50.0
                ]
            },
            "z-best_answer_by_ml": [
                "banksy"
            ]
        },
        "lines": [
            [
                0,
                0.09009009009009009,
                0.0989010989010989,
                0.13432835820895522,
                0.02611754966887417,
                0.1111111111111111,
                0,
                0,
                0.3925083936255734,
                -1
            ],
            [
                1,
                0.4594594594594595,
                0.21978021978021978,
                0.27611940298507465,
                0.15378565970453387,
                0.1111111111111111,
                0,
                0,
                0.14005532699673712,
                -1
            ],
            [
                0,
                0.45045045045045046,
                0.6813186813186813,
                0.5895522388059702,
                0.820096790626592,
                0.7777777777777778,
                0,
                0,
                0.4674362793776895,
                -1
            ]
        ]
    },
    "Which of these songs was written by the man nicknamed \u201cSlowhand\u201d?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.9993023738145068,
                    0.9993069322888964,
                    0.8345864661654135,
                    0.2256383712905452,
                    0.7162162162162162,
                    0,
                    1.0,
                    0.3927543912780133,
                    -1
                ],
                [
                    0.0003959499971717857,
                    0.00038682848991828246,
                    0.08421052631578947,
                    0.6922015182884748,
                    0.16216216216216217,
                    0,
                    0.0,
                    0.38622425802543914,
                    -1
                ],
                [
                    0.0003016761883213606,
                    0.000306239221185307,
                    0.081203007518797,
                    0.08216011042097998,
                    0.12162162162162163,
                    0,
                    0.0,
                    0.22102135069654755,
                    -1
                ]
            ],
            "fraction_answers": {
                "lay down sally": 0.7382578215790845,
                "lover lay down": 0.0723734293810647,
                "lay lady lay": 0.18936874903985082
            },
            "question": "which of these songs was written by the man nicknamed \u201cslowhand\u201d?",
            "rate_limited": false,
            "answers": [
                "lay down sally",
                "lay lady lay",
                "lover lay down"
            ],
            "ml_answers": {
                "lay down sally": 0.7041653695306317,
                "lover lay down": 0.06083339681590213,
                "lay lady lay": 0.12744901264898495
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "lay down sally": 6,
                "lover lay down": 0,
                "lay lady lay": 1
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    124000.0,
                    48.0,
                    38.0
                ],
                "wikipedia_search": [
                    0.6769151138716356,
                    2.0766045548654244,
                    0.24648033126293994
                ],
                "result_count_noun_chunks": [
                    1110000.0,
                    112000.0,
                    108000.0
                ],
                "word_relation_to_question": [
                    1.5710175651120533,
                    1.5448970321017566,
                    0.8840854027861902
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    3.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    53.0,
                    12.0,
                    9.0
                ],
                "result_count": [
                    106000.0,
                    42.0,
                    32.0
                ]
            },
            "z-best_answer_by_ml": [
                "lay down sally"
            ]
        },
        "lines": [
            [
                1,
                0.9993023738145068,
                0.9993069322888964,
                0.8345864661654135,
                0.2256383712905452,
                0.7162162162162162,
                0,
                1.0,
                0.3927543912780133,
                -1
            ],
            [
                0,
                0.0003959499971717857,
                0.00038682848991828246,
                0.08421052631578947,
                0.6922015182884748,
                0.16216216216216217,
                0,
                0.0,
                0.38622425802543914,
                -1
            ],
            [
                0,
                0.0003016761883213606,
                0.000306239221185307,
                0.081203007518797,
                0.08216011042097998,
                0.12162162162162163,
                0,
                0.0,
                0.22102135069654755,
                -1
            ]
        ]
    },
    "Which of these quantities is the largest?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.6744570957607674,
                    0.7564523308187443,
                    0.6524338216000239,
                    0,
                    0.9766233766233766,
                    0,
                    0,
                    1.0,
                    -1
                ],
                [
                    4.946018702245628e-06,
                    6.918771318464125e-06,
                    9.737818232836177e-07,
                    0,
                    0.012987012987012988,
                    0,
                    0,
                    0.0,
                    -1
                ],
                [
                    0.3255379582205304,
                    0.2435407504099372,
                    0.3475652046181528,
                    0,
                    0.01038961038961039,
                    0,
                    0,
                    0.0,
                    -1
                ]
            ],
            "fraction_answers": {
                "two half-dozens": 0.002599970311771396,
                "baker's dozen": 0.18540670472764614,
                "dozen": 0.8119933249605825
            },
            "question": "which of these quantities is the largest?",
            "rate_limited": false,
            "answers": [
                "dozen",
                "two half-dozens",
                "baker's dozen"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "two half-dozens": 0,
                "baker's dozen": 0,
                "dozen": 5
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    1640000.0,
                    15.0,
                    528000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    8710000.0,
                    13.0,
                    4640000.0
                ],
                "word_relation_to_question": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    1500000.0,
                    11.0,
                    724000.0
                ],
                "word_count_appended": [
                    376.0,
                    5.0,
                    4.0
                ]
            },
            "z-best_answer_by_ml": [
                "baker's dozen"
            ],
            "ml_answers": {
                "two half-dozens": 0.08111187708518634,
                "baker's dozen": 0.3017545263975648,
                "dozen": 0.2249168256240488
            }
        },
        "lines": [
            [
                0,
                0.6744570957607674,
                0.7564523308187443,
                0.6524338216000239,
                0,
                0.9766233766233766,
                0,
                0,
                1.0,
                -1
            ],
            [
                0,
                4.946018702245628e-06,
                6.918771318464125e-06,
                9.737818232836177e-07,
                0,
                0.012987012987012988,
                0,
                0,
                0.0,
                -1
            ],
            [
                1,
                0.3255379582205304,
                0.2435407504099372,
                0.3475652046181528,
                0,
                0.01038961038961039,
                0,
                0,
                0.0,
                -1
            ]
        ]
    },
    "The actor who played Don Draper provides the voice for what car company\u2019s ads?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.17886333396692644,
                    0.1888220454104405,
                    0.17805582290664101,
                    0.626183970856102,
                    0.3333333333333333,
                    0.6666666666666666,
                    0.6153846153846154,
                    0.32442230662339355,
                    1
                ],
                [
                    0.4219730089336628,
                    0.40947021152726565,
                    0.4215591915303176,
                    0.3315033014571949,
                    0.32154882154882153,
                    0.16666666666666666,
                    0.23076923076923078,
                    0.39121283550631375,
                    1
                ],
                [
                    0.39916365709941076,
                    0.4017077430622938,
                    0.40038498556304136,
                    0.0423127276867031,
                    0.3451178451178451,
                    0.16666666666666666,
                    0.15384615384615385,
                    0.28436485787029264,
                    1
                ]
            ],
            "fraction_answers": {
                "jaguar": 0.27419557961405094,
                "bmw": 0.3368379084924342,
                "mercedes-benz": 0.3889665118935149
            },
            "question": "the actor who played don draper provides the voice for what car company\u2019s ads?",
            "rate_limited": false,
            "answers": [
                "mercedes-benz",
                "bmw",
                "jaguar"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "jaguar": 0.10401655342441629,
                "bmw": 0.1015714578596584,
                "mercedes-benz": 0.48553323931250386
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    973000.0,
                    2110000.0,
                    2070000.0
                ],
                "wikipedia_search": [
                    3.7571038251366122,
                    1.9890198087431696,
                    0.2538763661202186
                ],
                "result_count_noun_chunks": [
                    925000.0,
                    2190000.0,
                    2080000.0
                ],
                "word_relation_to_question": [
                    1.6221115331169678,
                    1.9560641775315688,
                    1.4218242893514632
                ],
                "word_count_noun_chunks": [
                    16.0,
                    4.0,
                    4.0
                ],
                "word_count_raw": [
                    8.0,
                    3.0,
                    2.0
                ],
                "result_count": [
                    941000.0,
                    2220000.0,
                    2100000.0
                ],
                "word_count_appended": [
                    198.0,
                    191.0,
                    205.0
                ]
            },
            "z-best_answer_by_ml": [
                "mercedes-benz"
            ],
            "integer_answers": {
                "jaguar": 1,
                "bmw": 4,
                "mercedes-benz": 3
            }
        },
        "lines": [
            [
                1,
                0.17886333396692644,
                0.1888220454104405,
                0.17805582290664101,
                0.626183970856102,
                0.3333333333333333,
                0.6666666666666666,
                0.6153846153846154,
                0.32442230662339355,
                1
            ],
            [
                0,
                0.4219730089336628,
                0.40947021152726565,
                0.4215591915303176,
                0.3315033014571949,
                0.32154882154882153,
                0.16666666666666666,
                0.23076923076923078,
                0.39121283550631375,
                1
            ],
            [
                0,
                0.39916365709941076,
                0.4017077430622938,
                0.40038498556304136,
                0.0423127276867031,
                0.3451178451178451,
                0.16666666666666666,
                0.15384615384615385,
                0.28436485787029264,
                1
            ]
        ]
    },
    "Which of these substances expands when it freezes?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.2678955453149002,
                    0.17060451941028779,
                    0.18963257098494393,
                    0.0,
                    0.26296296296296295,
                    0.0,
                    0,
                    0.3333333333333333,
                    2
                ],
                [
                    0.6820276497695853,
                    0.8111762054979721,
                    0.805620251231339,
                    1.0,
                    0.37407407407407406,
                    1.0,
                    0,
                    0.0,
                    2
                ],
                [
                    0.05007680491551459,
                    0.018219275091740168,
                    0.004747177783717053,
                    0.0,
                    0.362962962962963,
                    0.0,
                    0,
                    0.6666666666666666,
                    2
                ]
            ],
            "fraction_answers": {
                "sodium chloride": 0.1749184188580612,
                "carbon dioxide": 0.6675568829389957,
                "dihydrogen monoxide": 0.15752469820294307
            },
            "question": "which of these substances expands when it freezes?",
            "rate_limited": false,
            "answers": [
                "sodium chloride",
                "carbon dioxide",
                "dihydrogen monoxide"
            ],
            "integer_answers": {
                "sodium chloride": 0,
                "carbon dioxide": 6,
                "dihydrogen monoxide": 1
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "sodium chloride": 0.274137664635192,
                "carbon dioxide": 0.44779290703213065,
                "dihydrogen monoxide": 0.3076194021948811
            },
            "question_type": 2,
            "data": {
                "result_count_important_words": [
                    265000.0,
                    1260000.0,
                    28300.0
                ],
                "wikipedia_search": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    1490000.0,
                    6330000.0,
                    37300.0
                ],
                "word_relation_to_question": [
                    0.3333333333333333,
                    0.0,
                    0.6666666666666666
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    71.0,
                    101.0,
                    98.0
                ],
                "result_count": [
                    87200.0,
                    222000.0,
                    16300.0
                ]
            },
            "z-best_answer_by_ml": [
                "carbon dioxide"
            ]
        },
        "lines": [
            [
                0,
                0.2678955453149002,
                0.17060451941028779,
                0.18963257098494393,
                0.0,
                0.26296296296296295,
                0.0,
                0,
                0.3333333333333333,
                2
            ],
            [
                0,
                0.6820276497695853,
                0.8111762054979721,
                0.805620251231339,
                1.0,
                0.37407407407407406,
                1.0,
                0,
                0.0,
                2
            ],
            [
                1,
                0.05007680491551459,
                0.018219275091740168,
                0.004747177783717053,
                0.0,
                0.362962962962963,
                0.0,
                0,
                0.6666666666666666,
                2
            ]
        ]
    },
    "Which U.S. president's wife was NOT born in North America?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.26991758241758246,
                    0.14650592562321207,
                    0.21687840290381127,
                    0.34878386668759187,
                    0.3092909535452323,
                    0.0,
                    0.05555555555555558,
                    0.28453601891157654,
                    -1
                ],
                [
                    0.3671016483516484,
                    0.3937474458520638,
                    0.4047186932849365,
                    0.39652199759360457,
                    0.3691931540342298,
                    0.5,
                    0.4444444444444444,
                    0.3359583181953565,
                    -1
                ],
                [
                    0.3629807692307692,
                    0.45974662852472414,
                    0.3784029038112523,
                    0.25469413571880367,
                    0.3215158924205379,
                    0.5,
                    0.5,
                    0.3795056628930669,
                    -1
                ]
            ],
            "fraction_answers": {
                "martin van buren": 0.21078850185021147,
                "john quincy adams": 0.5921329235888595,
                "rutherford b. hayes": 0.19707857456092903
            },
            "question": "which u.s. president's wife was not born in north america?",
            "rate_limited": false,
            "answers": [
                "john quincy adams",
                "rutherford b. hayes",
                "martin van buren"
            ],
            "ml_answers": {
                "martin van buren": 0.45325627786072276,
                "john quincy adams": 0.3453776498424303,
                "rutherford b. hayes": 0.6313322826616694
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "martin van buren": 1,
                "john quincy adams": 7,
                "rutherford b. hayes": 0
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    346000.0,
                    104000.0,
                    39400.0
                ],
                "wikipedia_search": [
                    1.5121613331240817,
                    1.034780024063955,
                    2.4530586428119636
                ],
                "result_count_noun_chunks": [
                    312000.0,
                    105000.0,
                    134000.0
                ],
                "word_relation_to_question": [
                    2.5855677730610815,
                    1.9685001816557217,
                    1.4459320452831967
                ],
                "word_count_noun_chunks": [
                    3.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    8.0,
                    1.0,
                    0.0
                ],
                "word_count_appended": [
                    156.0,
                    107.0,
                    146.0
                ],
                "result_count": [
                    134000.0,
                    77400.0,
                    79800.0
                ]
            },
            "z-best_answer_by_ml": [
                "rutherford b. hayes"
            ]
        },
        "lines": [
            [
                1,
                0.26991758241758246,
                0.14650592562321207,
                0.21687840290381127,
                0.34878386668759187,
                0.3092909535452323,
                0.0,
                0.05555555555555558,
                0.28453601891157654,
                -1
            ],
            [
                0,
                0.3671016483516484,
                0.3937474458520638,
                0.4047186932849365,
                0.39652199759360457,
                0.3691931540342298,
                0.5,
                0.4444444444444444,
                0.3359583181953565,
                -1
            ],
            [
                0,
                0.3629807692307692,
                0.45974662852472414,
                0.3784029038112523,
                0.25469413571880367,
                0.3215158924205379,
                0.5,
                0.5,
                0.3795056628930669,
                -1
            ]
        ]
    },
    "Who was the president of the Screen Actors Guild before its merger with AFTRA?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.0026176567838642126,
                    0.1820635872825435,
                    0.1419330289193303,
                    0.3790243902439025,
                    0.31666666666666665,
                    0.20454545454545456,
                    0.2553191489361702,
                    0.45000068106013225,
                    0
                ],
                [
                    0.002672765347735038,
                    0.22705458908218357,
                    0.3710045662100457,
                    0.10941734417344173,
                    0.21666666666666667,
                    0.0,
                    0.0425531914893617,
                    0.06080937186120113,
                    0
                ],
                [
                    0.9947095778684008,
                    0.590881823635273,
                    0.487062404870624,
                    0.5115582655826558,
                    0.4666666666666667,
                    0.7954545454545454,
                    0.7021276595744681,
                    0.48918994707866664,
                    0
                ]
            ],
            "fraction_answers": {
                "gabrielle carteris": 0.24152132680475802,
                "ken howard": 0.6297063613414127,
                "melissa gilbert": 0.12877231185382942
            },
            "question": "who was the president of the screen actors guild before its merger with aftra?",
            "rate_limited": false,
            "answers": [
                "gabrielle carteris",
                "melissa gilbert",
                "ken howard"
            ],
            "integer_answers": {
                "gabrielle carteris": 0,
                "ken howard": 8,
                "melissa gilbert": 0
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "gabrielle carteris": 0.13814346956147472,
                "ken howard": 0.5949794904720982,
                "melissa gilbert": -0.03643373154962164
            },
            "question_type": 0,
            "data": {
                "result_count_important_words": [
                    6070.0,
                    7570.0,
                    19700.0
                ],
                "wikipedia_search": [
                    2.274146341463415,
                    0.6565040650406504,
                    3.069349593495935
                ],
                "result_count_noun_chunks": [
                    7460.0,
                    19500.0,
                    25600.0
                ],
                "word_relation_to_question": [
                    2.7000040863607935,
                    0.3648562311672068,
                    2.9351396824719997
                ],
                "word_count_noun_chunks": [
                    9.0,
                    0.0,
                    35.0
                ],
                "word_count_raw": [
                    12.0,
                    2.0,
                    33.0
                ],
                "word_count_appended": [
                    114.0,
                    78.0,
                    168.0
                ],
                "result_count": [
                    95.0,
                    97.0,
                    36100.0
                ]
            },
            "z-best_answer_by_ml": [
                "ken howard"
            ]
        },
        "lines": [
            [
                0,
                0.0026176567838642126,
                0.1820635872825435,
                0.1419330289193303,
                0.3790243902439025,
                0.31666666666666665,
                0.20454545454545456,
                0.2553191489361702,
                0.45000068106013225,
                0
            ],
            [
                0,
                0.002672765347735038,
                0.22705458908218357,
                0.3710045662100457,
                0.10941734417344173,
                0.21666666666666667,
                0.0,
                0.0425531914893617,
                0.06080937186120113,
                0
            ],
            [
                1,
                0.9947095778684008,
                0.590881823635273,
                0.487062404870624,
                0.5115582655826558,
                0.4666666666666667,
                0.7954545454545454,
                0.7021276595744681,
                0.48918994707866664,
                0
            ]
        ]
    },
    "Jean Valjean, the protagonist of \u201cLes Mis\u00e9rables,\u201d is identified by what prisoner number?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.0037463976945244955,
                    0.034677723332076894,
                    0.0025626601662603913,
                    0.07152317880794704,
                    0.20212765957446807,
                    0.0,
                    0.0,
                    0.0046816479400749065,
                    1
                ],
                [
                    0.9960058648061075,
                    0.9611760271390878,
                    0.9969373085817863,
                    0.8211920529801325,
                    0.651595744680851,
                    1.0,
                    1.0,
                    0.799625468164794,
                    1
                ],
                [
                    0.0002477374993680166,
                    0.0041462495288352805,
                    0.0005000312519532471,
                    0.10728476821192054,
                    0.14627659574468085,
                    0.0,
                    0.0,
                    0.19569288389513106,
                    1
                ]
            ],
            "fraction_answers": {
                "y2k": 0.03991490843941898,
                "24601": 0.903316558294095,
                "867-5309": 0.05676853326648613
            },
            "question": "jean valjean, the protagonist of \u201cles mis\u00e9rables,\u201d is identified by what prisoner number?",
            "rate_limited": false,
            "answers": [
                "y2k",
                "24601",
                "867-5309"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "y2k": 0,
                "24601": 8,
                "867-5309": 0
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    92.0,
                    2550.0,
                    11.0
                ],
                "wikipedia_search": [
                    0.3576158940397351,
                    4.105960264900662,
                    0.5364238410596026
                ],
                "result_count_noun_chunks": [
                    82.0,
                    31900.0,
                    16.0
                ],
                "word_relation_to_question": [
                    0.028089887640449437,
                    4.797752808988764,
                    1.1741573033707864
                ],
                "word_count_noun_chunks": [
                    0.0,
                    30.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    16.0,
                    0.0
                ],
                "word_count_appended": [
                    76.0,
                    245.0,
                    55.0
                ],
                "result_count": [
                    741.0,
                    197000.0,
                    49.0
                ]
            },
            "z-best_answer_by_ml": [
                "24601"
            ],
            "ml_answers": {
                "y2k": 0.2199115127381312,
                "24601": 0.913882441902346,
                "867-5309": 0.09598759455771087
            }
        },
        "lines": [
            [
                0,
                0.0037463976945244955,
                0.034677723332076894,
                0.0025626601662603913,
                0.07152317880794704,
                0.20212765957446807,
                0.0,
                0.0,
                0.0046816479400749065,
                1
            ],
            [
                1,
                0.9960058648061075,
                0.9611760271390878,
                0.9969373085817863,
                0.8211920529801325,
                0.651595744680851,
                1.0,
                1.0,
                0.799625468164794,
                1
            ],
            [
                0,
                0.0002477374993680166,
                0.0041462495288352805,
                0.0005000312519532471,
                0.10728476821192054,
                0.14627659574468085,
                0.0,
                0.0,
                0.19569288389513106,
                1
            ]
        ]
    },
    "What is the grammatically correct way to announce people have arrived?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.04037051158409402,
                    0.05047453430794624,
                    0.8752904725019365,
                    0.0,
                    0.13636363636363635,
                    0,
                    0,
                    0.3608597285067873,
                    1
                ],
                [
                    0.9569306449563026,
                    0.9468580523461447,
                    0.08869093725793958,
                    0.5,
                    0.22727272727272727,
                    0,
                    0,
                    0.38367269984917046,
                    1
                ],
                [
                    0.0026988434596033222,
                    0.0026674133459089837,
                    0.03601859024012394,
                    0.5,
                    0.6363636363636364,
                    0,
                    0,
                    0.2554675716440422,
                    1
                ]
            ],
            "fraction_answers": {
                "they're here": 0.2438931472107334,
                "there here": 0.5172375102803808,
                "their here": 0.23886934250888583
            },
            "question": "what is the grammatically correct way to announce people have arrived?",
            "rate_limited": false,
            "answers": [
                "they're here",
                "there here",
                "their here"
            ],
            "integer_answers": {
                "they're here": 1,
                "there here": 4,
                "their here": 1
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "they're here": 0.32737761590931125,
                "there here": 0.336181860950438,
                "their here": 0.24434565325217114
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    137000.0,
                    2570000.0,
                    7240.0
                ],
                "wikipedia_search": [
                    0.0,
                    2.0,
                    2.0
                ],
                "result_count_noun_chunks": [
                    2260000.0,
                    229000.0,
                    93000.0
                ],
                "word_relation_to_question": [
                    1.4434389140271493,
                    1.5346907993966818,
                    1.0218702865761689
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    108000.0,
                    2560000.0,
                    7220.0
                ],
                "word_count_appended": [
                    3.0,
                    5.0,
                    14.0
                ]
            },
            "z-best_answer_by_ml": [
                "there here"
            ]
        },
        "lines": [
            [
                1,
                0.04037051158409402,
                0.05047453430794624,
                0.8752904725019365,
                0.0,
                0.13636363636363635,
                0,
                0,
                0.3608597285067873,
                1
            ],
            [
                0,
                0.9569306449563026,
                0.9468580523461447,
                0.08869093725793958,
                0.5,
                0.22727272727272727,
                0,
                0,
                0.38367269984917046,
                1
            ],
            [
                0,
                0.0026988434596033222,
                0.0026674133459089837,
                0.03601859024012394,
                0.5,
                0.6363636363636364,
                0,
                0,
                0.2554675716440422,
                1
            ]
        ]
    },
    "Basketball is NOT a major theme of which of these 90s movies?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.4793756967670011,
                    0.46396571873782627,
                    0.4794138727403042,
                    0.28125,
                    0.495,
                    0,
                    0,
                    0.3171683389074693,
                    -1
                ],
                [
                    0.4546106067845198,
                    0.27678223607323726,
                    0.45460699211595124,
                    0.28125,
                    0.41833333333333333,
                    0,
                    0,
                    0.1948717948717949,
                    -1
                ],
                [
                    0.06601369644847904,
                    0.2592520451889365,
                    0.06597913514374454,
                    0.4375,
                    0.08666666666666667,
                    0,
                    0,
                    0.4879598662207358,
                    -1
                ]
            ],
            "fraction_answers": {
                "white men can't jump": 0.1612754576157997,
                "point break": 0.3065150122737212,
                "eddie": 0.5322095301104791
            },
            "question": "basketball is not a major theme of which of these 90s movies?",
            "rate_limited": false,
            "answers": [
                "white men can't jump",
                "point break",
                "eddie"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "white men can't jump": 1,
                "point break": 1,
                "eddie": 4
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    92500.0,
                    573000.0,
                    618000.0
                ],
                "wikipedia_search": [
                    1.75,
                    1.75,
                    0.5
                ],
                "result_count_noun_chunks": [
                    51700.0,
                    114000.0,
                    1090000.0
                ],
                "word_relation_to_question": [
                    1.8283166109253066,
                    3.051282051282051,
                    0.12040133779264214
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    3.0,
                    49.0,
                    248.0
                ],
                "result_count": [
                    51800.0,
                    114000.0,
                    1090000.0
                ]
            },
            "z-best_answer_by_ml": [
                "point break"
            ],
            "ml_answers": {
                "white men can't jump": 0.31269209867170783,
                "point break": 0.33746982398282527,
                "eddie": 0.23075696298774143
            }
        },
        "lines": [
            [
                0,
                0.4793756967670011,
                0.46396571873782627,
                0.4794138727403042,
                0.28125,
                0.495,
                0,
                0,
                0.3171683389074693,
                -1
            ],
            [
                1,
                0.4546106067845198,
                0.27678223607323726,
                0.45460699211595124,
                0.28125,
                0.41833333333333333,
                0,
                0,
                0.1948717948717949,
                -1
            ],
            [
                0,
                0.06601369644847904,
                0.2592520451889365,
                0.06597913514374454,
                0.4375,
                0.08666666666666667,
                0,
                0,
                0.4879598662207358,
                -1
            ]
        ]
    },
    "What soda is named for a medical condition?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.02678102573078943,
                    0.09741060419235512,
                    0.12843884653629434,
                    0.024390243902439025,
                    0.28498985801217036,
                    0,
                    0.0,
                    0.27777777777777773,
                    1
                ],
                [
                    0.1549098547173114,
                    0.39827373612823674,
                    0.7176002651640703,
                    0.8048780487804879,
                    0.4513184584178499,
                    0,
                    1.0,
                    0.5998817966903073,
                    1
                ],
                [
                    0.8183091195518992,
                    0.5043156596794082,
                    0.1539608882996354,
                    0.17073170731707318,
                    0.26369168356997974,
                    0,
                    0.0,
                    0.12234042553191489,
                    1
                ]
            ],
            "fraction_answers": {
                "fanta": 0.2904784977071301,
                "pepsi": 0.5895517371283233,
                "faygo": 0.11996976516454658
            },
            "question": "what soda is named for a medical condition?",
            "rate_limited": false,
            "answers": [
                "faygo",
                "pepsi",
                "fanta"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "fanta": 0.31087938897178624,
                "pepsi": 0.7358283919065843,
                "faygo": 0.2203694847132336
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    79000.0,
                    323000.0,
                    409000.0
                ],
                "wikipedia_search": [
                    0.04878048780487805,
                    1.6097560975609757,
                    0.34146341463414637
                ],
                "result_count_noun_chunks": [
                    77500.0,
                    433000.0,
                    92900.0
                ],
                "word_relation_to_question": [
                    0.8333333333333333,
                    1.799645390070922,
                    0.3670212765957447
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    11.0,
                    0.0
                ],
                "result_count": [
                    61200.0,
                    354000.0,
                    1870000.0
                ],
                "word_count_appended": [
                    281.0,
                    445.0,
                    260.0
                ]
            },
            "z-best_answer_by_ml": [
                "pepsi"
            ],
            "integer_answers": {
                "fanta": 2,
                "pepsi": 5,
                "faygo": 0
            }
        },
        "lines": [
            [
                0,
                0.02678102573078943,
                0.09741060419235512,
                0.12843884653629434,
                0.024390243902439025,
                0.28498985801217036,
                0,
                0.0,
                0.27777777777777773,
                1
            ],
            [
                1,
                0.1549098547173114,
                0.39827373612823674,
                0.7176002651640703,
                0.8048780487804879,
                0.4513184584178499,
                0,
                1.0,
                0.5998817966903073,
                1
            ],
            [
                0,
                0.8183091195518992,
                0.5043156596794082,
                0.1539608882996354,
                0.17073170731707318,
                0.26369168356997974,
                0,
                0.0,
                0.12234042553191489,
                1
            ]
        ]
    },
    "Who was NOT a wife of Henry VIII?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.24485205843920255,
                    0.24483607381855405,
                    0.2506884627222132,
                    0.4230769230769231,
                    0.2643391521197007,
                    0.1923076923076923,
                    0.288135593220339,
                    0.299126723581039,
                    0
                ],
                [
                    0.4999719779717361,
                    0.49997337419900717,
                    0.49996707206111424,
                    0.3205128205128205,
                    0.46384039900249374,
                    0.5,
                    0.5,
                    0.36536381760670056,
                    0
                ],
                [
                    0.25517596358906136,
                    0.2551905519824388,
                    0.24934446521667258,
                    0.2564102564102564,
                    0.2718204488778055,
                    0.3076923076923077,
                    0.211864406779661,
                    0.3355094588122606,
                    0
                ]
            ],
            "fraction_answers": {
                "catherine of york": 0.08759263466153193,
                "catherine parr": 0.44815933017858406,
                "catherine howard": 0.46424803515988405
            },
            "question": "who was not a wife of henry viii?",
            "rate_limited": false,
            "answers": [
                "catherine parr",
                "catherine of york",
                "catherine howard"
            ],
            "ml_answers": {
                "catherine of york": 0.45879940857418183,
                "catherine parr": 0.21409287120629542,
                "catherine howard": 0.14941034261603925
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "catherine of york": 0,
                "catherine parr": 5,
                "catherine howard": 3
            },
            "question_type": 0,
            "data": {
                "result_count_important_words": [
                    345000.0,
                    36.0,
                    331000.0
                ],
                "wikipedia_search": [
                    0.46153846153846156,
                    1.0769230769230769,
                    1.4615384615384617
                ],
                "result_count_noun_chunks": [
                    371000.0,
                    49.0,
                    373000.0
                ],
                "word_relation_to_question": [
                    1.2052396585137664,
                    0.8078170943597968,
                    0.9869432471264368
                ],
                "word_count_noun_chunks": [
                    8.0,
                    0.0,
                    5.0
                ],
                "word_count_raw": [
                    25.0,
                    0.0,
                    34.0
                ],
                "result_count": [
                    346000.0,
                    38.0,
                    332000.0
                ],
                "word_count_appended": [
                    189.0,
                    29.0,
                    183.0
                ]
            },
            "z-best_answer_by_ml": [
                "catherine of york"
            ]
        },
        "lines": [
            [
                0,
                0.24485205843920255,
                0.24483607381855405,
                0.2506884627222132,
                0.4230769230769231,
                0.2643391521197007,
                0.1923076923076923,
                0.288135593220339,
                0.299126723581039,
                0
            ],
            [
                1,
                0.4999719779717361,
                0.49997337419900717,
                0.49996707206111424,
                0.3205128205128205,
                0.46384039900249374,
                0.5,
                0.5,
                0.36536381760670056,
                0
            ],
            [
                0,
                0.25517596358906136,
                0.2551905519824388,
                0.24934446521667258,
                0.2564102564102564,
                0.2718204488778055,
                0.3076923076923077,
                0.211864406779661,
                0.3355094588122606,
                0
            ]
        ]
    },
    "The creator of Wonder Woman also created an early version of what device?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.20551724137931035,
                    0.3020706455542022,
                    0.14915693904020752,
                    0.03469387755102041,
                    0.145,
                    0.0,
                    0.0,
                    0.16935483870967744,
                    1
                ],
                [
                    0.5241379310344828,
                    0.24604141291108406,
                    0.11802853437094682,
                    0.3508967223252938,
                    0.45,
                    0.0,
                    0.0,
                    0.6112903225806452,
                    1
                ],
                [
                    0.27034482758620687,
                    0.4518879415347138,
                    0.7328145265888456,
                    0.6144094001236857,
                    0.405,
                    1.0,
                    1.0,
                    0.2193548387096774,
                    1
                ]
            ],
            "fraction_answers": {
                "lie detector": 0.5867264418178911,
                "hearing aid": 0.2875493654028066,
                "magic marker": 0.12572419277930225
            },
            "question": "the creator of wonder woman also created an early version of what device?",
            "rate_limited": false,
            "answers": [
                "magic marker",
                "hearing aid",
                "lie detector"
            ],
            "integer_answers": {
                "lie detector": 5,
                "hearing aid": 3,
                "magic marker": 0
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "lie detector": 0.8758044270345577,
                "hearing aid": 0.4888414917514345,
                "magic marker": 0.3773899973836574
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    24800.0,
                    20200.0,
                    37100.0
                ],
                "wikipedia_search": [
                    0.24285714285714285,
                    2.4562770562770564,
                    4.3008658008658
                ],
                "result_count_noun_chunks": [
                    23000.0,
                    18200.0,
                    113000.0
                ],
                "word_relation_to_question": [
                    0.8467741935483871,
                    3.056451612903226,
                    1.096774193548387
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    7.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    25.0
                ],
                "result_count": [
                    44700.0,
                    114000.0,
                    58800.0
                ],
                "word_count_appended": [
                    29.0,
                    90.0,
                    81.0
                ]
            },
            "z-best_answer_by_ml": [
                "lie detector"
            ]
        },
        "lines": [
            [
                0,
                0.20551724137931035,
                0.3020706455542022,
                0.14915693904020752,
                0.03469387755102041,
                0.145,
                0.0,
                0.0,
                0.16935483870967744,
                1
            ],
            [
                0,
                0.5241379310344828,
                0.24604141291108406,
                0.11802853437094682,
                0.3508967223252938,
                0.45,
                0.0,
                0.0,
                0.6112903225806452,
                1
            ],
            [
                1,
                0.27034482758620687,
                0.4518879415347138,
                0.7328145265888456,
                0.6144094001236857,
                0.405,
                1.0,
                1.0,
                0.2193548387096774,
                1
            ]
        ]
    },
    "Lonnie Lynn's only Academy Award win was in what category?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.04886792452830189,
                    0.05982905982905983,
                    0.050799433313094514,
                    0.24921574580044525,
                    0.2624113475177305,
                    0.0,
                    0.0,
                    0.3537606837606838,
                    1
                ],
                [
                    0.04358490566037736,
                    0.052384891094568516,
                    0.04452539971665655,
                    0.261917411166045,
                    0.19148936170212766,
                    0.05,
                    0.047619047619047616,
                    0.3178739316239316,
                    1
                ],
                [
                    0.9075471698113208,
                    0.8877860490763717,
                    0.904675166970249,
                    0.4888668430335097,
                    0.5460992907801419,
                    0.95,
                    0.9523809523809523,
                    0.3283653846153846,
                    1
                ]
            ],
            "fraction_answers": {
                "best adapted screenplay": 0.1281105243436645,
                "best original song": 0.7457151070834913,
                "best cinematography": 0.12617436857284428
            },
            "question": "lonnie lynn's only academy award win was in what category?",
            "rate_limited": false,
            "answers": [
                "best adapted screenplay",
                "best cinematography",
                "best original song"
            ],
            "integer_answers": {
                "best adapted screenplay": 1,
                "best original song": 7,
                "best cinematography": 0
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "best adapted screenplay": 0.14230888235735303,
                "best original song": 0.8939226706320291,
                "best cinematography": 0.1094720761124364
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    2170.0,
                    1900.0,
                    32200.0
                ],
                "wikipedia_search": [
                    0.996862983201781,
                    1.04766964466418,
                    1.9554673721340388
                ],
                "result_count_noun_chunks": [
                    2510.0,
                    2200.0,
                    44700.0
                ],
                "word_relation_to_question": [
                    1.4150427350427353,
                    1.2714957264957265,
                    1.3134615384615385
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    19.0
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    20.0
                ],
                "word_count_appended": [
                    37.0,
                    27.0,
                    77.0
                ],
                "result_count": [
                    2590.0,
                    2310.0,
                    48100.0
                ]
            },
            "z-best_answer_by_ml": [
                "best original song"
            ]
        },
        "lines": [
            [
                0,
                0.04886792452830189,
                0.05982905982905983,
                0.050799433313094514,
                0.24921574580044525,
                0.2624113475177305,
                0.0,
                0.0,
                0.3537606837606838,
                1
            ],
            [
                0,
                0.04358490566037736,
                0.052384891094568516,
                0.04452539971665655,
                0.261917411166045,
                0.19148936170212766,
                0.05,
                0.047619047619047616,
                0.3178739316239316,
                1
            ],
            [
                1,
                0.9075471698113208,
                0.8877860490763717,
                0.904675166970249,
                0.4888668430335097,
                0.5460992907801419,
                0.95,
                0.9523809523809523,
                0.3283653846153846,
                1
            ]
        ]
    },
    "Which of these products was featured on \u201cShark Tank\u201d?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.4239577704809011,
                    0.9750270157915264,
                    0.3592233009708738,
                    0.03125,
                    0.18473895582329317,
                    0.0,
                    0.0,
                    0.009259259259259259,
                    -1
                ],
                [
                    0.573589924768278,
                    0.02486458578952603,
                    0.28779472954230234,
                    0.84375,
                    0.6626506024096386,
                    1.0,
                    1.0,
                    0.8111111111111111,
                    -1
                ],
                [
                    0.0024523047508208987,
                    0.00010839841894759663,
                    0.35298196948682387,
                    0.125,
                    0.15261044176706828,
                    0.0,
                    0.0,
                    0.17962962962962964,
                    -1
                ]
            ],
            "fraction_answers": {
                "scrub daddy": 0.650470119202607,
                "sticky buddy": 0.10159784300666128,
                "instant pot": 0.24793203779073172
            },
            "question": "which of these products was featured on \u201cshark tank\u201d?",
            "rate_limited": false,
            "answers": [
                "instant pot",
                "scrub daddy",
                "sticky buddy"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "scrub daddy": 6,
                "sticky buddy": 0,
                "instant pot": 2
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    6980000.0,
                    178000.0,
                    776.0
                ],
                "wikipedia_search": [
                    0.125,
                    3.375,
                    0.5
                ],
                "result_count_noun_chunks": [
                    51800.0,
                    41500.0,
                    50900.0
                ],
                "word_relation_to_question": [
                    0.037037037037037035,
                    3.2444444444444445,
                    0.7185185185185186
                ],
                "word_count_noun_chunks": [
                    0.0,
                    4.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    3.0,
                    0.0
                ],
                "result_count": [
                    10200.0,
                    13800.0,
                    59.0
                ],
                "word_count_appended": [
                    46.0,
                    165.0,
                    38.0
                ]
            },
            "z-best_answer_by_ml": [
                "scrub daddy"
            ],
            "ml_answers": {
                "scrub daddy": 0.8187049623961608,
                "sticky buddy": 0.12299721068763467,
                "instant pot": 0.38624644330166774
            }
        },
        "lines": [
            [
                0,
                0.4239577704809011,
                0.9750270157915264,
                0.3592233009708738,
                0.03125,
                0.18473895582329317,
                0.0,
                0.0,
                0.009259259259259259,
                -1
            ],
            [
                1,
                0.573589924768278,
                0.02486458578952603,
                0.28779472954230234,
                0.84375,
                0.6626506024096386,
                1.0,
                1.0,
                0.8111111111111111,
                -1
            ],
            [
                0,
                0.0024523047508208987,
                0.00010839841894759663,
                0.35298196948682387,
                0.125,
                0.15261044176706828,
                0.0,
                0.0,
                0.17962962962962964,
                -1
            ]
        ]
    },
    "What are the first words spoken by God in the King James Bible?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.9983657703164462,
                    0.9993904753142502,
                    0.5782790181585482,
                    0.5000000000000001,
                    0.8220338983050848,
                    1.0,
                    1.0,
                    0.2645157837231278,
                    1
                ],
                [
                    0.0003565592036844451,
                    0.00012650512345750004,
                    0.00019373814821555424,
                    0.33333333333333337,
                    0.0423728813559322,
                    0.0,
                    0.0,
                    0.32484779805496494,
                    1
                ],
                [
                    0.0012776704798692616,
                    0.00048301956229227286,
                    0.4215272436932362,
                    0.16666666666666669,
                    0.13559322033898305,
                    0.0,
                    0.0,
                    0.4106364182219073,
                    1
                ]
            ],
            "fraction_answers": {
                "hello, my children": 0.08765385190244851,
                "let there be light": 0.7703231182271821,
                "this is my gift": 0.14202302987036935
            },
            "question": "what are the first words spoken by god in the king james bible?",
            "rate_limited": false,
            "answers": [
                "let there be light",
                "hello, my children",
                "this is my gift"
            ],
            "ml_answers": {
                "hello, my children": 0.02256542164834872,
                "let there be light": 0.913882441902346,
                "this is my gift": 0.13155751315571343
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "hello, my children": 0,
                "let there be light": 7,
                "this is my gift": 1
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    86900.0,
                    11.0,
                    42.0
                ],
                "wikipedia_search": [
                    1.0,
                    0.6666666666666666,
                    0.3333333333333333
                ],
                "result_count_noun_chunks": [
                    98500.0,
                    33.0,
                    71800.0
                ],
                "word_relation_to_question": [
                    1.5870947023387667,
                    1.9490867883297898,
                    2.4638185093314435
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    8.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    33600.0,
                    12.0,
                    43.0
                ],
                "word_count_appended": [
                    97.0,
                    5.0,
                    16.0
                ]
            },
            "z-best_answer_by_ml": [
                "let there be light"
            ]
        },
        "lines": [
            [
                1,
                0.9983657703164462,
                0.9993904753142502,
                0.5782790181585482,
                0.5000000000000001,
                0.8220338983050848,
                1.0,
                1.0,
                0.2645157837231278,
                1
            ],
            [
                0,
                0.0003565592036844451,
                0.00012650512345750004,
                0.00019373814821555424,
                0.33333333333333337,
                0.0423728813559322,
                0.0,
                0.0,
                0.32484779805496494,
                1
            ],
            [
                0,
                0.0012776704798692616,
                0.00048301956229227286,
                0.4215272436932362,
                0.16666666666666669,
                0.13559322033898305,
                0.0,
                0.0,
                0.4106364182219073,
                1
            ]
        ]
    },
    "The inventor of the Erector Set made another toy that contained what?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.11764705882352944,
                    0.28191489361702127,
                    0.26146788990825687,
                    0.37416666666666665,
                    0.15000000000000002,
                    0,
                    0,
                    0.3833333333333333,
                    1
                ],
                [
                    0.4411764705882353,
                    0.2606382978723404,
                    0.2981651376146789,
                    0.43666666666666665,
                    0.425,
                    0,
                    0,
                    0.34020979020979025,
                    1
                ],
                [
                    0.4411764705882353,
                    0.4574468085106383,
                    0.44036697247706424,
                    0.18916666666666665,
                    0.425,
                    0,
                    0,
                    0.27645687645687644,
                    1
                ]
            ],
            "fraction_answers": {
                "uranium ore": 0.4771567192170642,
                "asbestos powder": 0.2567954017668397,
                "live ants": 0.26604787901609617
            },
            "question": "the inventor of the erector set made another toy that contained what?",
            "rate_limited": false,
            "answers": [
                "uranium ore",
                "live ants",
                "asbestos powder"
            ],
            "ml_answers": {
                "uranium ore": 0.40220802846215437,
                "asbestos powder": 0.20184659253212137,
                "live ants": 0.33674609718975684
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "uranium ore": 3,
                "asbestos powder": 2,
                "live ants": 1
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    41.0,
                    45.0,
                    8.0
                ],
                "wikipedia_search": [
                    0.5033333333333334,
                    0.25333333333333335,
                    1.2433333333333334
                ],
                "result_count_noun_chunks": [
                    52.0,
                    44.0,
                    13.0
                ],
                "word_relation_to_question": [
                    0.7000000000000001,
                    0.9587412587412587,
                    1.3412587412587413
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    14.0,
                    3.0,
                    3.0
                ],
                "result_count": [
                    13.0,
                    2.0,
                    2.0
                ]
            },
            "z-best_answer_by_ml": [
                "uranium ore"
            ]
        },
        "lines": [
            [
                1,
                0.11764705882352944,
                0.28191489361702127,
                0.26146788990825687,
                0.37416666666666665,
                0.15000000000000002,
                0,
                0,
                0.3833333333333333,
                1
            ],
            [
                0,
                0.4411764705882353,
                0.2606382978723404,
                0.2981651376146789,
                0.43666666666666665,
                0.425,
                0,
                0,
                0.34020979020979025,
                1
            ],
            [
                0,
                0.4411764705882353,
                0.4574468085106383,
                0.44036697247706424,
                0.18916666666666665,
                0.425,
                0,
                0,
                0.27645687645687644,
                1
            ]
        ]
    },
    "Which of these creatures is most likely to bark?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.047310434219053794,
                    0.012724598759511478,
                    0.11933410252183946,
                    0.038461538461538464,
                    0.2076771653543307,
                    0.125,
                    0.03137254901960784,
                    0.0,
                    -1
                ],
                [
                    0.5735580038885288,
                    0.06650041562759768,
                    0.16367232569639031,
                    0.2692307692307692,
                    0.15354330708661418,
                    0.0,
                    0.0,
                    0.4444444444444444,
                    -1
                ],
                [
                    0.37913156189241737,
                    0.9207749856128908,
                    0.7169935717817703,
                    0.6923076923076923,
                    0.6387795275590551,
                    0.875,
                    0.9686274509803922,
                    0.5555555555555555,
                    -1
                ]
            ],
            "fraction_answers": {
                "blue whale": 0.20886865824679307,
                "mime": 0.07273504854198522,
                "dog": 0.7183962932112217
            },
            "question": "which of these creatures is most likely to bark?",
            "rate_limited": false,
            "answers": [
                "mime",
                "blue whale",
                "dog"
            ],
            "ml_answers": {
                "blue whale": 0.08971790464534025,
                "mime": 0.15739986125370475,
                "dog": 0.8631092816978916
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "blue whale": 1,
                "mime": 0,
                "dog": 7
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    199000.0,
                    1040000.0,
                    14400000.0
                ],
                "wikipedia_search": [
                    0.07692307692307693,
                    0.5384615384615384,
                    1.3846153846153846
                ],
                "result_count_noun_chunks": [
                    724000.0,
                    993000.0,
                    4350000.0
                ],
                "word_relation_to_question": [
                    0.0,
                    1.3333333333333333,
                    1.6666666666666665
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    7.0
                ],
                "word_count_raw": [
                    8.0,
                    0.0,
                    247.0
                ],
                "word_count_appended": [
                    211.0,
                    156.0,
                    649.0
                ],
                "result_count": [
                    146000.0,
                    1770000.0,
                    1170000.0
                ]
            },
            "z-best_answer_by_ml": [
                "dog"
            ]
        },
        "lines": [
            [
                0,
                0.047310434219053794,
                0.012724598759511478,
                0.11933410252183946,
                0.038461538461538464,
                0.2076771653543307,
                0.125,
                0.03137254901960784,
                0.0,
                -1
            ],
            [
                0,
                0.5735580038885288,
                0.06650041562759768,
                0.16367232569639031,
                0.2692307692307692,
                0.15354330708661418,
                0.0,
                0.0,
                0.4444444444444444,
                -1
            ],
            [
                1,
                0.37913156189241737,
                0.9207749856128908,
                0.7169935717817703,
                0.6923076923076923,
                0.6387795275590551,
                0.875,
                0.9686274509803922,
                0.5555555555555555,
                -1
            ]
        ]
    },
    "Which of these phrases, written backwards, is a hip hop group?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.008999830191883173,
                    0.009074410163339383,
                    2.7466608757899868e-05,
                    0.03571428571428571,
                    0.4444444444444444,
                    0,
                    0,
                    0.4593548170357866,
                    -1
                ],
                [
                    0.0010188487009679063,
                    0.000989935654182478,
                    1.630829895000305e-05,
                    0.0,
                    0.2777777777777778,
                    0,
                    0,
                    0.47721870879420836,
                    -1
                ],
                [
                    0.9899813211071489,
                    0.9899356541824781,
                    0.9999562250922921,
                    0.9642857142857143,
                    0.2777777777777778,
                    0,
                    0,
                    0.06342647417000513,
                    -1
                ]
            ],
            "fraction_answers": {
                "beat chefs": 0.12617026320434777,
                "drummers ear": 0.15960254235974952,
                "blues rhythm": 0.7142271944359027
            },
            "question": "which of these phrases, written backwards, is a hip hop group?",
            "rate_limited": false,
            "answers": [
                "drummers ear",
                "beat chefs",
                "blues rhythm"
            ],
            "ml_answers": {
                "beat chefs": 0.2388333815237163,
                "drummers ear": 0.37874641334910214,
                "blues rhythm": 0.2630731868202319
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "beat chefs": 1,
                "drummers ear": 1,
                "blues rhythm": 4
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    55.0,
                    6.0,
                    6000.0
                ],
                "wikipedia_search": [
                    0.14285714285714285,
                    0.0,
                    3.857142857142857
                ],
                "result_count_noun_chunks": [
                    64.0,
                    38.0,
                    2330000.0
                ],
                "word_relation_to_question": [
                    2.7561289022147193,
                    2.8633122527652497,
                    0.38055884502003073
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    16.0,
                    10.0,
                    10.0
                ],
                "result_count": [
                    53.0,
                    6.0,
                    5830.0
                ]
            },
            "z-best_answer_by_ml": [
                "drummers ear"
            ]
        },
        "lines": [
            [
                1,
                0.008999830191883173,
                0.009074410163339383,
                2.7466608757899868e-05,
                0.03571428571428571,
                0.4444444444444444,
                0,
                0,
                0.4593548170357866,
                -1
            ],
            [
                0,
                0.0010188487009679063,
                0.000989935654182478,
                1.630829895000305e-05,
                0.0,
                0.2777777777777778,
                0,
                0,
                0.47721870879420836,
                -1
            ],
            [
                0,
                0.9899813211071489,
                0.9899356541824781,
                0.9999562250922921,
                0.9642857142857143,
                0.2777777777777778,
                0,
                0,
                0.06342647417000513,
                -1
            ]
        ]
    },
    "Which of these is a French territory?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.9951862075540963,
                    0.999515681486592,
                    0.9997471789021336,
                    0.48484848484848486,
                    0.6135458167330677,
                    1.0,
                    0,
                    0.6489685124864278,
                    -1
                ],
                [
                    0.00480759778209489,
                    0.00046785840410010687,
                    0.0002515632814591052,
                    0.18181818181818182,
                    0.350597609561753,
                    0.0,
                    0,
                    0.19136807817589577,
                    -1
                ],
                [
                    6.194663808861763e-06,
                    1.646010930788558e-05,
                    1.257816407295526e-06,
                    0.3333333333333333,
                    0.035856573705179286,
                    0.0,
                    0,
                    0.15966340933767642,
                    -1
                ]
            ],
            "fraction_answers": {
                "french guiana": 0.8202588402872575,
                "french stewart": 0.10418726986049782,
                "french cyprus": 0.07555388985224473
            },
            "question": "which of these is a french territory?",
            "rate_limited": false,
            "answers": [
                "french guiana",
                "french stewart",
                "french cyprus"
            ],
            "integer_answers": {
                "french guiana": 7,
                "french stewart": 0,
                "french cyprus": 0
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "french guiana": 0.5221530174453806,
                "french stewart": 0.22028136263221368,
                "french cyprus": 0.033997470516865094
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    23500000.0,
                    11000.0,
                    387.0
                ],
                "wikipedia_search": [
                    0.9696969696969697,
                    0.36363636363636365,
                    0.6666666666666666
                ],
                "result_count_noun_chunks": [
                    46100000.0,
                    11600.0,
                    58.0
                ],
                "word_relation_to_question": [
                    1.2979370249728557,
                    0.38273615635179153,
                    0.31932681867535284
                ],
                "word_count_noun_chunks": [
                    5.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    154.0,
                    88.0,
                    9.0
                ],
                "result_count": [
                    7390000.0,
                    35700.0,
                    46.0
                ]
            },
            "z-best_answer_by_ml": [
                "french guiana"
            ]
        },
        "lines": [
            [
                1,
                0.9951862075540963,
                0.999515681486592,
                0.9997471789021336,
                0.48484848484848486,
                0.6135458167330677,
                1.0,
                0,
                0.6489685124864278,
                -1
            ],
            [
                0,
                0.00480759778209489,
                0.00046785840410010687,
                0.0002515632814591052,
                0.18181818181818182,
                0.350597609561753,
                0.0,
                0,
                0.19136807817589577,
                -1
            ],
            [
                0,
                6.194663808861763e-06,
                1.646010930788558e-05,
                1.257816407295526e-06,
                0.3333333333333333,
                0.035856573705179286,
                0.0,
                0,
                0.15966340933767642,
                -1
            ]
        ]
    },
    "What makeup item often contains dried cochineal bugs as an ingredient?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.1310634328358209,
                    0.3827128351991669,
                    0.3779540994795666,
                    0.1989795918367347,
                    0.26628895184135976,
                    0.0,
                    0.02631578947368421,
                    0.36493183963172693,
                    1
                ],
                [
                    0.0830223880597015,
                    0.05927275883016576,
                    0.05895401416261411,
                    0.07312925170068027,
                    0.20679886685552407,
                    0.0,
                    0.0,
                    0.32929029912554414,
                    1
                ],
                [
                    0.7859141791044776,
                    0.5580144059706673,
                    0.5630918863578193,
                    0.7278911564625851,
                    0.5269121813031161,
                    1.0,
                    0.9736842105263158,
                    0.30577786124272893,
                    1
                ]
            ],
            "fraction_answers": {
                "lipstick": 0.6801607351209638,
                "eyeliner": 0.10130844734177874,
                "mascara": 0.2185308175372575
            },
            "question": "what makeup item often contains dried cochineal bugs as an ingredient?",
            "rate_limited": false,
            "answers": [
                "mascara",
                "eyeliner",
                "lipstick"
            ],
            "ml_answers": {
                "lipstick": 0.9189517110273231,
                "eyeliner": 0.10594197498779195,
                "mascara": 0.039475128375834154
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "lipstick": 7,
                "eyeliner": 0,
                "mascara": 1
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    44100.0,
                    6830.0,
                    64300.0
                ],
                "wikipedia_search": [
                    1.3928571428571428,
                    0.5119047619047619,
                    5.095238095238096
                ],
                "result_count_noun_chunks": [
                    44300.0,
                    6910.0,
                    66000.0
                ],
                "word_relation_to_question": [
                    1.4597273585269077,
                    1.3171611965021766,
                    1.2231114449709157
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    74.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    37.0
                ],
                "word_count_appended": [
                    188.0,
                    146.0,
                    372.0
                ],
                "result_count": [
                    5620.0,
                    3560.0,
                    33700.0
                ]
            },
            "z-best_answer_by_ml": [
                "lipstick"
            ]
        },
        "lines": [
            [
                0,
                0.1310634328358209,
                0.3827128351991669,
                0.3779540994795666,
                0.1989795918367347,
                0.26628895184135976,
                0.0,
                0.02631578947368421,
                0.36493183963172693,
                1
            ],
            [
                0,
                0.0830223880597015,
                0.05927275883016576,
                0.05895401416261411,
                0.07312925170068027,
                0.20679886685552407,
                0.0,
                0.0,
                0.32929029912554414,
                1
            ],
            [
                1,
                0.7859141791044776,
                0.5580144059706673,
                0.5630918863578193,
                0.7278911564625851,
                0.5269121813031161,
                1.0,
                0.9736842105263158,
                0.30577786124272893,
                1
            ]
        ]
    },
    "What term describes a person from the state between New York and Rhode Island?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.08500510030601836,
                    0.0020588015641124936,
                    0.9225881346612085,
                    0.28334892787524363,
                    0.2603036876355748,
                    0,
                    0,
                    0.31630291005291006,
                    1
                ],
                [
                    0.0010393531413303238,
                    0.000258647759525057,
                    0.0040936029816093355,
                    0.0659337231968811,
                    0.30151843817787416,
                    0,
                    0,
                    0.44298941798941804,
                    1
                ],
                [
                    0.9139555465526513,
                    0.9976825506763625,
                    0.07331826235718214,
                    0.6507173489278752,
                    0.43817787418655096,
                    0,
                    0,
                    0.24070767195767195,
                    1
                ]
            ],
            "fraction_answers": {
                "hoosier": 0.552426542443049,
                "nutmegger": 0.135972197207773,
                "cheesehead": 0.31160126034917796
            },
            "question": "what term describes a person from the state between new york and rhode island?",
            "rate_limited": false,
            "answers": [
                "cheesehead",
                "nutmegger",
                "hoosier"
            ],
            "ml_answers": {
                "hoosier": 0.502292025502087,
                "nutmegger": 0.24858765121447302,
                "cheesehead": 0.12075556259858401
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "hoosier": 4,
                "nutmegger": 1,
                "cheesehead": 1
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    7140.0,
                    897.0,
                    3460000.0
                ],
                "wikipedia_search": [
                    1.4167446393762182,
                    0.32966861598440544,
                    3.2535867446393762
                ],
                "result_count_noun_chunks": [
                    3020000.0,
                    13400.0,
                    240000.0
                ],
                "word_relation_to_question": [
                    1.8978174603174605,
                    2.6579365079365083,
                    1.4442460317460317
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    120.0,
                    139.0,
                    202.0
                ],
                "result_count": [
                    70500.0,
                    862.0,
                    758000.0
                ]
            },
            "z-best_answer_by_ml": [
                "hoosier"
            ]
        },
        "lines": [
            [
                0,
                0.08500510030601836,
                0.0020588015641124936,
                0.9225881346612085,
                0.28334892787524363,
                0.2603036876355748,
                0,
                0,
                0.31630291005291006,
                1
            ],
            [
                1,
                0.0010393531413303238,
                0.000258647759525057,
                0.0040936029816093355,
                0.0659337231968811,
                0.30151843817787416,
                0,
                0,
                0.44298941798941804,
                1
            ],
            [
                0,
                0.9139555465526513,
                0.9976825506763625,
                0.07331826235718214,
                0.6507173489278752,
                0.43817787418655096,
                0,
                0,
                0.24070767195767195,
                1
            ]
        ]
    },
    "Where in the home does the Maillard reaction typically occur?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.6843177189409368,
                    0.7840743696887178,
                    0.23880597014925373,
                    0.7032784793978824,
                    0.4774193548387097,
                    1.0,
                    1.0,
                    0.6190476190476191,
                    3
                ],
                [
                    0.16334012219959268,
                    0.040800037560448844,
                    0.3253731343283582,
                    0.12590891695369308,
                    0.2564516129032258,
                    0.0,
                    0.0,
                    0.21164021164021163,
                    3
                ],
                [
                    0.15234215885947047,
                    0.17512559275083336,
                    0.43582089552238806,
                    0.17081260364842454,
                    0.2661290322580645,
                    0.0,
                    0.0,
                    0.1693121693121693,
                    3
                ]
            ],
            "fraction_answers": {
                "bathroom": 0.17119280654391877,
                "bedroom": 0.14043925444819128,
                "kitchen": 0.68836793900789
            },
            "question": "where in the home does the maillard reaction typically occur?",
            "rate_limited": false,
            "answers": [
                "kitchen",
                "bedroom",
                "bathroom"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "bathroom": 1,
                "bedroom": 0,
                "kitchen": 7
            },
            "question_type": 3,
            "data": {
                "result_count_important_words": [
                    167000.0,
                    8690.0,
                    37300.0
                ],
                "wikipedia_search": [
                    2.109835438193647,
                    0.3777267508610792,
                    0.5124378109452736
                ],
                "result_count_noun_chunks": [
                    80000.0,
                    109000.0,
                    146000.0
                ],
                "word_relation_to_question": [
                    1.2380952380952381,
                    0.42328042328042326,
                    0.3386243386243386
                ],
                "word_count_noun_chunks": [
                    5.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    25.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    296.0,
                    159.0,
                    165.0
                ],
                "result_count": [
                    168000.0,
                    40100.0,
                    37400.0
                ]
            },
            "z-best_answer_by_ml": [
                "kitchen"
            ],
            "ml_answers": {
                "bathroom": 0.046738700115523814,
                "bedroom": 0.0006210489005737049,
                "kitchen": 0.9804667938220122
            }
        },
        "lines": [
            [
                1,
                0.6843177189409368,
                0.7840743696887178,
                0.23880597014925373,
                0.7032784793978824,
                0.4774193548387097,
                1.0,
                1.0,
                0.6190476190476191,
                3
            ],
            [
                0,
                0.16334012219959268,
                0.040800037560448844,
                0.3253731343283582,
                0.12590891695369308,
                0.2564516129032258,
                0.0,
                0.0,
                0.21164021164021163,
                3
            ],
            [
                0,
                0.15234215885947047,
                0.17512559275083336,
                0.43582089552238806,
                0.17081260364842454,
                0.2661290322580645,
                0.0,
                0.0,
                0.1693121693121693,
                3
            ]
        ]
    },
    "Who is the director of \u201cTyler Perry\u2019s Madea\u2019s Family Reunion\u201d?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.42028985507246375,
                    0.7872174590802806,
                    0.4444444444444444,
                    0.6392543859649122,
                    0.8018867924528302,
                    1.0,
                    1.0,
                    0.8773666146095275,
                    0
                ],
                [
                    0.30434782608695654,
                    0.10210444271239283,
                    0.30917874396135264,
                    0.15021929824561403,
                    0.10377358490566038,
                    0.0,
                    0.0,
                    0.10510204081632654,
                    0
                ],
                [
                    0.2753623188405797,
                    0.11067809820732658,
                    0.2463768115942029,
                    0.21052631578947367,
                    0.09433962264150944,
                    0.0,
                    0.0,
                    0.017531344574146134,
                    0
                ]
            ],
            "fraction_answers": {
                "tyler perry": 0.7463074439530574,
                "abraham lincoln": 0.11935181395590481,
                "george lucas": 0.1343407420910379
            },
            "question": "who is the director of \u201ctyler perry\u2019s madea\u2019s family reunion\u201d?",
            "rate_limited": false,
            "answers": [
                "tyler perry",
                "george lucas",
                "abraham lincoln"
            ],
            "integer_answers": {
                "tyler perry": 8,
                "abraham lincoln": 0,
                "george lucas": 0
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "tyler perry": 0.8649003386833115,
                "abraham lincoln": 0.17694818584050293,
                "george lucas": 0.12886753679681578
            },
            "question_type": 0,
            "data": {
                "result_count_important_words": [
                    101000.0,
                    13100.0,
                    14200.0
                ],
                "wikipedia_search": [
                    3.8355263157894735,
                    0.9013157894736842,
                    1.263157894736842
                ],
                "result_count_noun_chunks": [
                    92.0,
                    64.0,
                    51.0
                ],
                "word_relation_to_question": [
                    5.264199687657164,
                    0.6306122448979592,
                    0.10518806744487678
                ],
                "word_count_noun_chunks": [
                    228.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    242.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    255.0,
                    33.0,
                    30.0
                ],
                "result_count": [
                    87.0,
                    63.0,
                    57.0
                ]
            },
            "z-best_answer_by_ml": [
                "tyler perry"
            ]
        },
        "lines": [
            [
                1,
                0.42028985507246375,
                0.7872174590802806,
                0.4444444444444444,
                0.6392543859649122,
                0.8018867924528302,
                1.0,
                1.0,
                0.8773666146095275,
                0
            ],
            [
                0,
                0.30434782608695654,
                0.10210444271239283,
                0.30917874396135264,
                0.15021929824561403,
                0.10377358490566038,
                0.0,
                0.0,
                0.10510204081632654,
                0
            ],
            [
                0,
                0.2753623188405797,
                0.11067809820732658,
                0.2463768115942029,
                0.21052631578947367,
                0.09433962264150944,
                0.0,
                0.0,
                0.017531344574146134,
                0
            ]
        ]
    },
    "The first person to lead an expedition to the South Pole came from what country?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.29069405539993776,
                    0.27116402116402116,
                    0.26755852842809363,
                    0.383931694648721,
                    0.29812834224598933,
                    0.7142857142857143,
                    0.5,
                    0.1904380341880342,
                    1
                ],
                [
                    0.161531279178338,
                    0.21428571428571427,
                    0.19899665551839466,
                    0.38405329402673144,
                    0.3890374331550802,
                    0.0,
                    0.0,
                    0.32542735042735044,
                    1
                ],
                [
                    0.5477746654217243,
                    0.5145502645502645,
                    0.5334448160535117,
                    0.2320150113245475,
                    0.31283422459893045,
                    0.2857142857142857,
                    0.5,
                    0.4841346153846154,
                    1
                ]
            ],
            "fraction_answers": {
                "canada": 0.426308485380985,
                "iceland": 0.2091664658239511,
                "norway": 0.364525048795064
            },
            "question": "the first person to lead an expedition to the south pole came from what country?",
            "rate_limited": false,
            "answers": [
                "norway",
                "iceland",
                "canada"
            ],
            "ml_answers": {
                "canada": 0.17136538793423398,
                "iceland": 0.23543542009596774,
                "norway": 0.6169619801255819
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "canada": 4,
                "iceland": 2,
                "norway": 2
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    205000.0,
                    162000.0,
                    389000.0
                ],
                "wikipedia_search": [
                    2.6875218625410473,
                    2.6883730581871204,
                    1.6241050792718326
                ],
                "result_count_noun_chunks": [
                    160000.0,
                    119000.0,
                    319000.0
                ],
                "word_relation_to_question": [
                    1.142628205128205,
                    1.9525641025641023,
                    2.904807692307692
                ],
                "word_count_noun_chunks": [
                    5.0,
                    0.0,
                    2.0
                ],
                "word_count_raw": [
                    6.0,
                    0.0,
                    6.0
                ],
                "word_count_appended": [
                    223.0,
                    291.0,
                    234.0
                ],
                "result_count": [
                    93400.0,
                    51900.0,
                    176000.0
                ]
            },
            "z-best_answer_by_ml": [
                "norway"
            ]
        },
        "lines": [
            [
                1,
                0.29069405539993776,
                0.27116402116402116,
                0.26755852842809363,
                0.383931694648721,
                0.29812834224598933,
                0.7142857142857143,
                0.5,
                0.1904380341880342,
                1
            ],
            [
                0,
                0.161531279178338,
                0.21428571428571427,
                0.19899665551839466,
                0.38405329402673144,
                0.3890374331550802,
                0.0,
                0.0,
                0.32542735042735044,
                1
            ],
            [
                0,
                0.5477746654217243,
                0.5145502645502645,
                0.5334448160535117,
                0.2320150113245475,
                0.31283422459893045,
                0.2857142857142857,
                0.5,
                0.4841346153846154,
                1
            ]
        ]
    },
    "Which of these modes of transportation has only one wheel?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.3101503759398496,
                    0.023072525318523357,
                    0.31351351351351353,
                    0.09574468085106383,
                    0.11163337250293771,
                    0.0,
                    0.0,
                    0.07344632768361582,
                    -1
                ],
                [
                    0.4548872180451128,
                    0.9678209735380594,
                    0.46306306306306305,
                    0.5425531914893617,
                    0.40658049353701525,
                    0.8181818181818182,
                    0.62,
                    0.6381612737544942,
                    -1
                ],
                [
                    0.2349624060150376,
                    0.009106501143417184,
                    0.22342342342342342,
                    0.3617021276595745,
                    0.481786133960047,
                    0.18181818181818182,
                    0.38,
                    0.2883923985618901,
                    -1
                ]
            ],
            "fraction_answers": {
                "unicycle": 0.2701488965726965,
                "bus": 0.6139060039511156,
                "monster truck": 0.11594509947618797
            },
            "question": "which of these modes of transportation has only one wheel?",
            "rate_limited": false,
            "answers": [
                "monster truck",
                "bus",
                "unicycle"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "unicycle": 0.45017147172842376,
                "bus": 0.49195600476669005,
                "monster truck": 0.12085348819472241
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    1130000.0,
                    47400000.0,
                    446000.0
                ],
                "wikipedia_search": [
                    0.19148936170212766,
                    1.0851063829787233,
                    0.723404255319149
                ],
                "result_count_noun_chunks": [
                    1740000.0,
                    2570000.0,
                    1240000.0
                ],
                "word_relation_to_question": [
                    0.22033898305084745,
                    1.9144838212634825,
                    0.8651771956856702
                ],
                "word_count_noun_chunks": [
                    0.0,
                    27.0,
                    6.0
                ],
                "word_count_raw": [
                    0.0,
                    31.0,
                    19.0
                ],
                "result_count": [
                    1650000.0,
                    2420000.0,
                    1250000.0
                ],
                "word_count_appended": [
                    95.0,
                    346.0,
                    410.0
                ]
            },
            "z-best_answer_by_ml": [
                "bus"
            ],
            "integer_answers": {
                "unicycle": 1,
                "bus": 7,
                "monster truck": 0
            }
        },
        "lines": [
            [
                0,
                0.3101503759398496,
                0.023072525318523357,
                0.31351351351351353,
                0.09574468085106383,
                0.11163337250293771,
                0.0,
                0.0,
                0.07344632768361582,
                -1
            ],
            [
                0,
                0.4548872180451128,
                0.9678209735380594,
                0.46306306306306305,
                0.5425531914893617,
                0.40658049353701525,
                0.8181818181818182,
                0.62,
                0.6381612737544942,
                -1
            ],
            [
                1,
                0.2349624060150376,
                0.009106501143417184,
                0.22342342342342342,
                0.3617021276595745,
                0.481786133960047,
                0.18181818181818182,
                0.38,
                0.2883923985618901,
                -1
            ]
        ]
    },
    "Which of these astronomical objects orbits the Earth?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.40681362725450904,
                    0.3489787049109083,
                    0.34961832061068704,
                    0.40514200711569137,
                    0.48940914158305465,
                    0.46153846153846156,
                    0.3541666666666667,
                    0.43143402399127595,
                    -1
                ],
                [
                    0.3026052104208417,
                    0.30291177748804865,
                    0.3251908396946565,
                    0.3276821862348178,
                    0.09587513935340022,
                    0.05982905982905983,
                    0.027777777777777776,
                    0.34296619411123225,
                    -1
                ],
                [
                    0.2905811623246493,
                    0.34810951760104303,
                    0.3251908396946565,
                    0.26717580664949087,
                    0.41471571906354515,
                    0.47863247863247865,
                    0.6180555555555556,
                    0.2255997818974918,
                    -1
                ]
            ],
            "fraction_answers": {
                "sun": 0.3710076076773638,
                "milky way": 0.2231047731137293,
                "moon": 0.4058876192089068
            },
            "question": "which of these astronomical objects orbits the earth?",
            "rate_limited": false,
            "answers": [
                "moon",
                "milky way",
                "sun"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "sun": 2,
                "milky way": 0,
                "moon": 6
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    803000.0,
                    697000.0,
                    801000.0
                ],
                "wikipedia_search": [
                    1.6205680284627655,
                    1.3107287449392713,
                    1.0687032265979635
                ],
                "result_count_noun_chunks": [
                    2290000.0,
                    2130000.0,
                    2130000.0
                ],
                "word_relation_to_question": [
                    1.7257360959651038,
                    1.371864776444929,
                    0.9023991275899672
                ],
                "word_count_noun_chunks": [
                    54.0,
                    7.0,
                    56.0
                ],
                "word_count_raw": [
                    51.0,
                    4.0,
                    89.0
                ],
                "result_count": [
                    2030000.0,
                    1510000.0,
                    1450000.0
                ],
                "word_count_appended": [
                    439.0,
                    86.0,
                    372.0
                ]
            },
            "z-best_answer_by_ml": [
                "sun"
            ],
            "ml_answers": {
                "sun": 0.3957530712999296,
                "milky way": 0.3342977685794358,
                "moon": 0.38707451288002565
            }
        },
        "lines": [
            [
                1,
                0.40681362725450904,
                0.3489787049109083,
                0.34961832061068704,
                0.40514200711569137,
                0.48940914158305465,
                0.46153846153846156,
                0.3541666666666667,
                0.43143402399127595,
                -1
            ],
            [
                0,
                0.3026052104208417,
                0.30291177748804865,
                0.3251908396946565,
                0.3276821862348178,
                0.09587513935340022,
                0.05982905982905983,
                0.027777777777777776,
                0.34296619411123225,
                -1
            ],
            [
                0,
                0.2905811623246493,
                0.34810951760104303,
                0.3251908396946565,
                0.26717580664949087,
                0.41471571906354515,
                0.47863247863247865,
                0.6180555555555556,
                0.2255997818974918,
                -1
            ]
        ]
    },
    "Made famous in a documentary, where is the \u201cGrey Gardens\u201d home located?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.15300546448087432,
                    3.445726672430428e-05,
                    0.7613609326671426,
                    0.3482142857142857,
                    0.09090909090909091,
                    0.0,
                    0,
                    0.21373344147066775,
                    3
                ],
                [
                    0.47540983606557374,
                    0.955406031901164,
                    0.21859386152748037,
                    0.38125,
                    0.696969696969697,
                    1.0,
                    0,
                    0.5257637199243038,
                    3
                ],
                [
                    0.37158469945355194,
                    0.04455951083211167,
                    0.020045205805377112,
                    0.27053571428571427,
                    0.21212121212121213,
                    0.0,
                    0,
                    0.2605028386050284,
                    3
                ]
            ],
            "fraction_answers": {
                "east hampton, ny": 0.6076275923411741,
                "savannah, ga": 0.16847845444328508,
                "cape cod, ma": 0.22389395321554081
            },
            "question": "made famous in a documentary, where is the \u201cgrey gardens\u201d home located?",
            "rate_limited": false,
            "answers": [
                "cape cod, ma",
                "east hampton, ny",
                "savannah, ga"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "east hampton, ny": 0.6584819623141727,
                "savannah, ga": 0.13273993364102502,
                "cape cod, ma": 0.25415318406944926
            },
            "question_type": 3,
            "data": {
                "result_count_important_words": [
                    44.0,
                    1220000.0,
                    56900.0
                ],
                "wikipedia_search": [
                    1.3928571428571428,
                    1.525,
                    1.082142857142857
                ],
                "result_count_noun_chunks": [
                    256000.0,
                    73500.0,
                    6740.0
                ],
                "word_relation_to_question": [
                    0.6412003244120033,
                    1.5772911597729116,
                    0.7815085158150852
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    28.0,
                    87.0,
                    68.0
                ],
                "word_count_appended": [
                    3.0,
                    23.0,
                    7.0
                ]
            },
            "z-best_answer_by_ml": [
                "east hampton, ny"
            ],
            "integer_answers": {
                "east hampton, ny": 6,
                "savannah, ga": 0,
                "cape cod, ma": 1
            }
        },
        "lines": [
            [
                0,
                0.15300546448087432,
                3.445726672430428e-05,
                0.7613609326671426,
                0.3482142857142857,
                0.09090909090909091,
                0.0,
                0,
                0.21373344147066775,
                3
            ],
            [
                1,
                0.47540983606557374,
                0.955406031901164,
                0.21859386152748037,
                0.38125,
                0.696969696969697,
                1.0,
                0,
                0.5257637199243038,
                3
            ],
            [
                0,
                0.37158469945355194,
                0.04455951083211167,
                0.020045205805377112,
                0.27053571428571427,
                0.21212121212121213,
                0.0,
                0,
                0.2605028386050284,
                3
            ]
        ]
    },
    "Which of these is NOT a machine used for printing?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.40761750405186387,
                    0.39780521262002744,
                    0.3613138686131387,
                    0.5,
                    0.3479338842975207,
                    0,
                    0,
                    0.49782608695652175,
                    -1
                ],
                [
                    0.32820097244732577,
                    0.18038408779149517,
                    0.21350364963503649,
                    0.30978260869565216,
                    0.26859504132231404,
                    0,
                    0,
                    0.3637327677624602,
                    -1
                ],
                [
                    0.26418152350081037,
                    0.4218106995884774,
                    0.4251824817518248,
                    0.19021739130434784,
                    0.38347107438016526,
                    0,
                    0,
                    0.13844114528101803,
                    -1
                ]
            ],
            "fraction_answers": {
                "spirit duplicator": 0.39223189473111875,
                "hydraulophone": 0.1625011478203092,
                "hectograph": 0.4452669574485721
            },
            "question": "which of these is not a machine used for printing?",
            "rate_limited": false,
            "answers": [
                "hydraulophone",
                "hectograph",
                "spirit duplicator"
            ],
            "integer_answers": {
                "spirit duplicator": 3,
                "hydraulophone": 0,
                "hectograph": 3
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "spirit duplicator": 0.01312927939864498,
                "hydraulophone": 0.1487007374819756,
                "hectograph": 0.2521670751313442
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    14900.0,
                    46600.0,
                    11400.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.7608695652173914,
                    1.2391304347826086
                ],
                "result_count_noun_chunks": [
                    30400.0,
                    62800.0,
                    16400.0
                ],
                "word_relation_to_question": [
                    0.008695652173913044,
                    0.545068928950159,
                    1.4462354188759279
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    11400.0,
                    21200.0,
                    29100.0
                ],
                "word_count_appended": [
                    184.0,
                    280.0,
                    141.0
                ]
            },
            "z-best_answer_by_ml": [
                "hectograph"
            ]
        },
        "lines": [
            [
                1,
                0.40761750405186387,
                0.39780521262002744,
                0.3613138686131387,
                0.5,
                0.3479338842975207,
                0,
                0,
                0.49782608695652175,
                -1
            ],
            [
                0,
                0.32820097244732577,
                0.18038408779149517,
                0.21350364963503649,
                0.30978260869565216,
                0.26859504132231404,
                0,
                0,
                0.3637327677624602,
                -1
            ],
            [
                0,
                0.26418152350081037,
                0.4218106995884774,
                0.4251824817518248,
                0.19021739130434784,
                0.38347107438016526,
                0,
                0,
                0.13844114528101803,
                -1
            ]
        ]
    },
    "How does the second verse of \u201cThe Star-Spangled Banner\u201d begin?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.0019828155981493722,
                    0.009703419672900853,
                    0.002115398643227077,
                    0.5833333333333334,
                    0.03529411764705882,
                    0.0,
                    0.0,
                    0.19946808510638298,
                    5
                ],
                [
                    0.8162590879048248,
                    0.946865951952422,
                    0.8607484134510176,
                    0.06250000000000001,
                    0.49411764705882355,
                    0.6666666666666666,
                    1.0,
                    0.32535460992907805,
                    5
                ],
                [
                    0.1817580964970258,
                    0.043430628374677205,
                    0.13713618790575535,
                    0.3541666666666667,
                    0.47058823529411764,
                    0.3333333333333333,
                    0.0,
                    0.475177304964539,
                    5
                ]
            ],
            "fraction_answers": {
                "on the shore": 0.6465640471203541,
                "travels far": 0.10398714625013156,
                "where the foe": 0.2494488066295144
            },
            "question": "how does the second verse of \u201cthe star-spangled banner\u201d begin?",
            "rate_limited": false,
            "answers": [
                "travels far",
                "on the shore",
                "where the foe"
            ],
            "ml_answers": {
                "on the shore": 0.875043035451226,
                "travels far": 0.1202404771182588,
                "where the foe": 0.10264846202712737
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "on the shore": 6,
                "travels far": 1,
                "where the foe": 1
            },
            "question_type": 5,
            "data": {
                "result_count_important_words": [
                    2480.0,
                    242000.0,
                    11100.0
                ],
                "wikipedia_search": [
                    2.333333333333333,
                    0.25,
                    1.4166666666666665
                ],
                "result_count_noun_chunks": [
                    29.0,
                    11800.0,
                    1880.0
                ],
                "word_relation_to_question": [
                    0.7978723404255319,
                    1.3014184397163122,
                    1.900709219858156
                ],
                "word_count_noun_chunks": [
                    0.0,
                    6.0,
                    3.0
                ],
                "word_count_raw": [
                    0.0,
                    2.0,
                    0.0
                ],
                "result_count": [
                    12.0,
                    4940.0,
                    1100.0
                ],
                "word_count_appended": [
                    3.0,
                    42.0,
                    40.0
                ]
            },
            "z-best_answer_by_ml": [
                "on the shore"
            ]
        },
        "lines": [
            [
                0,
                0.0019828155981493722,
                0.009703419672900853,
                0.002115398643227077,
                0.5833333333333334,
                0.03529411764705882,
                0.0,
                0.0,
                0.19946808510638298,
                5
            ],
            [
                1,
                0.8162590879048248,
                0.946865951952422,
                0.8607484134510176,
                0.06250000000000001,
                0.49411764705882355,
                0.6666666666666666,
                1.0,
                0.32535460992907805,
                5
            ],
            [
                0,
                0.1817580964970258,
                0.043430628374677205,
                0.13713618790575535,
                0.3541666666666667,
                0.47058823529411764,
                0.3333333333333333,
                0.0,
                0.475177304964539,
                5
            ]
        ]
    },
    "Which of these substances is both artificially made and found in nature?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.22813688212927757,
                    0.30322128851540614,
                    0.3152603231597846,
                    0.2833333333333333,
                    0.29493545183714004,
                    0,
                    0,
                    0.5,
                    -1
                ],
                [
                    0.4505703422053232,
                    0.35154061624649857,
                    0.3385996409335727,
                    0.09999999999999999,
                    0.31479642502482624,
                    0,
                    0,
                    0.3181818181818182,
                    -1
                ],
                [
                    0.32129277566539927,
                    0.34523809523809523,
                    0.3461400359066427,
                    0.6166666666666667,
                    0.3902681231380338,
                    0,
                    0,
                    0.18181818181818182,
                    -1
                ]
            ],
            "fraction_answers": {
                "latex": 0.3669039797388366,
                "teflon": 0.3208145464958236,
                "nylon": 0.3122814737653398
            },
            "question": "which of these substances is both artificially made and found in nature?",
            "rate_limited": false,
            "answers": [
                "teflon",
                "nylon",
                "latex"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "latex": 3,
                "teflon": 1,
                "nylon": 2
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    433000.0,
                    502000.0,
                    493000.0
                ],
                "wikipedia_search": [
                    0.85,
                    0.3,
                    1.85
                ],
                "result_count_noun_chunks": [
                    878000.0,
                    943000.0,
                    964000.0
                ],
                "word_relation_to_question": [
                    0.5,
                    0.3181818181818182,
                    0.18181818181818182
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    1200000.0,
                    2370000.0,
                    1690000.0
                ],
                "word_count_appended": [
                    297.0,
                    317.0,
                    393.0
                ]
            },
            "z-best_answer_by_ml": [
                "nylon"
            ],
            "ml_answers": {
                "latex": 0.11533657078713312,
                "teflon": 0.12271480152881338,
                "nylon": 0.17796286209696216
            }
        },
        "lines": [
            [
                0,
                0.22813688212927757,
                0.30322128851540614,
                0.3152603231597846,
                0.2833333333333333,
                0.29493545183714004,
                0,
                0,
                0.5,
                -1
            ],
            [
                0,
                0.4505703422053232,
                0.35154061624649857,
                0.3385996409335727,
                0.09999999999999999,
                0.31479642502482624,
                0,
                0,
                0.3181818181818182,
                -1
            ],
            [
                1,
                0.32129277566539927,
                0.34523809523809523,
                0.3461400359066427,
                0.6166666666666667,
                0.3902681231380338,
                0,
                0,
                0.18181818181818182,
                -1
            ]
        ]
    },
    "Every U.S. state that starts with which of these letters has a Democratic governor?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.35009548058561424,
                    0.32857142857142857,
                    0.34213305174234426,
                    0.18421052631578946,
                    0.24835116517660852,
                    0.21662446520562298,
                    0.25625049924115345,
                    0.1935897435897436,
                    -1
                ],
                [
                    0.3341820496499045,
                    0.35454545454545455,
                    0.3590285110876452,
                    0.5,
                    0.432214568371684,
                    0.46232428184755087,
                    0.46409457624410894,
                    0.26282051282051283,
                    -1
                ],
                [
                    0.3157224697644812,
                    0.3168831168831169,
                    0.2988384371700106,
                    0.3157894736842105,
                    0.31943426645170747,
                    0.3210512529468262,
                    0.2796549245147376,
                    0.5435897435897437,
                    -1
                ]
            ],
            "fraction_answers": {
                "c": 0.3961512443208576,
                "w": 0.26497829505353815,
                "v": 0.3388704606256043
            },
            "question": "every u.s. state that starts with which of these letters has a democratic governor?",
            "rate_limited": false,
            "answers": [
                "w",
                "c",
                "v"
            ],
            "ml_answers": {
                "c": 0.4555383786631591,
                "w": 0.09095119911070756,
                "v": 0.053528916307502034
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "c": 6,
                "w": 1,
                "v": 1
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    2530000.0,
                    2730000.0,
                    2440000.0
                ],
                "wikipedia_search": [
                    0.3684210526315789,
                    1.0,
                    0.631578947368421
                ],
                "result_count_noun_chunks": [
                    6480000.0,
                    6800000.0,
                    5660000.0
                ],
                "word_relation_to_question": [
                    0.967948717948718,
                    1.314102564102564,
                    2.717948717948718
                ],
                "word_count_noun_chunks": [
                    2481.0,
                    5295.0,
                    3677.0
                ],
                "word_count_raw": [
                    3208.0,
                    5810.0,
                    3501.0
                ],
                "result_count": [
                    11000000.0,
                    10500000.0,
                    9920000.0
                ],
                "word_count_appended": [
                    3389.0,
                    5898.0,
                    4359.0
                ]
            },
            "z-best_answer_by_ml": [
                "c"
            ]
        },
        "lines": [
            [
                0,
                0.35009548058561424,
                0.32857142857142857,
                0.34213305174234426,
                0.18421052631578946,
                0.24835116517660852,
                0.21662446520562298,
                0.25625049924115345,
                0.1935897435897436,
                -1
            ],
            [
                1,
                0.3341820496499045,
                0.35454545454545455,
                0.3590285110876452,
                0.5,
                0.432214568371684,
                0.46232428184755087,
                0.46409457624410894,
                0.26282051282051283,
                -1
            ],
            [
                0,
                0.3157224697644812,
                0.3168831168831169,
                0.2988384371700106,
                0.3157894736842105,
                0.31943426645170747,
                0.3210512529468262,
                0.2796549245147376,
                0.5435897435897437,
                -1
            ]
        ]
    },
    "What does a rattlesnake typically do when it feels threatened?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.00011256936259253863,
                    0.0003210891343436938,
                    0.002738579349044856,
                    0.5416666666666667,
                    0.71875,
                    0,
                    0,
                    0.6157407407407408,
                    1
                ],
                [
                    0.9998808089101962,
                    0.9996575049233667,
                    0.9972562535201079,
                    0.25,
                    0.1875,
                    0,
                    0,
                    0.2222222222222222,
                    1
                ],
                [
                    6.621727211325802e-06,
                    2.1405942289579588e-05,
                    5.167130847254445e-06,
                    0.20833333333333334,
                    0.09375,
                    0,
                    0,
                    0.16203703703703703,
                    1
                ]
            ],
            "fraction_answers": {
                "sends an angry email": 0.0773589275284531,
                "eats its feelings": 0.6094194649293155,
                "rattles its tail": 0.31322160754223144
            },
            "question": "what does a rattlesnake typically do when it feels threatened?",
            "rate_limited": false,
            "answers": [
                "rattles its tail",
                "eats its feelings",
                "sends an angry email"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "sends an angry email": 0,
                "eats its feelings": 3,
                "rattles its tail": 3
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    30.0,
                    93400.0,
                    2.0
                ],
                "wikipedia_search": [
                    2.166666666666667,
                    1.0,
                    0.8333333333333334
                ],
                "result_count_noun_chunks": [
                    1060.0,
                    386000.0,
                    2.0
                ],
                "word_relation_to_question": [
                    1.8472222222222223,
                    0.6666666666666666,
                    0.4861111111111111
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    23.0,
                    6.0,
                    3.0
                ],
                "result_count": [
                    34.0,
                    302000.0,
                    2.0
                ]
            },
            "z-best_answer_by_ml": [
                "rattles its tail"
            ],
            "ml_answers": {
                "sends an angry email": 0.036194848504380775,
                "eats its feelings": 0.14338865108911322,
                "rattles its tail": 0.19689757435955854
            }
        },
        "lines": [
            [
                0,
                0.00011256936259253863,
                0.0003210891343436938,
                0.002738579349044856,
                0.5416666666666667,
                0.71875,
                0,
                0,
                0.6157407407407408,
                1
            ],
            [
                1,
                0.9998808089101962,
                0.9996575049233667,
                0.9972562535201079,
                0.25,
                0.1875,
                0,
                0,
                0.2222222222222222,
                1
            ],
            [
                0,
                6.621727211325802e-06,
                2.1405942289579588e-05,
                5.167130847254445e-06,
                0.20833333333333334,
                0.09375,
                0,
                0,
                0.16203703703703703,
                1
            ]
        ]
    },
    "Which of these was a name the ancient Greeks gave the planet Venus?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.00039939232644163836,
                    0.0017462640285930335,
                    0.0006105256922396356,
                    0.14285714285714288,
                    0.1444805194805195,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    -1
                ],
                [
                    0.9256943640890309,
                    0.7099297686336165,
                    0.8990683511697921,
                    0.634920634920635,
                    0.5698051948051948,
                    1.0,
                    1.0,
                    0.030303030303030304,
                    -1
                ],
                [
                    0.07390624358452746,
                    0.2883239673377906,
                    0.1003211231379683,
                    0.22222222222222224,
                    0.2857142857142857,
                    0.0,
                    0.0,
                    0.6363636363636364,
                    -1
                ]
            ],
            "fraction_answers": {
                "antimony": 0.20085643479505383,
                "flourine": 0.07792839721478374,
                "phosphorus": 0.7212151679901625
            },
            "question": "which of these was a name the ancient greeks gave the planet venus?",
            "rate_limited": false,
            "answers": [
                "flourine",
                "phosphorus",
                "antimony"
            ],
            "integer_answers": {
                "antimony": 1,
                "flourine": 0,
                "phosphorus": 7
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "antimony": 0.15831154825415947,
                "flourine": 0.1065609798116662,
                "phosphorus": 0.9137317575774503
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    642.0,
                    261000.0,
                    106000.0
                ],
                "wikipedia_search": [
                    0.42857142857142855,
                    1.9047619047619047,
                    0.6666666666666666
                ],
                "result_count_noun_chunks": [
                    639.0,
                    941000.0,
                    105000.0
                ],
                "word_relation_to_question": [
                    1.0,
                    0.09090909090909091,
                    1.9090909090909092
                ],
                "word_count_noun_chunks": [
                    0.0,
                    4.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    5.0,
                    0.0
                ],
                "word_count_appended": [
                    89.0,
                    351.0,
                    176.0
                ],
                "result_count": [
                    535.0,
                    1240000.0,
                    99000.0
                ]
            },
            "z-best_answer_by_ml": [
                "phosphorus"
            ]
        },
        "lines": [
            [
                0,
                0.00039939232644163836,
                0.0017462640285930335,
                0.0006105256922396356,
                0.14285714285714288,
                0.1444805194805195,
                0.0,
                0.0,
                0.3333333333333333,
                -1
            ],
            [
                1,
                0.9256943640890309,
                0.7099297686336165,
                0.8990683511697921,
                0.634920634920635,
                0.5698051948051948,
                1.0,
                1.0,
                0.030303030303030304,
                -1
            ],
            [
                0,
                0.07390624358452746,
                0.2883239673377906,
                0.1003211231379683,
                0.22222222222222224,
                0.2857142857142857,
                0.0,
                0.0,
                0.6363636363636364,
                -1
            ]
        ]
    },
    "Which of these film composers most recently won an Oscar?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.31591378801299086,
                    0.2916666666666667,
                    0.3132183908045977,
                    0.6345354645354646,
                    0.3634085213032581,
                    0.3333333333333333,
                    0.0,
                    0.2280720245937637,
                    -1
                ],
                [
                    0.47239444936521996,
                    0.5208333333333334,
                    0.47413793103448276,
                    0.23106617520410624,
                    0.37343358395989973,
                    0.4444444444444444,
                    1.0,
                    0.2910496267018006,
                    -1
                ],
                [
                    0.21169176262178918,
                    0.1875,
                    0.21264367816091953,
                    0.13439836026042923,
                    0.2631578947368421,
                    0.2222222222222222,
                    0.0,
                    0.4808783487044357,
                    -1
                ]
            ],
            "fraction_answers": {
                "ennio morricone": 0.31001852365625937,
                "danny elfman": 0.21406153333832975,
                "hans zimmer": 0.4759199430054109
            },
            "question": "which of these film composers most recently won an oscar?",
            "rate_limited": false,
            "answers": [
                "ennio morricone",
                "hans zimmer",
                "danny elfman"
            ],
            "ml_answers": {
                "ennio morricone": 0.09797814387893183,
                "danny elfman": 0.09289725147425429,
                "hans zimmer": 0.6212966781427292
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "ennio morricone": 1,
                "danny elfman": 1,
                "hans zimmer": 6
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    182000.0,
                    325000.0,
                    117000.0
                ],
                "wikipedia_search": [
                    3.172677322677323,
                    1.1553308760205312,
                    0.6719918013021462
                ],
                "result_count_noun_chunks": [
                    109000.0,
                    165000.0,
                    74000.0
                ],
                "word_relation_to_question": [
                    1.1403601229688185,
                    1.455248133509003,
                    2.4043917435221784
                ],
                "word_count_noun_chunks": [
                    3.0,
                    4.0,
                    2.0
                ],
                "word_count_raw": [
                    0.0,
                    4.0,
                    0.0
                ],
                "word_count_appended": [
                    145.0,
                    149.0,
                    105.0
                ],
                "result_count": [
                    107000.0,
                    160000.0,
                    71700.0
                ]
            },
            "z-best_answer_by_ml": [
                "hans zimmer"
            ]
        },
        "lines": [
            [
                1,
                0.31591378801299086,
                0.2916666666666667,
                0.3132183908045977,
                0.6345354645354646,
                0.3634085213032581,
                0.3333333333333333,
                0.0,
                0.2280720245937637,
                -1
            ],
            [
                0,
                0.47239444936521996,
                0.5208333333333334,
                0.47413793103448276,
                0.23106617520410624,
                0.37343358395989973,
                0.4444444444444444,
                1.0,
                0.2910496267018006,
                -1
            ],
            [
                0,
                0.21169176262178918,
                0.1875,
                0.21264367816091953,
                0.13439836026042923,
                0.2631578947368421,
                0.2222222222222222,
                0.0,
                0.4808783487044357,
                -1
            ]
        ]
    },
    "Which of these figure skating jumps was invented the most recently?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    4.493278435174649e-05,
                    0.4020296643247463,
                    0.17839660611334712,
                    0.013447286636385369,
                    0.2904761904761905,
                    0.22535211267605634,
                    0.2857142857142857,
                    0.22599456387447492,
                    -1
                ],
                [
                    0.999912665855767,
                    0.05542544886807182,
                    0.04492548678342217,
                    0.9759144154912742,
                    0.36507936507936506,
                    0.28169014084507044,
                    0.09523809523809523,
                    0.6332344946874228,
                    -1
                ],
                [
                    4.2401359881225565e-05,
                    0.5425448868071819,
                    0.7766779071032307,
                    0.010638297872340425,
                    0.34444444444444444,
                    0.49295774647887325,
                    0.6190476190476191,
                    0.1407709414381023,
                    -1
                ]
            ],
            "fraction_answers": {
                "salchow": 0.4314275141060611,
                "lutz": 0.2026819553249797,
                "axel": 0.3658905305689591
            },
            "question": "which of these figure skating jumps was invented the most recently?",
            "rate_limited": false,
            "answers": [
                "lutz",
                "salchow",
                "axel"
            ],
            "ml_answers": {
                "salchow": 0.3763568189887705,
                "lutz": 0.09010642568966808,
                "axel": 0.5012545782607161
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "salchow": 4,
                "lutz": 0,
                "axel": 4
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    103000.0,
                    14200.0,
                    139000.0
                ],
                "wikipedia_search": [
                    0.053789146545541476,
                    3.903657661965097,
                    0.0425531914893617
                ],
                "result_count_noun_chunks": [
                    164000.0,
                    41300.0,
                    714000.0
                ],
                "word_relation_to_question": [
                    1.1299728193723746,
                    3.166172473437114,
                    0.7038547071905115
                ],
                "word_count_noun_chunks": [
                    16.0,
                    20.0,
                    35.0
                ],
                "word_count_raw": [
                    12.0,
                    4.0,
                    26.0
                ],
                "result_count": [
                    71.0,
                    1580000.0,
                    67.0
                ],
                "word_count_appended": [
                    183.0,
                    230.0,
                    217.0
                ]
            },
            "z-best_answer_by_ml": [
                "axel"
            ]
        },
        "lines": [
            [
                1,
                4.493278435174649e-05,
                0.4020296643247463,
                0.17839660611334712,
                0.013447286636385369,
                0.2904761904761905,
                0.22535211267605634,
                0.2857142857142857,
                0.22599456387447492,
                -1
            ],
            [
                0,
                0.999912665855767,
                0.05542544886807182,
                0.04492548678342217,
                0.9759144154912742,
                0.36507936507936506,
                0.28169014084507044,
                0.09523809523809523,
                0.6332344946874228,
                -1
            ],
            [
                0,
                4.2401359881225565e-05,
                0.5425448868071819,
                0.7766779071032307,
                0.010638297872340425,
                0.34444444444444444,
                0.49295774647887325,
                0.6190476190476191,
                0.1407709414381023,
                -1
            ]
        ]
    },
    "Romaine, Iceberg and Butterhead are all varieties of what?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.9991748749419834,
                    0.996717397791704,
                    0.9981567140820088,
                    0.9895833333333334,
                    0.9740698985343855,
                    1.0,
                    1.0,
                    1.0,
                    1
                ],
                [
                    0.0006446289515754731,
                    0.002387347060578932,
                    0.0013216012242200815,
                    0.0,
                    0.012401352874859075,
                    0.0,
                    0.0,
                    0.0,
                    1
                ],
                [
                    0.00018049610644113248,
                    0.0008952551477170994,
                    0.0005216846937710848,
                    0.010416666666666666,
                    0.013528748590755355,
                    0.0,
                    0.0,
                    0.0,
                    1
                ]
            ],
            "fraction_answers": {
                "lettuce": 0.9947127773354268,
                "race cars": 0.0031928564006689177,
                "disney dwarfs": 0.0020943662639041957
            },
            "question": "romaine, iceberg and butterhead are all varieties of what?",
            "rate_limited": false,
            "answers": [
                "lettuce",
                "disney dwarfs",
                "race cars"
            ],
            "integer_answers": {
                "lettuce": 8,
                "race cars": 0,
                "disney dwarfs": 0
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "lettuce": 0.913882441902346,
                "race cars": 0.10688941363616723,
                "disney dwarfs": 0.11426711914367982
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    16700.0,
                    40.0,
                    15.0
                ],
                "wikipedia_search": [
                    3.9583333333333335,
                    0.0,
                    0.041666666666666664
                ],
                "result_count_noun_chunks": [
                    28700.0,
                    38.0,
                    15.0
                ],
                "word_relation_to_question": [
                    4.0,
                    0.0,
                    0.0
                ],
                "word_count_noun_chunks": [
                    675.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    656.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    864.0,
                    11.0,
                    12.0
                ],
                "result_count": [
                    77500.0,
                    50.0,
                    14.0
                ]
            },
            "z-best_answer_by_ml": [
                "lettuce"
            ]
        },
        "lines": [
            [
                1,
                0.9991748749419834,
                0.996717397791704,
                0.9981567140820088,
                0.9895833333333334,
                0.9740698985343855,
                1.0,
                1.0,
                1.0,
                1
            ],
            [
                0,
                0.0006446289515754731,
                0.002387347060578932,
                0.0013216012242200815,
                0.0,
                0.012401352874859075,
                0.0,
                0.0,
                0.0,
                1
            ],
            [
                0,
                0.00018049610644113248,
                0.0008952551477170994,
                0.0005216846937710848,
                0.010416666666666666,
                0.013528748590755355,
                0.0,
                0.0,
                0.0,
                1
            ]
        ]
    },
    "Rejected in the late 1700s, what was the name of the proposed 14th U.S. colony?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.7267950963222417,
                    0.6513859275053305,
                    0.6509253350350989,
                    0.008620689655172414,
                    0.5787671232876712,
                    0.0,
                    0.0,
                    0.2851183252427184,
                    1
                ],
                [
                    0.25977816695855227,
                    0.22068230277185502,
                    0.2431397574984046,
                    0.24233716475095785,
                    0.3732876712328767,
                    0.9333333333333333,
                    1.0,
                    0.6542020631067961,
                    1
                ],
                [
                    0.013426736719206071,
                    0.1279317697228145,
                    0.10593490746649649,
                    0.7490421455938697,
                    0.04794520547945205,
                    0.06666666666666667,
                    0.0,
                    0.060679611650485424,
                    1
                ]
            ],
            "fraction_answers": {
                "roanoke": 0.36270156213102916,
                "new albion": 0.14645338041237388,
                "vandalia": 0.490845057456597
            },
            "question": "rejected in the late 1700s, what was the name of the proposed 14th u.s. colony?",
            "rate_limited": false,
            "answers": [
                "roanoke",
                "vandalia",
                "new albion"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "roanoke": 0.2249168256240488,
                "new albion": 0.18143847456983075,
                "vandalia": 0.8330408235856895
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    6110.0,
                    2070.0,
                    1200.0
                ],
                "wikipedia_search": [
                    0.017241379310344827,
                    0.4846743295019157,
                    1.4980842911877394
                ],
                "result_count_noun_chunks": [
                    10200.0,
                    3810.0,
                    1660.0
                ],
                "word_relation_to_question": [
                    1.1404733009708738,
                    2.6168082524271847,
                    0.24271844660194175
                ],
                "word_count_noun_chunks": [
                    0.0,
                    14.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_appended": [
                    169.0,
                    109.0,
                    14.0
                ],
                "result_count": [
                    4980.0,
                    1780.0,
                    92.0
                ]
            },
            "z-best_answer_by_ml": [
                "vandalia"
            ],
            "integer_answers": {
                "roanoke": 4,
                "new albion": 1,
                "vandalia": 3
            }
        },
        "lines": [
            [
                0,
                0.7267950963222417,
                0.6513859275053305,
                0.6509253350350989,
                0.008620689655172414,
                0.5787671232876712,
                0.0,
                0.0,
                0.2851183252427184,
                1
            ],
            [
                1,
                0.25977816695855227,
                0.22068230277185502,
                0.2431397574984046,
                0.24233716475095785,
                0.3732876712328767,
                0.9333333333333333,
                1.0,
                0.6542020631067961,
                1
            ],
            [
                0,
                0.013426736719206071,
                0.1279317697228145,
                0.10593490746649649,
                0.7490421455938697,
                0.04794520547945205,
                0.06666666666666667,
                0.0,
                0.060679611650485424,
                1
            ]
        ]
    },
    "Which of these are you most likely to find in a toolbox?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.9460616438356164,
                    0.7303754266211604,
                    0.9714178965066318,
                    0.05555555555555555,
                    0.7932692307692307,
                    1.0,
                    1.0,
                    0.225,
                    -1
                ],
                [
                    0.030393835616438356,
                    0.11604095563139932,
                    0.016813002054922473,
                    0.8888888888888888,
                    0.13221153846153846,
                    0.0,
                    0.0,
                    0.2,
                    -1
                ],
                [
                    0.023544520547945206,
                    0.15358361774744028,
                    0.011769101438445732,
                    0.05555555555555555,
                    0.07451923076923077,
                    0.0,
                    0.0,
                    0.575,
                    -1
                ]
            ],
            "fraction_answers": {
                "hammerhead shark": 0.1117465032573272,
                "hammer": 0.7152099691610244,
                "mc hammer": 0.17304352758164845
            },
            "question": "which of these are you most likely to find in a toolbox?",
            "rate_limited": false,
            "answers": [
                "hammer",
                "mc hammer",
                "hammerhead shark"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "hammerhead shark": 0.20932155230609972,
                "hammer": 0.9098894055394503,
                "mc hammer": 0.21624753382234477
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    642000.0,
                    102000.0,
                    135000.0
                ],
                "wikipedia_search": [
                    0.1111111111111111,
                    1.7777777777777777,
                    0.1111111111111111
                ],
                "result_count_noun_chunks": [
                    10400000.0,
                    180000.0,
                    126000.0
                ],
                "word_relation_to_question": [
                    0.45,
                    0.4,
                    1.15
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    4420000.0,
                    142000.0,
                    110000.0
                ],
                "word_count_appended": [
                    330.0,
                    55.0,
                    31.0
                ]
            },
            "z-best_answer_by_ml": [
                "hammer"
            ],
            "integer_answers": {
                "hammerhead shark": 1,
                "hammer": 6,
                "mc hammer": 1
            }
        },
        "lines": [
            [
                1,
                0.9460616438356164,
                0.7303754266211604,
                0.9714178965066318,
                0.05555555555555555,
                0.7932692307692307,
                1.0,
                1.0,
                0.225,
                -1
            ],
            [
                0,
                0.030393835616438356,
                0.11604095563139932,
                0.016813002054922473,
                0.8888888888888888,
                0.13221153846153846,
                0.0,
                0.0,
                0.2,
                -1
            ],
            [
                0,
                0.023544520547945206,
                0.15358361774744028,
                0.011769101438445732,
                0.05555555555555555,
                0.07451923076923077,
                0.0,
                0.0,
                0.575,
                -1
            ]
        ]
    },
    "Which of these is usually found on the ocean floor?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.9120493616976278,
                    0.9639544413116852,
                    0.8733401430030644,
                    0.28108974358974365,
                    0.3659147869674185,
                    0,
                    0,
                    0.05729166666666667,
                    -1
                ],
                [
                    5.022880542682588e-05,
                    2.2347732722450834e-05,
                    0.09193054136874361,
                    0.03541666666666667,
                    0.15037593984962405,
                    0,
                    0,
                    0.05952380952380953,
                    -1
                ],
                [
                    0.08790040949694529,
                    0.03602321095559239,
                    0.03472931562819203,
                    0.6834935897435898,
                    0.48370927318295737,
                    0,
                    0,
                    0.8831845238095238,
                    -1
                ]
            ],
            "fraction_answers": {
                "sweet potato": 0.5756066905393677,
                "cherry tomato": 0.05621992232449886,
                "sea cucumber": 0.36817338713613346
            },
            "question": "which of these is usually found on the ocean floor?",
            "rate_limited": false,
            "answers": [
                "sweet potato",
                "cherry tomato",
                "sea cucumber"
            ],
            "ml_answers": {
                "sweet potato": 0.13683584030985063,
                "cherry tomato": 0.12327497553604201,
                "sea cucumber": 0.1748120226073087
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "sweet potato": 3,
                "cherry tomato": 0,
                "sea cucumber": 3
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    2890000.0,
                    67.0,
                    108000.0
                ],
                "wikipedia_search": [
                    1.1243589743589744,
                    0.14166666666666666,
                    2.7339743589743586
                ],
                "result_count_noun_chunks": [
                    3420000.0,
                    360000.0,
                    136000.0
                ],
                "word_relation_to_question": [
                    0.22916666666666666,
                    0.23809523809523808,
                    3.532738095238095
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    146.0,
                    60.0,
                    193.0
                ],
                "result_count": [
                    1380000.0,
                    76.0,
                    133000.0
                ]
            },
            "z-best_answer_by_ml": [
                "sea cucumber"
            ]
        },
        "lines": [
            [
                0,
                0.9120493616976278,
                0.9639544413116852,
                0.8733401430030644,
                0.28108974358974365,
                0.3659147869674185,
                0,
                0,
                0.05729166666666667,
                -1
            ],
            [
                0,
                5.022880542682588e-05,
                2.2347732722450834e-05,
                0.09193054136874361,
                0.03541666666666667,
                0.15037593984962405,
                0,
                0,
                0.05952380952380953,
                -1
            ],
            [
                1,
                0.08790040949694529,
                0.03602321095559239,
                0.03472931562819203,
                0.6834935897435898,
                0.48370927318295737,
                0,
                0,
                0.8831845238095238,
                -1
            ]
        ]
    },
    "The '90s band The Lightning Seeds took their name from which song?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.13846587704455723,
                    0.11369509043927649,
                    0.21517895644674429,
                    0.241156116068292,
                    0.41935483870967744,
                    1.0,
                    1.0,
                    0.17001338688085676,
                    -1
                ],
                [
                    0.3976311336717428,
                    0.42506459948320413,
                    0.599396291504959,
                    0.4220824843673154,
                    0.3387096774193548,
                    0.0,
                    0.0,
                    0.35609103078982596,
                    -1
                ],
                [
                    0.46390298928369994,
                    0.46124031007751937,
                    0.18542475204829667,
                    0.33676139956439266,
                    0.24193548387096775,
                    0.0,
                    0.0,
                    0.4738955823293172,
                    -1
                ]
            ],
            "fraction_answers": {
                "when doves cry": 0.27039506464677426,
                "raspberry beret": 0.41223303319867555,
                "purple rain": 0.3173719021545503
            },
            "question": "the '90s band the lightning seeds took their name from which song?",
            "rate_limited": false,
            "answers": [
                "raspberry beret",
                "purple rain",
                "when doves cry"
            ],
            "integer_answers": {
                "when doves cry": 3,
                "raspberry beret": 3,
                "purple rain": 2
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "when doves cry": 0.43848234230307626,
                "raspberry beret": 0.8614042263175246,
                "purple rain": 0.26155327573975523
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    8800.0,
                    32900.0,
                    35700.0
                ],
                "wikipedia_search": [
                    0.964624464273168,
                    1.6883299374692615,
                    1.3470455982575706
                ],
                "result_count_noun_chunks": [
                    9980.0,
                    27800.0,
                    8600.0
                ],
                "word_relation_to_question": [
                    0.5100401606425703,
                    1.0682730923694779,
                    1.4216867469879517
                ],
                "word_count_noun_chunks": [
                    3.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    9820.0,
                    28200.0,
                    32900.0
                ],
                "word_count_appended": [
                    26.0,
                    21.0,
                    15.0
                ]
            },
            "z-best_answer_by_ml": [
                "raspberry beret"
            ]
        },
        "lines": [
            [
                1,
                0.13846587704455723,
                0.11369509043927649,
                0.21517895644674429,
                0.241156116068292,
                0.41935483870967744,
                1.0,
                1.0,
                0.17001338688085676,
                -1
            ],
            [
                0,
                0.3976311336717428,
                0.42506459948320413,
                0.599396291504959,
                0.4220824843673154,
                0.3387096774193548,
                0.0,
                0.0,
                0.35609103078982596,
                -1
            ],
            [
                0,
                0.46390298928369994,
                0.46124031007751937,
                0.18542475204829667,
                0.33676139956439266,
                0.24193548387096775,
                0.0,
                0.0,
                0.4738955823293172,
                -1
            ]
        ]
    },
    "Which Las Vegas hotel features a replica of the Rialto Bridge?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.3305122494432071,
                    0.1875302078298695,
                    0.2300556586270872,
                    0.1981892595339231,
                    0.5229357798165137,
                    0.028169014084507043,
                    0.012345679012345678,
                    0.27706851351340184,
                    -1
                ],
                [
                    0.6057906458797327,
                    0.21314644755920734,
                    0.23376623376623376,
                    0.0324301175737756,
                    0.37920489296636084,
                    0.971830985915493,
                    0.9506172839506173,
                    0.2525322496913865,
                    -1
                ],
                [
                    0.06369710467706013,
                    0.5993233446109232,
                    0.536178107606679,
                    0.7693806228923012,
                    0.09785932721712538,
                    0.0,
                    0.037037037037037035,
                    0.4703992367952117,
                    -1
                ]
            ],
            "fraction_answers": {
                "caesars palace": 0.3217343476045422,
                "luxor": 0.22335079523260692,
                "the venetian": 0.4549148571628509
            },
            "question": "which las vegas hotel features a replica of the rialto bridge?",
            "rate_limited": false,
            "answers": [
                "luxor",
                "the venetian",
                "caesars palace"
            ],
            "integer_answers": {
                "caesars palace": 4,
                "luxor": 1,
                "the venetian": 3
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "caesars palace": 0.2656577290724015,
                "luxor": 0.37488849847423017,
                "the venetian": 0.8609488739023382
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    7760.0,
                    8820.0,
                    24800.0
                ],
                "wikipedia_search": [
                    1.1891355572035387,
                    0.19458070544265357,
                    4.6162837373538075
                ],
                "result_count_noun_chunks": [
                    12400.0,
                    12600.0,
                    28900.0
                ],
                "word_relation_to_question": [
                    1.1082740540536073,
                    1.010128998765546,
                    1.8815969471808467
                ],
                "word_count_noun_chunks": [
                    2.0,
                    69.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    77.0,
                    3.0
                ],
                "word_count_appended": [
                    171.0,
                    124.0,
                    32.0
                ],
                "result_count": [
                    74200.0,
                    136000.0,
                    14300.0
                ]
            },
            "z-best_answer_by_ml": [
                "the venetian"
            ]
        },
        "lines": [
            [
                0,
                0.3305122494432071,
                0.1875302078298695,
                0.2300556586270872,
                0.1981892595339231,
                0.5229357798165137,
                0.028169014084507043,
                0.012345679012345678,
                0.27706851351340184,
                -1
            ],
            [
                1,
                0.6057906458797327,
                0.21314644755920734,
                0.23376623376623376,
                0.0324301175737756,
                0.37920489296636084,
                0.971830985915493,
                0.9506172839506173,
                0.2525322496913865,
                -1
            ],
            [
                0,
                0.06369710467706013,
                0.5993233446109232,
                0.536178107606679,
                0.7693806228923012,
                0.09785932721712538,
                0.0,
                0.037037037037037035,
                0.4703992367952117,
                -1
            ]
        ]
    },
    "What TV series derived from a nearly 20-year-old Michael Crichton screenplay?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.0894533406957482,
                    0.07442196531791907,
                    0.0013430340976066939,
                    0.022727272727272728,
                    0.013141426783479349,
                    0.0,
                    0.0,
                    0.271323829555192,
                    1
                ],
                [
                    0.19271120927664273,
                    0.06394508670520231,
                    0.0034590374600229957,
                    0.30612627286125815,
                    0.07321652065081352,
                    0.0,
                    0.0,
                    0.19393191555659423,
                    1
                ],
                [
                    0.7178354500276091,
                    0.8616329479768786,
                    0.9951979284423703,
                    0.6711464544114691,
                    0.9136420525657072,
                    1.0,
                    1.0,
                    0.5347442548882138,
                    1
                ]
            ],
            "fraction_answers": {
                "numb3rs": 0.10417375531381674,
                "the expanse": 0.059051358647152254,
                "er": 0.8367748860390309
            },
            "question": "what tv series derived from a nearly 20-year-old michael crichton screenplay?",
            "rate_limited": false,
            "answers": [
                "the expanse",
                "numb3rs",
                "er"
            ],
            "integer_answers": {
                "numb3rs": 0,
                "the expanse": 0,
                "er": 8
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "numb3rs": 0.04711192426200772,
                "the expanse": 0.05734504968723344,
                "er": 0.8830690529682085
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    4120.0,
                    3540.0,
                    47700.0
                ],
                "wikipedia_search": [
                    0.09090909090909091,
                    1.2245050914450326,
                    2.6845858176458766
                ],
                "result_count_noun_chunks": [
                    13900.0,
                    35800.0,
                    10300000.0
                ],
                "word_relation_to_question": [
                    1.3566191477759602,
                    0.9696595777829711,
                    2.6737212744410686
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    1062.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1194.0
                ],
                "result_count": [
                    16200.0,
                    34900.0,
                    130000.0
                ],
                "word_count_appended": [
                    21.0,
                    117.0,
                    1460.0
                ]
            },
            "z-best_answer_by_ml": [
                "er"
            ]
        },
        "lines": [
            [
                0,
                0.0894533406957482,
                0.07442196531791907,
                0.0013430340976066939,
                0.022727272727272728,
                0.013141426783479349,
                0.0,
                0.0,
                0.271323829555192,
                1
            ],
            [
                0,
                0.19271120927664273,
                0.06394508670520231,
                0.0034590374600229957,
                0.30612627286125815,
                0.07321652065081352,
                0.0,
                0.0,
                0.19393191555659423,
                1
            ],
            [
                1,
                0.7178354500276091,
                0.8616329479768786,
                0.9951979284423703,
                0.6711464544114691,
                0.9136420525657072,
                1.0,
                1.0,
                0.5347442548882138,
                1
            ]
        ]
    },
    "Which of these video games was NOT produced by FromSoftware?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.16652943598188558,
                    0.04642265520233496,
                    0.14501510574018128,
                    0.3576882323610072,
                    0.10243055555555558,
                    0.0,
                    0.0,
                    0.29839514511234994,
                    -1
                ],
                [
                    0.4215726636475916,
                    0.49341326812337305,
                    0.4416918429003021,
                    0.4148591373722264,
                    0.4114583333333333,
                    0.5,
                    0.5,
                    0.3488103598714593,
                    -1
                ],
                [
                    0.4118979003705229,
                    0.46016407667429204,
                    0.4132930513595166,
                    0.22745263026676638,
                    0.4861111111111111,
                    0.5,
                    0.5,
                    0.3527944950161908,
                    -1
                ]
            ],
            "fraction_answers": {
                "dark souls": 0.7208797175116715,
                "beyond: two souls": 0.11704859868792857,
                "demon's souls": 0.16207168380040007
            },
            "question": "which of these video games was not produced by fromsoftware?",
            "rate_limited": false,
            "answers": [
                "dark souls",
                "beyond: two souls",
                "demon's souls"
            ],
            "ml_answers": {
                "dark souls": 0.07213645102366295,
                "beyond: two souls": 0.5149776020043608,
                "demon's souls": 0.497764194048575
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "dark souls": 7,
                "beyond: two souls": 0,
                "demon's souls": 1
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    1150000.0,
                    16700.0,
                    101000.0
                ],
                "wikipedia_search": [
                    0.8538706058339567,
                    0.5108451757666417,
                    1.6352842183994016
                ],
                "result_count_noun_chunks": [
                    235000.0,
                    38600.0,
                    57400.0
                ],
                "word_relation_to_question": [
                    1.2096291293259005,
                    0.9071378407712443,
                    0.8832330299028552
                ],
                "word_count_noun_chunks": [
                    75.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    58.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    229.0,
                    51.0,
                    8.0
                ],
                "result_count": [
                    162000.0,
                    38100.0,
                    42800.0
                ]
            },
            "z-best_answer_by_ml": [
                "beyond: two souls"
            ]
        },
        "lines": [
            [
                0,
                0.16652943598188558,
                0.04642265520233496,
                0.14501510574018128,
                0.3576882323610072,
                0.10243055555555558,
                0.0,
                0.0,
                0.29839514511234994,
                -1
            ],
            [
                1,
                0.4215726636475916,
                0.49341326812337305,
                0.4416918429003021,
                0.4148591373722264,
                0.4114583333333333,
                0.5,
                0.5,
                0.3488103598714593,
                -1
            ],
            [
                0,
                0.4118979003705229,
                0.46016407667429204,
                0.4132930513595166,
                0.22745263026676638,
                0.4861111111111111,
                0.5,
                0.5,
                0.3527944950161908,
                -1
            ]
        ]
    },
    "Which actor currently stars in a show that is both one word long and pluralized?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.20757707449256096,
                    0.5625458597168247,
                    0.0007607636164800418,
                    0.30431887366818877,
                    0.3055555555555556,
                    0,
                    0,
                    0.5187310979618671,
                    5
                ],
                [
                    0.0009714198067385858,
                    0.002636084463407343,
                    0.0007369897534650405,
                    0.33656773211567736,
                    0.3333333333333333,
                    0,
                    0,
                    0.203767258382643,
                    5
                ],
                [
                    0.7914515057007004,
                    0.4348180558197679,
                    0.9985022466300549,
                    0.359113394216134,
                    0.3611111111111111,
                    0,
                    0,
                    0.2775016436554898,
                    5
                ]
            ],
            "fraction_answers": {
                "william h. macy": 0.31658153750191287,
                "taraji p. henson": 0.1463354696425441,
                "paul giamatti": 0.5370829928555431
            },
            "question": "which actor currently stars in a show that is both one word long and pluralized?",
            "rate_limited": false,
            "answers": [
                "william h. macy",
                "taraji p. henson",
                "paul giamatti"
            ],
            "ml_answers": {
                "william h. macy": 0.34563749930433507,
                "taraji p. henson": 0.25308316960079735,
                "paul giamatti": 0.35187854956222786
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "william h. macy": 2,
                "taraji p. henson": 0,
                "paul giamatti": 4
            },
            "question_type": 5,
            "data": {
                "result_count_important_words": [
                    20700.0,
                    97.0,
                    16000.0
                ],
                "wikipedia_search": [
                    1.217275494672755,
                    1.3462709284627095,
                    1.436453576864536
                ],
                "result_count_noun_chunks": [
                    96.0,
                    93.0,
                    126000.0
                ],
                "word_relation_to_question": [
                    2.593655489809336,
                    1.018836291913215,
                    1.387508218277449
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    22.0,
                    24.0,
                    26.0
                ],
                "result_count": [
                    20300.0,
                    95.0,
                    77400.0
                ]
            },
            "z-best_answer_by_ml": [
                "paul giamatti"
            ]
        },
        "lines": [
            [
                0,
                0.20757707449256096,
                0.5625458597168247,
                0.0007607636164800418,
                0.30431887366818877,
                0.3055555555555556,
                0,
                0,
                0.5187310979618671,
                5
            ],
            [
                0,
                0.0009714198067385858,
                0.002636084463407343,
                0.0007369897534650405,
                0.33656773211567736,
                0.3333333333333333,
                0,
                0,
                0.203767258382643,
                5
            ],
            [
                1,
                0.7914515057007004,
                0.4348180558197679,
                0.9985022466300549,
                0.359113394216134,
                0.3611111111111111,
                0,
                0,
                0.2775016436554898,
                5
            ]
        ]
    },
    "Which of these is NOT the title of a current TV show?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.4080473658772354,
                    0.43640897755610975,
                    0.28885767790262173,
                    0.36738147822985545,
                    0.1927966101694915,
                    0,
                    0,
                    0.2927848526272873,
                    5
                ],
                [
                    0.2051715804736588,
                    0.20554383272587762,
                    0.26217228464419473,
                    0.3006615407830119,
                    0.4957627118644068,
                    0,
                    0,
                    0.32068254310861694,
                    5
                ],
                [
                    0.38678105364910587,
                    0.35804718971801264,
                    0.44897003745318353,
                    0.33195698098713267,
                    0.3114406779661017,
                    0,
                    0,
                    0.38653260426409575,
                    5
                ]
            ],
            "fraction_answers": {
                "chicago med": 0.3379076792124663,
                "chicago police": 0.25875715198745597,
                "chicage fire": 0.4033351688000777
            },
            "question": "which of these is not the title of a current tv show?",
            "rate_limited": false,
            "answers": [
                "chicago med",
                "chicage fire",
                "chicago police"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "chicago med": 0.40875304058654305,
                "chicago police": 0.3321768530507761,
                "chicage fire": 0.2733635926809105
            },
            "question_type": 5,
            "data": {
                "result_count_important_words": [
                    66300.0,
                    307000.0,
                    148000.0
                ],
                "wikipedia_search": [
                    0.7957111306208675,
                    1.1960307553019287,
                    1.0082581140772038
                ],
                "result_count_noun_chunks": [
                    451000.0,
                    508000.0,
                    109000.0
                ],
                "word_relation_to_question": [
                    1.243290884236276,
                    1.0759047413482983,
                    0.6808043744154255
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    145.0,
                    2.0,
                    89.0
                ],
                "result_count": [
                    76100.0,
                    244000.0,
                    93700.0
                ]
            },
            "z-best_answer_by_ml": [
                "chicago med"
            ],
            "integer_answers": {
                "chicago med": 2,
                "chicago police": 0,
                "chicage fire": 4
            }
        },
        "lines": [
            [
                0,
                0.4080473658772354,
                0.43640897755610975,
                0.28885767790262173,
                0.36738147822985545,
                0.1927966101694915,
                0,
                0,
                0.2927848526272873,
                5
            ],
            [
                0,
                0.2051715804736588,
                0.20554383272587762,
                0.26217228464419473,
                0.3006615407830119,
                0.4957627118644068,
                0,
                0,
                0.32068254310861694,
                5
            ],
            [
                1,
                0.38678105364910587,
                0.35804718971801264,
                0.44897003745318353,
                0.33195698098713267,
                0.3114406779661017,
                0,
                0,
                0.38653260426409575,
                5
            ]
        ]
    },
    "Which of these things is NOT found inside an atom?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.19918603279462072,
                    0.022393439668378867,
                    0.2688564476885645,
                    0.32073643410852715,
                    0.3201948627103631,
                    0.21875,
                    0.1652542372881356,
                    0.3595848595848596,
                    -1
                ],
                [
                    0.4824820101450985,
                    0.49396233216184554,
                    0.4215328467153285,
                    0.29761904761904767,
                    0.370682019486271,
                    0.5,
                    0.5,
                    0.3015873015873016,
                    -1
                ],
                [
                    0.3183319570602807,
                    0.4836442281697756,
                    0.30961070559610704,
                    0.38164451827242524,
                    0.3091231178033658,
                    0.28125,
                    0.3347457627118644,
                    0.33882783882783885,
                    -1
                ]
            ],
            "fraction_answers": {
                "wonton": 0.1580336105712768,
                "proton": 0.5312609215391376,
                "neutron": 0.3107054678895856
            },
            "question": "which of these things is not found inside an atom?",
            "rate_limited": false,
            "answers": [
                "proton",
                "wonton",
                "neutron"
            ],
            "integer_answers": {
                "wonton": 2,
                "proton": 5,
                "neutron": 1
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "wonton": 0.4554419393895839,
                "proton": 0.11440680367605682,
                "neutron": 0.279678434253844
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    10600000.0,
                    134000.0,
                    363000.0
                ],
                "wikipedia_search": [
                    1.0755813953488373,
                    1.2142857142857142,
                    0.7101328903654485
                ],
                "result_count_noun_chunks": [
                    380000.0,
                    129000.0,
                    313000.0
                ],
                "word_relation_to_question": [
                    0.8424908424908425,
                    1.1904761904761905,
                    0.9670329670329669
                ],
                "word_count_noun_chunks": [
                    45.0,
                    0.0,
                    35.0
                ],
                "word_count_raw": [
                    79.0,
                    0.0,
                    39.0
                ],
                "word_count_appended": [
                    406.0,
                    292.0,
                    431.0
                ],
                "result_count": [
                    1530000.0,
                    89100.0,
                    924000.0
                ]
            },
            "z-best_answer_by_ml": [
                "wonton"
            ]
        },
        "lines": [
            [
                0,
                0.19918603279462072,
                0.022393439668378867,
                0.2688564476885645,
                0.32073643410852715,
                0.3201948627103631,
                0.21875,
                0.1652542372881356,
                0.3595848595848596,
                -1
            ],
            [
                1,
                0.4824820101450985,
                0.49396233216184554,
                0.4215328467153285,
                0.29761904761904767,
                0.370682019486271,
                0.5,
                0.5,
                0.3015873015873016,
                -1
            ],
            [
                0,
                0.3183319570602807,
                0.4836442281697756,
                0.30961070559610704,
                0.38164451827242524,
                0.3091231178033658,
                0.28125,
                0.3347457627118644,
                0.33882783882783885,
                -1
            ]
        ]
    },
    "What actor famously yelled \"Not the bees! Not the bees!\" in a 2006 film?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.11764705882352944,
                    0.3211336449285538,
                    0.46480026195153895,
                    0.3038935430478885,
                    0.25806451612903225,
                    0.0,
                    0.0,
                    0.18154561902300337,
                    1
                ],
                [
                    0.5,
                    0.4998228052931068,
                    0.09888670595939753,
                    0.3826627183370782,
                    0.45161290322580644,
                    0.5,
                    0.5,
                    0.4259110881364694,
                    1
                ],
                [
                    0.38235294117647056,
                    0.17904354977833947,
                    0.4363130320890635,
                    0.3134437386150333,
                    0.29032258064516125,
                    0.5,
                    0.5,
                    0.39254329284052725,
                    1
                ]
            ],
            "fraction_answers": {
                "macauley culkin": 0.1602759447620354,
                "nicolas cage": 0.5882288390241134,
                "oprah winfrey": 0.2514952162138511
            },
            "question": "what actor famously yelled \"not the bees! not the bees!\" in a 2006 film?",
            "rate_limited": false,
            "answers": [
                "nicolas cage",
                "macauley culkin",
                "oprah winfrey"
            ],
            "ml_answers": {
                "macauley culkin": 0.7373998509736898,
                "nicolas cage": 0.13567927380282355,
                "oprah winfrey": 0.4062803887607577
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "macauley culkin": 1,
                "nicolas cage": 6,
                "oprah winfrey": 1
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    53500.0,
                    53.0,
                    96000.0
                ],
                "wikipedia_search": [
                    1.176638741712669,
                    0.7040236899775306,
                    1.1193375683098006
                ],
                "result_count_noun_chunks": [
                    215000.0,
                    2450000.0,
                    389000.0
                ],
                "word_relation_to_question": [
                    3.1845438097699663,
                    0.7408891186353063,
                    1.0745670715947275
                ],
                "word_count_noun_chunks": [
                    11.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    3.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    15.0,
                    3.0,
                    13.0
                ],
                "result_count": [
                    13.0,
                    0,
                    4.0
                ]
            },
            "z-best_answer_by_ml": [
                "macauley culkin"
            ]
        },
        "lines": [
            [
                1,
                0.11764705882352944,
                0.3211336449285538,
                0.46480026195153895,
                0.3038935430478885,
                0.25806451612903225,
                0.0,
                0.0,
                0.18154561902300337,
                1
            ],
            [
                0,
                0.5,
                0.4998228052931068,
                0.09888670595939753,
                0.3826627183370782,
                0.45161290322580644,
                0.5,
                0.5,
                0.4259110881364694,
                1
            ],
            [
                0,
                0.38235294117647056,
                0.17904354977833947,
                0.4363130320890635,
                0.3134437386150333,
                0.29032258064516125,
                0.5,
                0.5,
                0.39254329284052725,
                1
            ]
        ]
    },
    "What form of transportation counts Jay-Z as a prominent investor?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.5209066591581022,
                    0.54743699950233,
                    0.2865421764327109,
                    0.16511018786127168,
                    0.481994459833795,
                    0.3333333333333333,
                    0.6666666666666666,
                    0.48397692098160117,
                    1
                ],
                [
                    0.010981275517387019,
                    0.013708546351174048,
                    0.5473277527366388,
                    0.4782153179190752,
                    0.18005540166204986,
                    0.3333333333333333,
                    0.16666666666666666,
                    0.21501027660154026,
                    1
                ],
                [
                    0.4681120653245108,
                    0.43885445414649593,
                    0.16613007083065034,
                    0.35667449421965314,
                    0.3379501385041551,
                    0.3333333333333333,
                    0.16666666666666666,
                    0.30101280241685857,
                    1
                ]
            ],
            "fraction_answers": {
                "boats": 0.32109175318029043,
                "aviation": 0.43574592547122637,
                "e-bikes": 0.24316232134848315
            },
            "question": "what form of transportation counts jay-z as a prominent investor?",
            "rate_limited": false,
            "answers": [
                "aviation",
                "e-bikes",
                "boats"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "boats": 0,
                "aviation": 6,
                "e-bikes": 2
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    2420000.0,
                    60600.0,
                    1940000.0
                ],
                "wikipedia_search": [
                    0.6604407514450867,
                    1.9128612716763007,
                    1.4266979768786126
                ],
                "result_count_noun_chunks": [
                    890000.0,
                    1700000.0,
                    516000.0
                ],
                "word_relation_to_question": [
                    2.903861525889607,
                    1.2900616596092416,
                    1.8060768145011514
                ],
                "word_count_noun_chunks": [
                    1.0,
                    1.0,
                    1.0
                ],
                "word_count_raw": [
                    4.0,
                    1.0,
                    1.0
                ],
                "result_count": [
                    1480000.0,
                    31200.0,
                    1330000.0
                ],
                "word_count_appended": [
                    174.0,
                    65.0,
                    122.0
                ]
            },
            "z-best_answer_by_ml": [
                "boats"
            ],
            "ml_answers": {
                "boats": 0.4092490992330646,
                "aviation": 0.24998270155744307,
                "e-bikes": 0.21146828478862315
            }
        },
        "lines": [
            [
                1,
                0.5209066591581022,
                0.54743699950233,
                0.2865421764327109,
                0.16511018786127168,
                0.481994459833795,
                0.3333333333333333,
                0.6666666666666666,
                0.48397692098160117,
                1
            ],
            [
                0,
                0.010981275517387019,
                0.013708546351174048,
                0.5473277527366388,
                0.4782153179190752,
                0.18005540166204986,
                0.3333333333333333,
                0.16666666666666666,
                0.21501027660154026,
                1
            ],
            [
                0,
                0.4681120653245108,
                0.43885445414649593,
                0.16613007083065034,
                0.35667449421965314,
                0.3379501385041551,
                0.3333333333333333,
                0.16666666666666666,
                0.30101280241685857,
                1
            ]
        ]
    },
    "Mardi Gras is celebrated right before what other observance?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.6182065217391305,
                    0.5906853464596744,
                    0.5747126436781609,
                    0.4016225749559083,
                    0.45754716981132076,
                    1.0,
                    1.0,
                    0.5109411764705882,
                    1
                ],
                [
                    0.15421195652173914,
                    0.16319575918212798,
                    0.14310344827586208,
                    0.28557319223985894,
                    0.2971698113207547,
                    0.0,
                    0.0,
                    0.30427450980392157,
                    1
                ],
                [
                    0.22758152173913043,
                    0.24611889435819764,
                    0.28218390804597704,
                    0.3128042328042328,
                    0.24528301886792453,
                    0.0,
                    0.0,
                    0.18478431372549017,
                    1
                ]
            ],
            "fraction_answers": {
                "kwanzaa": 0.16844108466803306,
                "ramadan": 0.1873444861926191,
                "lent": 0.6442144291393479
            },
            "question": "mardi gras is celebrated right before what other observance?",
            "rate_limited": false,
            "answers": [
                "lent",
                "kwanzaa",
                "ramadan"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "kwanzaa": 0,
                "ramadan": 0,
                "lent": 8
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    156000.0,
                    43100.0,
                    65000.0
                ],
                "wikipedia_search": [
                    1.2048677248677249,
                    0.8567195767195768,
                    0.9384126984126984
                ],
                "result_count_noun_chunks": [
                    200000.0,
                    49800.0,
                    98200.0
                ],
                "word_relation_to_question": [
                    2.554705882352941,
                    1.521372549019608,
                    0.9239215686274509
                ],
                "word_count_noun_chunks": [
                    72.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    105.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    182000.0,
                    45400.0,
                    67000.0
                ],
                "word_count_appended": [
                    388.0,
                    252.0,
                    208.0
                ]
            },
            "z-best_answer_by_ml": [
                "lent"
            ],
            "ml_answers": {
                "kwanzaa": 0.11813861766361383,
                "ramadan": 0.17679635866033183,
                "lent": 0.956047286314242
            }
        },
        "lines": [
            [
                1,
                0.6182065217391305,
                0.5906853464596744,
                0.5747126436781609,
                0.4016225749559083,
                0.45754716981132076,
                1.0,
                1.0,
                0.5109411764705882,
                1
            ],
            [
                0,
                0.15421195652173914,
                0.16319575918212798,
                0.14310344827586208,
                0.28557319223985894,
                0.2971698113207547,
                0.0,
                0.0,
                0.30427450980392157,
                1
            ],
            [
                0,
                0.22758152173913043,
                0.24611889435819764,
                0.28218390804597704,
                0.3128042328042328,
                0.24528301886792453,
                0.0,
                0.0,
                0.18478431372549017,
                1
            ]
        ]
    },
    "In baking, yeast helps bread rise, but scientifically yeast is what?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.6519607843137255,
                    0.061611374407582936,
                    0.027140266399849917,
                    0.8807241145950824,
                    0.3435326842837274,
                    0.5833333333333334,
                    0.6,
                    0.8,
                    1
                ],
                [
                    0.17598039215686276,
                    0.10604265402843602,
                    0.9380276405478082,
                    0.06468531468531469,
                    0.3157162726008345,
                    0.25,
                    0.23333333333333334,
                    0.2,
                    1
                ],
                [
                    0.17205882352941176,
                    0.832345971563981,
                    0.034832093052341945,
                    0.05459057071960298,
                    0.34075104311543813,
                    0.16666666666666666,
                    0.16666666666666666,
                    0.0,
                    1
                ]
            ],
            "fraction_answers": {
                "fungus": 0.49353781966666266,
                "plant": 0.28547320091907363,
                "bacteria": 0.22098897941426368
            },
            "question": "in baking, yeast helps bread rise, but scientifically yeast is what?",
            "rate_limited": false,
            "answers": [
                "fungus",
                "plant",
                "bacteria"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "fungus": 0.5396660716930533,
                "plant": 0.158872836972298,
                "bacteria": 0.19047841481215633
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    208000.0,
                    358000.0,
                    2810000.0
                ],
                "wikipedia_search": [
                    3.5228964583803295,
                    0.25874125874125875,
                    0.21836228287841192
                ],
                "result_count_noun_chunks": [
                    434000.0,
                    15000000.0,
                    557000.0
                ],
                "word_relation_to_question": [
                    4.0,
                    1.0,
                    0.0
                ],
                "word_count_noun_chunks": [
                    7.0,
                    3.0,
                    2.0
                ],
                "word_count_raw": [
                    18.0,
                    7.0,
                    5.0
                ],
                "result_count": [
                    1330000.0,
                    359000.0,
                    351000.0
                ],
                "word_count_appended": [
                    247.0,
                    227.0,
                    245.0
                ]
            },
            "z-best_answer_by_ml": [
                "fungus"
            ],
            "integer_answers": {
                "fungus": 6,
                "plant": 1,
                "bacteria": 1
            }
        },
        "lines": [
            [
                1,
                0.6519607843137255,
                0.061611374407582936,
                0.027140266399849917,
                0.8807241145950824,
                0.3435326842837274,
                0.5833333333333334,
                0.6,
                0.8,
                1
            ],
            [
                0,
                0.17598039215686276,
                0.10604265402843602,
                0.9380276405478082,
                0.06468531468531469,
                0.3157162726008345,
                0.25,
                0.23333333333333334,
                0.2,
                1
            ],
            [
                0,
                0.17205882352941176,
                0.832345971563981,
                0.034832093052341945,
                0.05459057071960298,
                0.34075104311543813,
                0.16666666666666666,
                0.16666666666666666,
                0.0,
                1
            ]
        ]
    },
    "Which of these is classified as a neurological condition or disorder?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.02063002638354684,
                    0.02636768680154846,
                    0.014135083080338198,
                    0.28756674294431733,
                    0.5100671140939598,
                    0.0,
                    0.0,
                    0.1696688968948362,
                    -1
                ],
                [
                    0.11793127562859596,
                    0.30750127821196405,
                    0.2611562693826919,
                    0.20404271548436306,
                    0.23993288590604026,
                    0.0,
                    0.0,
                    0.20523817693143478,
                    -1
                ],
                [
                    0.8614386979878572,
                    0.6661310349864875,
                    0.72470864753697,
                    0.5083905415713196,
                    0.25,
                    1.0,
                    1.0,
                    0.6250929261737291,
                    -1
                ]
            ],
            "fraction_answers": {
                "halitosis": 0.12855444377481834,
                "cystic fibrosis": 0.16697532519313626,
                "multiple sclerosis": 0.7044702310320454
            },
            "question": "which of these is classified as a neurological condition or disorder?",
            "rate_limited": false,
            "answers": [
                "halitosis",
                "cystic fibrosis",
                "multiple sclerosis"
            ],
            "ml_answers": {
                "halitosis": 0.31006538440586606,
                "cystic fibrosis": 0.2065588152451903,
                "multiple sclerosis": 0.8888578591118336
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "halitosis": 1,
                "cystic fibrosis": 0,
                "multiple sclerosis": 7
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    36100.0,
                    421000.0,
                    912000.0
                ],
                "wikipedia_search": [
                    0.8627002288329519,
                    0.6121281464530892,
                    1.5251716247139586
                ],
                "result_count_noun_chunks": [
                    86600.0,
                    1600000.0,
                    4440000.0
                ],
                "word_relation_to_question": [
                    0.5090066906845085,
                    0.6157145307943044,
                    1.8752787785211873
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    3.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    6.0
                ],
                "result_count": [
                    64900.0,
                    371000.0,
                    2710000.0
                ],
                "word_count_appended": [
                    304.0,
                    143.0,
                    149.0
                ]
            },
            "z-best_answer_by_ml": [
                "multiple sclerosis"
            ]
        },
        "lines": [
            [
                0,
                0.02063002638354684,
                0.02636768680154846,
                0.014135083080338198,
                0.28756674294431733,
                0.5100671140939598,
                0.0,
                0.0,
                0.1696688968948362,
                -1
            ],
            [
                0,
                0.11793127562859596,
                0.30750127821196405,
                0.2611562693826919,
                0.20404271548436306,
                0.23993288590604026,
                0.0,
                0.0,
                0.20523817693143478,
                -1
            ],
            [
                1,
                0.8614386979878572,
                0.6661310349864875,
                0.72470864753697,
                0.5083905415713196,
                0.25,
                1.0,
                1.0,
                0.6250929261737291,
                -1
            ]
        ]
    },
    "Which of these Uranus moons is NOT named after a Shakespearean character?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.12088922787024992,
                    0.12463642795377722,
                    0.12106013998484239,
                    0.4713541666666667,
                    0.28610354223433243,
                    0.18867924528301888,
                    0.22499999999999998,
                    0.47067345783814374,
                    -1
                ],
                [
                    0.3905289604737763,
                    0.38778397924691455,
                    0.3901074405956043,
                    0.20208333333333334,
                    0.33310626702997276,
                    0.34905660377358494,
                    0.29166666666666663,
                    0.21220033955857381,
                    -1
                ],
                [
                    0.4885818116559738,
                    0.48757959279930824,
                    0.4888324194195533,
                    0.3265625,
                    0.3807901907356948,
                    0.46226415094339623,
                    0.48333333333333334,
                    0.3171262026032824,
                    -1
                ]
            ],
            "fraction_answers": {
                "oberon": 0.4979009480422422,
                "trinculo": 0.14123244962736448,
                "umbriel": 0.36086660233039336
            },
            "question": "which of these uranus moons is not named after a shakespearean character?",
            "rate_limited": false,
            "answers": [
                "oberon",
                "umbriel",
                "trinculo"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "oberon": 0.05189704615446326,
                "trinculo": 0.26808905881738015,
                "umbriel": 0.03459786659908878
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    19100.0,
                    5710.0,
                    632.0
                ],
                "wikipedia_search": [
                    0.22916666666666666,
                    2.3833333333333333,
                    1.3875
                ],
                "result_count_noun_chunks": [
                    17000.0,
                    4930.0,
                    501.0
                ],
                "word_relation_to_question": [
                    0.2932654216185625,
                    2.8779966044142617,
                    1.828737973967176
                ],
                "word_count_noun_chunks": [
                    33.0,
                    16.0,
                    4.0
                ],
                "word_count_raw": [
                    33.0,
                    25.0,
                    2.0
                ],
                "result_count": [
                    16900.0,
                    4880.0,
                    509.0
                ],
                "word_count_appended": [
                    314.0,
                    245.0,
                    175.0
                ]
            },
            "z-best_answer_by_ml": [
                "trinculo"
            ],
            "integer_answers": {
                "oberon": 6,
                "trinculo": 0,
                "umbriel": 2
            }
        },
        "lines": [
            [
                0,
                0.12088922787024992,
                0.12463642795377722,
                0.12106013998484239,
                0.4713541666666667,
                0.28610354223433243,
                0.18867924528301888,
                0.22499999999999998,
                0.47067345783814374,
                -1
            ],
            [
                1,
                0.3905289604737763,
                0.38778397924691455,
                0.3901074405956043,
                0.20208333333333334,
                0.33310626702997276,
                0.34905660377358494,
                0.29166666666666663,
                0.21220033955857381,
                -1
            ],
            [
                0,
                0.4885818116559738,
                0.48757959279930824,
                0.4888324194195533,
                0.3265625,
                0.3807901907356948,
                0.46226415094339623,
                0.48333333333333334,
                0.3171262026032824,
                -1
            ]
        ]
    },
    "Which of these knots is typically used to add another line to a rope?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.0009852044444962904,
                    0.11312205204303355,
                    0.1277205040091638,
                    0.22344321310926718,
                    0.18021978021978025,
                    0.06521739130434784,
                    0,
                    0.24096405230672474,
                    -1
                ],
                [
                    0.4996805372570509,
                    0.3971858604059569,
                    0.39032073310423826,
                    0.30480029762384553,
                    0.46153846153846156,
                    0.43478260869565216,
                    0,
                    0.3186672034222614,
                    -1
                ],
                [
                    0.4993342582984528,
                    0.48969208755100957,
                    0.48195876288659795,
                    0.4717564892668873,
                    0.35824175824175825,
                    0.5,
                    0,
                    0.44036874427101386,
                    -1
                ]
            ],
            "fraction_answers": {
                "grantchester": 0.07389939985265148,
                "rolling hitch": 0.19800694227215238,
                "bowline": 0.7280936578751961
            },
            "question": "which of these knots is typically used to add another line to a rope?",
            "rate_limited": false,
            "answers": [
                "bowline",
                "rolling hitch",
                "grantchester"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "grantchester": 0.44295287972254116,
                "rolling hitch": 0.09358269526906902,
                "bowline": 0.030552545492369486
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    146000.0,
                    38800.0,
                    3890.0
                ],
                "wikipedia_search": [
                    2.2124542951258626,
                    1.5615976190092358,
                    0.22594808586490164
                ],
                "result_count_noun_chunks": [
                    260000.0,
                    76600.0,
                    12600.0
                ],
                "word_relation_to_question": [
                    2.072287581546202,
                    1.4506623726219088,
                    0.4770500458318893
                ],
                "word_count_noun_chunks": [
                    20.0,
                    3.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    4280000.0,
                    2740.0,
                    5710.0
                ],
                "word_count_appended": [
                    291.0,
                    35.0,
                    129.0
                ]
            },
            "z-best_answer_by_ml": [
                "grantchester"
            ],
            "integer_answers": {
                "grantchester": 0,
                "rolling hitch": 0,
                "bowline": 7
            }
        },
        "lines": [
            [
                0,
                0.0009852044444962904,
                0.11312205204303355,
                0.1277205040091638,
                0.22344321310926718,
                0.18021978021978025,
                0.06521739130434784,
                0,
                0.24096405230672474,
                -1
            ],
            [
                1,
                0.4996805372570509,
                0.3971858604059569,
                0.39032073310423826,
                0.30480029762384553,
                0.46153846153846156,
                0.43478260869565216,
                0,
                0.3186672034222614,
                -1
            ],
            [
                0,
                0.4993342582984528,
                0.48969208755100957,
                0.48195876288659795,
                0.4717564892668873,
                0.35824175824175825,
                0.5,
                0,
                0.44036874427101386,
                -1
            ]
        ]
    },
    "Which of these celebrities is known for having aviophobia?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.3873239436619718,
                    0.39263803680981596,
                    0.44025157232704404,
                    0.8344907407407408,
                    0.3442622950819672,
                    0.125,
                    0.25,
                    0.2512820512820513,
                    -1
                ],
                [
                    0.36619718309859156,
                    0.3987730061349693,
                    0.33962264150943394,
                    0.07423941798941798,
                    0.47540983606557374,
                    0.5,
                    0.5,
                    0.4615384615384615,
                    -1
                ],
                [
                    0.24647887323943662,
                    0.2085889570552147,
                    0.22012578616352202,
                    0.09126984126984126,
                    0.18032786885245902,
                    0.375,
                    0.25,
                    0.2871794871794872,
                    -1
                ]
            ],
            "fraction_answers": {
                "angelina jolie": 0.3781560799879489,
                "john travolta": 0.2323713517199951,
                "john madden": 0.38947256829205606
            },
            "question": "which of these celebrities is known for having aviophobia?",
            "rate_limited": false,
            "answers": [
                "angelina jolie",
                "john madden",
                "john travolta"
            ],
            "integer_answers": {
                "angelina jolie": 3,
                "john travolta": 0,
                "john madden": 5
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "angelina jolie": 0.1605105096737254,
                "john travolta": 0.2672855921949274,
                "john madden": 0.5307058178733622
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    64.0,
                    65.0,
                    34.0
                ],
                "wikipedia_search": [
                    2.5034722222222223,
                    0.22271825396825395,
                    0.2738095238095238
                ],
                "result_count_noun_chunks": [
                    70.0,
                    54.0,
                    35.0
                ],
                "word_relation_to_question": [
                    0.7538461538461538,
                    1.3846153846153846,
                    0.8615384615384616
                ],
                "word_count_noun_chunks": [
                    1.0,
                    4.0,
                    3.0
                ],
                "word_count_raw": [
                    1.0,
                    2.0,
                    1.0
                ],
                "word_count_appended": [
                    21.0,
                    29.0,
                    11.0
                ],
                "result_count": [
                    55.0,
                    52.0,
                    35.0
                ]
            },
            "z-best_answer_by_ml": [
                "john madden"
            ]
        },
        "lines": [
            [
                0,
                0.3873239436619718,
                0.39263803680981596,
                0.44025157232704404,
                0.8344907407407408,
                0.3442622950819672,
                0.125,
                0.25,
                0.2512820512820513,
                -1
            ],
            [
                1,
                0.36619718309859156,
                0.3987730061349693,
                0.33962264150943394,
                0.07423941798941798,
                0.47540983606557374,
                0.5,
                0.5,
                0.4615384615384615,
                -1
            ],
            [
                0,
                0.24647887323943662,
                0.2085889570552147,
                0.22012578616352202,
                0.09126984126984126,
                0.18032786885245902,
                0.375,
                0.25,
                0.2871794871794872,
                -1
            ]
        ]
    },
    "By definition, an Anglophile would be most interested in which of these things?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.4099384629621061,
                    0.09737267963807736,
                    0.10553170133273852,
                    0.26666666666666666,
                    0.7063829787234043,
                    0,
                    0.5,
                    0.39999999999999997,
                    -1
                ],
                [
                    0.5894600471938185,
                    0.9022606094904416,
                    0.8936039703340211,
                    0.7333333333333334,
                    0.2553191489361702,
                    0,
                    0.5,
                    0.6,
                    -1
                ],
                [
                    0.000601489844075325,
                    0.0003667108714810161,
                    0.0008643283332403948,
                    0.0,
                    0.03829787234042553,
                    0,
                    0.0,
                    0.0,
                    -1
                ]
            ],
            "fraction_answers": {
                "geometry": 0.3551274984747133,
                "downton abbey": 0.6391395870411121,
                "trout fishing": 0.00573291448417461
            },
            "question": "by definition, an anglophile would be most interested in which of these things?",
            "rate_limited": false,
            "answers": [
                "geometry",
                "downton abbey",
                "trout fishing"
            ],
            "ml_answers": {
                "geometry": 0.26300993473703954,
                "downton abbey": 0.34880882256852,
                "trout fishing": 0.07705753372418261
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "geometry": 2,
                "downton abbey": 5,
                "trout fishing": 0
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    218000.0,
                    2020000.0,
                    821.0
                ],
                "wikipedia_search": [
                    0.8,
                    2.2,
                    0.0
                ],
                "result_count_noun_chunks": [
                    7570.0,
                    64100.0,
                    62.0
                ],
                "word_relation_to_question": [
                    1.2,
                    1.8,
                    0.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    0.0
                ],
                "word_count_appended": [
                    166.0,
                    60.0,
                    9.0
                ],
                "result_count": [
                    44300.0,
                    63700.0,
                    65.0
                ]
            },
            "z-best_answer_by_ml": [
                "downton abbey"
            ]
        },
        "lines": [
            [
                0,
                0.4099384629621061,
                0.09737267963807736,
                0.10553170133273852,
                0.26666666666666666,
                0.7063829787234043,
                0,
                0.5,
                0.39999999999999997,
                -1
            ],
            [
                1,
                0.5894600471938185,
                0.9022606094904416,
                0.8936039703340211,
                0.7333333333333334,
                0.2553191489361702,
                0,
                0.5,
                0.6,
                -1
            ],
            [
                0,
                0.000601489844075325,
                0.0003667108714810161,
                0.0008643283332403948,
                0.0,
                0.03829787234042553,
                0,
                0.0,
                0.0,
                -1
            ]
        ]
    },
    "Who wrote a #1 hit song for the Monkees?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.37676767676767675,
                    0.7856810785681079,
                    0.3296378418329638,
                    0.5364181100799782,
                    0,
                    0,
                    0,
                    0.2327374654563902,
                    0
                ],
                [
                    0.23636363636363636,
                    0.13365876336587634,
                    0.3288987435328899,
                    0.29496536982118027,
                    0,
                    0,
                    0,
                    0.22363009357930613,
                    0
                ],
                [
                    0.38686868686868686,
                    0.08066015806601581,
                    0.34146341463414637,
                    0.1686165200988415,
                    0,
                    0,
                    0,
                    0.5436324409643037,
                    0
                ]
            ],
            "fraction_answers": {
                "neil diamond": 0.24350332133257782,
                "james taylor": 0.4522484345410234,
                "jackson browne": 0.30424824412639884
            },
            "question": "who wrote a #1 hit song for the monkees?",
            "rate_limited": false,
            "answers": [
                "james taylor",
                "neil diamond",
                "jackson browne"
            ],
            "integer_answers": {
                "neil diamond": 0,
                "james taylor": 2,
                "jackson browne": 3
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "neil diamond": 0.1843212801331381,
                "james taylor": 0.40334467607961344,
                "jackson browne": 0.12377893266611639
            },
            "question_type": 0,
            "data": {
                "result_count_important_words": [
                    676000.0,
                    115000.0,
                    69400.0
                ],
                "wikipedia_search": [
                    2.1456724403199128,
                    1.179861479284721,
                    0.674466080395366
                ],
                "result_count_noun_chunks": [
                    446000000.0,
                    445000000.0,
                    462000000.0
                ],
                "word_relation_to_question": [
                    0.9309498618255607,
                    0.8945203743172244,
                    2.1745297638572145
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    37300000.0,
                    23400000.0,
                    38300000.0
                ]
            },
            "z-best_answer_by_ml": [
                "james taylor"
            ]
        },
        "lines": [
            [
                0,
                0.37676767676767675,
                0.7856810785681079,
                0.3296378418329638,
                0.5364181100799782,
                0,
                0,
                0,
                0.2327374654563902,
                0
            ],
            [
                1,
                0.23636363636363636,
                0.13365876336587634,
                0.3288987435328899,
                0.29496536982118027,
                0,
                0,
                0,
                0.22363009357930613,
                0
            ],
            [
                0,
                0.38686868686868686,
                0.08066015806601581,
                0.34146341463414637,
                0.1686165200988415,
                0,
                0,
                0,
                0.5436324409643037,
                0
            ]
        ]
    },
    "Anne of Green Gables literally means Anne of what?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.5416666666666666,
                    0.576,
                    0.7593648370347399,
                    0.08333333333333333,
                    0.5121951219512195,
                    0,
                    0,
                    0.4333333333333334,
                    1
                ],
                [
                    0.052083333333333336,
                    0.04,
                    0.00010336903540787035,
                    0.430952380952381,
                    0.07317073170731707,
                    0,
                    0,
                    0.37240818224081823,
                    1
                ],
                [
                    0.40625,
                    0.384,
                    0.2405317939298522,
                    0.4857142857142857,
                    0.4146341463414634,
                    0,
                    0,
                    0.19425848442584848,
                    1
                ]
            ],
            "fraction_answers": {
                "green pastures": 0.4843155487198822,
                "green walls": 0.35423145173524156,
                "green jars": 0.16145299954487624
            },
            "question": "anne of green gables literally means anne of what?",
            "rate_limited": false,
            "answers": [
                "green pastures",
                "green jars",
                "green walls"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "green pastures": 5,
                "green walls": 1,
                "green jars": 0
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    72.0,
                    5.0,
                    48.0
                ],
                "wikipedia_search": [
                    0.3333333333333333,
                    1.723809523809524,
                    1.9428571428571428
                ],
                "result_count_noun_chunks": [
                    191000.0,
                    26.0,
                    60500.0
                ],
                "word_relation_to_question": [
                    1.7333333333333332,
                    1.4896327289632727,
                    0.7770339377033937
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    21.0,
                    3.0,
                    17.0
                ],
                "result_count": [
                    52.0,
                    5.0,
                    39.0
                ]
            },
            "z-best_answer_by_ml": [
                "green walls"
            ],
            "ml_answers": {
                "green pastures": 0.21179521614911734,
                "green walls": 0.4414862843922968,
                "green jars": 0.14168324067310084
            }
        },
        "lines": [
            [
                0,
                0.5416666666666666,
                0.576,
                0.7593648370347399,
                0.08333333333333333,
                0.5121951219512195,
                0,
                0,
                0.4333333333333334,
                1
            ],
            [
                0,
                0.052083333333333336,
                0.04,
                0.00010336903540787035,
                0.430952380952381,
                0.07317073170731707,
                0,
                0,
                0.37240818224081823,
                1
            ],
            [
                1,
                0.40625,
                0.384,
                0.2405317939298522,
                0.4857142857142857,
                0.4146341463414634,
                0,
                0,
                0.19425848442584848,
                1
            ]
        ]
    },
    "What Japanese word means \u201cempty orchestra\u201d?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.12200081333875559,
                    0.385049365303244,
                    0.10551330798479087,
                    0.15625,
                    0.19176598049837487,
                    0.0,
                    0.0,
                    0.30268595041322316,
                    1
                ],
                [
                    0.04839365595770639,
                    0.29619181946403383,
                    0.3925855513307985,
                    0.4270833333333333,
                    0.21451787648970747,
                    0.017921146953405017,
                    0.012195121951219513,
                    0.2551078971533517,
                    1
                ],
                [
                    0.829605530703538,
                    0.31875881523272215,
                    0.5019011406844106,
                    0.4166666666666667,
                    0.5937161430119177,
                    0.982078853046595,
                    0.9878048780487805,
                    0.44220615243342515,
                    1
                ]
            ],
            "fraction_answers": {
                "sake": 0.15790817719229858,
                "anime": 0.20799955032919445,
                "karaoke": 0.634092272478507
            },
            "question": "what japanese word means \u201cempty orchestra\u201d?",
            "rate_limited": false,
            "answers": [
                "sake",
                "anime",
                "karaoke"
            ],
            "ml_answers": {
                "sake": 0.2100969737166074,
                "anime": 0.122070214520792,
                "karaoke": 0.8529472477947512
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "sake": 1,
                "anime": 1,
                "karaoke": 6
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    819000.0,
                    630000.0,
                    678000.0
                ],
                "wikipedia_search": [
                    0.625,
                    1.7083333333333333,
                    1.6666666666666667
                ],
                "result_count_noun_chunks": [
                    11100.0,
                    41300.0,
                    52800.0
                ],
                "word_relation_to_question": [
                    1.2107438016528926,
                    1.0204315886134068,
                    1.7688246097337006
                ],
                "word_count_noun_chunks": [
                    0.0,
                    5.0,
                    274.0
                ],
                "word_count_raw": [
                    0.0,
                    4.0,
                    324.0
                ],
                "word_count_appended": [
                    177.0,
                    198.0,
                    548.0
                ],
                "result_count": [
                    3000.0,
                    1190.0,
                    20400.0
                ]
            },
            "z-best_answer_by_ml": [
                "karaoke"
            ]
        },
        "lines": [
            [
                0,
                0.12200081333875559,
                0.385049365303244,
                0.10551330798479087,
                0.15625,
                0.19176598049837487,
                0.0,
                0.0,
                0.30268595041322316,
                1
            ],
            [
                0,
                0.04839365595770639,
                0.29619181946403383,
                0.3925855513307985,
                0.4270833333333333,
                0.21451787648970747,
                0.017921146953405017,
                0.012195121951219513,
                0.2551078971533517,
                1
            ],
            [
                1,
                0.829605530703538,
                0.31875881523272215,
                0.5019011406844106,
                0.4166666666666667,
                0.5937161430119177,
                0.982078853046595,
                0.9878048780487805,
                0.44220615243342515,
                1
            ]
        ]
    },
    "What is Telluride, Colorado named after?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.9788611519602264,
                    0.9974133746484117,
                    0.8732936646832342,
                    0,
                    0.5446428571428571,
                    0,
                    0,
                    0.375,
                    1
                ],
                [
                    0.021034132660726724,
                    0.002571110032427017,
                    0.08505425271263563,
                    0,
                    0.38392857142857145,
                    0,
                    0,
                    0.25,
                    1
                ],
                [
                    0.00010471537904690793,
                    1.5515319161197518e-05,
                    0.041652082604130204,
                    0,
                    0.07142857142857142,
                    0,
                    0,
                    0.375,
                    1
                ]
            ],
            "fraction_answers": {
                "a european city": 0.09764017694618195,
                "an element": 0.7538422096869459,
                "a governor": 0.14851761336687214
            },
            "question": "what is telluride, colorado named after?",
            "rate_limited": false,
            "answers": [
                "an element",
                "a governor",
                "a european city"
            ],
            "ml_answers": {
                "a european city": 0.10814260320510308,
                "an element": 0.2383590047067267,
                "a governor": 0.3061956541846559
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "a european city": 0,
                "an element": 5,
                "a governor": 0
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    1800000.0,
                    4640.0,
                    28.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    49900000.0,
                    4860000.0,
                    2380000.0
                ],
                "word_relation_to_question": [
                    0.375,
                    0.25,
                    0.375
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    215000.0,
                    4620.0,
                    23.0
                ],
                "word_count_appended": [
                    61.0,
                    43.0,
                    8.0
                ]
            },
            "z-best_answer_by_ml": [
                "a governor"
            ]
        },
        "lines": [
            [
                1,
                0.9788611519602264,
                0.9974133746484117,
                0.8732936646832342,
                0,
                0.5446428571428571,
                0,
                0,
                0.375,
                1
            ],
            [
                0,
                0.021034132660726724,
                0.002571110032427017,
                0.08505425271263563,
                0,
                0.38392857142857145,
                0,
                0,
                0.25,
                1
            ],
            [
                0,
                0.00010471537904690793,
                1.5515319161197518e-05,
                0.041652082604130204,
                0,
                0.07142857142857142,
                0,
                0,
                0.375,
                1
            ]
        ]
    },
    "Which of these is NOT a suit in a traditional Tarot deck?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.4207738369415016,
                    0.4253075571177504,
                    0.4253075571177504,
                    0.24043062200956938,
                    0.3964984552008239,
                    0.5,
                    0.5,
                    0.47500000000000003,
                    -1
                ],
                [
                    0.27567941040994937,
                    0.28295254833040423,
                    0.28295254833040423,
                    0.3181818181818182,
                    0.2976313079299691,
                    0.1643835616438356,
                    0.2554347826086957,
                    0.17142857142857146,
                    -1
                ],
                [
                    0.303546752648549,
                    0.29173989455184535,
                    0.29173989455184535,
                    0.44138755980861244,
                    0.305870236869207,
                    0.3356164383561644,
                    0.24456521739130432,
                    0.3535714285714286,
                    -1
                ]
            ],
            "fraction_answers": {
                "gloves": 0.15417049290315107,
                "cups": 0.3579906443127609,
                "swords": 0.4878388627840881
            },
            "question": "which of these is not a suit in a traditional tarot deck?",
            "rate_limited": false,
            "answers": [
                "gloves",
                "swords",
                "cups"
            ],
            "ml_answers": {
                "gloves": 0.4915177483982176,
                "cups": 0.12016641982327624,
                "swords": 0.04345003797003597
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "gloves": 1,
                "cups": 1,
                "swords": 6
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    170000.0,
                    494000.0,
                    474000.0
                ],
                "wikipedia_search": [
                    1.0382775119617225,
                    0.7272727272727273,
                    0.23444976076555024
                ],
                "result_count_noun_chunks": [
                    170000.0,
                    494000.0,
                    474000.0
                ],
                "word_relation_to_question": [
                    0.2,
                    2.6285714285714286,
                    1.1714285714285715
                ],
                "word_count_noun_chunks": [
                    0.0,
                    49.0,
                    24.0
                ],
                "word_count_raw": [
                    0.0,
                    45.0,
                    47.0
                ],
                "result_count": [
                    344000.0,
                    974000.0,
                    853000.0
                ],
                "word_count_appended": [
                    201.0,
                    393.0,
                    377.0
                ]
            },
            "z-best_answer_by_ml": [
                "gloves"
            ]
        },
        "lines": [
            [
                1,
                0.4207738369415016,
                0.4253075571177504,
                0.4253075571177504,
                0.24043062200956938,
                0.3964984552008239,
                0.5,
                0.5,
                0.47500000000000003,
                -1
            ],
            [
                0,
                0.27567941040994937,
                0.28295254833040423,
                0.28295254833040423,
                0.3181818181818182,
                0.2976313079299691,
                0.1643835616438356,
                0.2554347826086957,
                0.17142857142857146,
                -1
            ],
            [
                0,
                0.303546752648549,
                0.29173989455184535,
                0.29173989455184535,
                0.44138755980861244,
                0.305870236869207,
                0.3356164383561644,
                0.24456521739130432,
                0.3535714285714286,
                -1
            ]
        ]
    },
    "Table tennis is also known as what?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.9828598381368139,
                    0.9941022547173763,
                    0.9861424035544245,
                    0.20198170731707318,
                    0.507563025210084,
                    1.0,
                    1.0,
                    0.6984897518878102,
                    1
                ],
                [
                    0.01703952985714577,
                    0.005862654322692219,
                    0.013774053814829006,
                    0.6676829268292683,
                    0.27058823529411763,
                    0.0,
                    0.0,
                    0.15480043149946063,
                    1
                ],
                [
                    0.00010063200604038986,
                    3.509095993147662e-05,
                    8.35426307464629e-05,
                    0.13033536585365854,
                    0.2218487394957983,
                    0.0,
                    0.0,
                    0.14670981661272922,
                    1
                ]
            ],
            "fraction_answers": {
                "hunky-dory": 0.1412184789521892,
                "argle-bargle": 0.062389148444863055,
                "ping-pong": 0.7963923726029478
            },
            "question": "table tennis is also known as what?",
            "rate_limited": false,
            "answers": [
                "ping-pong",
                "hunky-dory",
                "argle-bargle"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "hunky-dory": 0.25357881172903707,
                "argle-bargle": 0.06455876473624103,
                "ping-pong": 0.9244764620258943
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    23400000.0,
                    138000.0,
                    826.0
                ],
                "wikipedia_search": [
                    0.40396341463414637,
                    1.3353658536585367,
                    0.2606707317073171
                ],
                "result_count_noun_chunks": [
                    9880000.0,
                    138000.0,
                    837.0
                ],
                "word_relation_to_question": [
                    2.0954692556634305,
                    0.46440129449838186,
                    0.4401294498381877
                ],
                "word_count_noun_chunks": [
                    27.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    78.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    7960000.0,
                    138000.0,
                    815.0
                ],
                "word_count_appended": [
                    302.0,
                    161.0,
                    132.0
                ]
            },
            "z-best_answer_by_ml": [
                "ping-pong"
            ],
            "integer_answers": {
                "hunky-dory": 1,
                "argle-bargle": 0,
                "ping-pong": 7
            }
        },
        "lines": [
            [
                1,
                0.9828598381368139,
                0.9941022547173763,
                0.9861424035544245,
                0.20198170731707318,
                0.507563025210084,
                1.0,
                1.0,
                0.6984897518878102,
                1
            ],
            [
                0,
                0.01703952985714577,
                0.005862654322692219,
                0.013774053814829006,
                0.6676829268292683,
                0.27058823529411763,
                0.0,
                0.0,
                0.15480043149946063,
                1
            ],
            [
                0,
                0.00010063200604038986,
                3.509095993147662e-05,
                8.35426307464629e-05,
                0.13033536585365854,
                0.2218487394957983,
                0.0,
                0.0,
                0.14670981661272922,
                1
            ]
        ]
    },
    "Which of these countries is closest to the International Date Line?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.4871495327102804,
                    0.9917355371900827,
                    0.00623542077875471,
                    0.32236730863275115,
                    0.3326293558606125,
                    0.3333333333333333,
                    0.6,
                    0.3497355513171739,
                    -1
                ],
                [
                    0.30899532710280375,
                    0.004382526564344747,
                    0.005368144027752856,
                    0.3777416673453997,
                    0.3146779303062302,
                    0.0,
                    0.0,
                    0.3198667827968758,
                    -1
                ],
                [
                    0.2038551401869159,
                    0.003881936245572609,
                    0.9883964351934924,
                    0.29989102402184914,
                    0.3526927138331573,
                    0.6666666666666666,
                    0.4,
                    0.33039766588595026,
                    -1
                ]
            ],
            "fraction_answers": {
                "brazil": 0.1663790472679259,
                "japan": 0.42789825497787365,
                "spain": 0.40572269775420056
            },
            "question": "which of these countries is closest to the international date line?",
            "rate_limited": false,
            "answers": [
                "japan",
                "brazil",
                "spain"
            ],
            "integer_answers": {
                "brazil": 1,
                "japan": 4,
                "spain": 3
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "brazil": 0.29450523494608494,
                "japan": 0.32490711645735254,
                "spain": 0.42297071167532574
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    1050000000.0,
                    4640000.0,
                    4110000.0
                ],
                "wikipedia_search": [
                    1.2894692345310046,
                    1.5109666693815988,
                    1.1995640960873966
                ],
                "result_count_noun_chunks": [
                    4170000.0,
                    3590000.0,
                    661000000.0
                ],
                "word_relation_to_question": [
                    1.3989422052686955,
                    1.2794671311875032,
                    1.321590663543801
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    2.0
                ],
                "word_count_raw": [
                    3.0,
                    0.0,
                    2.0
                ],
                "word_count_appended": [
                    315.0,
                    298.0,
                    334.0
                ],
                "result_count": [
                    8340000.0,
                    5290000.0,
                    3490000.0
                ]
            },
            "z-best_answer_by_ml": [
                "spain"
            ]
        },
        "lines": [
            [
                1,
                0.4871495327102804,
                0.9917355371900827,
                0.00623542077875471,
                0.32236730863275115,
                0.3326293558606125,
                0.3333333333333333,
                0.6,
                0.3497355513171739,
                -1
            ],
            [
                0,
                0.30899532710280375,
                0.004382526564344747,
                0.005368144027752856,
                0.3777416673453997,
                0.3146779303062302,
                0.0,
                0.0,
                0.3198667827968758,
                -1
            ],
            [
                0,
                0.2038551401869159,
                0.003881936245572609,
                0.9883964351934924,
                0.29989102402184914,
                0.3526927138331573,
                0.6666666666666666,
                0.4,
                0.33039766588595026,
                -1
            ]
        ]
    },
    "How did Mason jars get their name?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    3.865962442948059e-06,
                    4.816929696718399e-06,
                    4.2820953753621836e-06,
                    0.0,
                    0.47619047619047616,
                    0,
                    0,
                    0.0,
                    5
                ],
                [
                    0.999995618575898,
                    0.9999946050387397,
                    0.9999952141286981,
                    0.0,
                    0.2857142857142857,
                    0,
                    0,
                    0.36499526403031024,
                    5
                ],
                [
                    5.154616590597413e-07,
                    5.780315636062079e-07,
                    5.037759265131981e-07,
                    1.0,
                    0.23809523809523808,
                    0,
                    0,
                    0.6350047359696898,
                    5
                ]
            ],
            "fraction_answers": {
                "named after inventor": 0.07936724019633186,
                "invented in mason, al": 0.608449164581322,
                "masons used them": 0.3121835952223462
            },
            "question": "how did mason jars get their name?",
            "rate_limited": false,
            "answers": [
                "named after inventor",
                "invented in mason, al",
                "masons used them"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "named after inventor": 0.3104525995772774,
                "invented in mason, al": 0.10841342879265708,
                "masons used them": 0.11731243394393773
            },
            "question_type": 5,
            "data": {
                "result_count_important_words": [
                    25.0,
                    5190000.0,
                    3.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_noun_chunks": [
                    17.0,
                    3970000.0,
                    2.0
                ],
                "word_relation_to_question": [
                    0.0,
                    0.7299905280606205,
                    1.2700094719393795
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    15.0,
                    3880000.0,
                    2.0
                ],
                "word_count_appended": [
                    10.0,
                    6.0,
                    5.0
                ]
            },
            "z-best_answer_by_ml": [
                "named after inventor"
            ],
            "integer_answers": {
                "named after inventor": 1,
                "invented in mason, al": 3,
                "masons used them": 2
            }
        },
        "lines": [
            [
                1,
                3.865962442948059e-06,
                4.816929696718399e-06,
                4.2820953753621836e-06,
                0.0,
                0.47619047619047616,
                0,
                0,
                0.0,
                5
            ],
            [
                0,
                0.999995618575898,
                0.9999946050387397,
                0.9999952141286981,
                0.0,
                0.2857142857142857,
                0,
                0,
                0.36499526403031024,
                5
            ],
            [
                0,
                5.154616590597413e-07,
                5.780315636062079e-07,
                5.037759265131981e-07,
                1.0,
                0.23809523809523808,
                0,
                0,
                0.6350047359696898,
                5
            ]
        ]
    },
    "Who is NOT considered an official member of the Eagles?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.4998632852553148,
                    0.4998647089818995,
                    0.3868131868131868,
                    0.33333333333333337,
                    0.42699724517906334,
                    0.5,
                    0.5,
                    0.48167814772292383,
                    0
                ],
                [
                    0.21802583908674553,
                    0.21802503595892847,
                    0.2824175824175824,
                    0.2691658223573117,
                    0.25757575757575757,
                    0.25,
                    0.2,
                    0.2690877392369929,
                    0
                ],
                [
                    0.28211087565793974,
                    0.28211025505917203,
                    0.33076923076923076,
                    0.39750084430935495,
                    0.3154269972451791,
                    0.25,
                    0.3,
                    0.2492341130400832,
                    0
                ]
            ],
            "fraction_answers": {
                "j.d. souther": 0.09286252317856959,
                "randy meisner": 0.5089255558416703,
                "bernie leadon": 0.3982119209797601
            },
            "question": "who is not considered an official member of the eagles?",
            "rate_limited": false,
            "answers": [
                "j.d. souther",
                "randy meisner",
                "bernie leadon"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "j.d. souther": 0,
                "randy meisner": 7,
                "bernie leadon": 1
            },
            "question_type": 0,
            "data": {
                "result_count_important_words": [
                    95.0,
                    198000.0,
                    153000.0
                ],
                "wikipedia_search": [
                    1.0,
                    1.3850050658561297,
                    0.6149949341438703
                ],
                "result_count_noun_chunks": [
                    103000.0,
                    198000.0,
                    154000.0
                ],
                "word_relation_to_question": [
                    0.10993111366245695,
                    1.3854735645780423,
                    1.504595321759501
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    3.0,
                    2.0
                ],
                "result_count": [
                    96.0,
                    198000.0,
                    153000.0
                ],
                "word_count_appended": [
                    53.0,
                    176.0,
                    134.0
                ]
            },
            "z-best_answer_by_ml": [
                "j.d. souther"
            ],
            "ml_answers": {
                "j.d. souther": 0.594932615834693,
                "randy meisner": -0.006763274889340932,
                "bernie leadon": 0.1290413291441453
            }
        },
        "lines": [
            [
                1,
                0.4998632852553148,
                0.4998647089818995,
                0.3868131868131868,
                0.33333333333333337,
                0.42699724517906334,
                0.5,
                0.5,
                0.48167814772292383,
                0
            ],
            [
                0,
                0.21802583908674553,
                0.21802503595892847,
                0.2824175824175824,
                0.2691658223573117,
                0.25757575757575757,
                0.25,
                0.2,
                0.2690877392369929,
                0
            ],
            [
                0,
                0.28211087565793974,
                0.28211025505917203,
                0.33076923076923076,
                0.39750084430935495,
                0.3154269972451791,
                0.25,
                0.3,
                0.2492341130400832,
                0
            ]
        ]
    },
    "What SportsCenter anchor shares their last name with Linus & Lucy from \u201cPeanuts\u201d?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.4433361041839845,
                    0.46987951807228917,
                    0.002823529411764706,
                    0.2833333333333333,
                    0.16666666666666666,
                    0.5,
                    0.16666666666666666,
                    0.16291829639287267,
                    1
                ],
                [
                    0.0800775838182322,
                    0.3253012048192771,
                    0.22070588235294117,
                    0.20833333333333331,
                    0.6666666666666666,
                    0.0,
                    0.6666666666666666,
                    0.5220013037809648,
                    1
                ],
                [
                    0.47658631199778334,
                    0.20481927710843373,
                    0.7764705882352941,
                    0.5083333333333333,
                    0.16666666666666666,
                    0.5,
                    0.16666666666666666,
                    0.31508039982616254,
                    1
                ]
            ],
            "fraction_answers": {
                "jemele hill": 0.2744530143409472,
                "kenny mayne": 0.3893279054792926,
                "scott van pelt": 0.3362190801797602
            },
            "question": "what sportscenter anchor shares their last name with linus & lucy from \u201cpeanuts\u201d?",
            "rate_limited": false,
            "answers": [
                "jemele hill",
                "scott van pelt",
                "kenny mayne"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "jemele hill": 0.4882839325124068,
                "kenny mayne": 0.4428280975136468,
                "scott van pelt": 0.2957224010721621
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    39.0,
                    27.0,
                    17.0
                ],
                "wikipedia_search": [
                    0.5666666666666667,
                    0.41666666666666663,
                    1.0166666666666666
                ],
                "result_count_noun_chunks": [
                    60.0,
                    4690.0,
                    16500.0
                ],
                "word_relation_to_question": [
                    0.488754889178618,
                    1.5660039113428943,
                    0.9452411994784876
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    2.0
                ],
                "word_count_raw": [
                    1.0,
                    4.0,
                    1.0
                ],
                "result_count": [
                    160000.0,
                    28900.0,
                    172000.0
                ],
                "word_count_appended": [
                    1.0,
                    4.0,
                    1.0
                ]
            },
            "z-best_answer_by_ml": [
                "jemele hill"
            ],
            "integer_answers": {
                "jemele hill": 2,
                "kenny mayne": 3,
                "scott van pelt": 3
            }
        },
        "lines": [
            [
                0,
                0.4433361041839845,
                0.46987951807228917,
                0.002823529411764706,
                0.2833333333333333,
                0.16666666666666666,
                0.5,
                0.16666666666666666,
                0.16291829639287267,
                1
            ],
            [
                1,
                0.0800775838182322,
                0.3253012048192771,
                0.22070588235294117,
                0.20833333333333331,
                0.6666666666666666,
                0.0,
                0.6666666666666666,
                0.5220013037809648,
                1
            ],
            [
                0,
                0.47658631199778334,
                0.20481927710843373,
                0.7764705882352941,
                0.5083333333333333,
                0.16666666666666666,
                0.5,
                0.16666666666666666,
                0.31508039982616254,
                1
            ]
        ]
    },
    "How do you let someone on Tinder know you're interested?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.9994865023474179,
                    0.09543109128357308,
                    0.5416340023353855,
                    0.9128571428571428,
                    0.8317757009345794,
                    1.0,
                    1.0,
                    0.5759562841530054,
                    5
                ],
                [
                    0.0,
                    0.9045207782529969,
                    0.4583056942837877,
                    0.06469387755102039,
                    0.028037383177570093,
                    0.0,
                    0.0,
                    0.13770491803278687,
                    5
                ],
                [
                    0.0005134976525821597,
                    4.813046342997598e-05,
                    6.030338082681417e-05,
                    0.02244897959183673,
                    0.14018691588785046,
                    0.0,
                    0.0,
                    0.2863387978142076,
                    5
                ]
            ],
            "fraction_answers": {
                "draw circle around face": 0.19915783141227025,
                "shake phone": 0.05619957809884172,
                "swipe right": 0.744642590488888
            },
            "question": "how do you let someone on tinder know you're interested?",
            "rate_limited": false,
            "answers": [
                "swipe right",
                "draw circle around face",
                "shake phone"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "draw circle around face": 0.2025803503934468,
                "shake phone": 0.11812258444029897,
                "swipe right": 0.8676870656151953
            },
            "question_type": 5,
            "data": {
                "result_count_important_words": [
                    115000.0,
                    1090000.0,
                    58.0
                ],
                "wikipedia_search": [
                    4.564285714285715,
                    0.32346938775510203,
                    0.11224489795918367
                ],
                "result_count_noun_chunks": [
                    2470000.0,
                    2090000.0,
                    275.0
                ],
                "word_relation_to_question": [
                    1.7278688524590162,
                    0.4131147540983606,
                    0.8590163934426229
                ],
                "word_count_noun_chunks": [
                    9.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    7.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    109000.0,
                    0,
                    56.0
                ],
                "word_count_appended": [
                    89.0,
                    3.0,
                    15.0
                ]
            },
            "z-best_answer_by_ml": [
                "swipe right"
            ],
            "integer_answers": {
                "draw circle around face": 1,
                "shake phone": 0,
                "swipe right": 7
            }
        },
        "lines": [
            [
                1,
                0.9994865023474179,
                0.09543109128357308,
                0.5416340023353855,
                0.9128571428571428,
                0.8317757009345794,
                1.0,
                1.0,
                0.5759562841530054,
                5
            ],
            [
                0,
                0.0,
                0.9045207782529969,
                0.4583056942837877,
                0.06469387755102039,
                0.028037383177570093,
                0.0,
                0.0,
                0.13770491803278687,
                5
            ],
            [
                0,
                0.0005134976525821597,
                4.813046342997598e-05,
                6.030338082681417e-05,
                0.02244897959183673,
                0.14018691588785046,
                0.0,
                0.0,
                0.2863387978142076,
                5
            ]
        ]
    },
    "Which of these sharks is NOT a Lamniforme?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.47362869198312235,
                    0.4879041207368499,
                    0.4766582376969461,
                    0.4614093959731544,
                    0.29824561403508776,
                    0,
                    0,
                    0.3377016129032258,
                    -1
                ],
                [
                    0.1214252226910455,
                    0.0672116593918769,
                    0.09832717370161448,
                    0.1493288590604027,
                    0.29824561403508776,
                    0,
                    0,
                    0.2664650537634409,
                    -1
                ],
                [
                    0.40494608532583215,
                    0.4448842198712732,
                    0.4250145886014394,
                    0.389261744966443,
                    0.4035087719298246,
                    0,
                    0,
                    0.3958333333333333,
                    -1
                ]
            ],
            "fraction_answers": {
                "goblin shark": 0.15481744222387125,
                "great white shark": 0.6663321391188439,
                "hammerhead shark": 0.1788504186572848
            },
            "question": "which of these sharks is not a lamniforme?",
            "rate_limited": false,
            "answers": [
                "goblin shark",
                "great white shark",
                "hammerhead shark"
            ],
            "ml_answers": {
                "goblin shark": 0.36581019159421807,
                "great white shark": -0.017202986476045404,
                "hammerhead shark": 0.22306601951898822
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "goblin shark": 1,
                "great white shark": 5,
                "hammerhead shark": 0
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    32700.0,
                    1170000.0,
                    149000.0
                ],
                "wikipedia_search": [
                    0.07718120805369127,
                    0.7013422818791947,
                    0.2214765100671141
                ],
                "result_count_noun_chunks": [
                    24000.0,
                    413000.0,
                    77100.0
                ],
                "word_relation_to_question": [
                    0.6491935483870968,
                    0.9341397849462365,
                    0.4166666666666667
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    22500.0,
                    323000.0,
                    81100.0
                ],
                "word_count_appended": [
                    161.0,
                    161.0,
                    77.0
                ]
            },
            "z-best_answer_by_ml": [
                "goblin shark"
            ]
        },
        "lines": [
            [
                0,
                0.47362869198312235,
                0.4879041207368499,
                0.4766582376969461,
                0.4614093959731544,
                0.29824561403508776,
                0,
                0,
                0.3377016129032258,
                -1
            ],
            [
                0,
                0.1214252226910455,
                0.0672116593918769,
                0.09832717370161448,
                0.1493288590604027,
                0.29824561403508776,
                0,
                0,
                0.2664650537634409,
                -1
            ],
            [
                1,
                0.40494608532583215,
                0.4448842198712732,
                0.4250145886014394,
                0.389261744966443,
                0.4035087719298246,
                0,
                0,
                0.3958333333333333,
                -1
            ]
        ]
    },
    "Which of these restaurant brands has its original location in Europe?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.05960350710491081,
                    0.5309303458353629,
                    0.06648869188535737,
                    0.6653508771929825,
                    0.9061032863849765,
                    0.5,
                    0.5,
                    0.35748785852412795,
                    -1
                ],
                [
                    0.9026907960091565,
                    0.15781782756941062,
                    0.8915529139172921,
                    0.2655701754385965,
                    0.007042253521126761,
                    0.0,
                    0.0,
                    0.37056374828395555,
                    -1
                ],
                [
                    0.037705696885932706,
                    0.3112518265952265,
                    0.04195839419735053,
                    0.06907894736842105,
                    0.08685446009389672,
                    0.5,
                    0.5,
                    0.27194839319191655,
                    -1
                ]
            ],
            "fraction_answers": {
                "mr. chow": 0.227349714791593,
                "benihana": 0.4482455708659648,
                "p.f. chang's": 0.3244047143424423
            },
            "question": "which of these restaurant brands has its original location in europe?",
            "rate_limited": false,
            "answers": [
                "benihana",
                "p.f. chang's",
                "mr. chow"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "mr. chow": 0.46771845995148004,
                "benihana": 0.4797540111926683,
                "p.f. chang's": 0.16114360788663068
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    109000.0,
                    32400.0,
                    63900.0
                ],
                "wikipedia_search": [
                    2.66140350877193,
                    1.062280701754386,
                    0.2763157894736842
                ],
                "result_count_noun_chunks": [
                    132000.0,
                    1770000.0,
                    83300.0
                ],
                "word_relation_to_question": [
                    1.7874392926206397,
                    1.8528187414197776,
                    1.3597419659595826
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    1.0
                ],
                "result_count": [
                    138000.0,
                    2090000.0,
                    87300.0
                ],
                "word_count_appended": [
                    386.0,
                    3.0,
                    37.0
                ]
            },
            "z-best_answer_by_ml": [
                "benihana"
            ],
            "integer_answers": {
                "mr. chow": 0,
                "benihana": 5,
                "p.f. chang's": 3
            }
        },
        "lines": [
            [
                0,
                0.05960350710491081,
                0.5309303458353629,
                0.06648869188535737,
                0.6653508771929825,
                0.9061032863849765,
                0.5,
                0.5,
                0.35748785852412795,
                -1
            ],
            [
                0,
                0.9026907960091565,
                0.15781782756941062,
                0.8915529139172921,
                0.2655701754385965,
                0.007042253521126761,
                0.0,
                0.0,
                0.37056374828395555,
                -1
            ],
            [
                1,
                0.037705696885932706,
                0.3112518265952265,
                0.04195839419735053,
                0.06907894736842105,
                0.08685446009389672,
                0.5,
                0.5,
                0.27194839319191655,
                -1
            ]
        ]
    },
    "In which of these movies is the title NOT spoken by any character?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.49347935127905035,
                    0.48710091636024044,
                    0.48986630939824977,
                    0.10225247408658128,
                    0.31847725162488394,
                    0.5,
                    0.5,
                    0.19762974051896204,
                    -1
                ],
                [
                    0.4872094967396756,
                    0.4856709274499504,
                    0.4790484877854109,
                    0.42356714037803106,
                    0.31847725162488394,
                    0.5,
                    0.5,
                    0.4201097804391217,
                    -1
                ],
                [
                    0.019311151981274033,
                    0.027228156189809116,
                    0.031085202816339308,
                    0.47418038553538766,
                    0.3630454967502321,
                    0.0,
                    0.0,
                    0.3822604790419162,
                    -1
                ]
            ],
            "fraction_answers": {
                "inception": 0.22779848918300805,
                "speed": 0.6757222819212605,
                "gravity": 0.09647922889573157
            },
            "question": "in which of these movies is the title not spoken by any character?",
            "rate_limited": false,
            "answers": [
                "inception",
                "gravity",
                "speed"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "inception": 3,
                "speed": 5,
                "gravity": 0
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    442000.0,
                    491000.0,
                    16200000.0
                ],
                "wikipedia_search": [
                    2.386485155480512,
                    0.4585971577318137,
                    0.15491768678767412
                ],
                "result_count_noun_chunks": [
                    711000.0,
                    1470000.0,
                    32900000.0
                ],
                "word_relation_to_question": [
                    1.8142215568862274,
                    0.4793413173652694,
                    0.7064371257485029
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count": [
                    780000.0,
                    1530000.0,
                    57500000.0
                ],
                "word_count_appended": [
                    391.0,
                    391.0,
                    295.0
                ]
            },
            "z-best_answer_by_ml": [
                "inception"
            ],
            "ml_answers": {
                "inception": 0.517719195487487,
                "speed": 0.2620932921153911,
                "gravity": 0.5095537363533316
            }
        },
        "lines": [
            [
                0,
                0.49347935127905035,
                0.48710091636024044,
                0.48986630939824977,
                0.10225247408658128,
                0.31847725162488394,
                0.5,
                0.5,
                0.19762974051896204,
                -1
            ],
            [
                1,
                0.4872094967396756,
                0.4856709274499504,
                0.4790484877854109,
                0.42356714037803106,
                0.31847725162488394,
                0.5,
                0.5,
                0.4201097804391217,
                -1
            ],
            [
                0,
                0.019311151981274033,
                0.027228156189809116,
                0.031085202816339308,
                0.47418038553538766,
                0.3630454967502321,
                0.0,
                0.0,
                0.3822604790419162,
                -1
            ]
        ]
    },
    "Tom from MySpace shares his name with a key character in what film franchise?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.6560571614160442,
                    0.1972168905950096,
                    0.11936936936936937,
                    0.23463153875525009,
                    0.37662337662337664,
                    1.0,
                    0.1111111111111111,
                    0.4172133228143372,
                    1
                ],
                [
                    0.13543358233192596,
                    0.3800383877159309,
                    0.31756756756756754,
                    0.2946366297569047,
                    0.1645021645021645,
                    0.0,
                    0.0,
                    0.27095885588152235,
                    1
                ],
                [
                    0.20850925625202987,
                    0.4227447216890595,
                    0.5630630630630631,
                    0.4707318314878452,
                    0.4588744588744589,
                    0.0,
                    0.8888888888888888,
                    0.31182782130414044,
                    1
                ]
            ],
            "fraction_answers": {
                "the godfather": 0.195392148469502,
                "the matrix": 0.4155800051949357,
                "harry potter": 0.38902784633556226
            },
            "question": "tom from myspace shares his name with a key character in what film franchise?",
            "rate_limited": false,
            "answers": [
                "harry potter",
                "the godfather",
                "the matrix"
            ],
            "integer_answers": {
                "the godfather": 0,
                "the matrix": 5,
                "harry potter": 3
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "the godfather": 0.2336782518506687,
                "the matrix": 0.707818978158403,
                "harry potter": 0.504275390742622
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    411000.0,
                    792000.0,
                    881000.0
                ],
                "wikipedia_search": [
                    1.1731576937762505,
                    1.4731831487845235,
                    2.353659157439226
                ],
                "result_count_noun_chunks": [
                    530000.0,
                    1410000.0,
                    2500000.0
                ],
                "word_relation_to_question": [
                    2.5032799368860235,
                    1.6257531352891343,
                    1.8709669278248429
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    8.0
                ],
                "word_count_appended": [
                    87.0,
                    38.0,
                    106.0
                ],
                "result_count": [
                    2020000.0,
                    417000.0,
                    642000.0
                ]
            },
            "z-best_answer_by_ml": [
                "the matrix"
            ]
        },
        "lines": [
            [
                0,
                0.6560571614160442,
                0.1972168905950096,
                0.11936936936936937,
                0.23463153875525009,
                0.37662337662337664,
                1.0,
                0.1111111111111111,
                0.4172133228143372,
                1
            ],
            [
                0,
                0.13543358233192596,
                0.3800383877159309,
                0.31756756756756754,
                0.2946366297569047,
                0.1645021645021645,
                0.0,
                0.0,
                0.27095885588152235,
                1
            ],
            [
                1,
                0.20850925625202987,
                0.4227447216890595,
                0.5630630630630631,
                0.4707318314878452,
                0.4588744588744589,
                0.0,
                0.8888888888888888,
                0.31182782130414044,
                1
            ]
        ]
    },
    "Which player won Rookie of the Year in their sport most recently?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.9567283641820911,
                    6.01194668732245e-05,
                    0.4505208333333333,
                    0.39216705261113155,
                    0.3236994219653179,
                    0.6666666666666666,
                    1.0,
                    0.43552479320772747,
                    -1
                ],
                [
                    0.013631815907953976,
                    5.822095739301741e-05,
                    0.16796875,
                    0.16188290419375945,
                    0.33236994219653176,
                    0.0,
                    0.0,
                    0.1712213666847193,
                    -1
                ],
                [
                    0.029639819909954977,
                    0.9998816595757337,
                    0.3815104166666667,
                    0.445950043195109,
                    0.3439306358381503,
                    0.3333333333333333,
                    0.0,
                    0.3932538401075533,
                    -1
                ]
            ],
            "fraction_answers": {
                "mike trout": 0.5281709064291427,
                "von miller": 0.10589162499254468,
                "blake griffin": 0.36593746857831266
            },
            "question": "which player won rookie of the year in their sport most recently?",
            "rate_limited": false,
            "answers": [
                "mike trout",
                "von miller",
                "blake griffin"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "mike trout": 0.8051875977946925,
                "von miller": 0.21567197219500994,
                "blake griffin": -0.008812511717545672
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    95.0,
                    92.0,
                    1580000.0
                ],
                "wikipedia_search": [
                    1.9608352630556578,
                    0.8094145209687973,
                    2.229750215975545
                ],
                "result_count_noun_chunks": [
                    692000.0,
                    258000.0,
                    586000.0
                ],
                "word_relation_to_question": [
                    2.6131487592463647,
                    1.0273282001083157,
                    2.35952304064532
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    15300000.0,
                    218000.0,
                    474000.0
                ],
                "word_count_appended": [
                    112.0,
                    115.0,
                    119.0
                ]
            },
            "z-best_answer_by_ml": [
                "mike trout"
            ],
            "integer_answers": {
                "mike trout": 5,
                "von miller": 0,
                "blake griffin": 3
            }
        },
        "lines": [
            [
                1,
                0.9567283641820911,
                6.01194668732245e-05,
                0.4505208333333333,
                0.39216705261113155,
                0.3236994219653179,
                0.6666666666666666,
                1.0,
                0.43552479320772747,
                -1
            ],
            [
                0,
                0.013631815907953976,
                5.822095739301741e-05,
                0.16796875,
                0.16188290419375945,
                0.33236994219653176,
                0.0,
                0.0,
                0.1712213666847193,
                -1
            ],
            [
                0,
                0.029639819909954977,
                0.9998816595757337,
                0.3815104166666667,
                0.445950043195109,
                0.3439306358381503,
                0.3333333333333333,
                0.0,
                0.3932538401075533,
                -1
            ]
        ]
    },
    "In computing, what unit is half a byte?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.5044319097502015,
                    0.491701244813278,
                    0.9075244112578977,
                    1.0,
                    0.662777129521587,
                    0.8524590163934426,
                    0.8709677419354839,
                    0.33741258741258745,
                    1
                ],
                [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.01866977829638273,
                    0.0,
                    0.0,
                    0.25018402649981597,
                    1
                ],
                [
                    0.49556809024979853,
                    0.508298755186722,
                    0.09247558874210224,
                    0.0,
                    0.3185530921820303,
                    0.14754098360655737,
                    0.12903225806451613,
                    0.41240338608759664,
                    1
                ]
            ],
            "fraction_answers": {
                "demibyte": 0.033606725599524837,
                "nibble": 0.7034092551355597,
                "octet": 0.2629840192649154
            },
            "question": "in computing, what unit is half a byte?",
            "rate_limited": false,
            "answers": [
                "nibble",
                "demibyte",
                "octet"
            ],
            "integer_answers": {
                "demibyte": 0,
                "nibble": 6,
                "octet": 2
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "demibyte": 0.08111187708518634,
                "nibble": 0.8008347005510988,
                "octet": 0.3652546574583225
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    71100.0,
                    0,
                    73500.0
                ],
                "wikipedia_search": [
                    4.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    632000.0,
                    0,
                    64400.0
                ],
                "word_relation_to_question": [
                    1.3496503496503498,
                    1.0007361059992639,
                    1.6496135443503865
                ],
                "word_count_noun_chunks": [
                    156.0,
                    0.0,
                    27.0
                ],
                "word_count_raw": [
                    162.0,
                    0.0,
                    24.0
                ],
                "word_count_appended": [
                    568.0,
                    16.0,
                    273.0
                ],
                "result_count": [
                    62600.0,
                    0,
                    61500.0
                ]
            },
            "z-best_answer_by_ml": [
                "nibble"
            ]
        },
        "lines": [
            [
                1,
                0.5044319097502015,
                0.491701244813278,
                0.9075244112578977,
                1.0,
                0.662777129521587,
                0.8524590163934426,
                0.8709677419354839,
                0.33741258741258745,
                1
            ],
            [
                0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.01866977829638273,
                0.0,
                0.0,
                0.25018402649981597,
                1
            ],
            [
                0,
                0.49556809024979853,
                0.508298755186722,
                0.09247558874210224,
                0.0,
                0.3185530921820303,
                0.14754098360655737,
                0.12903225806451613,
                0.41240338608759664,
                1
            ]
        ]
    },
    "Which of these has NEVER been named Pantone\u2019s Color of the Year?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.40995838558639874,
                    0.4692336378892411,
                    0.24099810486418194,
                    0.3277777777777778,
                    0.18666666666666665,
                    0,
                    0.33333333333333337,
                    0.4291101055806938,
                    -1
                ],
                [
                    0.09210063831910748,
                    0.04316613835539812,
                    0.4348283849231417,
                    0.40416666666666673,
                    0.4033333333333333,
                    0,
                    0.33333333333333337,
                    0.318420060331825,
                    -1
                ],
                [
                    0.4979409760944938,
                    0.48760022375536083,
                    0.3241735102126764,
                    0.26805555555555555,
                    0.41000000000000003,
                    0,
                    0.33333333333333337,
                    0.25246983408748114,
                    -1
                ]
            ],
            "fraction_answers": {
                "chili pepper": 0.2646933048460283,
                "cucumber": 0.31512056808620187,
                "sand dollar": 0.4201861270677698
            },
            "question": "which of these has never been named pantone\u2019s color of the year?",
            "rate_limited": false,
            "answers": [
                "cucumber",
                "sand dollar",
                "chili pepper"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "chili pepper": 0.16453024060828475,
                "cucumber": 0.5000546414881034,
                "sand dollar": 0.2802397567076428
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    330000.0,
                    4900000.0,
                    133000.0
                ],
                "wikipedia_search": [
                    1.0333333333333334,
                    0.575,
                    1.3916666666666668
                ],
                "result_count_noun_chunks": [
                    24600000.0,
                    6190000.0,
                    16700000.0
                ],
                "word_relation_to_question": [
                    0.42533936651583715,
                    1.08947963800905,
                    1.4851809954751132
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    1.0
                ],
                "result_count": [
                    4150000.0,
                    18800000.0,
                    94900.0
                ],
                "word_count_appended": [
                    188.0,
                    58.0,
                    54.0
                ]
            },
            "z-best_answer_by_ml": [
                "cucumber"
            ],
            "integer_answers": {
                "chili pepper": 2,
                "cucumber": 3,
                "sand dollar": 2
            }
        },
        "lines": [
            [
                1,
                0.40995838558639874,
                0.4692336378892411,
                0.24099810486418194,
                0.3277777777777778,
                0.18666666666666665,
                0,
                0.33333333333333337,
                0.4291101055806938,
                -1
            ],
            [
                0,
                0.09210063831910748,
                0.04316613835539812,
                0.4348283849231417,
                0.40416666666666673,
                0.4033333333333333,
                0,
                0.33333333333333337,
                0.318420060331825,
                -1
            ],
            [
                0,
                0.4979409760944938,
                0.48760022375536083,
                0.3241735102126764,
                0.26805555555555555,
                0.41000000000000003,
                0,
                0.33333333333333337,
                0.25246983408748114,
                -1
            ]
        ]
    },
    "What book that heavily influenced \u201cThe Matrix\u201d makes a cameo in the movie?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.5933739904400858,
                    0.248790601243953,
                    0.8049942500410712,
                    0.8431818181818183,
                    0.9901960784313726,
                    0,
                    1.0,
                    0.6391208791208791,
                    1
                ],
                [
                    0.36426569968683037,
                    0.7325501036627505,
                    0.14588467225234106,
                    0.030303030303030304,
                    0.0,
                    0,
                    0.0,
                    0.172002442002442,
                    1
                ],
                [
                    0.042360309873083896,
                    0.018659295093296474,
                    0.04912107770658781,
                    0.1265151515151515,
                    0.00980392156862745,
                    0,
                    0.0,
                    0.1888766788766789,
                    1
                ]
            ],
            "fraction_answers": {
                "simulacra & simulation": 0.20642942112962778,
                "neuromancer": 0.7313796596370257,
                "gravity's rainbow": 0.062190919233346575
            },
            "question": "what book that heavily influenced \u201cthe matrix\u201d makes a cameo in the movie?",
            "rate_limited": false,
            "answers": [
                "neuromancer",
                "simulacra & simulation",
                "gravity's rainbow"
            ],
            "ml_answers": {
                "simulacra & simulation": 0.4678792658095853,
                "neuromancer": 0.6788386686971437,
                "gravity's rainbow": 0.17345351917704982
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "simulacra & simulation": 1,
                "neuromancer": 6,
                "gravity's rainbow": 0
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    1080.0,
                    3180.0,
                    81.0
                ],
                "wikipedia_search": [
                    5.0590909090909095,
                    0.18181818181818182,
                    0.759090909090909
                ],
                "result_count_noun_chunks": [
                    49000.0,
                    8880.0,
                    2990.0
                ],
                "word_relation_to_question": [
                    3.195604395604396,
                    0.86001221001221,
                    0.9443833943833945
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    18.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    303.0,
                    0.0,
                    3.0
                ],
                "result_count": [
                    10800.0,
                    6630.0,
                    771.0
                ]
            },
            "z-best_answer_by_ml": [
                "neuromancer"
            ]
        },
        "lines": [
            [
                0,
                0.5933739904400858,
                0.248790601243953,
                0.8049942500410712,
                0.8431818181818183,
                0.9901960784313726,
                0,
                1.0,
                0.6391208791208791,
                1
            ],
            [
                1,
                0.36426569968683037,
                0.7325501036627505,
                0.14588467225234106,
                0.030303030303030304,
                0.0,
                0,
                0.0,
                0.172002442002442,
                1
            ],
            [
                0,
                0.042360309873083896,
                0.018659295093296474,
                0.04912107770658781,
                0.1265151515151515,
                0.00980392156862745,
                0,
                0.0,
                0.1888766788766789,
                1
            ]
        ]
    },
    "Which of these things was created by a person who chose to remain anonymous?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.9006774162770728,
                    0.9404849375459221,
                    0.6513409961685823,
                    0.8973684210526316,
                    0.7728174603174603,
                    0,
                    1.0,
                    0.6345454545454545,
                    0
                ],
                [
                    0.05982253601755558,
                    0.03054476750288653,
                    0.16365626710454298,
                    0.08947368421052632,
                    0.15575396825396826,
                    0,
                    0.0,
                    0.18545454545454546,
                    0
                ],
                [
                    0.039500047705371626,
                    0.028970294951191352,
                    0.18500273672687464,
                    0.013157894736842105,
                    0.07142857142857142,
                    0,
                    0.0,
                    0.18,
                    0
                ]
            ],
            "fraction_answers": {
                "hoverboards": 0.09781510979200359,
                "fidget spinners": 0.07400850650697872,
                "bitcoin": 0.8281763837010176
            },
            "question": "which of these things was created by a person who chose to remain anonymous?",
            "rate_limited": false,
            "answers": [
                "bitcoin",
                "hoverboards",
                "fidget spinners"
            ],
            "integer_answers": {
                "hoverboards": 0,
                "fidget spinners": 0,
                "bitcoin": 7
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "hoverboards": 0.13980366567215813,
                "fidget spinners": 0.1895649728058835,
                "bitcoin": 0.7609167030434091
            },
            "question_type": 0,
            "data": {
                "result_count_important_words": [
                    896000.0,
                    29100.0,
                    27600.0
                ],
                "wikipedia_search": [
                    3.5894736842105264,
                    0.35789473684210527,
                    0.05263157894736842
                ],
                "result_count_noun_chunks": [
                    1190000.0,
                    299000.0,
                    338000.0
                ],
                "word_relation_to_question": [
                    3.172727272727273,
                    0.9272727272727272,
                    0.9
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    21.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    779.0,
                    157.0,
                    72.0
                ],
                "result_count": [
                    944000.0,
                    62700.0,
                    41400.0
                ]
            },
            "z-best_answer_by_ml": [
                "bitcoin"
            ]
        },
        "lines": [
            [
                1,
                0.9006774162770728,
                0.9404849375459221,
                0.6513409961685823,
                0.8973684210526316,
                0.7728174603174603,
                0,
                1.0,
                0.6345454545454545,
                0
            ],
            [
                0,
                0.05982253601755558,
                0.03054476750288653,
                0.16365626710454298,
                0.08947368421052632,
                0.15575396825396826,
                0,
                0.0,
                0.18545454545454546,
                0
            ],
            [
                0,
                0.039500047705371626,
                0.028970294951191352,
                0.18500273672687464,
                0.013157894736842105,
                0.07142857142857142,
                0,
                0.0,
                0.18,
                0
            ]
        ]
    },
    "In which town were a President, Governor, Senator, NFL owner and late night host all born?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.6736904585250038,
                    0.24619960343688038,
                    0.23523685918234913,
                    0.38285929032272115,
                    0.2777777777777778,
                    0,
                    0,
                    0.20675162062046612,
                    -1
                ],
                [
                    0.011722807538210417,
                    0.023463317911434238,
                    0.02498377676833225,
                    0.17637664562531583,
                    0.16666666666666666,
                    0,
                    0,
                    0.2880894042290477,
                    -1
                ],
                [
                    0.3145867339367859,
                    0.7303370786516854,
                    0.7397793640493187,
                    0.44076406405196294,
                    0.5555555555555556,
                    0,
                    0,
                    0.5051589751504861,
                    -1
                ]
            ],
            "fraction_answers": {
                "muncie, in": 0.5476969618992991,
                "hope, ar": 0.11521710312316785,
                "brookline, ma": 0.33708593497753303
            },
            "question": "in which town were a president, governor, senator, nfl owner and late night host all born?",
            "rate_limited": false,
            "answers": [
                "brookline, ma",
                "hope, ar",
                "muncie, in"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "muncie, in": 0.32104219078713936,
                "hope, ar": 0.1673487251384288,
                "brookline, ma": 0.4557999605182272
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    745.0,
                    71.0,
                    2210.0
                ],
                "wikipedia_search": [
                    3.062874322581769,
                    1.4110131650025266,
                    3.5261125124157036
                ],
                "result_count_noun_chunks": [
                    725.0,
                    77.0,
                    2280.0
                ],
                "word_relation_to_question": [
                    1.654012964963729,
                    2.3047152338323817,
                    4.041271801203889
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    4540.0,
                    79.0,
                    2120.0
                ],
                "word_count_appended": [
                    5.0,
                    3.0,
                    10.0
                ]
            },
            "z-best_answer_by_ml": [
                "brookline, ma"
            ],
            "integer_answers": {
                "muncie, in": 5,
                "hope, ar": 0,
                "brookline, ma": 1
            }
        },
        "lines": [
            [
                1,
                0.6736904585250038,
                0.24619960343688038,
                0.23523685918234913,
                0.38285929032272115,
                0.2777777777777778,
                0,
                0,
                0.20675162062046612,
                -1
            ],
            [
                0,
                0.011722807538210417,
                0.023463317911434238,
                0.02498377676833225,
                0.17637664562531583,
                0.16666666666666666,
                0,
                0,
                0.2880894042290477,
                -1
            ],
            [
                0,
                0.3145867339367859,
                0.7303370786516854,
                0.7397793640493187,
                0.44076406405196294,
                0.5555555555555556,
                0,
                0,
                0.5051589751504861,
                -1
            ]
        ]
    },
    "Though it now conveys something different, which of these words originally meant \u201cparrot\u201d?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.3409090909090909,
                    0.4908616187989556,
                    0.25743443551402884,
                    0.0,
                    0.34831460674157305,
                    0,
                    0.0,
                    0.11981566820276497,
                    -1
                ],
                [
                    0.22237076648841356,
                    0.3028720626631854,
                    0.7424951087457252,
                    0.041666666666666664,
                    0.29775280898876405,
                    0,
                    0.0,
                    0.31163594470046085,
                    -1
                ],
                [
                    0.43672014260249553,
                    0.206266318537859,
                    7.045574024594473e-05,
                    0.9583333333333334,
                    0.3539325842696629,
                    0,
                    1.0,
                    0.5685483870967742,
                    -1
                ]
            ],
            "fraction_answers": {
                "thespian": 0.2741133368933165,
                "popinjay": 0.5034101745114816,
                "warble": 0.22247648859520192
            },
            "question": "though it now conveys something different, which of these words originally meant \u201cparrot\u201d?",
            "rate_limited": false,
            "answers": [
                "warble",
                "thespian",
                "popinjay"
            ],
            "ml_answers": {
                "thespian": 0.21908477526450548,
                "popinjay": 0.8103853293271703,
                "warble": 0.4897934443769495
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "thespian": 1,
                "popinjay": 5,
                "warble": 1
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    18800.0,
                    11600.0,
                    7900.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.125,
                    2.875
                ],
                "result_count_noun_chunks": [
                    285000.0,
                    822000.0,
                    78.0
                ],
                "word_relation_to_question": [
                    0.4792626728110599,
                    1.2465437788018434,
                    2.274193548387097
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count": [
                    15300.0,
                    9980.0,
                    19600.0
                ],
                "word_count_appended": [
                    124.0,
                    106.0,
                    126.0
                ]
            },
            "z-best_answer_by_ml": [
                "popinjay"
            ]
        },
        "lines": [
            [
                0,
                0.3409090909090909,
                0.4908616187989556,
                0.25743443551402884,
                0.0,
                0.34831460674157305,
                0,
                0.0,
                0.11981566820276497,
                -1
            ],
            [
                0,
                0.22237076648841356,
                0.3028720626631854,
                0.7424951087457252,
                0.041666666666666664,
                0.29775280898876405,
                0,
                0.0,
                0.31163594470046085,
                -1
            ],
            [
                1,
                0.43672014260249553,
                0.206266318537859,
                7.045574024594473e-05,
                0.9583333333333334,
                0.3539325842696629,
                0,
                1.0,
                0.5685483870967742,
                -1
            ]
        ]
    },
    "What gargantuan fruit is the subject of a Roald Dahl children's book?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.003573023671281822,
                    0.00115524748955834,
                    0.0044603033006244425,
                    0.18108318034175963,
                    0.25,
                    0.0,
                    0.0,
                    0.20537535699714404,
                    1
                ],
                [
                    0.004912907548012505,
                    0.003554607660179508,
                    0.0053523639607493305,
                    0.33319799463352634,
                    0.11290322580645161,
                    0.1,
                    0.2,
                    0.25844553243574053,
                    1
                ],
                [
                    0.9915140687807057,
                    0.9952901448502621,
                    0.9901873327386262,
                    0.48571882502471403,
                    0.6370967741935484,
                    0.9,
                    0.8,
                    0.5361791105671154,
                    1
                ]
            ],
            "fraction_answers": {
                "loquat": 0.1272958290055825,
                "dragonfruit": 0.08070588897504606,
                "peach": 0.7919982820193715
            },
            "question": "what gargantuan fruit is the subject of a roald dahl children's book?",
            "rate_limited": false,
            "answers": [
                "dragonfruit",
                "loquat",
                "peach"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "loquat": 0,
                "dragonfruit": 0,
                "peach": 8
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    13.0,
                    40.0,
                    11200.0
                ],
                "wikipedia_search": [
                    0.7243327213670385,
                    1.3327919785341054,
                    1.9428753000988561
                ],
                "result_count_noun_chunks": [
                    10.0,
                    12.0,
                    2220.0
                ],
                "word_relation_to_question": [
                    0.8215014279885762,
                    1.0337821297429621,
                    2.1447164422684617
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    9.0
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    4.0
                ],
                "result_count": [
                    8.0,
                    11.0,
                    2220.0
                ],
                "word_count_appended": [
                    62.0,
                    28.0,
                    158.0
                ]
            },
            "z-best_answer_by_ml": [
                "peach"
            ],
            "ml_answers": {
                "loquat": 0.09979309482616147,
                "dragonfruit": 0.11082712897212879,
                "peach": 0.5604963532906949
            }
        },
        "lines": [
            [
                0,
                0.003573023671281822,
                0.00115524748955834,
                0.0044603033006244425,
                0.18108318034175963,
                0.25,
                0.0,
                0.0,
                0.20537535699714404,
                1
            ],
            [
                0,
                0.004912907548012505,
                0.003554607660179508,
                0.0053523639607493305,
                0.33319799463352634,
                0.11290322580645161,
                0.1,
                0.2,
                0.25844553243574053,
                1
            ],
            [
                1,
                0.9915140687807057,
                0.9952901448502621,
                0.9901873327386262,
                0.48571882502471403,
                0.6370967741935484,
                0.9,
                0.8,
                0.5361791105671154,
                1
            ]
        ]
    },
    "The only person who owns more U.S. land than Ted Turner made his fortune in what business?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.08648244958924571,
                    0.43359375,
                    0.11808004907222819,
                    0.33905134292231065,
                    0.8769230769230769,
                    0,
                    0,
                    0.14206282175032175,
                    1
                ],
                [
                    0.0666168782673637,
                    0.369140625,
                    0.8372948934212544,
                    0.3952115626309175,
                    0.07692307692307693,
                    0,
                    0,
                    0.5095941924066925,
                    1
                ],
                [
                    0.8469006721433906,
                    0.197265625,
                    0.04462505750651741,
                    0.26573709444677185,
                    0.046153846153846156,
                    0,
                    0,
                    0.34834298584298584,
                    1
                ]
            ],
            "fraction_answers": {
                "pharmaceuticals": 0.33269891504286386,
                "cable tv": 0.3757968714415509,
                "fast food": 0.29150421351558536
            },
            "question": "the only person who owns more u.s. land than ted turner made his fortune in what business?",
            "rate_limited": false,
            "answers": [
                "pharmaceuticals",
                "cable tv",
                "fast food"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "pharmaceuticals": 2,
                "cable tv": 3,
                "fast food": 1
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    2220000.0,
                    1890000.0,
                    1010000.0
                ],
                "wikipedia_search": [
                    1.6952567146115534,
                    1.9760578131545874,
                    1.3286854722338592
                ],
                "result_count_noun_chunks": [
                    770000.0,
                    5460000.0,
                    291000.0
                ],
                "word_relation_to_question": [
                    0.8523769305019305,
                    3.0575651544401548,
                    2.090057915057915
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    579000.0,
                    446000.0,
                    5670000.0
                ],
                "word_count_appended": [
                    171.0,
                    15.0,
                    9.0
                ]
            },
            "z-best_answer_by_ml": [
                "fast food"
            ],
            "ml_answers": {
                "pharmaceuticals": 0.27645801300583,
                "cable tv": 0.31175066580746624,
                "fast food": 0.36591565463690273
            }
        },
        "lines": [
            [
                0,
                0.08648244958924571,
                0.43359375,
                0.11808004907222819,
                0.33905134292231065,
                0.8769230769230769,
                0,
                0,
                0.14206282175032175,
                1
            ],
            [
                1,
                0.0666168782673637,
                0.369140625,
                0.8372948934212544,
                0.3952115626309175,
                0.07692307692307693,
                0,
                0,
                0.5095941924066925,
                1
            ],
            [
                0,
                0.8469006721433906,
                0.197265625,
                0.04462505750651741,
                0.26573709444677185,
                0.046153846153846156,
                0,
                0,
                0.34834298584298584,
                1
            ]
        ]
    },
    "Which game is an example of combinatorics?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.9182623018324069,
                    0.901010101010101,
                    0.9065048359567609,
                    0.14516129032258066,
                    0.3037716615698267,
                    0,
                    0,
                    0.6093418259023354,
                    -1
                ],
                [
                    0.02347127856701668,
                    0.03333333333333333,
                    0.02920538592831405,
                    0.3544142614601019,
                    0.07543323139653414,
                    0,
                    0,
                    0.13588110403397027,
                    -1
                ],
                [
                    0.058266419600576484,
                    0.06565656565656566,
                    0.06428977811492509,
                    0.5004244482173175,
                    0.6207951070336392,
                    0,
                    0,
                    0.25477707006369427,
                    -1
                ]
            ],
            "fraction_answers": {
                "sudoku": 0.26070156478111967,
                "risk": 0.6306753360990018,
                "crossword puzzles": 0.1086230991198784
            },
            "question": "which game is an example of combinatorics?",
            "rate_limited": false,
            "answers": [
                "risk",
                "crossword puzzles",
                "sudoku"
            ],
            "integer_answers": {
                "sudoku": 2,
                "risk": 4,
                "crossword puzzles": 0
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "sudoku": 0.23188621270258894,
                "risk": 0.06749324500677205,
                "crossword puzzles": 0.15670631515779096
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    446000.0,
                    16500.0,
                    32500.0
                ],
                "wikipedia_search": [
                    0.2903225806451613,
                    0.7088285229202038,
                    1.000848896434635
                ],
                "result_count_noun_chunks": [
                    478000.0,
                    15400.0,
                    33900.0
                ],
                "word_relation_to_question": [
                    1.2186836518046709,
                    0.27176220806794055,
                    0.5095541401273885
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    298.0,
                    74.0,
                    609.0
                ],
                "result_count": [
                    446000.0,
                    11400.0,
                    28300.0
                ]
            },
            "z-best_answer_by_ml": [
                "sudoku"
            ]
        },
        "lines": [
            [
                0,
                0.9182623018324069,
                0.901010101010101,
                0.9065048359567609,
                0.14516129032258066,
                0.3037716615698267,
                0,
                0,
                0.6093418259023354,
                -1
            ],
            [
                0,
                0.02347127856701668,
                0.03333333333333333,
                0.02920538592831405,
                0.3544142614601019,
                0.07543323139653414,
                0,
                0,
                0.13588110403397027,
                -1
            ],
            [
                1,
                0.058266419600576484,
                0.06565656565656566,
                0.06428977811492509,
                0.5004244482173175,
                0.6207951070336392,
                0,
                0,
                0.25477707006369427,
                -1
            ]
        ]
    },
    "Queen Victoria is credited with starting what fashion trend?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.336734693877551,
                    0.3333333333333333,
                    0.3099201065246338,
                    0.5936507936507937,
                    0.25316455696202533,
                    0,
                    0.0,
                    0.16086538461538463,
                    1
                ],
                [
                    0.19387755102040816,
                    0.329004329004329,
                    0.4793608521970706,
                    0.11904761904761905,
                    0.189873417721519,
                    0,
                    0.0,
                    0.10317307692307694,
                    1
                ],
                [
                    0.46938775510204084,
                    0.33766233766233766,
                    0.2107190412782956,
                    0.2873015873015873,
                    0.5569620253164557,
                    0,
                    1.0,
                    0.7359615384615384,
                    1
                ]
            ],
            "fraction_answers": {
                "white wedding dress": 0.5139991835888936,
                "mini dress": 0.28395269556624597,
                "little black dress": 0.2020481208448604
            },
            "question": "queen victoria is credited with starting what fashion trend?",
            "rate_limited": false,
            "answers": [
                "mini dress",
                "little black dress",
                "white wedding dress"
            ],
            "ml_answers": {
                "white wedding dress": 0.713087299003893,
                "mini dress": 0.19859246635688194,
                "little black dress": 0.15780690901300734
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "white wedding dress": 5,
                "mini dress": 1,
                "little black dress": 1
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    77.0,
                    76.0,
                    78.0
                ],
                "wikipedia_search": [
                    2.3746031746031746,
                    0.4761904761904762,
                    1.1492063492063491
                ],
                "result_count_noun_chunks": [
                    931000.0,
                    1440000.0,
                    633000.0
                ],
                "word_relation_to_question": [
                    0.8043269230769231,
                    0.5158653846153847,
                    3.6798076923076923
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    8.0
                ],
                "word_count_appended": [
                    20.0,
                    15.0,
                    44.0
                ],
                "result_count": [
                    33.0,
                    19.0,
                    46.0
                ]
            },
            "z-best_answer_by_ml": [
                "white wedding dress"
            ]
        },
        "lines": [
            [
                0,
                0.336734693877551,
                0.3333333333333333,
                0.3099201065246338,
                0.5936507936507937,
                0.25316455696202533,
                0,
                0.0,
                0.16086538461538463,
                1
            ],
            [
                0,
                0.19387755102040816,
                0.329004329004329,
                0.4793608521970706,
                0.11904761904761905,
                0.189873417721519,
                0,
                0.0,
                0.10317307692307694,
                1
            ],
            [
                1,
                0.46938775510204084,
                0.33766233766233766,
                0.2107190412782956,
                0.2873015873015873,
                0.5569620253164557,
                0,
                1.0,
                0.7359615384615384,
                1
            ]
        ]
    },
    "According to the old saying, what kind of animal can NOT change its spots?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.4789315274642588,
                    0.47848948374760997,
                    0.4713839750260146,
                    0.4863121874177415,
                    0.36457473162675474,
                    0.4708029197080292,
                    0.5,
                    0.35655053529305025,
                    1
                ],
                [
                    0.4800601956358164,
                    0.35930528999362654,
                    0.4708636836628512,
                    0.3476472195281712,
                    0.3063583815028902,
                    0.08029197080291972,
                    0.012345679012345678,
                    0.2619488296135003,
                    1
                ],
                [
                    0.041008276899924756,
                    0.16220522625876355,
                    0.057752341311134214,
                    0.1660405930540873,
                    0.32906688687035507,
                    0.4489051094890511,
                    0.4876543209876543,
                    0.38150063509344945,
                    1
                ]
            ],
            "fraction_answers": {
                "tiger": 0.48146665250889503,
                "leopard": 0.4202946875619697,
                "zebra": 0.09823865992913523
            },
            "question": "according to the old saying, what kind of animal can not change its spots?",
            "rate_limited": false,
            "answers": [
                "zebra",
                "leopard",
                "tiger"
            ],
            "ml_answers": {
                "tiger": 0.2238531646605841,
                "leopard": 0.44114659925129657,
                "zebra": 0.5095537363533316
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "tiger": 4,
                "leopard": 4,
                "zebra": 0
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    1350000.0,
                    8830000.0,
                    21200000.0
                ],
                "wikipedia_search": [
                    0.16425375098710188,
                    1.8282333656619458,
                    4.007512883350953
                ],
                "result_count_noun_chunks": [
                    1100000.0,
                    1120000.0,
                    17000000.0
                ],
                "word_relation_to_question": [
                    1.7213935764833967,
                    2.856614044637997,
                    1.4219923788786064
                ],
                "word_count_noun_chunks": [
                    8.0,
                    115.0,
                    14.0
                ],
                "word_count_raw": [
                    0.0,
                    79.0,
                    2.0
                ],
                "word_count_appended": [
                    328.0,
                    469.0,
                    414.0
                ],
                "result_count": [
                    1120000.0,
                    1060000.0,
                    24400000.0
                ]
            },
            "z-best_answer_by_ml": [
                "zebra"
            ]
        },
        "lines": [
            [
                0,
                0.4789315274642588,
                0.47848948374760997,
                0.4713839750260146,
                0.4863121874177415,
                0.36457473162675474,
                0.4708029197080292,
                0.5,
                0.35655053529305025,
                1
            ],
            [
                1,
                0.4800601956358164,
                0.35930528999362654,
                0.4708636836628512,
                0.3476472195281712,
                0.3063583815028902,
                0.08029197080291972,
                0.012345679012345678,
                0.2619488296135003,
                1
            ],
            [
                0,
                0.041008276899924756,
                0.16220522625876355,
                0.057752341311134214,
                0.1660405930540873,
                0.32906688687035507,
                0.4489051094890511,
                0.4876543209876543,
                0.38150063509344945,
                1
            ]
        ]
    },
    "Who defeated Napoleon at the Battle of Waterloo?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.009013282732447819,
                    0.000336965067954622,
                    0.00022366360993066427,
                    0.0375,
                    0.08796296296296297,
                    0.0,
                    0.0,
                    0.0,
                    0
                ],
                [
                    0.9894280292762266,
                    0.9163085181222178,
                    0.9823656593033098,
                    0.9625,
                    0.44907407407407407,
                    1.0,
                    1.0,
                    0.9679694727104533,
                    0
                ],
                [
                    0.0015586879913255624,
                    0.08335451680982756,
                    0.017410677086759554,
                    0.0,
                    0.46296296296296297,
                    0.0,
                    0.0,
                    0.03203052728954672,
                    0
                ]
            ],
            "fraction_answers": {
                "jack skellington": 0.016879609296662008,
                "beef wellington": 0.0746646715175528,
                "the duke of wellington": 0.9084557191857853
            },
            "question": "who defeated napoleon at the battle of waterloo?",
            "rate_limited": false,
            "answers": [
                "jack skellington",
                "the duke of wellington",
                "beef wellington"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "jack skellington": 0,
                "beef wellington": 1,
                "the duke of wellington": 7
            },
            "question_type": 0,
            "data": {
                "result_count_important_words": [
                    57.0,
                    155000.0,
                    14100.0
                ],
                "wikipedia_search": [
                    0.15,
                    3.85,
                    0.0
                ],
                "result_count_noun_chunks": [
                    51.0,
                    224000.0,
                    3970.0
                ],
                "word_relation_to_question": [
                    0.0,
                    3.8718778908418128,
                    0.12812210915818686
                ],
                "word_count_noun_chunks": [
                    0.0,
                    14.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    13.0,
                    0.0
                ],
                "result_count": [
                    13300.0,
                    1460000.0,
                    2300.0
                ],
                "word_count_appended": [
                    19.0,
                    97.0,
                    100.0
                ]
            },
            "z-best_answer_by_ml": [
                "the duke of wellington"
            ],
            "ml_answers": {
                "jack skellington": 0.10554678524095656,
                "beef wellington": 0.3274034884342619,
                "the duke of wellington": 0.9694356029265825
            }
        },
        "lines": [
            [
                0,
                0.009013282732447819,
                0.000336965067954622,
                0.00022366360993066427,
                0.0375,
                0.08796296296296297,
                0.0,
                0.0,
                0.0,
                0
            ],
            [
                1,
                0.9894280292762266,
                0.9163085181222178,
                0.9823656593033098,
                0.9625,
                0.44907407407407407,
                1.0,
                1.0,
                0.9679694727104533,
                0
            ],
            [
                0,
                0.0015586879913255624,
                0.08335451680982756,
                0.017410677086759554,
                0.0,
                0.46296296296296297,
                0.0,
                0.0,
                0.03203052728954672,
                0
            ]
        ]
    },
    "What generation of the iPod was the first to offer video?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.497787610619469,
                    0.47260600285850407,
                    0.4297994269340974,
                    0.5,
                    0.2459016393442623,
                    0.0,
                    0.0,
                    0.23331828708246655,
                    1
                ],
                [
                    0.25,
                    0.017627441638875654,
                    0.31614135625596945,
                    0.5,
                    0.5355191256830601,
                    1.0,
                    1.0,
                    0.5074691386473428,
                    1
                ],
                [
                    0.252212389380531,
                    0.5097665555026203,
                    0.25405921680993315,
                    0.0,
                    0.2185792349726776,
                    0.0,
                    0.0,
                    0.2592125742701907,
                    1
                ]
            ],
            "fraction_answers": {
                "third generation": 0.29742662085484994,
                "u2 special edition": 0.5158446327781561,
                "fifth generation": 0.1867287463669941
            },
            "question": "what generation of the ipod was the first to offer video?",
            "rate_limited": false,
            "answers": [
                "third generation",
                "u2 special edition",
                "fifth generation"
            ],
            "ml_answers": {
                "third generation": 0.1527874030808538,
                "u2 special edition": 0.8187049623961608,
                "fifth generation": 0.4492745548733527
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "third generation": 3,
                "u2 special edition": 4,
                "fifth generation": 1
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    992000.0,
                    37000.0,
                    1070000.0
                ],
                "wikipedia_search": [
                    1.5,
                    1.5,
                    0.0
                ],
                "result_count_noun_chunks": [
                    450000.0,
                    331000.0,
                    266000.0
                ],
                "word_relation_to_question": [
                    0.9332731483298662,
                    2.029876554589371,
                    1.0368502970807627
                ],
                "word_count_noun_chunks": [
                    0.0,
                    2.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    3.0,
                    0.0
                ],
                "word_count_appended": [
                    45.0,
                    98.0,
                    40.0
                ],
                "result_count": [
                    225000.0,
                    113000.0,
                    114000.0
                ]
            },
            "z-best_answer_by_ml": [
                "u2 special edition"
            ]
        },
        "lines": [
            [
                0,
                0.497787610619469,
                0.47260600285850407,
                0.4297994269340974,
                0.5,
                0.2459016393442623,
                0.0,
                0.0,
                0.23331828708246655,
                1
            ],
            [
                0,
                0.25,
                0.017627441638875654,
                0.31614135625596945,
                0.5,
                0.5355191256830601,
                1.0,
                1.0,
                0.5074691386473428,
                1
            ],
            [
                1,
                0.252212389380531,
                0.5097665555026203,
                0.25405921680993315,
                0.0,
                0.2185792349726776,
                0.0,
                0.0,
                0.2592125742701907,
                1
            ]
        ]
    },
    "Which of these is NOT a geometric shape?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.41556728232189977,
                    0.41001028453890986,
                    0.29144805876180485,
                    0.35,
                    0.3330188679245283,
                    0.3125,
                    0.5,
                    0.425,
                    -1
                ],
                [
                    0.49076517150395776,
                    0.4885155982173466,
                    0.48399790136411336,
                    0.4,
                    0.3872641509433962,
                    0.5,
                    0.5,
                    0.5,
                    -1
                ],
                [
                    0.09366754617414247,
                    0.10147411724374356,
                    0.22455403987408187,
                    0.25,
                    0.27971698113207544,
                    0.1875,
                    0.0,
                    0.07500000000000001,
                    -1
                ]
            ],
            "fraction_answers": {
                "hexagon": 0.6970218288939891,
                "octagon": 0.24061387661321432,
                "tarragon": 0.06236429449279652
            },
            "question": "which of these is not a geometric shape?",
            "rate_limited": false,
            "answers": [
                "octagon",
                "tarragon",
                "hexagon"
            ],
            "integer_answers": {
                "hexagon": 8,
                "octagon": 0,
                "tarragon": 0
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "hexagon": 0.023530557765317026,
                "octagon": 0.45403619938902473,
                "tarragon": 0.5868025643459921
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    1050000.0,
                    134000.0,
                    4650000.0
                ],
                "wikipedia_search": [
                    0.3,
                    0.2,
                    0.5
                ],
                "result_count_noun_chunks": [
                    1590000.0,
                    122000.0,
                    2100000.0
                ],
                "word_relation_to_question": [
                    0.3,
                    0.0,
                    1.7
                ],
                "word_count_noun_chunks": [
                    3.0,
                    0.0,
                    5.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    5.0
                ],
                "word_count_appended": [
                    354.0,
                    239.0,
                    467.0
                ],
                "result_count": [
                    1280000.0,
                    140000.0,
                    6160000.0
                ]
            },
            "z-best_answer_by_ml": [
                "tarragon"
            ]
        },
        "lines": [
            [
                0,
                0.41556728232189977,
                0.41001028453890986,
                0.29144805876180485,
                0.35,
                0.3330188679245283,
                0.3125,
                0.5,
                0.425,
                -1
            ],
            [
                1,
                0.49076517150395776,
                0.4885155982173466,
                0.48399790136411336,
                0.4,
                0.3872641509433962,
                0.5,
                0.5,
                0.5,
                -1
            ],
            [
                0,
                0.09366754617414247,
                0.10147411724374356,
                0.22455403987408187,
                0.25,
                0.27971698113207544,
                0.1875,
                0.0,
                0.07500000000000001,
                -1
            ]
        ]
    },
    "What tech mogul became a billionaire the youngest?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.08726899383983573,
                    0.13887427669647553,
                    0.04895461499235084,
                    0.4163832199546485,
                    0.3875,
                    0.0,
                    0.4090909090909091,
                    0.5765766885398025,
                    1
                ],
                [
                    0.6724845995893224,
                    0.6102051551814834,
                    0.8669046404895462,
                    0.34268707482993194,
                    0.37083333333333335,
                    1.0,
                    0.5454545454545454,
                    0.3067500165705574,
                    1
                ],
                [
                    0.2402464065708419,
                    0.25092056812204105,
                    0.08414074451810301,
                    0.24092970521541948,
                    0.24166666666666667,
                    0.0,
                    0.045454545454545456,
                    0.11667329488964008,
                    1
                ]
            ],
            "fraction_answers": {
                "mark zuckerberg": 0.58941492068109,
                "evan spiegel": 0.2580810878892527,
                "larry page": 0.1525039914296572
            },
            "question": "what tech mogul became a billionaire the youngest?",
            "rate_limited": false,
            "answers": [
                "evan spiegel",
                "mark zuckerberg",
                "larry page"
            ],
            "integer_answers": {
                "mark zuckerberg": 5,
                "evan spiegel": 3,
                "larry page": 0
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "mark zuckerberg": 0.685239407342589,
                "evan spiegel": 0.4067904729342754,
                "larry page": 0.2904337672074828
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    26400.0,
                    116000.0,
                    47700.0
                ],
                "wikipedia_search": [
                    1.2491496598639455,
                    1.0280612244897958,
                    0.7227891156462585
                ],
                "result_count_noun_chunks": [
                    9600.0,
                    170000.0,
                    16500.0
                ],
                "word_relation_to_question": [
                    2.30630675415921,
                    1.2270000662822296,
                    0.4666931795585603
                ],
                "word_count_noun_chunks": [
                    0.0,
                    10.0,
                    0.0
                ],
                "word_count_raw": [
                    9.0,
                    12.0,
                    1.0
                ],
                "word_count_appended": [
                    93.0,
                    89.0,
                    58.0
                ],
                "result_count": [
                    17000.0,
                    131000.0,
                    46800.0
                ]
            },
            "z-best_answer_by_ml": [
                "mark zuckerberg"
            ]
        },
        "lines": [
            [
                0,
                0.08726899383983573,
                0.13887427669647553,
                0.04895461499235084,
                0.4163832199546485,
                0.3875,
                0.0,
                0.4090909090909091,
                0.5765766885398025,
                1
            ],
            [
                1,
                0.6724845995893224,
                0.6102051551814834,
                0.8669046404895462,
                0.34268707482993194,
                0.37083333333333335,
                1.0,
                0.5454545454545454,
                0.3067500165705574,
                1
            ],
            [
                0,
                0.2402464065708419,
                0.25092056812204105,
                0.08414074451810301,
                0.24092970521541948,
                0.24166666666666667,
                0.0,
                0.045454545454545456,
                0.11667329488964008,
                1
            ]
        ]
    },
    "Which of these is NOT a marsupial?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.4922487151402814,
                    0.453623670212766,
                    0.4548689744419282,
                    0.5,
                    0.47118506493506496,
                    0.5,
                    0.5,
                    0.5,
                    -1
                ],
                [
                    0.46482433229421183,
                    0.45861037234042556,
                    0.4560012940795859,
                    0.5,
                    0.2922077922077922,
                    0.4772727272727273,
                    0.3666666666666667,
                    0.2164179104477612,
                    -1
                ],
                [
                    0.04292695256550677,
                    0.0877659574468085,
                    0.08912973147848591,
                    0.0,
                    0.23660714285714285,
                    0.022727272727272707,
                    0.13333333333333336,
                    0.28358208955223885,
                    -1
                ]
            ],
            "fraction_answers": {
                "cuscus": 0.19199972617270733,
                "quintana roo": 0.03201839381748988,
                "wombat": 0.7759818800098027
            },
            "question": "which of these is not a marsupial?",
            "rate_limited": false,
            "answers": [
                "quintana roo",
                "cuscus",
                "wombat"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "cuscus": 1,
                "quintana roo": 0,
                "wombat": 7
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    55800.0,
                    49800.0,
                    496000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_noun_chunks": [
                    55800.0,
                    54400.0,
                    508000.0
                ],
                "word_relation_to_question": [
                    0.0,
                    0.5671641791044776,
                    0.43283582089552236
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    21.0
                ],
                "word_count_raw": [
                    0.0,
                    4.0,
                    11.0
                ],
                "word_count_appended": [
                    71.0,
                    512.0,
                    649.0
                ],
                "result_count": [
                    36800.0,
                    167000.0,
                    2170000.0
                ]
            },
            "z-best_answer_by_ml": [
                "quintana roo"
            ],
            "ml_answers": {
                "cuscus": 0.4521887882864278,
                "quintana roo": 0.5995435019724512,
                "wombat": 0.14081243861902404
            }
        },
        "lines": [
            [
                1,
                0.4922487151402814,
                0.453623670212766,
                0.4548689744419282,
                0.5,
                0.47118506493506496,
                0.5,
                0.5,
                0.5,
                -1
            ],
            [
                0,
                0.46482433229421183,
                0.45861037234042556,
                0.4560012940795859,
                0.5,
                0.2922077922077922,
                0.4772727272727273,
                0.3666666666666667,
                0.2164179104477612,
                -1
            ],
            [
                0,
                0.04292695256550677,
                0.0877659574468085,
                0.08912973147848591,
                0.0,
                0.23660714285714285,
                0.022727272727272707,
                0.13333333333333336,
                0.28358208955223885,
                -1
            ]
        ]
    },
    "Which human sense is most closely associated with the bony labyrinth?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.3405336721728081,
                    0.31464475592073465,
                    0.38633741057004384,
                    0.3904761904761905,
                    0.3086816720257235,
                    0.36470588235294116,
                    0.024390243902439025,
                    0.3387387387387387,
                    -1
                ],
                [
                    0.301143583227446,
                    0.3243112614789754,
                    0.041310870066928226,
                    0.5,
                    0.40353697749196143,
                    0.29411764705882354,
                    0.926829268292683,
                    0.49909909909909916,
                    -1
                ],
                [
                    0.3583227445997459,
                    0.36104398260028997,
                    0.572351719363028,
                    0.10952380952380951,
                    0.2877813504823151,
                    0.3411764705882353,
                    0.04878048780487805,
                    0.1621621621621622,
                    -1
                ]
            ],
            "fraction_answers": {
                "touch": 0.3085635707699524,
                "sight": 0.280142840890558,
                "hearing": 0.4112935883394896
            },
            "question": "which human sense is most closely associated with the bony labyrinth?",
            "rate_limited": false,
            "answers": [
                "touch",
                "hearing",
                "sight"
            ],
            "ml_answers": {
                "touch": 0.15074245362831734,
                "sight": 0.08186756214592845,
                "hearing": 0.7098283180072342
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "touch": 1,
                "sight": 3,
                "hearing": 4
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    65100.0,
                    67100.0,
                    74700.0
                ],
                "wikipedia_search": [
                    1.9523809523809523,
                    2.5,
                    0.5476190476190476
                ],
                "result_count_noun_chunks": [
                    83700000.0,
                    8950000.0,
                    124000000.0
                ],
                "word_relation_to_question": [
                    1.016216216216216,
                    1.4972972972972973,
                    0.4864864864864865
                ],
                "word_count_noun_chunks": [
                    31.0,
                    25.0,
                    29.0
                ],
                "word_count_raw": [
                    1.0,
                    38.0,
                    2.0
                ],
                "word_count_appended": [
                    192.0,
                    251.0,
                    179.0
                ],
                "result_count": [
                    53600.0,
                    47400.0,
                    56400.0
                ]
            },
            "z-best_answer_by_ml": [
                "hearing"
            ]
        },
        "lines": [
            [
                0,
                0.3405336721728081,
                0.31464475592073465,
                0.38633741057004384,
                0.3904761904761905,
                0.3086816720257235,
                0.36470588235294116,
                0.024390243902439025,
                0.3387387387387387,
                -1
            ],
            [
                1,
                0.301143583227446,
                0.3243112614789754,
                0.041310870066928226,
                0.5,
                0.40353697749196143,
                0.29411764705882354,
                0.926829268292683,
                0.49909909909909916,
                -1
            ],
            [
                0,
                0.3583227445997459,
                0.36104398260028997,
                0.572351719363028,
                0.10952380952380951,
                0.2877813504823151,
                0.3411764705882353,
                0.04878048780487805,
                0.1621621621621622,
                -1
            ]
        ]
    },
    "Catching a catfish with your bare hands is called what?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.15516457832889696,
                    0.6795849021948756,
                    0.6009666794986954,
                    0.06666666666666667,
                    0.19572368421052633,
                    0.0,
                    0.0,
                    0.0564516129032258,
                    1
                ],
                [
                    0.0008439914302408621,
                    0.002050999479597147,
                    0.001240429445228624,
                    0.06666666666666667,
                    0.05592105263157895,
                    0.0,
                    0.0,
                    0.075,
                    1
                ],
                [
                    0.8439914302408622,
                    0.3183640983255273,
                    0.39779289105607596,
                    0.8666666666666666,
                    0.7483552631578947,
                    1.0,
                    1.0,
                    0.8685483870967742,
                    1
                ]
            ],
            "fraction_answers": {
                "noodling": 0.7554648420679752,
                "whiskering": 0.02521539245666403,
                "strumming": 0.21931976547536083
            },
            "question": "catching a catfish with your bare hands is called what?",
            "rate_limited": false,
            "answers": [
                "strumming",
                "whiskering",
                "noodling"
            ],
            "ml_answers": {
                "noodling": 0.8547383047801711,
                "whiskering": 0.12720443928359632,
                "strumming": 0.2678872446452576
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "noodling": 6,
                "whiskering": 0,
                "strumming": 2
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    22200.0,
                    67.0,
                    10400.0
                ],
                "wikipedia_search": [
                    0.3333333333333333,
                    0.3333333333333333,
                    4.333333333333333
                ],
                "result_count_noun_chunks": [
                    28100.0,
                    58.0,
                    18600.0
                ],
                "word_relation_to_question": [
                    0.282258064516129,
                    0.375,
                    4.342741935483871
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    126.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    203.0
                ],
                "result_count": [
                    7170.0,
                    39.0,
                    39000.0
                ],
                "word_count_appended": [
                    119.0,
                    34.0,
                    455.0
                ]
            },
            "z-best_answer_by_ml": [
                "noodling"
            ]
        },
        "lines": [
            [
                0,
                0.15516457832889696,
                0.6795849021948756,
                0.6009666794986954,
                0.06666666666666667,
                0.19572368421052633,
                0.0,
                0.0,
                0.0564516129032258,
                1
            ],
            [
                0,
                0.0008439914302408621,
                0.002050999479597147,
                0.001240429445228624,
                0.06666666666666667,
                0.05592105263157895,
                0.0,
                0.0,
                0.075,
                1
            ],
            [
                1,
                0.8439914302408622,
                0.3183640983255273,
                0.39779289105607596,
                0.8666666666666666,
                0.7483552631578947,
                1.0,
                1.0,
                0.8685483870967742,
                1
            ]
        ]
    },
    "What is the total cost of all the vowels on \u201cWheel of Fortune\u201d?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.3076923076923077,
                    0.2619589977220957,
                    0.2615268329554044,
                    0.0,
                    0.375,
                    0,
                    0,
                    0.10128205128205128,
                    1
                ],
                [
                    0.4166666666666667,
                    0.6309794988610479,
                    0.6311413454270597,
                    1.0,
                    0.375,
                    0,
                    0,
                    0.48333333333333334,
                    1
                ],
                [
                    0.27564102564102566,
                    0.1070615034168565,
                    0.1073318216175359,
                    0.0,
                    0.25,
                    0,
                    0,
                    0.4153846153846154,
                    1
                ]
            ],
            "fraction_answers": {
                "$2,500": 0.5895201407146846,
                "$1,250": 0.21791003160864317,
                "$2,900": 0.19256982767667222
            },
            "question": "what is the total cost of all the vowels on \u201cwheel of fortune\u201d?",
            "rate_limited": false,
            "answers": [
                "$1,250",
                "$2,500",
                "$2,900"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "$2,500": 5,
                "$1,250": 1,
                "$2,900": 0
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    3450.0,
                    8310.0,
                    1410.0
                ],
                "wikipedia_search": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    3460.0,
                    8350.0,
                    1420.0
                ],
                "word_relation_to_question": [
                    0.40512820512820513,
                    1.9333333333333333,
                    1.6615384615384616
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    48.0,
                    65.0,
                    43.0
                ],
                "word_count_appended": [
                    15.0,
                    15.0,
                    10.0
                ]
            },
            "z-best_answer_by_ml": [
                "$1,250"
            ],
            "ml_answers": {
                "$2,500": 0.3142943258489904,
                "$1,250": 0.4921682753982507,
                "$2,900": 0.1883962477338258
            }
        },
        "lines": [
            [
                1,
                0.3076923076923077,
                0.2619589977220957,
                0.2615268329554044,
                0.0,
                0.375,
                0,
                0,
                0.10128205128205128,
                1
            ],
            [
                0,
                0.4166666666666667,
                0.6309794988610479,
                0.6311413454270597,
                1.0,
                0.375,
                0,
                0,
                0.48333333333333334,
                1
            ],
            [
                0,
                0.27564102564102566,
                0.1070615034168565,
                0.1073318216175359,
                0.0,
                0.25,
                0,
                0,
                0.4153846153846154,
                1
            ]
        ]
    },
    "Which brand mascot was NOT a real person?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.42267552182163187,
                    0.42063492063492064,
                    0.39192270692050973,
                    0.3032407407407408,
                    0.3002915451895044,
                    0.5,
                    0.5,
                    0.27135681135681133,
                    -1
                ],
                [
                    0.36432637571157495,
                    0.49946294307196565,
                    0.49941210703215,
                    0.4207175925925926,
                    0.36297376093294464,
                    0.5,
                    0.5,
                    0.3857111657111657,
                    -1
                ],
                [
                    0.21299810246679318,
                    0.07990213629311371,
                    0.10866518604734021,
                    0.2760416666666667,
                    0.33673469387755106,
                    0.0,
                    0.0,
                    0.34293202293202296,
                    -1
                ]
            ],
            "fraction_answers": {
                "betty crocker": 0.660681547929128,
                "sara lee": 0.11684901373690162,
                "little debbie": 0.22246943833397032
            },
            "question": "which brand mascot was not a real person?",
            "rate_limited": false,
            "answers": [
                "little debbie",
                "sara lee",
                "betty crocker"
            ],
            "integer_answers": {
                "betty crocker": 6,
                "sara lee": 0,
                "little debbie": 2
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "betty crocker": 0.11525077966893688,
                "sara lee": 0.500162139461744,
                "little debbie": 0.3861919812673261
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    13300.0,
                    90.0,
                    70400.0
                ],
                "wikipedia_search": [
                    1.574074074074074,
                    0.6342592592592593,
                    1.7916666666666667
                ],
                "result_count_noun_chunks": [
                    18200.0,
                    99.0,
                    65900.0
                ],
                "word_relation_to_question": [
                    1.829145509145509,
                    0.9143106743106744,
                    1.2565438165438165
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_count_appended": [
                    137.0,
                    94.0,
                    112.0
                ],
                "result_count": [
                    16300.0,
                    28600.0,
                    60500.0
                ]
            },
            "z-best_answer_by_ml": [
                "sara lee"
            ]
        },
        "lines": [
            [
                0,
                0.42267552182163187,
                0.42063492063492064,
                0.39192270692050973,
                0.3032407407407408,
                0.3002915451895044,
                0.5,
                0.5,
                0.27135681135681133,
                -1
            ],
            [
                0,
                0.36432637571157495,
                0.49946294307196565,
                0.49941210703215,
                0.4207175925925926,
                0.36297376093294464,
                0.5,
                0.5,
                0.3857111657111657,
                -1
            ],
            [
                1,
                0.21299810246679318,
                0.07990213629311371,
                0.10866518604734021,
                0.2760416666666667,
                0.33673469387755106,
                0.0,
                0.0,
                0.34293202293202296,
                -1
            ]
        ]
    },
    "What iconic painting once hung in Napoleon's bedroom?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "mona lisa"
            ],
            "question": "what iconic painting once hung in napoleon's bedroom?",
            "answers": [
                "the starry night",
                "mona lisa",
                "the birth of venus"
            ],
            "integer_answers": {
                "the birth of venus": 1,
                "mona lisa": 6,
                "the starry night": 1
            },
            "data": {
                "result_count_important_words": [
                    84.0,
                    132000.0,
                    42800.0
                ],
                "wikipedia_search": [
                    2.1902173913043477,
                    0.6177536231884058,
                    0.19202898550724637
                ],
                "result_count_noun_chunks": [
                    48200.0,
                    260000.0,
                    50300.0
                ],
                "word_relation_to_question": [
                    1.8031914893617023,
                    0.4893617021276596,
                    2.707446808510638
                ],
                "word_count_noun_chunks": [
                    0.0,
                    23.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    45.0,
                    1.0
                ],
                "word_count_appended": [
                    49.0,
                    217.0,
                    72.0
                ],
                "result_count": [
                    34800.0,
                    170000.0,
                    48600.0
                ]
            },
            "negative_question": false,
            "fraction_answers": {
                "the birth of venus": 0.1823443364938314,
                "mona lisa": 0.6291628049252049,
                "the starry night": 0.1884928585809637
            },
            "lines": [
                [
                    0.13733228097868982,
                    0.0004803183824706663,
                    0.13444909344490935,
                    0.7300724637681159,
                    0.14497041420118342,
                    0.0,
                    0.0,
                    0.3606382978723405,
                    1
                ],
                [
                    0.6708760852407262,
                    0.7547860295967613,
                    0.7252440725244073,
                    0.20591787439613526,
                    0.6420118343195266,
                    0.9583333333333334,
                    0.9782608695652174,
                    0.09787234042553192,
                    1
                ],
                [
                    0.19179163378058406,
                    0.24473365202076805,
                    0.1403068340306834,
                    0.06400966183574879,
                    0.21301775147928995,
                    0.041666666666666664,
                    0.021739130434782608,
                    0.5414893617021276,
                    1
                ]
            ],
            "rate_limited": false,
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "the birth of venus": 0.37660404146212684,
                "mona lisa": 0.8609233863065419,
                "the starry night": 0.15376296313828747
            },
            "question_type": 1
        },
        "lines": [
            [
                0,
                0.13733228097868982,
                0.0004803183824706663,
                0.13444909344490935,
                0.7300724637681159,
                0.14497041420118342,
                0.0,
                0.0,
                0.3606382978723405,
                1
            ],
            [
                1,
                0.6708760852407262,
                0.7547860295967613,
                0.7252440725244073,
                0.20591787439613526,
                0.6420118343195266,
                0.9583333333333334,
                0.9782608695652174,
                0.09787234042553192,
                1
            ],
            [
                0,
                0.19179163378058406,
                0.24473365202076805,
                0.1403068340306834,
                0.06400966183574879,
                0.21301775147928995,
                0.041666666666666664,
                0.021739130434782608,
                0.5414893617021276,
                1
            ]
        ]
    },
    "Which of these is NOT among the four \u201cC\u2019s\u201d of diamond buying?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.2558084400189663,
                    0.3076923076923077,
                    0.1952054794520548,
                    0.15657439199123635,
                    0.2742663656884876,
                    0.21572580645161288,
                    0.19746376811594202,
                    0.32420196439533455,
                    -1
                ],
                [
                    0.3108108108108108,
                    0.3428254437869822,
                    0.3574486301369863,
                    0.45210846887589246,
                    0.3177200902934537,
                    0.2842741935483871,
                    0.302536231884058,
                    0.27823818293431557,
                    -1
                ],
                [
                    0.43338074917022285,
                    0.3494822485207101,
                    0.4473458904109589,
                    0.39131713913287125,
                    0.40801354401805867,
                    0.5,
                    0.5,
                    0.39755985267034993,
                    -1
                ]
            ],
            "fraction_answers": {
                "color": 0.5182653690485145,
                "core": 0.1432251440192071,
                "cut": 0.3385094869322785
            },
            "question": "which of these is not among the four \u201cc\u2019s\u201d of diamond buying?",
            "rate_limited": false,
            "answers": [
                "color",
                "cut",
                "core"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "color": 7,
                "core": 0,
                "cut": 1
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    5200000.0,
                    4250000.0,
                    4070000.0
                ],
                "wikipedia_search": [
                    1.3737024320350546,
                    0.19156612449643015,
                    0.4347314434685151
                ],
                "result_count_noun_chunks": [
                    712000.0,
                    333000.0,
                    123000.0
                ],
                "word_relation_to_question": [
                    1.0547882136279927,
                    1.3305709023941068,
                    0.6146408839779005
                ],
                "word_count_noun_chunks": [
                    141.0,
                    107.0,
                    0.0
                ],
                "word_count_raw": [
                    167.0,
                    109.0,
                    0.0
                ],
                "result_count": [
                    1030000.0,
                    798000.0,
                    281000.0
                ],
                "word_count_appended": [
                    400.0,
                    323.0,
                    163.0
                ]
            },
            "z-best_answer_by_ml": [
                "core"
            ],
            "ml_answers": {
                "color": 0.1319099410991172,
                "core": 0.6599525615736389,
                "cut": 0.12862508909535855
            }
        },
        "lines": [
            [
                0,
                0.2558084400189663,
                0.3076923076923077,
                0.1952054794520548,
                0.15657439199123635,
                0.2742663656884876,
                0.21572580645161288,
                0.19746376811594202,
                0.32420196439533455,
                -1
            ],
            [
                0,
                0.3108108108108108,
                0.3428254437869822,
                0.3574486301369863,
                0.45210846887589246,
                0.3177200902934537,
                0.2842741935483871,
                0.302536231884058,
                0.27823818293431557,
                -1
            ],
            [
                1,
                0.43338074917022285,
                0.3494822485207101,
                0.4473458904109589,
                0.39131713913287125,
                0.40801354401805867,
                0.5,
                0.5,
                0.39755985267034993,
                -1
            ]
        ]
    },
    "What is the correct pronunciation of the performer who sings \u201cSmooth Operator\u201d?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.7849462365591398,
                    0.6101694915254238,
                    0.71241961414791,
                    0.0,
                    0.7513513513513513,
                    1.0,
                    1.0,
                    0.6666666666666666,
                    1
                ],
                [
                    0.16129032258064516,
                    0.22033898305084745,
                    0.0012057877813504824,
                    0.0,
                    0.12972972972972974,
                    0.0,
                    0.0,
                    0.0,
                    1
                ],
                [
                    0.053763440860215055,
                    0.1694915254237288,
                    0.28637459807073956,
                    1.0,
                    0.11891891891891893,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    1
                ]
            ],
            "fraction_answers": {
                "sayd": 0.0640706028928216,
                "shah-day": 0.6906941700313115,
                "say-dee": 0.24523522707586695
            },
            "question": "what is the correct pronunciation of the performer who sings \u201csmooth operator\u201d?",
            "rate_limited": false,
            "answers": [
                "shah-day",
                "sayd",
                "say-dee"
            ],
            "ml_answers": {
                "sayd": 0.32464191200898934,
                "shah-day": 0.9251694470974915,
                "say-dee": 0.3310299851679257
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "sayd": 0,
                "shah-day": 7,
                "say-dee": 1
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    36.0,
                    13.0,
                    10.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_noun_chunks": [
                    7090.0,
                    12.0,
                    2850.0
                ],
                "word_relation_to_question": [
                    2.0,
                    0.0,
                    1.0
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    139.0,
                    24.0,
                    22.0
                ],
                "result_count": [
                    73.0,
                    15.0,
                    5.0
                ]
            },
            "z-best_answer_by_ml": [
                "shah-day"
            ]
        },
        "lines": [
            [
                1,
                0.7849462365591398,
                0.6101694915254238,
                0.71241961414791,
                0.0,
                0.7513513513513513,
                1.0,
                1.0,
                0.6666666666666666,
                1
            ],
            [
                0,
                0.16129032258064516,
                0.22033898305084745,
                0.0012057877813504824,
                0.0,
                0.12972972972972974,
                0.0,
                0.0,
                0.0,
                1
            ],
            [
                0,
                0.053763440860215055,
                0.1694915254237288,
                0.28637459807073956,
                1.0,
                0.11891891891891893,
                0.0,
                0.0,
                0.3333333333333333,
                1
            ]
        ]
    },
    "In which version of \u201cDragnet\u201d is the line \u201cJust the facts, ma\u2019am\u201d first said?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.30952380952380953,
                    0.2222222222222222,
                    0.3983739837398374,
                    0.012077294685990338,
                    0.3333333333333333,
                    0,
                    0,
                    0.3975193798449613,
                    -1
                ],
                [
                    0.23809523809523808,
                    0.2222222222222222,
                    0.41192411924119243,
                    0.8140096618357487,
                    0.3333333333333333,
                    0,
                    0,
                    0.16511627906976745,
                    -1
                ],
                [
                    0.4523809523809524,
                    0.5555555555555556,
                    0.1897018970189702,
                    0.17391304347826086,
                    0.3333333333333333,
                    0,
                    0,
                    0.43736434108527134,
                    -1
                ]
            ],
            "fraction_answers": {
                "'80s movie": 0.35704152047539056,
                "'50s movie": 0.36411680896625037,
                "'50s tv show": 0.278841670558359
            },
            "question": "in which version of \u201cdragnet\u201d is the line \u201cjust the facts, ma\u2019am\u201d first said?",
            "rate_limited": false,
            "answers": [
                "'50s tv show",
                "'50s movie",
                "'80s movie"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "'80s movie": 0.48792640956760913,
                "'50s movie": 0.13825722834525014,
                "'50s tv show": 0.19217838309924093
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    12.0,
                    12.0,
                    30.0
                ],
                "wikipedia_search": [
                    0.036231884057971016,
                    2.442028985507246,
                    0.5217391304347826
                ],
                "result_count_noun_chunks": [
                    147000.0,
                    152000.0,
                    70000.0
                ],
                "word_relation_to_question": [
                    1.9875968992248063,
                    0.8255813953488372,
                    2.1868217054263566
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    13.0,
                    10.0,
                    19.0
                ],
                "word_count_appended": [
                    2.0,
                    2.0,
                    2.0
                ]
            },
            "z-best_answer_by_ml": [
                "'80s movie"
            ],
            "integer_answers": {
                "'80s movie": 3,
                "'50s movie": 2,
                "'50s tv show": 1
            }
        },
        "lines": [
            [
                0,
                0.30952380952380953,
                0.2222222222222222,
                0.3983739837398374,
                0.012077294685990338,
                0.3333333333333333,
                0,
                0,
                0.3975193798449613,
                -1
            ],
            [
                0,
                0.23809523809523808,
                0.2222222222222222,
                0.41192411924119243,
                0.8140096618357487,
                0.3333333333333333,
                0,
                0,
                0.16511627906976745,
                -1
            ],
            [
                1,
                0.4523809523809524,
                0.5555555555555556,
                0.1897018970189702,
                0.17391304347826086,
                0.3333333333333333,
                0,
                0,
                0.43736434108527134,
                -1
            ]
        ]
    },
    "Which of these companies went public first?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.2581681180902971,
                    0.9770552196372082,
                    0.9876956558131749,
                    0.5554804804804805,
                    0.276690888764673,
                    1.0,
                    1.0,
                    0.44230769230769235,
                    -1
                ],
                [
                    0.013031931319869064,
                    0.0110919793376847,
                    0.006668340727100248,
                    0.12832475332475332,
                    0.35941866964784797,
                    0.0,
                    0.0,
                    0.07417582417582419,
                    -1
                ],
                [
                    0.7287999505898338,
                    0.011852801025107116,
                    0.005636003459724896,
                    0.31619476619476616,
                    0.363890441587479,
                    0.0,
                    0.0,
                    0.4835164835164835,
                    -1
                ]
            ],
            "fraction_answers": {
                "ferrari": 0.07408893731663493,
                "facebook": 0.6871747568866907,
                "alibaba": 0.23873630579667432
            },
            "question": "which of these companies went public first?",
            "rate_limited": false,
            "answers": [
                "facebook",
                "ferrari",
                "alibaba"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "ferrari": 0,
                "facebook": 5,
                "alibaba": 3
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    244000000.0,
                    2770000.0,
                    2960000.0
                ],
                "wikipedia_search": [
                    1.6664414414414415,
                    0.38497425997425994,
                    0.9485842985842985
                ],
                "result_count_noun_chunks": [
                    354000000.0,
                    2390000.0,
                    2020000.0
                ],
                "word_relation_to_question": [
                    0.8846153846153846,
                    0.14835164835164835,
                    0.9670329670329669
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    13.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    41800000.0,
                    2110000.0,
                    118000000.0
                ],
                "word_count_appended": [
                    495.0,
                    643.0,
                    651.0
                ]
            },
            "z-best_answer_by_ml": [
                "facebook"
            ],
            "ml_answers": {
                "ferrari": 0.3274203947547246,
                "facebook": 0.9027876141721933,
                "alibaba": 0.2784459706195779
            }
        },
        "lines": [
            [
                1,
                0.2581681180902971,
                0.9770552196372082,
                0.9876956558131749,
                0.5554804804804805,
                0.276690888764673,
                1.0,
                1.0,
                0.44230769230769235,
                -1
            ],
            [
                0,
                0.013031931319869064,
                0.0110919793376847,
                0.006668340727100248,
                0.12832475332475332,
                0.35941866964784797,
                0.0,
                0.0,
                0.07417582417582419,
                -1
            ],
            [
                0,
                0.7287999505898338,
                0.011852801025107116,
                0.005636003459724896,
                0.31619476619476616,
                0.363890441587479,
                0.0,
                0.0,
                0.4835164835164835,
                -1
            ]
        ]
    },
    "What topic would a herpetologist study?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.9647474859100453,
                    0.9153815737445631,
                    0.48366013071895425,
                    0.7333333333333334,
                    0.44871794871794873,
                    0,
                    0,
                    0.2642857142857143,
                    1
                ],
                [
                    0.025085644822632334,
                    0.039343614076710165,
                    0.3888888888888889,
                    0.0,
                    0.2692307692307692,
                    0,
                    0,
                    0.1,
                    1
                ],
                [
                    0.010166869267322355,
                    0.04527481217872677,
                    0.12745098039215685,
                    0.26666666666666666,
                    0.28205128205128205,
                    0,
                    0,
                    0.6357142857142857,
                    1
                ]
            ],
            "fraction_answers": {
                "venereal disease": 0.6350210311184266,
                "mushroom farming": 0.13709148616983344,
                "crocodile teeth": 0.22788748271174006
            },
            "question": "what topic would a herpetologist study?",
            "rate_limited": false,
            "answers": [
                "venereal disease",
                "mushroom farming",
                "crocodile teeth"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "venereal disease": 0.40415307350088014,
                "mushroom farming": 0.23944883325939403,
                "crocodile teeth": 0.14239745210228014
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    46300.0,
                    1990.0,
                    2290.0
                ],
                "wikipedia_search": [
                    1.4666666666666668,
                    0.0,
                    0.5333333333333333
                ],
                "result_count_noun_chunks": [
                    296000.0,
                    238000.0,
                    78000.0
                ],
                "word_relation_to_question": [
                    0.5285714285714286,
                    0.2,
                    1.2714285714285714
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    35.0,
                    21.0,
                    22.0
                ],
                "result_count": [
                    8730.0,
                    227.0,
                    92.0
                ]
            },
            "z-best_answer_by_ml": [
                "venereal disease"
            ],
            "integer_answers": {
                "venereal disease": 5,
                "mushroom farming": 0,
                "crocodile teeth": 1
            }
        },
        "lines": [
            [
                0,
                0.9647474859100453,
                0.9153815737445631,
                0.48366013071895425,
                0.7333333333333334,
                0.44871794871794873,
                0,
                0,
                0.2642857142857143,
                1
            ],
            [
                0,
                0.025085644822632334,
                0.039343614076710165,
                0.3888888888888889,
                0.0,
                0.2692307692307692,
                0,
                0,
                0.1,
                1
            ],
            [
                1,
                0.010166869267322355,
                0.04527481217872677,
                0.12745098039215685,
                0.26666666666666666,
                0.28205128205128205,
                0,
                0,
                0.6357142857142857,
                1
            ]
        ]
    },
    "In the U.K., who appoints the Prime Minister?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.5596211795092553,
                    0.4617400419287212,
                    0.9736390263609737,
                    0.0,
                    0.24686192468619247,
                    0.4,
                    0.0625,
                    0.3999999999999999,
                    0
                ],
                [
                    0.22083512699095997,
                    0.27882599580712786,
                    0.009958590041409959,
                    0.393939393939394,
                    0.2384937238493724,
                    0.0,
                    0.0625,
                    0.5666666666666667,
                    0
                ],
                [
                    0.21954369349978475,
                    0.25943396226415094,
                    0.016402383597616404,
                    0.6060606060606061,
                    0.5146443514644351,
                    0.6,
                    0.875,
                    0.03333333333333333,
                    0
                ]
            ],
            "fraction_answers": {
                "the parliament": 0.22140243716186636,
                "the people": 0.38804527156064283,
                "the queen": 0.3905522912774908
            },
            "question": "in the u.k., who appoints the prime minister?",
            "rate_limited": false,
            "answers": [
                "the people",
                "the parliament",
                "the queen"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "the parliament": 1,
                "the people": 3,
                "the queen": 4
            },
            "question_type": 0,
            "data": {
                "result_count_important_words": [
                    881000.0,
                    532000.0,
                    495000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.7878787878787878,
                    1.212121212121212
                ],
                "result_count_noun_chunks": [
                    48200000.0,
                    493000.0,
                    812000.0
                ],
                "word_relation_to_question": [
                    1.2,
                    1.7000000000000002,
                    0.1
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    3.0
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    14.0
                ],
                "result_count": [
                    1300000.0,
                    513000.0,
                    510000.0
                ],
                "word_count_appended": [
                    59.0,
                    57.0,
                    123.0
                ]
            },
            "z-best_answer_by_ml": [
                "the queen"
            ],
            "ml_answers": {
                "the parliament": 0.24573040450733613,
                "the people": 0.09756494754071021,
                "the queen": 0.8313904313678637
            }
        },
        "lines": [
            [
                0,
                0.5596211795092553,
                0.4617400419287212,
                0.9736390263609737,
                0.0,
                0.24686192468619247,
                0.4,
                0.0625,
                0.3999999999999999,
                0
            ],
            [
                0,
                0.22083512699095997,
                0.27882599580712786,
                0.009958590041409959,
                0.393939393939394,
                0.2384937238493724,
                0.0,
                0.0625,
                0.5666666666666667,
                0
            ],
            [
                1,
                0.21954369349978475,
                0.25943396226415094,
                0.016402383597616404,
                0.6060606060606061,
                0.5146443514644351,
                0.6,
                0.875,
                0.03333333333333333,
                0
            ]
        ]
    },
    "Which writer has stated that his/her trademark series of books would never be adapted for film?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.051633675172907234,
                    0.31277533039647576,
                    0.0997001499250375,
                    0.19386621622785488,
                    0.2904411764705882,
                    0.041666666666666685,
                    0,
                    0.3145943744154721,
                    -1
                ],
                [
                    0.47370617696160267,
                    0.34165442976015664,
                    0.4605697151424288,
                    0.3061337837721452,
                    0.3382352941176471,
                    0.5,
                    0,
                    0.29496279608726594,
                    -1
                ],
                [
                    0.4746601478654901,
                    0.3455702398433676,
                    0.4397301349325337,
                    0.5,
                    0.3713235294117647,
                    0.4583333333333333,
                    0,
                    0.3904428294972619,
                    -1
                ]
            ],
            "fraction_answers": {
                "jeff kinney": 0.14855422431892817,
                "sue grafton": 0.22421080118821532,
                "james patterson": 0.6272349744928565
            },
            "question": "which writer has stated that his/her trademark series of books would never be adapted for film?",
            "rate_limited": false,
            "answers": [
                "james patterson",
                "sue grafton",
                "jeff kinney"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "jeff kinney": 0,
                "sue grafton": 1,
                "james patterson": 6
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    76500.0,
                    64700.0,
                    63100.0
                ],
                "wikipedia_search": [
                    3.6736054052657416,
                    2.3263945947342584,
                    0.0
                ],
                "result_count_noun_chunks": [
                    26700.0,
                    2630.0,
                    4020.0
                ],
                "word_relation_to_question": [
                    2.2248675070143347,
                    2.4604464469528082,
                    1.314686046032857
                ],
                "word_count_noun_chunks": [
                    11.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    7520.0,
                    441.0,
                    425.0
                ],
                "word_count_appended": [
                    57.0,
                    44.0,
                    35.0
                ]
            },
            "z-best_answer_by_ml": [
                "sue grafton"
            ],
            "ml_answers": {
                "jeff kinney": 0.18325904500854404,
                "sue grafton": 0.4984215362594342,
                "james patterson": 0.15831154825415947
            }
        },
        "lines": [
            [
                0,
                0.051633675172907234,
                0.31277533039647576,
                0.0997001499250375,
                0.19386621622785488,
                0.2904411764705882,
                0.041666666666666685,
                0,
                0.3145943744154721,
                -1
            ],
            [
                1,
                0.47370617696160267,
                0.34165442976015664,
                0.4605697151424288,
                0.3061337837721452,
                0.3382352941176471,
                0.5,
                0,
                0.29496279608726594,
                -1
            ],
            [
                0,
                0.4746601478654901,
                0.3455702398433676,
                0.4397301349325337,
                0.5,
                0.3713235294117647,
                0.4583333333333333,
                0,
                0.3904428294972619,
                -1
            ]
        ]
    },
    "In Harry Potter's Quidditch, what ALWAYS happens when one team catches the snitch?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.17757009345794392,
                    0.024261603375527425,
                    0.0130475302889096,
                    0.0625,
                    0.10714285714285714,
                    0.0,
                    0.0,
                    0.26156299840510366,
                    1
                ],
                [
                    0.04672897196261682,
                    0.006329113924050633,
                    0.003727865796831314,
                    0.9375,
                    0.10714285714285714,
                    0.0,
                    0.0,
                    0.6523432707643234,
                    1
                ],
                [
                    0.7757009345794392,
                    0.9694092827004219,
                    0.983224603914259,
                    0.0,
                    0.7857142857142857,
                    1.0,
                    1.0,
                    0.08609373083057294,
                    1
                ]
            ],
            "fraction_answers": {
                "the game ends": 0.7000178547173723,
                "that team loses": 0.2192215099488349,
                "that team wins": 0.08076063533379271
            },
            "question": "in harry potter's quidditch, what always happens when one team catches the snitch?",
            "rate_limited": false,
            "answers": [
                "that team wins",
                "that team loses",
                "the game ends"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "the game ends": 6,
                "that team loses": 2,
                "that team wins": 0
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    23.0,
                    6.0,
                    919.0
                ],
                "wikipedia_search": [
                    0.0625,
                    0.9375,
                    0.0
                ],
                "result_count_noun_chunks": [
                    28.0,
                    8.0,
                    2110.0
                ],
                "word_relation_to_question": [
                    0.5231259968102073,
                    1.3046865415286468,
                    0.17218746166114587
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    6.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    8.0
                ],
                "result_count": [
                    19.0,
                    5.0,
                    83.0
                ],
                "word_count_appended": [
                    6.0,
                    6.0,
                    44.0
                ]
            },
            "z-best_answer_by_ml": [
                "the game ends"
            ],
            "ml_answers": {
                "the game ends": 0.8892867890383769,
                "that team loses": 0.17834529713700997,
                "that team wins": 0.08702519083885397
            }
        },
        "lines": [
            [
                0,
                0.17757009345794392,
                0.024261603375527425,
                0.0130475302889096,
                0.0625,
                0.10714285714285714,
                0.0,
                0.0,
                0.26156299840510366,
                1
            ],
            [
                0,
                0.04672897196261682,
                0.006329113924050633,
                0.003727865796831314,
                0.9375,
                0.10714285714285714,
                0.0,
                0.0,
                0.6523432707643234,
                1
            ],
            [
                1,
                0.7757009345794392,
                0.9694092827004219,
                0.983224603914259,
                0.0,
                0.7857142857142857,
                1.0,
                1.0,
                0.08609373083057294,
                1
            ]
        ]
    },
    "Laurie Metcalf, Amy Morton and Tracy Letts are members of a theatre company from what city?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.2863157894736842,
                    0.29067245119305857,
                    0.6195190947666195,
                    0.2384799271652288,
                    0.13864306784660768,
                    0.14953271028037382,
                    0.06837606837606838,
                    0.4433388919770243,
                    1
                ],
                [
                    0.45052631578947366,
                    0.455531453362256,
                    0.16265912305516267,
                    0.2840352524392133,
                    0.06489675516224189,
                    0.0,
                    0.0,
                    0.31660876412821937,
                    1
                ],
                [
                    0.2631578947368421,
                    0.25379609544468545,
                    0.21782178217821782,
                    0.47748482039555795,
                    0.7964601769911505,
                    0.8504672897196262,
                    0.9316239316239316,
                    0.24005234389475633,
                    1
                ]
            ],
            "fraction_answers": {
                "new york": 0.27935975013483316,
                "los angeles": 0.21678220799207087,
                "chicago": 0.5038580418730959
            },
            "question": "laurie metcalf, amy morton and tracy letts are members of a theatre company from what city?",
            "rate_limited": false,
            "answers": [
                "new york",
                "los angeles",
                "chicago"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "new york": 2,
                "los angeles": 2,
                "chicago": 4
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    1340.0,
                    2100.0,
                    1170.0
                ],
                "wikipedia_search": [
                    0.9539197086609152,
                    1.1361410097568532,
                    1.9099392815822318
                ],
                "result_count_noun_chunks": [
                    4380.0,
                    1150.0,
                    1540.0
                ],
                "word_relation_to_question": [
                    2.2166944598851215,
                    1.5830438206410968,
                    1.2002617194737817
                ],
                "word_count_noun_chunks": [
                    16.0,
                    0.0,
                    91.0
                ],
                "word_count_raw": [
                    8.0,
                    0.0,
                    109.0
                ],
                "result_count": [
                    1360.0,
                    2140.0,
                    1250.0
                ],
                "word_count_appended": [
                    47.0,
                    22.0,
                    270.0
                ]
            },
            "z-best_answer_by_ml": [
                "chicago"
            ],
            "ml_answers": {
                "new york": 0.17000031782028446,
                "los angeles": 0.4802956384688352,
                "chicago": 0.7511353136098265
            }
        },
        "lines": [
            [
                0,
                0.2863157894736842,
                0.29067245119305857,
                0.6195190947666195,
                0.2384799271652288,
                0.13864306784660768,
                0.14953271028037382,
                0.06837606837606838,
                0.4433388919770243,
                1
            ],
            [
                0,
                0.45052631578947366,
                0.455531453362256,
                0.16265912305516267,
                0.2840352524392133,
                0.06489675516224189,
                0.0,
                0.0,
                0.31660876412821937,
                1
            ],
            [
                1,
                0.2631578947368421,
                0.25379609544468545,
                0.21782178217821782,
                0.47748482039555795,
                0.7964601769911505,
                0.8504672897196262,
                0.9316239316239316,
                0.24005234389475633,
                1
            ]
        ]
    },
    "Which of these two U.S. cities are in the same time zone?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.14516129032258066,
                    0.16923076923076924,
                    0.2127659574468085,
                    0.014492753623188406,
                    0.3333333333333333,
                    0,
                    0,
                    0.19410714285714287,
                    -1
                ],
                [
                    0.6774193548387096,
                    0.6923076923076923,
                    0.5531914893617021,
                    0.7671497584541062,
                    0.3333333333333333,
                    0,
                    0,
                    0.4603571428571429,
                    -1
                ],
                [
                    0.1774193548387097,
                    0.13846153846153847,
                    0.23404255319148937,
                    0.2183574879227053,
                    0.3333333333333333,
                    0,
                    0,
                    0.3455357142857143,
                    -1
                ]
            ],
            "fraction_answers": {
                "el paso / pierre": 0.1781818744689705,
                "pensacola / sioux falls": 0.2411916636722484,
                "bismarck / cheyenne": 0.580626461858781
            },
            "question": "which of these two u.s. cities are in the same time zone?",
            "rate_limited": false,
            "answers": [
                "el paso / pierre",
                "bismarck / cheyenne",
                "pensacola / sioux falls"
            ],
            "integer_answers": {
                "el paso / pierre": 1,
                "pensacola / sioux falls": 0,
                "bismarck / cheyenne": 5
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "el paso / pierre": 0.2691029898638849,
                "pensacola / sioux falls": 0.19047946912120073,
                "bismarck / cheyenne": 0.3024635858587149
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    11.0,
                    45.0,
                    9.0
                ],
                "wikipedia_search": [
                    0.043478260869565216,
                    2.3014492753623186,
                    0.6550724637681159
                ],
                "result_count_noun_chunks": [
                    10.0,
                    26.0,
                    11.0
                ],
                "word_relation_to_question": [
                    0.7764285714285715,
                    1.8414285714285716,
                    1.3821428571428571
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    3.0,
                    3.0,
                    3.0
                ],
                "result_count": [
                    9.0,
                    42.0,
                    11.0
                ]
            },
            "z-best_answer_by_ml": [
                "bismarck / cheyenne"
            ]
        },
        "lines": [
            [
                0,
                0.14516129032258066,
                0.16923076923076924,
                0.2127659574468085,
                0.014492753623188406,
                0.3333333333333333,
                0,
                0,
                0.19410714285714287,
                -1
            ],
            [
                0,
                0.6774193548387096,
                0.6923076923076923,
                0.5531914893617021,
                0.7671497584541062,
                0.3333333333333333,
                0,
                0,
                0.4603571428571429,
                -1
            ],
            [
                1,
                0.1774193548387097,
                0.13846153846153847,
                0.23404255319148937,
                0.2183574879227053,
                0.3333333333333333,
                0,
                0,
                0.3455357142857143,
                -1
            ]
        ]
    },
    "Until it was banned, lithium was a key ingredient in which of these brands?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.16244686749214562,
                    0.5950960901259112,
                    0.6200569440050617,
                    0.13513513513513514,
                    0.055900621118012424,
                    0.0,
                    0.0,
                    0.14117647058823532,
                    -1
                ],
                [
                    0.8371835150619109,
                    0.40357852882703776,
                    0.37646314457450175,
                    0.8648648648648649,
                    0.9192546583850931,
                    1.0,
                    1.0,
                    0.7147058823529412,
                    -1
                ],
                [
                    0.00036961744594344855,
                    0.0013253810470510272,
                    0.0034799114204365706,
                    0.0,
                    0.024844720496894408,
                    0.0,
                    0.0,
                    0.14411764705882357,
                    -1
                ]
            ],
            "fraction_answers": {
                "cracker jack": 0.21372651605806267,
                "7up": 0.7645063242582937,
                "good and plenty": 0.021767159683643626
            },
            "question": "until it was banned, lithium was a key ingredient in which of these brands?",
            "rate_limited": false,
            "answers": [
                "cracker jack",
                "7up",
                "good and plenty"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "cracker jack": 2,
                "7up": 6,
                "good and plenty": 0
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    898.0,
                    609.0,
                    2.0
                ],
                "wikipedia_search": [
                    0.40540540540540543,
                    2.5945945945945947,
                    0.0
                ],
                "result_count_noun_chunks": [
                    1960.0,
                    1190.0,
                    11.0
                ],
                "word_relation_to_question": [
                    0.5647058823529412,
                    2.8588235294117643,
                    0.5764705882352942
                ],
                "word_count_noun_chunks": [
                    0.0,
                    11.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    6.0,
                    0.0
                ],
                "result_count": [
                    879.0,
                    4530.0,
                    2.0
                ],
                "word_count_appended": [
                    9.0,
                    148.0,
                    4.0
                ]
            },
            "z-best_answer_by_ml": [
                "7up"
            ],
            "ml_answers": {
                "cracker jack": 0.24486083521933805,
                "7up": 0.8547383047801711,
                "good and plenty": 0.11567280937357681
            }
        },
        "lines": [
            [
                0,
                0.16244686749214562,
                0.5950960901259112,
                0.6200569440050617,
                0.13513513513513514,
                0.055900621118012424,
                0.0,
                0.0,
                0.14117647058823532,
                -1
            ],
            [
                1,
                0.8371835150619109,
                0.40357852882703776,
                0.37646314457450175,
                0.8648648648648649,
                0.9192546583850931,
                1.0,
                1.0,
                0.7147058823529412,
                -1
            ],
            [
                0,
                0.00036961744594344855,
                0.0013253810470510272,
                0.0034799114204365706,
                0.0,
                0.024844720496894408,
                0.0,
                0.0,
                0.14411764705882357,
                -1
            ]
        ]
    },
    "Aside from blood cells, what would you also find inside your blood vessels?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.5478708977488473,
                    0.5350877192982456,
                    0.5772739237381209,
                    0.08711650922177237,
                    0.3744619799139168,
                    0.5862068965517241,
                    0.34210526315789475,
                    0.28466179035130423,
                    1
                ],
                [
                    0.42175210197992946,
                    0.4166666666666667,
                    0.4202481551741352,
                    0.8813045434098065,
                    0.4418938307030129,
                    0.41379310344827586,
                    0.6578947368421053,
                    0.6163691374837473,
                    1
                ],
                [
                    0.030377000271223215,
                    0.04824561403508772,
                    0.002477921087743821,
                    0.031578947368421054,
                    0.1836441893830703,
                    0.0,
                    0.0,
                    0.09896907216494846,
                    1
                ]
            ],
            "fraction_answers": {
                "marrow": 0.53374028446346,
                "plasma": 0.4168481224977283,
                "plastids": 0.049411593038811825
            },
            "question": "aside from blood cells, what would you also find inside your blood vessels?",
            "rate_limited": false,
            "answers": [
                "plasma",
                "marrow",
                "plastids"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "marrow": 4,
                "plasma": 4,
                "plastids": 0
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    122000.0,
                    95000.0,
                    11000.0
                ],
                "wikipedia_search": [
                    0.4355825461088619,
                    4.406522717049032,
                    0.15789473684210525
                ],
                "result_count_noun_chunks": [
                    6360000.0,
                    4630000.0,
                    27300.0
                ],
                "word_relation_to_question": [
                    1.4233089517565212,
                    3.0818456874187365,
                    0.4948453608247423
                ],
                "word_count_noun_chunks": [
                    17.0,
                    12.0,
                    0.0
                ],
                "word_count_raw": [
                    13.0,
                    25.0,
                    0.0
                ],
                "result_count": [
                    404000.0,
                    311000.0,
                    22400.0
                ],
                "word_count_appended": [
                    261.0,
                    308.0,
                    128.0
                ]
            },
            "z-best_answer_by_ml": [
                "plasma"
            ],
            "ml_answers": {
                "marrow": 0.4361180078718439,
                "plasma": 0.5104204661575243,
                "plastids": 0.2427340108264955
            }
        },
        "lines": [
            [
                1,
                0.5478708977488473,
                0.5350877192982456,
                0.5772739237381209,
                0.08711650922177237,
                0.3744619799139168,
                0.5862068965517241,
                0.34210526315789475,
                0.28466179035130423,
                1
            ],
            [
                0,
                0.42175210197992946,
                0.4166666666666667,
                0.4202481551741352,
                0.8813045434098065,
                0.4418938307030129,
                0.41379310344827586,
                0.6578947368421053,
                0.6163691374837473,
                1
            ],
            [
                0,
                0.030377000271223215,
                0.04824561403508772,
                0.002477921087743821,
                0.031578947368421054,
                0.1836441893830703,
                0.0,
                0.0,
                0.09896907216494846,
                1
            ]
        ]
    },
    "Which of these Kentucky Derby winners was named for its trainer?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.010139905018611218,
                    0.08578183046268152,
                    0.008594229588704727,
                    0.7333333333333334,
                    0.5462962962962963,
                    0,
                    1.0,
                    0.8468693808312129,
                    -1
                ],
                [
                    0.9780515979976896,
                    0.8611955420466059,
                    0.8287292817679558,
                    0.0,
                    0.3611111111111111,
                    0,
                    0.0,
                    0.014499999999999999,
                    -1
                ],
                [
                    0.01180849698369914,
                    0.0530226274907126,
                    0.16267648864333947,
                    0.26666666666666666,
                    0.09259259259259259,
                    0,
                    0.0,
                    0.1386306191687871,
                    -1
                ]
            ],
            "fraction_answers": {
                "lieut. gibson": 0.10362821307797107,
                "paul jones": 0.4347982189890517,
                "clyde van dusen": 0.4615735679329771
            },
            "question": "which of these kentucky derby winners was named for its trainer?",
            "rate_limited": false,
            "answers": [
                "clyde van dusen",
                "paul jones",
                "lieut. gibson"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "lieut. gibson": 0.13716344581096565,
                "paul jones": 0.16676257384639762,
                "clyde van dusen": 0.6730338783822989
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    5080.0,
                    51000.0,
                    3140.0
                ],
                "wikipedia_search": [
                    2.2,
                    0.0,
                    0.8
                ],
                "result_count_noun_chunks": [
                    84.0,
                    8100.0,
                    1590.0
                ],
                "word_relation_to_question": [
                    4.234346904156064,
                    0.0725,
                    0.6931530958439356
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    4.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    79.0,
                    7620.0,
                    92.0
                ],
                "word_count_appended": [
                    59.0,
                    39.0,
                    10.0
                ]
            },
            "z-best_answer_by_ml": [
                "clyde van dusen"
            ],
            "integer_answers": {
                "lieut. gibson": 0,
                "paul jones": 3,
                "clyde van dusen": 4
            }
        },
        "lines": [
            [
                1,
                0.010139905018611218,
                0.08578183046268152,
                0.008594229588704727,
                0.7333333333333334,
                0.5462962962962963,
                0,
                1.0,
                0.8468693808312129,
                -1
            ],
            [
                0,
                0.9780515979976896,
                0.8611955420466059,
                0.8287292817679558,
                0.0,
                0.3611111111111111,
                0,
                0.0,
                0.014499999999999999,
                -1
            ],
            [
                0,
                0.01180849698369914,
                0.0530226274907126,
                0.16267648864333947,
                0.26666666666666666,
                0.09259259259259259,
                0,
                0.0,
                0.1386306191687871,
                -1
            ]
        ]
    },
    "Which of these Hebrew texts does NOT form a significant part of the Christian Old Testament?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.11251554082055532,
                    0.24976686353745725,
                    0.100326264274062,
                    0.3473424212550529,
                    0.29618937644341803,
                    0.02941176470588236,
                    0.020000000000000018,
                    0.34554948906112715,
                    -1
                ],
                [
                    0.46891835888934935,
                    0.4693814112527199,
                    0.47104404567699837,
                    0.4468275234427879,
                    0.3643187066974596,
                    0.5,
                    0.5,
                    0.3336488209070504,
                    -1
                ],
                [
                    0.41856610029009533,
                    0.2808517252098228,
                    0.42862969004893964,
                    0.2058300553021592,
                    0.3394919168591224,
                    0.47058823529411764,
                    0.48,
                    0.32080169003182246,
                    -1
                ]
            ],
            "fraction_answers": {
                "talmud": 0.26381014674098013,
                "ketuvim": 0.11146528328340861,
                "torah": 0.6247245699756112
            },
            "question": "which of these hebrew texts does not form a significant part of the christian old testament?",
            "rate_limited": false,
            "answers": [
                "torah",
                "ketuvim",
                "talmud"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "talmud": 0.49872368376094406,
                "ketuvim": 0.5095537363533316,
                "torah": 0.1928641593510131
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    1610000.0,
                    197000.0,
                    1410000.0
                ],
                "wikipedia_search": [
                    2.1372061024292597,
                    0.7444146718009689,
                    4.118379225769771
                ],
                "result_count_noun_chunks": [
                    1960000.0,
                    142000.0,
                    350000.0
                ],
                "word_relation_to_question": [
                    2.1623071531442197,
                    2.3289165073012947,
                    2.508776339554485
                ],
                "word_count_noun_chunks": [
                    16.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    24.0,
                    0.0,
                    1.0
                ],
                "result_count": [
                    1870000.0,
                    150000.0,
                    393000.0
                ],
                "word_count_appended": [
                    353.0,
                    235.0,
                    278.0
                ]
            },
            "z-best_answer_by_ml": [
                "ketuvim"
            ],
            "integer_answers": {
                "talmud": 2,
                "ketuvim": 0,
                "torah": 6
            }
        },
        "lines": [
            [
                0,
                0.11251554082055532,
                0.24976686353745725,
                0.100326264274062,
                0.3473424212550529,
                0.29618937644341803,
                0.02941176470588236,
                0.020000000000000018,
                0.34554948906112715,
                -1
            ],
            [
                0,
                0.46891835888934935,
                0.4693814112527199,
                0.47104404567699837,
                0.4468275234427879,
                0.3643187066974596,
                0.5,
                0.5,
                0.3336488209070504,
                -1
            ],
            [
                1,
                0.41856610029009533,
                0.2808517252098228,
                0.42862969004893964,
                0.2058300553021592,
                0.3394919168591224,
                0.47058823529411764,
                0.48,
                0.32080169003182246,
                -1
            ]
        ]
    },
    "How many of the three Baltic countries border Russia?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.30345911949685533,
                    0.8466322019929572,
                    0.4666666666666667,
                    0.02857142857142857,
                    0.465016146393972,
                    0.676923076923077,
                    0.6666666666666666,
                    0.2884231536926148,
                    5
                ],
                [
                    0.46855345911949686,
                    0.123623286131715,
                    0.3652173913043478,
                    0.2357142857142857,
                    0.2303552206673843,
                    0.020512820512820513,
                    0.01881720430107527,
                    0.6097804391217565,
                    5
                ],
                [
                    0.2279874213836478,
                    0.02974451187532779,
                    0.1681159420289855,
                    0.7357142857142858,
                    0.30462863293864373,
                    0.30256410256410254,
                    0.31451612903225806,
                    0.10179640718562875,
                    5
                ]
            ],
            "fraction_answers": {
                "none": 0.27313342909036,
                "three": 0.46779480755052977,
                "two": 0.25907176335911025
            },
            "question": "how many of the three baltic countries border russia?",
            "rate_limited": false,
            "answers": [
                "three",
                "two",
                "none"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "none": 1,
                "three": 5,
                "two": 2
            },
            "question_type": 5,
            "data": {
                "result_count_important_words": [
                    11300000.0,
                    1650000.0,
                    397000.0
                ],
                "wikipedia_search": [
                    0.05714285714285714,
                    0.4714285714285714,
                    1.4714285714285715
                ],
                "result_count_noun_chunks": [
                    4830000.0,
                    3780000.0,
                    1740000.0
                ],
                "word_relation_to_question": [
                    0.8652694610778443,
                    1.8293413173652695,
                    0.30538922155688625
                ],
                "word_count_noun_chunks": [
                    264.0,
                    8.0,
                    118.0
                ],
                "word_count_raw": [
                    248.0,
                    7.0,
                    117.0
                ],
                "result_count": [
                    1930000.0,
                    2980000.0,
                    1450000.0
                ],
                "word_count_appended": [
                    432.0,
                    214.0,
                    283.0
                ]
            },
            "z-best_answer_by_ml": [
                "three"
            ],
            "ml_answers": {
                "none": 0.3227400118454825,
                "three": 0.5065169872436784,
                "two": 0.06883808650129174
            }
        },
        "lines": [
            [
                1,
                0.30345911949685533,
                0.8466322019929572,
                0.4666666666666667,
                0.02857142857142857,
                0.465016146393972,
                0.676923076923077,
                0.6666666666666666,
                0.2884231536926148,
                5
            ],
            [
                0,
                0.46855345911949686,
                0.123623286131715,
                0.3652173913043478,
                0.2357142857142857,
                0.2303552206673843,
                0.020512820512820513,
                0.01881720430107527,
                0.6097804391217565,
                5
            ],
            [
                0,
                0.2279874213836478,
                0.02974451187532779,
                0.1681159420289855,
                0.7357142857142858,
                0.30462863293864373,
                0.30256410256410254,
                0.31451612903225806,
                0.10179640718562875,
                5
            ]
        ]
    },
    "Which of these actors was a high school cheerleader?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.2745762711864407,
                    0.30697674418604654,
                    0.28139904610492844,
                    0.36481481481481487,
                    0.43661971830985913,
                    0.0,
                    0.0,
                    0.18614718614718614,
                    -1
                ],
                [
                    0.5372881355932203,
                    0.5637209302325581,
                    0.5310015898251192,
                    0.10555555555555556,
                    0.2676056338028169,
                    1.0,
                    1.0,
                    0.5432900432900433,
                    -1
                ],
                [
                    0.188135593220339,
                    0.12930232558139534,
                    0.1875993640699523,
                    0.5296296296296297,
                    0.29577464788732394,
                    0.0,
                    0.0,
                    0.27056277056277056,
                    -1
                ]
            ],
            "fraction_answers": {
                "john travolta": 0.20012554136892635,
                "george clooney": 0.23131672259365949,
                "michael douglas": 0.5685577360374142
            },
            "question": "which of these actors was a high school cheerleader?",
            "rate_limited": false,
            "answers": [
                "george clooney",
                "michael douglas",
                "john travolta"
            ],
            "integer_answers": {
                "john travolta": 1,
                "george clooney": 1,
                "michael douglas": 6
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "john travolta": 0.05353627477475271,
                "george clooney": 0.23805375214817015,
                "michael douglas": 0.8857440448556347
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    330000.0,
                    606000.0,
                    139000.0
                ],
                "wikipedia_search": [
                    1.0944444444444446,
                    0.31666666666666665,
                    1.588888888888889
                ],
                "result_count_noun_chunks": [
                    177000.0,
                    334000.0,
                    118000.0
                ],
                "word_relation_to_question": [
                    0.5584415584415584,
                    1.62987012987013,
                    0.8116883116883117
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_appended": [
                    62.0,
                    38.0,
                    42.0
                ],
                "result_count": [
                    162000.0,
                    317000.0,
                    111000.0
                ]
            },
            "z-best_answer_by_ml": [
                "michael douglas"
            ]
        },
        "lines": [
            [
                0,
                0.2745762711864407,
                0.30697674418604654,
                0.28139904610492844,
                0.36481481481481487,
                0.43661971830985913,
                0.0,
                0.0,
                0.18614718614718614,
                -1
            ],
            [
                1,
                0.5372881355932203,
                0.5637209302325581,
                0.5310015898251192,
                0.10555555555555556,
                0.2676056338028169,
                1.0,
                1.0,
                0.5432900432900433,
                -1
            ],
            [
                0,
                0.188135593220339,
                0.12930232558139534,
                0.1875993640699523,
                0.5296296296296297,
                0.29577464788732394,
                0.0,
                0.0,
                0.27056277056277056,
                -1
            ]
        ]
    },
    "Which of these foods is cultivated in a paddy?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.36208246346555323,
                    0.7741079808835797,
                    0.41405581472382474,
                    1.0,
                    0.6383616383616384,
                    0.916083916083916,
                    1.0,
                    0.375,
                    -1
                ],
                [
                    0.6379030387381118,
                    0.22583588649723318,
                    0.5859280397035256,
                    0.0,
                    0.3106893106893107,
                    0.08391608391608392,
                    0.0,
                    0.3,
                    -1
                ],
                [
                    1.4497796334957086e-05,
                    5.6132619187173563e-05,
                    1.6145572649608263e-05,
                    0.0,
                    0.05094905094905095,
                    0.0,
                    0.0,
                    0.325,
                    -1
                ]
            ],
            "fraction_answers": {
                "cake": 0.2680340449430332,
                "rice": 0.684961476689814,
                "dunkaroos": 0.04700447836715284
            },
            "question": "which of these foods is cultivated in a paddy?",
            "rate_limited": false,
            "answers": [
                "rice",
                "cake",
                "dunkaroos"
            ],
            "ml_answers": {
                "cake": 0.1730920011463898,
                "rice": 0.8449405674129946,
                "dunkaroos": 0.08111187708518634
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "cake": 2,
                "rice": 6,
                "dunkaroos": 0
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    593000.0,
                    173000.0,
                    43.0
                ],
                "wikipedia_search": [
                    3.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    1590000.0,
                    2250000.0,
                    62.0
                ],
                "word_relation_to_question": [
                    0.75,
                    0.6,
                    0.65
                ],
                "word_count_noun_chunks": [
                    131.0,
                    12.0,
                    0.0
                ],
                "word_count_raw": [
                    281.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    999000.0,
                    1760000.0,
                    40.0
                ],
                "word_count_appended": [
                    639.0,
                    311.0,
                    51.0
                ]
            },
            "z-best_answer_by_ml": [
                "rice"
            ]
        },
        "lines": [
            [
                1,
                0.36208246346555323,
                0.7741079808835797,
                0.41405581472382474,
                1.0,
                0.6383616383616384,
                0.916083916083916,
                1.0,
                0.375,
                -1
            ],
            [
                0,
                0.6379030387381118,
                0.22583588649723318,
                0.5859280397035256,
                0.0,
                0.3106893106893107,
                0.08391608391608392,
                0.0,
                0.3,
                -1
            ],
            [
                0,
                1.4497796334957086e-05,
                5.6132619187173563e-05,
                1.6145572649608263e-05,
                0.0,
                0.05094905094905095,
                0.0,
                0.0,
                0.325,
                -1
            ]
        ]
    },
    "Which of these phrases appears in a Shakespeare play?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    4.0185225854987404e-05,
                    0.00010884124705947231,
                    6.298805903870773e-05,
                    0.813953488372093,
                    0.32038834951456313,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.8304821150855366,
                    -1
                ],
                [
                    0.9999579456938726,
                    0.9998882563196856,
                    0.9999354372394853,
                    0.13953488372093023,
                    0.6407766990291263,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.08709175738724728,
                    -1
                ],
                [
                    1.8690802723249958e-06,
                    2.9024332549192616e-06,
                    1.5747014759676935e-06,
                    0.046511627906976744,
                    0.038834951456310676,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.08242612752721618,
                    -1
                ]
            ],
            "fraction_answers": {
                "at a loss": 0.5667314557571268,
                "in such a pickle": 0.32896282927135156,
                "up a dark creek": 0.10430571497152169
            },
            "question": "which of these phrases appears in a shakespeare play?",
            "rate_limited": false,
            "answers": [
                "in such a pickle",
                "at a loss",
                "up a dark creek"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "at a loss": 0.23918979363715048,
                "in such a pickle": 0.3303591567027979,
                "up a dark creek": 0.10025283369461017
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    75.0,
                    689000.0,
                    2.0
                ],
                "wikipedia_search": [
                    0.813953488372093,
                    0.13953488372093023,
                    0.046511627906976744
                ],
                "result_count_noun_chunks": [
                    80.0,
                    1270000.0,
                    2.0
                ],
                "word_relation_to_question": [
                    3.3219284603421464,
                    0.3483670295489891,
                    0.3297045101088647
                ],
                "word_count_noun_chunks": [
                    1.0,
                    1.0,
                    1.0
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    1.0
                ],
                "word_count_appended": [
                    33.0,
                    66.0,
                    4.0
                ],
                "result_count": [
                    43.0,
                    1070000.0,
                    2.0
                ]
            },
            "z-best_answer_by_ml": [
                "in such a pickle"
            ],
            "integer_answers": {
                "at a loss": 4,
                "in such a pickle": 4,
                "up a dark creek": 0
            }
        },
        "lines": [
            [
                1,
                4.0185225854987404e-05,
                0.00010884124705947231,
                6.298805903870773e-05,
                0.813953488372093,
                0.32038834951456313,
                0.3333333333333333,
                0.3333333333333333,
                0.8304821150855366,
                -1
            ],
            [
                0,
                0.9999579456938726,
                0.9998882563196856,
                0.9999354372394853,
                0.13953488372093023,
                0.6407766990291263,
                0.3333333333333333,
                0.3333333333333333,
                0.08709175738724728,
                -1
            ],
            [
                0,
                1.8690802723249958e-06,
                2.9024332549192616e-06,
                1.5747014759676935e-06,
                0.046511627906976744,
                0.038834951456310676,
                0.3333333333333333,
                0.3333333333333333,
                0.08242612752721618,
                -1
            ]
        ]
    },
    "J.K. Rowling\u2019s first book published in England was titled \u201cHarry Potter and the\u201d what?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.5797613434509048,
                    0.6310326325984034,
                    0.5087632272956735,
                    0.0700354609929078,
                    0.3333333333333333,
                    0.0,
                    0,
                    0.23102704302111912,
                    1
                ],
                [
                    0.4202201383735059,
                    0.3689459955299043,
                    0.4912196677337537,
                    0.874113475177305,
                    0.3333333333333333,
                    1.0,
                    0,
                    0.6777933340307635,
                    1
                ],
                [
                    1.8518175589340937e-05,
                    2.1371871692281043e-05,
                    1.7104970572871778e-05,
                    0.05585106382978724,
                    0.3333333333333333,
                    0.0,
                    0,
                    0.09117962294811746,
                    1
                ]
            ],
            "fraction_answers": {
                "magician's stone": 0.06863157358987035,
                "philosopher's stone": 0.33627900581319164,
                "sorcerer's stone": 0.595089420596938
            },
            "question": "j.k. rowling\u2019s first book published in england was titled \u201charry potter and the\u201d what?",
            "rate_limited": false,
            "answers": [
                "philosopher's stone",
                "sorcerer's stone",
                "magician's stone"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "magician's stone": 0.33585826406288266,
                "philosopher's stone": 0.16578873910940856,
                "sorcerer's stone": 0.38247109123997114
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    561000.0,
                    328000.0,
                    19.0
                ],
                "wikipedia_search": [
                    0.42021276595744683,
                    5.24468085106383,
                    0.3351063829787234
                ],
                "result_count_noun_chunks": [
                    1160000.0,
                    1120000.0,
                    39.0
                ],
                "word_relation_to_question": [
                    1.8482163441689528,
                    5.422346672246107,
                    0.7294369835849396
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    407000.0,
                    295000.0,
                    13.0
                ],
                "word_count_appended": [
                    3.0,
                    3.0,
                    3.0
                ]
            },
            "z-best_answer_by_ml": [
                "sorcerer's stone"
            ],
            "integer_answers": {
                "magician's stone": 0,
                "philosopher's stone": 4,
                "sorcerer's stone": 3
            }
        },
        "lines": [
            [
                1,
                0.5797613434509048,
                0.6310326325984034,
                0.5087632272956735,
                0.0700354609929078,
                0.3333333333333333,
                0.0,
                0,
                0.23102704302111912,
                1
            ],
            [
                0,
                0.4202201383735059,
                0.3689459955299043,
                0.4912196677337537,
                0.874113475177305,
                0.3333333333333333,
                1.0,
                0,
                0.6777933340307635,
                1
            ],
            [
                0,
                1.8518175589340937e-05,
                2.1371871692281043e-05,
                1.7104970572871778e-05,
                0.05585106382978724,
                0.3333333333333333,
                0.0,
                0,
                0.09117962294811746,
                1
            ]
        ]
    },
    "Which former NFL star does NOT have a football video game named after him?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.4541364296081277,
                    0.4551083591331269,
                    0.43934058898847633,
                    0.414966373785995,
                    0.35119047619047616,
                    0,
                    0.0,
                    0.27398177392833956,
                    -1
                ],
                [
                    0.17779390420899854,
                    0.17079463364293085,
                    0.1494878361075544,
                    0.2899205822012069,
                    0.3214285714285714,
                    0,
                    0.5,
                    0.3644735462984171,
                    -1
                ],
                [
                    0.36806966618287373,
                    0.37409700722394224,
                    0.4111715749039693,
                    0.2951130440127982,
                    0.3273809523809524,
                    0,
                    0.5,
                    0.36154467977324334,
                    -1
                ]
            ],
            "fraction_answers": {
                "kurt warner": 0.2464637358634917,
                "brett favre": 0.4360288360320916,
                "emmitt smith": 0.3175074281044167
            },
            "question": "which former nfl star does not have a football video game named after him?",
            "rate_limited": false,
            "answers": [
                "emmitt smith",
                "brett favre",
                "kurt warner"
            ],
            "integer_answers": {
                "kurt warner": 0,
                "brett favre": 5,
                "emmitt smith": 2
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "kurt warner": 0.23410413826596677,
                "brett favre": 0.2640841530850334,
                "emmitt smith": 0.26172351907536195
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    43500.0,
                    319000.0,
                    122000.0
                ],
                "wikipedia_search": [
                    1.020403514568061,
                    2.5209530135855176,
                    2.458643471846422
                ],
                "result_count_noun_chunks": [
                    37900.0,
                    219000.0,
                    55500.0
                ],
                "word_relation_to_question": [
                    2.712218712859925,
                    1.6263174444189945,
                    1.6614638427210804
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    125.0,
                    150.0,
                    145.0
                ],
                "result_count": [
                    31600.0,
                    222000.0,
                    90900.0
                ]
            },
            "z-best_answer_by_ml": [
                "brett favre"
            ]
        },
        "lines": [
            [
                0,
                0.4541364296081277,
                0.4551083591331269,
                0.43934058898847633,
                0.414966373785995,
                0.35119047619047616,
                0,
                0.0,
                0.27398177392833956,
                -1
            ],
            [
                1,
                0.17779390420899854,
                0.17079463364293085,
                0.1494878361075544,
                0.2899205822012069,
                0.3214285714285714,
                0,
                0.5,
                0.3644735462984171,
                -1
            ],
            [
                0,
                0.36806966618287373,
                0.37409700722394224,
                0.4111715749039693,
                0.2951130440127982,
                0.3273809523809524,
                0,
                0.5,
                0.36154467977324334,
                -1
            ]
        ]
    },
    "How do you spell the last name of Duke University\u2019s men's basketball coach?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.03125,
                    0.0,
                    5.464182285121031e-05,
                    0.0,
                    0.06031746031746032,
                    0.0,
                    0.0,
                    0.34437885346976255,
                    5
                ],
                [
                    0.96875,
                    1.0,
                    0.9999453581771488,
                    1.0,
                    0.8888888888888888,
                    1.0,
                    1.0,
                    0.3110455201364292,
                    5
                ],
                [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.050793650793650794,
                    0.0,
                    0.0,
                    0.34457562639380823,
                    5
                ]
            ],
            "fraction_answers": {
                "khzyrweski": 0.049421159648432376,
                "crzyzewski": 0.05450011945125926,
                "krzyzewski": 0.8960787209003084
            },
            "question": "how do you spell the last name of duke university\u2019s men's basketball coach?",
            "rate_limited": false,
            "answers": [
                "crzyzewski",
                "krzyzewski",
                "khzyrweski"
            ],
            "integer_answers": {
                "khzyrweski": 1,
                "crzyzewski": 0,
                "krzyzewski": 7
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "khzyrweski": 0.08111187708518634,
                "crzyzewski": 0.11059718514847422,
                "krzyzewski": 0.913882441902346
            },
            "question_type": 5,
            "data": {
                "result_count_important_words": [
                    0,
                    21600.0,
                    0
                ],
                "wikipedia_search": [
                    0.0,
                    5.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    3.0,
                    54900.0,
                    0
                ],
                "word_relation_to_question": [
                    2.0662731208185754,
                    1.8662731208185752,
                    2.0674537583628494
                ],
                "word_count_noun_chunks": [
                    0.0,
                    131.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    34.0,
                    0.0
                ],
                "word_count_appended": [
                    19.0,
                    280.0,
                    16.0
                ],
                "result_count": [
                    3.0,
                    93.0,
                    0
                ]
            },
            "z-best_answer_by_ml": [
                "krzyzewski"
            ]
        },
        "lines": [
            [
                0,
                0.03125,
                0.0,
                5.464182285121031e-05,
                0.0,
                0.06031746031746032,
                0.0,
                0.0,
                0.34437885346976255,
                5
            ],
            [
                1,
                0.96875,
                1.0,
                0.9999453581771488,
                1.0,
                0.8888888888888888,
                1.0,
                1.0,
                0.3110455201364292,
                5
            ],
            [
                0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.050793650793650794,
                0.0,
                0.0,
                0.34457562639380823,
                5
            ]
        ]
    },
    "Which of these consists of frozen water?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.11426437640029873,
                    0.062318560092897234,
                    0.2337917485265226,
                    0.6818181818181819,
                    0.518562874251497,
                    0,
                    0,
                    0.6428571428571429,
                    -1
                ],
                [
                    0.7766990291262136,
                    0.12773369460034836,
                    0.550098231827112,
                    0.0,
                    0.046706586826347304,
                    0,
                    0,
                    0.07142857142857144,
                    -1
                ],
                [
                    0.10903659447348768,
                    0.8099477453067544,
                    0.21611001964636542,
                    0.3181818181818182,
                    0.4347305389221557,
                    0,
                    0,
                    0.28571428571428575,
                    -1
                ]
            ],
            "fraction_answers": {
                "garden rake": 0.2621110189680988,
                "snowflake": 0.3756021473244234,
                "drake": 0.3622868337074779
            },
            "question": "which of these consists of frozen water?",
            "rate_limited": false,
            "answers": [
                "snowflake",
                "garden rake",
                "drake"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "garden rake": 0.16899747347945748,
                "snowflake": 0.2619244004261189,
                "drake": 0.24863678511346082
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    644000.0,
                    1320000.0,
                    8370000.0
                ],
                "wikipedia_search": [
                    1.3636363636363638,
                    0.0,
                    0.6363636363636364
                ],
                "result_count_noun_chunks": [
                    1190000.0,
                    2800000.0,
                    1100000.0
                ],
                "word_relation_to_question": [
                    1.2857142857142856,
                    0.14285714285714285,
                    0.5714285714285714
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    1530000.0,
                    10400000.0,
                    1460000.0
                ],
                "word_count_appended": [
                    433.0,
                    39.0,
                    363.0
                ]
            },
            "z-best_answer_by_ml": [
                "snowflake"
            ],
            "integer_answers": {
                "garden rake": 2,
                "snowflake": 3,
                "drake": 1
            }
        },
        "lines": [
            [
                1,
                0.11426437640029873,
                0.062318560092897234,
                0.2337917485265226,
                0.6818181818181819,
                0.518562874251497,
                0,
                0,
                0.6428571428571429,
                -1
            ],
            [
                0,
                0.7766990291262136,
                0.12773369460034836,
                0.550098231827112,
                0.0,
                0.046706586826347304,
                0,
                0,
                0.07142857142857144,
                -1
            ],
            [
                0,
                0.10903659447348768,
                0.8099477453067544,
                0.21611001964636542,
                0.3181818181818182,
                0.4347305389221557,
                0,
                0,
                0.28571428571428575,
                -1
            ]
        ]
    },
    "Which of the following is a dice-based game?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.00630763459340621,
                    0.05385609651012495,
                    0.007196096093123621,
                    0.0,
                    0.15638575152041703,
                    0.0,
                    0.0,
                    0.38701845285988384,
                    -1
                ],
                [
                    0.6455946886750478,
                    0.6057733735458853,
                    0.5978599780256763,
                    0.0,
                    0.2867072111207645,
                    0.0,
                    0.0,
                    0.010596026490066225,
                    -1
                ],
                [
                    0.348097676731546,
                    0.34037052994398964,
                    0.3949439258812001,
                    1.0,
                    0.5569070373588184,
                    1.0,
                    1.0,
                    0.6023855206500499,
                    -1
                ]
            ],
            "fraction_answers": {
                "sparkle": 0.26831640973218,
                "quirkle": 0.07634550394711945,
                "farkle": 0.6553380863207006
            },
            "question": "which of the following is a dice-based game?",
            "rate_limited": false,
            "answers": [
                "quirkle",
                "sparkle",
                "farkle"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "sparkle": 0.10841342879265708,
                "quirkle": 0.21662790320181802,
                "farkle": 0.8187049623961608
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    62500.0,
                    703000.0,
                    395000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    4.0
                ],
                "result_count_noun_chunks": [
                    7270.0,
                    604000.0,
                    399000.0
                ],
                "word_relation_to_question": [
                    1.1610553585796515,
                    0.031788079470198675,
                    1.8071565619501497
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    8.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    7.0
                ],
                "result_count": [
                    7230.0,
                    740000.0,
                    399000.0
                ],
                "word_count_appended": [
                    180.0,
                    330.0,
                    641.0
                ]
            },
            "z-best_answer_by_ml": [
                "farkle"
            ],
            "integer_answers": {
                "sparkle": 3,
                "quirkle": 0,
                "farkle": 5
            }
        },
        "lines": [
            [
                0,
                0.00630763459340621,
                0.05385609651012495,
                0.007196096093123621,
                0.0,
                0.15638575152041703,
                0.0,
                0.0,
                0.38701845285988384,
                -1
            ],
            [
                0,
                0.6455946886750478,
                0.6057733735458853,
                0.5978599780256763,
                0.0,
                0.2867072111207645,
                0.0,
                0.0,
                0.010596026490066225,
                -1
            ],
            [
                1,
                0.348097676731546,
                0.34037052994398964,
                0.3949439258812001,
                1.0,
                0.5569070373588184,
                1.0,
                1.0,
                0.6023855206500499,
                -1
            ]
        ]
    },
    "What word describes joining a cause just to feel good about it?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.0004657216894949905,
                    0.0004640592091955117,
                    0.001369035635192269,
                    0.08739837398373984,
                    0.09777777777777778,
                    0,
                    0,
                    0.19734488865280367,
                    1
                ],
                [
                    2.3883163563845666e-05,
                    2.3797908163872394e-05,
                    4.02657539762432e-05,
                    0.2516759378120097,
                    0.04666666666666667,
                    0,
                    0,
                    0.24648031907345513,
                    1
                ],
                [
                    0.9995103951469412,
                    0.9995121428826406,
                    0.9985906986108315,
                    0.6609256882042505,
                    0.8555555555555555,
                    0,
                    0,
                    0.5561747922737411,
                    1
                ]
            ],
            "fraction_answers": {
                "slacktivism": 0.8450448787789934,
                "gung-faux": 0.09081847839630591,
                "joinerism": 0.06413664282470068
            },
            "question": "what word describes joining a cause just to feel good about it?",
            "rate_limited": false,
            "answers": [
                "joinerism",
                "gung-faux",
                "slacktivism"
            ],
            "integer_answers": {
                "slacktivism": 6,
                "gung-faux": 0,
                "joinerism": 0
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "slacktivism": 0.40475624971005275,
                "gung-faux": 0.036194848504380775,
                "joinerism": 0.04967666415389235
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    39.0,
                    2.0,
                    84000.0
                ],
                "wikipedia_search": [
                    0.524390243902439,
                    1.5100556268720582,
                    3.9655541292255028
                ],
                "result_count_noun_chunks": [
                    68.0,
                    2.0,
                    49600.0
                ],
                "word_relation_to_question": [
                    1.184069331916822,
                    1.4788819144407308,
                    3.337048753642447
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    44.0,
                    21.0,
                    385.0
                ],
                "result_count": [
                    39.0,
                    2.0,
                    83700.0
                ]
            },
            "z-best_answer_by_ml": [
                "slacktivism"
            ]
        },
        "lines": [
            [
                0,
                0.0004657216894949905,
                0.0004640592091955117,
                0.001369035635192269,
                0.08739837398373984,
                0.09777777777777778,
                0,
                0,
                0.19734488865280367,
                1
            ],
            [
                0,
                2.3883163563845666e-05,
                2.3797908163872394e-05,
                4.02657539762432e-05,
                0.2516759378120097,
                0.04666666666666667,
                0,
                0,
                0.24648031907345513,
                1
            ],
            [
                1,
                0.9995103951469412,
                0.9995121428826406,
                0.9985906986108315,
                0.6609256882042505,
                0.8555555555555555,
                0,
                0,
                0.5561747922737411,
                1
            ]
        ]
    },
    "One of Apple\u2019s biggest flops was a product named after a man who did what?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.5547445255474452,
                    0.5652173913043478,
                    0.5563380281690141,
                    0.3467086834733893,
                    0.42105263157894735,
                    0,
                    0,
                    0.3235279213540083,
                    1
                ],
                [
                    0.1386861313868613,
                    0.13768115942028986,
                    0.14084507042253522,
                    0.2574229691876751,
                    0.3157894736842105,
                    0,
                    0,
                    0.5228159521637783,
                    1
                ],
                [
                    0.30656934306569344,
                    0.2971014492753623,
                    0.3028169014084507,
                    0.39586834733893556,
                    0.2631578947368421,
                    0,
                    0,
                    0.15365612648221347,
                    1
                ]
            ],
            "fraction_answers": {
                "invented the transistor": 0.2522067927108917,
                "developed calculus": 0.28652834371791625,
                "discovered saturn": 0.46126486357119206
            },
            "question": "one of apple\u2019s biggest flops was a product named after a man who did what?",
            "rate_limited": false,
            "answers": [
                "discovered saturn",
                "invented the transistor",
                "developed calculus"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "invented the transistor": 0.1390019413424628,
                "developed calculus": 0.21785344624757622,
                "discovered saturn": 0.32926722979193807
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    78.0,
                    19.0,
                    41.0
                ],
                "wikipedia_search": [
                    1.040126050420168,
                    0.7722689075630252,
                    1.1876050420168067
                ],
                "result_count_noun_chunks": [
                    79.0,
                    20.0,
                    43.0
                ],
                "word_relation_to_question": [
                    1.294111685416033,
                    2.0912638086551127,
                    0.6146245059288538
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    8.0,
                    6.0,
                    5.0
                ],
                "result_count": [
                    76.0,
                    19.0,
                    42.0
                ]
            },
            "z-best_answer_by_ml": [
                "discovered saturn"
            ],
            "integer_answers": {
                "invented the transistor": 1,
                "developed calculus": 1,
                "discovered saturn": 4
            }
        },
        "lines": [
            [
                0,
                0.5547445255474452,
                0.5652173913043478,
                0.5563380281690141,
                0.3467086834733893,
                0.42105263157894735,
                0,
                0,
                0.3235279213540083,
                1
            ],
            [
                0,
                0.1386861313868613,
                0.13768115942028986,
                0.14084507042253522,
                0.2574229691876751,
                0.3157894736842105,
                0,
                0,
                0.5228159521637783,
                1
            ],
            [
                1,
                0.30656934306569344,
                0.2971014492753623,
                0.3028169014084507,
                0.39586834733893556,
                0.2631578947368421,
                0,
                0,
                0.15365612648221347,
                1
            ]
        ]
    },
    "What dish is made with ham, poached eggs and Hollandaise sauce?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.0013426283364766987,
                    0.003653352605432228,
                    0.005699929408566556,
                    0.0,
                    0.15771812080536912,
                    0.0,
                    0.0,
                    0.07073509015256588,
                    1
                ],
                [
                    0.9984807696226377,
                    0.9958515669013083,
                    0.9941992256865126,
                    0.8927489177489177,
                    0.7046979865771812,
                    1.0,
                    1.0,
                    0.8758668515950069,
                    1
                ],
                [
                    0.0001766020408856366,
                    0.0004950804932595075,
                    0.0001008449049207929,
                    0.10725108225108224,
                    0.13758389261744966,
                    0.0,
                    0.0,
                    0.05339805825242719,
                    1
                ]
            ],
            "fraction_answers": {
                "eggs benedict": 0.9327306647664455,
                "benedict cumberbatch": 0.03737569507000313,
                "pope benedict": 0.02989364016355131
            },
            "question": "what dish is made with ham, poached eggs and hollandaise sauce?",
            "rate_limited": false,
            "answers": [
                "pope benedict",
                "eggs benedict",
                "benedict cumberbatch"
            ],
            "integer_answers": {
                "eggs benedict": 8,
                "benedict cumberbatch": 0,
                "pope benedict": 0
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "eggs benedict": 0.913882441902346,
                "benedict cumberbatch": 0.06309835395033804,
                "pope benedict": 0.17416954101789928
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    642.0,
                    175000.0,
                    87.0
                ],
                "wikipedia_search": [
                    0.0,
                    5.356493506493507,
                    0.6435064935064935
                ],
                "result_count_noun_chunks": [
                    5200.0,
                    907000.0,
                    92.0
                ],
                "word_relation_to_question": [
                    0.42441054091539526,
                    5.2552011095700415,
                    0.32038834951456313
                ],
                "word_count_noun_chunks": [
                    0.0,
                    81.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    142.0,
                    0.0
                ],
                "result_count": [
                    593.0,
                    441000.0,
                    78.0
                ],
                "word_count_appended": [
                    47.0,
                    210.0,
                    41.0
                ]
            },
            "z-best_answer_by_ml": [
                "eggs benedict"
            ]
        },
        "lines": [
            [
                0,
                0.0013426283364766987,
                0.003653352605432228,
                0.005699929408566556,
                0.0,
                0.15771812080536912,
                0.0,
                0.0,
                0.07073509015256588,
                1
            ],
            [
                1,
                0.9984807696226377,
                0.9958515669013083,
                0.9941992256865126,
                0.8927489177489177,
                0.7046979865771812,
                1.0,
                1.0,
                0.8758668515950069,
                1
            ],
            [
                0,
                0.0001766020408856366,
                0.0004950804932595075,
                0.0001008449049207929,
                0.10725108225108224,
                0.13758389261744966,
                0.0,
                0.0,
                0.05339805825242719,
                1
            ]
        ]
    },
    "Though perhaps more famous as butter, which of these is a location in Florida?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.9510086455331412,
                    0.46716243280129505,
                    0.4366754617414248,
                    0.19230769230769232,
                    0.6272727272727273,
                    0.0,
                    0,
                    0.2195009895391575,
                    -1
                ],
                [
                    0.0234149855907781,
                    0.5327909920921114,
                    0.4230430958663149,
                    0.5,
                    0.37272727272727274,
                    1.0,
                    0,
                    0.33228670537830846,
                    -1
                ],
                [
                    0.025576368876080693,
                    4.657510659348259e-05,
                    0.14028144239226034,
                    0.3076923076923077,
                    0.0,
                    0.0,
                    0,
                    0.4482123050825341,
                    -1
                ]
            ],
            "fraction_answers": {
                "tillamook": 0.4134182784564912,
                "kerrygold": 0.45489472166496936,
                "land o\u2019 lakes": 0.13168699987853946
            },
            "question": "though perhaps more famous as butter, which of these is a location in florida?",
            "rate_limited": false,
            "answers": [
                "tillamook",
                "kerrygold",
                "land o\u2019 lakes"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "tillamook": 0.13630479822135302,
                "kerrygold": 0.2369953266564221,
                "land o\u2019 lakes": 0.15078529286622652
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    662000.0,
                    755000.0,
                    66.0
                ],
                "wikipedia_search": [
                    0.38461538461538464,
                    1.0,
                    0.6153846153846154
                ],
                "result_count_noun_chunks": [
                    993000.0,
                    962000.0,
                    319000.0
                ],
                "word_relation_to_question": [
                    0.8780039581566299,
                    1.3291468215132336,
                    1.7928492203301363
                ],
                "word_count_noun_chunks": [
                    0.0,
                    8.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    2640.0,
                    65.0,
                    71.0
                ],
                "word_count_appended": [
                    207.0,
                    123.0,
                    0.0
                ]
            },
            "z-best_answer_by_ml": [
                "kerrygold"
            ],
            "integer_answers": {
                "tillamook": 3,
                "kerrygold": 3,
                "land o\u2019 lakes": 1
            }
        },
        "lines": [
            [
                0,
                0.9510086455331412,
                0.46716243280129505,
                0.4366754617414248,
                0.19230769230769232,
                0.6272727272727273,
                0.0,
                0,
                0.2195009895391575,
                -1
            ],
            [
                0,
                0.0234149855907781,
                0.5327909920921114,
                0.4230430958663149,
                0.5,
                0.37272727272727274,
                1.0,
                0,
                0.33228670537830846,
                -1
            ],
            [
                1,
                0.025576368876080693,
                4.657510659348259e-05,
                0.14028144239226034,
                0.3076923076923077,
                0.0,
                0.0,
                0,
                0.4482123050825341,
                -1
            ]
        ]
    },
    "Which Oscar-winning actress has NOT won the award for playing a real person?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.4693245778611632,
                    0.38994708994708993,
                    0.4735237483953787,
                    0.15571548907652633,
                    0.2886435331230284,
                    0.5,
                    0,
                    0.36453355564713924,
                    -1
                ],
                [
                    0.05347091932457787,
                    0.20687830687830688,
                    0.04589216944801028,
                    0.3973421303873431,
                    0.35962145110410093,
                    0.5,
                    0,
                    0.33415758647956795,
                    -1
                ],
                [
                    0.4772045028142589,
                    0.4031746031746032,
                    0.480584082156611,
                    0.44694238053613056,
                    0.3517350157728707,
                    0.0,
                    0,
                    0.30130885787329287,
                    -1
                ]
            ],
            "fraction_answers": {
                "hilary swank": 0.2968715879063522,
                "emma thompson": 0.24523200169990692,
                "susan sarandon": 0.4578964103937408
            },
            "question": "which oscar-winning actress has not won the award for playing a real person?",
            "rate_limited": false,
            "answers": [
                "emma thompson",
                "susan sarandon",
                "hilary swank"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "hilary swank": 0.3553788171081836,
                "emma thompson": 0.43978612305595893,
                "susan sarandon": 0.36876797387439847
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    2080000.0,
                    5540000.0,
                    1830000.0
                ],
                "wikipedia_search": [
                    5.508552174775579,
                    1.6425259138025097,
                    0.8489219114219113
                ],
                "result_count_noun_chunks": [
                    330000.0,
                    5660000.0,
                    242000.0
                ],
                "word_relation_to_question": [
                    2.167463109645772,
                    2.6534786163269133,
                    3.179058274027314
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    134.0,
                    89.0,
                    94.0
                ],
                "result_count": [
                    327000.0,
                    4760000.0,
                    243000.0
                ]
            },
            "z-best_answer_by_ml": [
                "emma thompson"
            ],
            "integer_answers": {
                "hilary swank": 2,
                "emma thompson": 2,
                "susan sarandon": 3
            }
        },
        "lines": [
            [
                1,
                0.4693245778611632,
                0.38994708994708993,
                0.4735237483953787,
                0.15571548907652633,
                0.2886435331230284,
                0.5,
                0,
                0.36453355564713924,
                -1
            ],
            [
                0,
                0.05347091932457787,
                0.20687830687830688,
                0.04589216944801028,
                0.3973421303873431,
                0.35962145110410093,
                0.5,
                0,
                0.33415758647956795,
                -1
            ],
            [
                0,
                0.4772045028142589,
                0.4031746031746032,
                0.480584082156611,
                0.44694238053613056,
                0.3517350157728707,
                0.0,
                0,
                0.30130885787329287,
                -1
            ]
        ]
    },
    "According to Alexa, which of these is NOT one the top five most popular sports sites?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.4999202883004875,
                    0.4915433403805497,
                    0.46672582076308783,
                    0.29144171779141104,
                    0.44297082228116713,
                    0.375,
                    0,
                    0.29087766390109,
                    -1
                ],
                [
                    0.17623272667153467,
                    0.3859861069163395,
                    0.17435669920141972,
                    0.37903374233128834,
                    0.3952254641909814,
                    0.125,
                    0,
                    0.3563526923702619,
                    -1
                ],
                [
                    0.3238469850279778,
                    0.12247055270311086,
                    0.35891748003549245,
                    0.3295245398773006,
                    0.16180371352785144,
                    0.5,
                    0,
                    0.35276964372864816,
                    -1
                ]
            ],
            "fraction_answers": {
                "deadspin": 0.38590488145703394,
                "sports illustrated": 0.4308035909480498,
                "yahoo sports": 0.18329152759491624
            },
            "question": "according to alexa, which of these is not one the top five most popular sports sites?",
            "rate_limited": false,
            "answers": [
                "yahoo sports",
                "sports illustrated",
                "deadspin"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "deadspin": 2,
                "sports illustrated": 3,
                "yahoo sports": 2
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    67200.0,
                    906000.0,
                    3000000.0
                ],
                "wikipedia_search": [
                    2.0855828220858896,
                    1.2096625766871165,
                    1.7047546012269938
                ],
                "result_count_noun_chunks": [
                    37500.0,
                    367000.0,
                    159000.0
                ],
                "word_relation_to_question": [
                    2.0912233609891,
                    1.4364730762973807,
                    1.4723035627135186
                ],
                "word_count_noun_chunks": [
                    1.0,
                    3.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    81.0,
                    329000.0,
                    179000.0
                ],
                "word_count_appended": [
                    43.0,
                    79.0,
                    255.0
                ]
            },
            "z-best_answer_by_ml": [
                "deadspin"
            ],
            "ml_answers": {
                "deadspin": 0.43168687157450536,
                "sports illustrated": 0.16891572906999433,
                "yahoo sports": 0.1844539530261841
            }
        },
        "lines": [
            [
                0,
                0.4999202883004875,
                0.4915433403805497,
                0.46672582076308783,
                0.29144171779141104,
                0.44297082228116713,
                0.375,
                0,
                0.29087766390109,
                -1
            ],
            [
                0,
                0.17623272667153467,
                0.3859861069163395,
                0.17435669920141972,
                0.37903374233128834,
                0.3952254641909814,
                0.125,
                0,
                0.3563526923702619,
                -1
            ],
            [
                1,
                0.3238469850279778,
                0.12247055270311086,
                0.35891748003549245,
                0.3295245398773006,
                0.16180371352785144,
                0.5,
                0,
                0.35276964372864816,
                -1
            ]
        ]
    },
    "The best-selling book \u201cThe Chocolate War\u201d is about what?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.07142857142857142,
                    1.725099624503315e-05,
                    1.8159476522823435e-05,
                    0.061224489795918366,
                    0.3333333333333333,
                    0,
                    0,
                    0.2487012987012987,
                    1
                ],
                [
                    0.9285714285714286,
                    0.21218725381390777,
                    0.19491171467830487,
                    0.2571428571428571,
                    0.5333333333333333,
                    0,
                    0,
                    0.13020631850419084,
                    1
                ],
                [
                    0.0,
                    0.7877954951898473,
                    0.8050701258451723,
                    0.6816326530612244,
                    0.13333333333333333,
                    0,
                    0,
                    0.6210923827945104,
                    1
                ]
            ],
            "fraction_answers": {
                "high school conformity": 0.11912051728864827,
                "the rise of hershey's": 0.5048206650373479,
                "sugar addiction": 0.3760588176740038
            },
            "question": "the best-selling book \u201cthe chocolate war\u201d is about what?",
            "rate_limited": false,
            "answers": [
                "high school conformity",
                "sugar addiction",
                "the rise of hershey's"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "high school conformity": 0,
                "the rise of hershey's": 4,
                "sugar addiction": 2
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    3.0,
                    36900.0,
                    137000.0
                ],
                "wikipedia_search": [
                    0.30612244897959184,
                    1.2857142857142856,
                    3.4081632653061225
                ],
                "result_count_noun_chunks": [
                    3.0,
                    32200.0,
                    133000.0
                ],
                "word_relation_to_question": [
                    1.2435064935064934,
                    0.6510315925209542,
                    3.105461913972552
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    3.0,
                    39.0,
                    0
                ],
                "word_count_appended": [
                    5.0,
                    8.0,
                    2.0
                ]
            },
            "z-best_answer_by_ml": [
                "sugar addiction"
            ],
            "ml_answers": {
                "high school conformity": 0.27220122369332844,
                "the rise of hershey's": 0.23360595186604016,
                "sugar addiction": 0.35565216794696713
            }
        },
        "lines": [
            [
                1,
                0.07142857142857142,
                1.725099624503315e-05,
                1.8159476522823435e-05,
                0.061224489795918366,
                0.3333333333333333,
                0,
                0,
                0.2487012987012987,
                1
            ],
            [
                0,
                0.9285714285714286,
                0.21218725381390777,
                0.19491171467830487,
                0.2571428571428571,
                0.5333333333333333,
                0,
                0,
                0.13020631850419084,
                1
            ],
            [
                0,
                0.0,
                0.7877954951898473,
                0.8050701258451723,
                0.6816326530612244,
                0.13333333333333333,
                0,
                0,
                0.6210923827945104,
                1
            ]
        ]
    },
    "The \u201cAmerican Craftsman\u201d style of house was an architectural reaction to what?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.0008874164349523754,
                    0.2778761061946903,
                    0.015849282296650717,
                    0.45732900136798904,
                    0.1981981981981982,
                    0.0,
                    0.0,
                    0.16206909278807116,
                    1
                ],
                [
                    0.9983434893214223,
                    0.43495575221238936,
                    0.9688995215311005,
                    0.3689329685362517,
                    0.6396396396396397,
                    1.0,
                    1.0,
                    0.3314768337393273,
                    1
                ],
                [
                    0.0007690942436253919,
                    0.28716814159292037,
                    0.015251196172248804,
                    0.17373803009575922,
                    0.16216216216216217,
                    0.0,
                    0.0,
                    0.5064540734726016,
                    1
                ]
            ],
            "fraction_answers": {
                "world war i": 0.13902613716006895,
                "industrial revolution": 0.7177810256225163,
                "the great depression": 0.1431928372174147
            },
            "question": "the \u201camerican craftsman\u201d style of house was an architectural reaction to what?",
            "rate_limited": false,
            "answers": [
                "world war i",
                "industrial revolution",
                "the great depression"
            ],
            "ml_answers": {
                "world war i": 0.21223728201881534,
                "industrial revolution": 0.8709913948164357,
                "the great depression": 0.27917803761435717
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "world war i": 1,
                "industrial revolution": 6,
                "the great depression": 1
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    6280000.0,
                    9830000.0,
                    6490000.0
                ],
                "wikipedia_search": [
                    2.286645006839945,
                    1.8446648426812584,
                    0.868690150478796
                ],
                "result_count_noun_chunks": [
                    53.0,
                    3240.0,
                    51.0
                ],
                "word_relation_to_question": [
                    0.8103454639403558,
                    1.6573841686966364,
                    2.532270367363008
                ],
                "word_count_noun_chunks": [
                    0.0,
                    12.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    9.0,
                    0.0
                ],
                "result_count": [
                    60.0,
                    67500.0,
                    52.0
                ],
                "word_count_appended": [
                    22.0,
                    71.0,
                    18.0
                ]
            },
            "z-best_answer_by_ml": [
                "industrial revolution"
            ]
        },
        "lines": [
            [
                0,
                0.0008874164349523754,
                0.2778761061946903,
                0.015849282296650717,
                0.45732900136798904,
                0.1981981981981982,
                0.0,
                0.0,
                0.16206909278807116,
                1
            ],
            [
                1,
                0.9983434893214223,
                0.43495575221238936,
                0.9688995215311005,
                0.3689329685362517,
                0.6396396396396397,
                1.0,
                1.0,
                0.3314768337393273,
                1
            ],
            [
                0,
                0.0007690942436253919,
                0.28716814159292037,
                0.015251196172248804,
                0.17373803009575922,
                0.16216216216216217,
                0.0,
                0.0,
                0.5064540734726016,
                1
            ]
        ]
    },
    "Which three-letter-titled movie grossed the most worldwide?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.26068821689259647,
                    0.30430164630908124,
                    0.2406480117820324,
                    0.0,
                    0.35370611183355005,
                    0.2,
                    0.14285714285714285,
                    0.6011904761904762,
                    -1
                ],
                [
                    0.205422314911366,
                    0.2703133297928837,
                    0.2203240058910162,
                    0.0,
                    0.36150845253576075,
                    0.4,
                    0.7142857142857143,
                    0.3898809523809524,
                    -1
                ],
                [
                    0.5338894681960376,
                    0.42538502389803506,
                    0.5390279823269514,
                    1.0,
                    0.2847854356306892,
                    0.4,
                    0.14285714285714285,
                    0.008928571428571428,
                    -1
                ]
            ],
            "fraction_answers": {
                "big": 0.41685920304217844,
                "saw": 0.2629239507331099,
                "ray": 0.32021684622471164
            },
            "question": "which three-letter-titled movie grossed the most worldwide?",
            "rate_limited": false,
            "answers": [
                "saw",
                "ray",
                "big"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "big": 0.2931177097801207,
                "saw": 0.19660718520663167,
                "ray": 0.25555583464652304
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    573000.0,
                    509000.0,
                    801000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_noun_chunks": [
                    817000.0,
                    748000.0,
                    1830000.0
                ],
                "word_relation_to_question": [
                    1.8035714285714286,
                    1.1696428571428572,
                    0.026785714285714284
                ],
                "word_count_noun_chunks": [
                    2.0,
                    4.0,
                    4.0
                ],
                "word_count_raw": [
                    1.0,
                    5.0,
                    1.0
                ],
                "result_count": [
                    12500000.0,
                    9850000.0,
                    25600000.0
                ],
                "word_count_appended": [
                    272.0,
                    278.0,
                    219.0
                ]
            },
            "z-best_answer_by_ml": [
                "big"
            ],
            "integer_answers": {
                "big": 4,
                "saw": 1,
                "ray": 3
            }
        },
        "lines": [
            [
                0,
                0.26068821689259647,
                0.30430164630908124,
                0.2406480117820324,
                0.0,
                0.35370611183355005,
                0.2,
                0.14285714285714285,
                0.6011904761904762,
                -1
            ],
            [
                0,
                0.205422314911366,
                0.2703133297928837,
                0.2203240058910162,
                0.0,
                0.36150845253576075,
                0.4,
                0.7142857142857143,
                0.3898809523809524,
                -1
            ],
            [
                1,
                0.5338894681960376,
                0.42538502389803506,
                0.5390279823269514,
                1.0,
                0.2847854356306892,
                0.4,
                0.14285714285714285,
                0.008928571428571428,
                -1
            ]
        ]
    },
    "Talking is discouraged on what Amtrak car?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.9890978618572243,
                    0.9919523425651395,
                    0.9034803079468092,
                    0.47410358565737054,
                    0.088,
                    0,
                    0.0,
                    0.13222486144101345,
                    1
                ],
                [
                    0.010516059051716214,
                    0.00776072907130853,
                    0.0567538334287714,
                    0.017928286852589643,
                    0.0,
                    0,
                    0.0,
                    0.028296195754628054,
                    1
                ],
                [
                    0.00038607909105951133,
                    0.0002869283635518999,
                    0.03976585862441942,
                    0.5079681274900398,
                    0.912,
                    0,
                    1.0,
                    0.8394789428043584,
                    1
                ]
            ],
            "fraction_answers": {
                "sports argument car": 0.5112655656382225,
                "quiet car": 0.4714122766247756,
                "meet & greet car": 0.017322157737001978
            },
            "question": "talking is discouraged on what amtrak car?",
            "rate_limited": false,
            "answers": [
                "sports argument car",
                "meet & greet car",
                "quiet car"
            ],
            "ml_answers": {
                "sports argument car": 0.30195038370570754,
                "quiet car": 0.6522742330382578,
                "meet & greet car": 0.15646435950425283
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "sports argument car": 3,
                "quiet car": 4,
                "meet & greet car": 0
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    3630000.0,
                    28400.0,
                    1050.0
                ],
                "wikipedia_search": [
                    0.9482071713147411,
                    0.035856573705179286,
                    1.0159362549800797
                ],
                "result_count_noun_chunks": [
                    1420000.0,
                    89200.0,
                    62500.0
                ],
                "word_relation_to_question": [
                    0.39667458432304037,
                    0.08488858726388417,
                    2.5184368284130754
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    3.0
                ],
                "result_count": [
                    2690000.0,
                    28600.0,
                    1050.0
                ],
                "word_count_appended": [
                    11.0,
                    0.0,
                    114.0
                ]
            },
            "z-best_answer_by_ml": [
                "quiet car"
            ]
        },
        "lines": [
            [
                0,
                0.9890978618572243,
                0.9919523425651395,
                0.9034803079468092,
                0.47410358565737054,
                0.088,
                0,
                0.0,
                0.13222486144101345,
                1
            ],
            [
                0,
                0.010516059051716214,
                0.00776072907130853,
                0.0567538334287714,
                0.017928286852589643,
                0.0,
                0,
                0.0,
                0.028296195754628054,
                1
            ],
            [
                1,
                0.00038607909105951133,
                0.0002869283635518999,
                0.03976585862441942,
                0.5079681274900398,
                0.912,
                0,
                1.0,
                0.8394789428043584,
                1
            ]
        ]
    },
    "Which of these is NOT a name of one of the Florida Keys?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.49999539974473345,
                    0.4276547559134373,
                    0.4999606981660523,
                    0.33333333333333337,
                    0.41304347826086957,
                    0,
                    0,
                    0.22679498088469696,
                    -1
                ],
                [
                    0.4996874999008566,
                    0.4906894816305989,
                    0.4840724146633074,
                    0.5,
                    0.39751552795031053,
                    0,
                    0,
                    0.4618308145875529,
                    -1
                ],
                [
                    0.00031710035441001594,
                    0.08165576245596379,
                    0.015966887170640265,
                    0.16666666666666669,
                    0.1894409937888199,
                    0,
                    0,
                    0.3113742045277501,
                    -1
                ]
            ],
            "fraction_answers": {
                "pigeon key": 0.7448594616785832,
                "turtle key": 0.0554014204224579,
                "fat deer key": 0.199739117898959
            },
            "question": "which of these is not a name of one of the florida keys?",
            "rate_limited": false,
            "answers": [
                "fat deer key",
                "turtle key",
                "pigeon key"
            ],
            "ml_answers": {
                "pigeon key": 0.09184919608600704,
                "turtle key": 0.2995443092739973,
                "fat deer key": 0.25100550444564534
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "pigeon key": 5,
                "turtle key": 0,
                "fat deer key": 1
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    23000.0,
                    2960.0,
                    133000.0
                ],
                "wikipedia_search": [
                    0.3333333333333333,
                    0.0,
                    0.6666666666666666
                ],
                "result_count_noun_chunks": [
                    38.0,
                    15400.0,
                    468000.0
                ],
                "word_relation_to_question": [
                    1.0928200764612122,
                    0.15267674164978848,
                    0.7545031818889995
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    28.0,
                    33.0,
                    100.0
                ],
                "result_count": [
                    29.0,
                    1970.0,
                    3150000.0
                ]
            },
            "z-best_answer_by_ml": [
                "turtle key"
            ]
        },
        "lines": [
            [
                0,
                0.49999539974473345,
                0.4276547559134373,
                0.4999606981660523,
                0.33333333333333337,
                0.41304347826086957,
                0,
                0,
                0.22679498088469696,
                -1
            ],
            [
                1,
                0.4996874999008566,
                0.4906894816305989,
                0.4840724146633074,
                0.5,
                0.39751552795031053,
                0,
                0,
                0.4618308145875529,
                -1
            ],
            [
                0,
                0.00031710035441001594,
                0.08165576245596379,
                0.015966887170640265,
                0.16666666666666669,
                0.1894409937888199,
                0,
                0,
                0.3113742045277501,
                -1
            ]
        ]
    },
    "Which of these verbs has two meanings that are opposites of each other?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.166073546856465,
                    0.3168896321070234,
                    0.34715312327252623,
                    0,
                    0.3218232044198895,
                    0.0,
                    0.0,
                    0.0,
                    -1
                ],
                [
                    0.3748517200474496,
                    0.20903010033444816,
                    0.04477611940298507,
                    0,
                    0.27071823204419887,
                    0.0,
                    0.3333333333333333,
                    0.25,
                    -1
                ],
                [
                    0.45907473309608543,
                    0.47408026755852845,
                    0.6080707573244887,
                    0,
                    0.4074585635359116,
                    1.0,
                    0.6666666666666666,
                    0.75,
                    -1
                ]
            ],
            "fraction_answers": {
                "cleave": 0.6236215697402401,
                "jut": 0.2118156435946307,
                "branch": 0.16456278666512916
            },
            "question": "which of these verbs has two meanings that are opposites of each other?",
            "rate_limited": false,
            "answers": [
                "branch",
                "jut",
                "cleave"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "cleave": 7,
                "jut": 0,
                "branch": 0
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    379000.0,
                    250000.0,
                    567000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    3140000.0,
                    405000.0,
                    5500000.0
                ],
                "word_relation_to_question": [
                    0.0,
                    0.5,
                    1.5
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    10.0
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    2.0
                ],
                "word_count_appended": [
                    233.0,
                    196.0,
                    295.0
                ],
                "result_count": [
                    2800000.0,
                    6320000.0,
                    7740000.0
                ]
            },
            "z-best_answer_by_ml": [
                "cleave"
            ],
            "ml_answers": {
                "cleave": 0.5783633369960238,
                "jut": 0.437175552681577,
                "branch": 0.16102412009042846
            }
        },
        "lines": [
            [
                0,
                0.166073546856465,
                0.3168896321070234,
                0.34715312327252623,
                0,
                0.3218232044198895,
                0.0,
                0.0,
                0.0,
                -1
            ],
            [
                0,
                0.3748517200474496,
                0.20903010033444816,
                0.04477611940298507,
                0,
                0.27071823204419887,
                0.0,
                0.3333333333333333,
                0.25,
                -1
            ],
            [
                1,
                0.45907473309608543,
                0.47408026755852845,
                0.6080707573244887,
                0,
                0.4074585635359116,
                1.0,
                0.6666666666666666,
                0.75,
                -1
            ]
        ]
    },
    "Which of these is NOT a real animal?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.2770992366412214,
                    0.38810837932764675,
                    0.38810837932764675,
                    0.16379310344827586,
                    0.32231128924515695,
                    0.5,
                    0.5,
                    0.39825021872265964,
                    -1
                ],
                [
                    0.272264631043257,
                    0.3702960361264426,
                    0.3702960361264426,
                    0.39655172413793105,
                    0.2859051436205745,
                    0.0,
                    0.0,
                    0.2674103237095363,
                    -1
                ],
                [
                    0.45063613231552163,
                    0.24159558454591068,
                    0.24159558454591068,
                    0.4396551724137931,
                    0.39178356713426854,
                    0.5,
                    0.5,
                    0.33433945756780403,
                    -1
                ]
            ],
            "fraction_answers": {
                "liger": 0.5093190263089541,
                "wholphin": 0.22509862536919784,
                "jackalope": 0.2655823483218481
            },
            "question": "which of these is not a real animal?",
            "rate_limited": false,
            "answers": [
                "jackalope",
                "liger",
                "wholphin"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "liger": 5,
                "wholphin": 2,
                "jackalope": 1
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    446000.0,
                    517000.0,
                    1030000.0
                ],
                "wikipedia_search": [
                    1.3448275862068966,
                    0.41379310344827586,
                    0.2413793103448276
                ],
                "result_count_noun_chunks": [
                    446000.0,
                    517000.0,
                    1030000.0
                ],
                "word_relation_to_question": [
                    0.4069991251093613,
                    0.9303587051618547,
                    0.6626421697287839
                ],
                "word_count_noun_chunks": [
                    0.0,
                    6.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    7.0,
                    0.0
                ],
                "result_count": [
                    876000.0,
                    895000.0,
                    194000.0
                ],
                "word_count_appended": [
                    532.0,
                    641.0,
                    324.0
                ]
            },
            "z-best_answer_by_ml": [
                "wholphin"
            ],
            "ml_answers": {
                "liger": 0.15095270913709402,
                "wholphin": 0.5295760613631644,
                "jackalope": 0.4680856822192782
            }
        },
        "lines": [
            [
                1,
                0.2770992366412214,
                0.38810837932764675,
                0.38810837932764675,
                0.16379310344827586,
                0.32231128924515695,
                0.5,
                0.5,
                0.39825021872265964,
                -1
            ],
            [
                0,
                0.272264631043257,
                0.3702960361264426,
                0.3702960361264426,
                0.39655172413793105,
                0.2859051436205745,
                0.0,
                0.0,
                0.2674103237095363,
                -1
            ],
            [
                0,
                0.45063613231552163,
                0.24159558454591068,
                0.24159558454591068,
                0.4396551724137931,
                0.39178356713426854,
                0.5,
                0.5,
                0.33433945756780403,
                -1
            ]
        ]
    },
    "Which Hawaiian island has active volcanoes?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.16608996539792387,
                    0.031641451114364734,
                    0.22109158186864014,
                    0.0,
                    0.17314930991217065,
                    0.04,
                    0.029411764705882353,
                    0.6946236985597674,
                    -1
                ],
                [
                    0.5207612456747405,
                    0.9110135025215552,
                    0.4616096207215541,
                    0.3795045045045045,
                    0.45420326223337515,
                    0.46,
                    0.6764705882352942,
                    0.14333183482754092,
                    -1
                ],
                [
                    0.31314878892733566,
                    0.05734504636408004,
                    0.3172987974098057,
                    0.6204954954954954,
                    0.3726474278544542,
                    0.5,
                    0.29411764705882354,
                    0.16204446661269178,
                    -1
                ]
            ],
            "fraction_answers": {
                "maui": 0.5008618198398206,
                "oahu": 0.3296372087153358,
                "big island": 0.16950097144484363
            },
            "question": "which hawaiian island has active volcanoes?",
            "rate_limited": false,
            "answers": [
                "big island",
                "maui",
                "oahu"
            ],
            "integer_answers": {
                "maui": 5,
                "oahu": 2,
                "big island": 1
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "maui": 0.40864063466044376,
                "oahu": 0.264464176859897,
                "big island": 0.12353021267826436
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    389000.0,
                    11200000.0,
                    705000.0
                ],
                "wikipedia_search": [
                    0.0,
                    1.1385135135135136,
                    1.8614864864864864
                ],
                "result_count_noun_chunks": [
                    239000.0,
                    499000.0,
                    343000.0
                ],
                "word_relation_to_question": [
                    2.7784947942390694,
                    0.5733273393101637,
                    0.6481778664507671
                ],
                "word_count_noun_chunks": [
                    2.0,
                    23.0,
                    25.0
                ],
                "word_count_raw": [
                    1.0,
                    23.0,
                    10.0
                ],
                "word_count_appended": [
                    138.0,
                    362.0,
                    297.0
                ],
                "result_count": [
                    288000.0,
                    903000.0,
                    543000.0
                ]
            },
            "z-best_answer_by_ml": [
                "maui"
            ]
        },
        "lines": [
            [
                1,
                0.16608996539792387,
                0.031641451114364734,
                0.22109158186864014,
                0.0,
                0.17314930991217065,
                0.04,
                0.029411764705882353,
                0.6946236985597674,
                -1
            ],
            [
                0,
                0.5207612456747405,
                0.9110135025215552,
                0.4616096207215541,
                0.3795045045045045,
                0.45420326223337515,
                0.46,
                0.6764705882352942,
                0.14333183482754092,
                -1
            ],
            [
                0,
                0.31314878892733566,
                0.05734504636408004,
                0.3172987974098057,
                0.6204954954954954,
                0.3726474278544542,
                0.5,
                0.29411764705882354,
                0.16204446661269178,
                -1
            ]
        ]
    },
    "Pixies, Bon Iver, Iron & Wine and Bauhaus were all once signed to which record label?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.3333333333333333,
                    0.002991921811110003,
                    0.18488888888888888,
                    0.3812222222222222,
                    0.02702702702702703,
                    0.0,
                    0.0,
                    0.30173102360602366,
                    -1
                ],
                [
                    0.3333333333333333,
                    0.9893288122070409,
                    0.25955555555555554,
                    0.6154444444444445,
                    0.9459459459459459,
                    1.0,
                    1.0,
                    0.5137362637362638,
                    -1
                ],
                [
                    0.3333333333333333,
                    0.007679265981849008,
                    0.5555555555555556,
                    0.0033333333333333335,
                    0.02702702702702703,
                    0.0,
                    0.0,
                    0.18453271265771268,
                    -1
                ]
            ],
            "fraction_answers": {
                "4ad": 0.707168044402823,
                "geffen": 0.13893265348610137,
                "subpop": 0.15389930211107564
            },
            "question": "pixies, bon iver, iron & wine and bauhaus were all once signed to which record label?",
            "rate_limited": false,
            "answers": [
                "subpop",
                "4ad",
                "geffen"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "4ad": 0.900211676063975,
                "geffen": 0.19665349022386708,
                "subpop": 0.2242492785804548
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    30.0,
                    9920.0,
                    77.0
                ],
                "wikipedia_search": [
                    2.287333333333333,
                    3.6926666666666668,
                    0.02
                ],
                "result_count_noun_chunks": [
                    41600.0,
                    58400.0,
                    125000.0
                ],
                "word_relation_to_question": [
                    2.413848188848189,
                    4.1098901098901095,
                    1.4762617012617012
                ],
                "word_count_noun_chunks": [
                    0.0,
                    72.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    34.0,
                    0.0
                ],
                "word_count_appended": [
                    1.0,
                    35.0,
                    1.0
                ],
                "result_count": [
                    229000.0,
                    229000.0,
                    229000.0
                ]
            },
            "z-best_answer_by_ml": [
                "4ad"
            ],
            "integer_answers": {
                "4ad": 6,
                "geffen": 1,
                "subpop": 1
            }
        },
        "lines": [
            [
                0,
                0.3333333333333333,
                0.002991921811110003,
                0.18488888888888888,
                0.3812222222222222,
                0.02702702702702703,
                0.0,
                0.0,
                0.30173102360602366,
                -1
            ],
            [
                1,
                0.3333333333333333,
                0.9893288122070409,
                0.25955555555555554,
                0.6154444444444445,
                0.9459459459459459,
                1.0,
                1.0,
                0.5137362637362638,
                -1
            ],
            [
                0,
                0.3333333333333333,
                0.007679265981849008,
                0.5555555555555556,
                0.0033333333333333335,
                0.02702702702702703,
                0.0,
                0.0,
                0.18453271265771268,
                -1
            ]
        ]
    },
    "Which of these would an oologist study?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.06464124111182935,
                    0.8440462277626283,
                    0.1628032345013477,
                    0.0,
                    0.1485148514851485,
                    0,
                    0,
                    0.0,
                    -1
                ],
                [
                    0.049773755656108594,
                    0.005323983898195039,
                    0.044743935309973046,
                    0.08333333333333333,
                    0.21782178217821782,
                    0,
                    0,
                    1.0,
                    -1
                ],
                [
                    0.885585003232062,
                    0.15062978833917673,
                    0.7924528301886793,
                    0.9166666666666666,
                    0.6336633663366337,
                    0,
                    0,
                    0.0,
                    -1
                ]
            ],
            "fraction_answers": {
                "ice cave": 0.20333425914349232,
                "human liver": 0.23349946506263797,
                "ostrich egg": 0.5631662757938697
            },
            "question": "which of these would an oologist study?",
            "rate_limited": false,
            "answers": [
                "ice cave",
                "human liver",
                "ostrich egg"
            ],
            "ml_answers": {
                "ice cave": 0.3165694602784449,
                "human liver": 0.1597590304663733,
                "ostrich egg": 0.47850672869838173
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "ice cave": 1,
                "human liver": 1,
                "ostrich egg": 4
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    13000.0,
                    82.0,
                    2320.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.08333333333333333,
                    0.9166666666666666
                ],
                "result_count_noun_chunks": [
                    302.0,
                    83.0,
                    1470.0
                ],
                "word_relation_to_question": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    15.0,
                    22.0,
                    64.0
                ],
                "result_count": [
                    100.0,
                    77.0,
                    1370.0
                ]
            },
            "z-best_answer_by_ml": [
                "ostrich egg"
            ]
        },
        "lines": [
            [
                0,
                0.06464124111182935,
                0.8440462277626283,
                0.1628032345013477,
                0.0,
                0.1485148514851485,
                0,
                0,
                0.0,
                -1
            ],
            [
                0,
                0.049773755656108594,
                0.005323983898195039,
                0.044743935309973046,
                0.08333333333333333,
                0.21782178217821782,
                0,
                0,
                1.0,
                -1
            ],
            [
                1,
                0.885585003232062,
                0.15062978833917673,
                0.7924528301886793,
                0.9166666666666666,
                0.6336633663366337,
                0,
                0,
                0.0,
                -1
            ]
        ]
    },
    "Who holds the record as the youngest solo artist with a Billboard #1 hit?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.3281786941580756,
                    0.300462249614792,
                    0.3333333333333333,
                    0.33041159359030725,
                    0.34782608695652173,
                    0.625,
                    0.34782608695652173,
                    0.2954332203823272,
                    0
                ],
                [
                    0.3281786941580756,
                    0.46841294298921415,
                    0.3333333333333333,
                    0.5173005833887676,
                    0.21739130434782608,
                    0.125,
                    0.21739130434782608,
                    0.4040553510717387,
                    0
                ],
                [
                    0.3436426116838488,
                    0.23112480739599384,
                    0.3333333333333333,
                    0.15228782302092517,
                    0.43478260869565216,
                    0.25,
                    0.43478260869565216,
                    0.3005114285459341,
                    0
                ]
            ],
            "fraction_answers": {
                "justin bieber": 0.36355890812398484,
                "stevie wonder": 0.3100581526714174,
                "michael jackson": 0.3263829392045977
            },
            "question": "who holds the record as the youngest solo artist with a billboard #1 hit?",
            "rate_limited": false,
            "answers": [
                "justin bieber",
                "michael jackson",
                "stevie wonder"
            ],
            "integer_answers": {
                "justin bieber": 2,
                "stevie wonder": 3,
                "michael jackson": 3
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "justin bieber": 0.4940140918232028,
                "stevie wonder": 0.3922654324524057,
                "michael jackson": 0.21877190902081659
            },
            "question_type": 0,
            "data": {
                "result_count_important_words": [
                    97500.0,
                    152000.0,
                    75000.0
                ],
                "wikipedia_search": [
                    2.643292748722458,
                    4.138404667110141,
                    1.2183025841674013
                ],
                "result_count_noun_chunks": [
                    751000.0,
                    751000.0,
                    751000.0
                ],
                "word_relation_to_question": [
                    2.0680325426762907,
                    2.8283874575021706,
                    2.1035799998215388
                ],
                "word_count_noun_chunks": [
                    15.0,
                    3.0,
                    6.0
                ],
                "word_count_raw": [
                    8.0,
                    5.0,
                    10.0
                ],
                "word_count_appended": [
                    8.0,
                    5.0,
                    10.0
                ],
                "result_count": [
                    1910000.0,
                    1910000.0,
                    2000000.0
                ]
            },
            "z-best_answer_by_ml": [
                "justin bieber"
            ]
        },
        "lines": [
            [
                0,
                0.3281786941580756,
                0.300462249614792,
                0.3333333333333333,
                0.33041159359030725,
                0.34782608695652173,
                0.625,
                0.34782608695652173,
                0.2954332203823272,
                0
            ],
            [
                0,
                0.3281786941580756,
                0.46841294298921415,
                0.3333333333333333,
                0.5173005833887676,
                0.21739130434782608,
                0.125,
                0.21739130434782608,
                0.4040553510717387,
                0
            ],
            [
                1,
                0.3436426116838488,
                0.23112480739599384,
                0.3333333333333333,
                0.15228782302092517,
                0.43478260869565216,
                0.25,
                0.43478260869565216,
                0.3005114285459341,
                0
            ]
        ]
    },
    "Which of these states does NOT touch the Mason-Dixon Line?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.12962962962962965,
                    0.3576725613234455,
                    0.05509924709103353,
                    0.3343458009483992,
                    0.44342762063227953,
                    0.3793103448275862,
                    0.3666666666666667,
                    0.34013709218127086,
                    -1
                ],
                [
                    0.41260558804418457,
                    0.3394181403308614,
                    0.48032169746748804,
                    0.3213965038300738,
                    0.23710482529118138,
                    0.15517241379310343,
                    0.2,
                    0.34037100317305347,
                    -1
                ],
                [
                    0.45776478232618584,
                    0.30290929834569313,
                    0.46457905544147843,
                    0.344257695221527,
                    0.3194675540765391,
                    0.46551724137931033,
                    0.43333333333333335,
                    0.3194919046456757,
                    -1
                ]
            ],
            "fraction_answers": {
                "tennessee": 0.2231697838075643,
                "delaware": 0.3784024570175135,
                "west virginia": 0.3984277591749222
            },
            "question": "which of these states does not touch the mason-dixon line?",
            "rate_limited": false,
            "answers": [
                "west virginia",
                "delaware",
                "tennessee"
            ],
            "ml_answers": {
                "tennessee": 0.6609575406739909,
                "delaware": 0.10414100491120391,
                "west virginia": 0.20548646371500914
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "tennessee": 2,
                "delaware": 4,
                "west virginia": 2
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    499000.0,
                    563000.0,
                    691000.0
                ],
                "wikipedia_search": [
                    0.6626167962064031,
                    0.7144139846797048,
                    0.622969219113892
                ],
                "result_count_noun_chunks": [
                    2600000.0,
                    115000.0,
                    207000.0
                ],
                "word_relation_to_question": [
                    0.6394516312749166,
                    0.6385159873077861,
                    0.7220323814172972
                ],
                "word_count_noun_chunks": [
                    7.0,
                    20.0,
                    2.0
                ],
                "word_count_raw": [
                    4.0,
                    9.0,
                    2.0
                ],
                "result_count": [
                    1140000.0,
                    269000.0,
                    130000.0
                ],
                "word_count_appended": [
                    68.0,
                    316.0,
                    217.0
                ]
            },
            "z-best_answer_by_ml": [
                "tennessee"
            ]
        },
        "lines": [
            [
                0,
                0.12962962962962965,
                0.3576725613234455,
                0.05509924709103353,
                0.3343458009483992,
                0.44342762063227953,
                0.3793103448275862,
                0.3666666666666667,
                0.34013709218127086,
                -1
            ],
            [
                0,
                0.41260558804418457,
                0.3394181403308614,
                0.48032169746748804,
                0.3213965038300738,
                0.23710482529118138,
                0.15517241379310343,
                0.2,
                0.34037100317305347,
                -1
            ],
            [
                1,
                0.45776478232618584,
                0.30290929834569313,
                0.46457905544147843,
                0.344257695221527,
                0.3194675540765391,
                0.46551724137931033,
                0.43333333333333335,
                0.3194919046456757,
                -1
            ]
        ]
    },
    "One of Tupac Shakur\u2019s biggest posthumous hits samples what singer?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.030400572246065807,
                    0.029178674351585013,
                    0.09443269908386187,
                    0.36768018018018017,
                    0.14084507042253522,
                    0,
                    0.0,
                    0.5682993197278912,
                    1
                ],
                [
                    0.7725321888412017,
                    0.7744956772334294,
                    0.4498473103124266,
                    0.4671546546546546,
                    0.19718309859154928,
                    0,
                    0.0,
                    0.12639455782312928,
                    1
                ],
                [
                    0.19706723891273248,
                    0.1963256484149856,
                    0.45571999060371154,
                    0.16516516516516516,
                    0.6619718309859155,
                    0,
                    1.0,
                    0.3053061224489796,
                    1
                ]
            ],
            "fraction_answers": {
                "john mellencamp": 0.39822964106519876,
                "bruce hornsby": 0.42593657093306997,
                "christopher cross": 0.17583378800173133
            },
            "question": "one of tupac shakur\u2019s biggest posthumous hits samples what singer?",
            "rate_limited": false,
            "answers": [
                "christopher cross",
                "john mellencamp",
                "bruce hornsby"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "john mellencamp": 3,
                "bruce hornsby": 3,
                "christopher cross": 1
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    81.0,
                    2150.0,
                    545.0
                ],
                "wikipedia_search": [
                    1.1030405405405406,
                    1.4014639639639639,
                    0.4954954954954955
                ],
                "result_count_noun_chunks": [
                    80400.0,
                    383000.0,
                    388000.0
                ],
                "word_relation_to_question": [
                    2.841496598639456,
                    0.6319727891156464,
                    1.5265306122448978
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    7.0
                ],
                "word_count_appended": [
                    10.0,
                    14.0,
                    47.0
                ],
                "result_count": [
                    85.0,
                    2160.0,
                    551.0
                ]
            },
            "z-best_answer_by_ml": [
                "bruce hornsby"
            ],
            "ml_answers": {
                "john mellencamp": 0.27104034102772984,
                "bruce hornsby": 0.6009807159329703,
                "christopher cross": 0.1757535581778849
            }
        },
        "lines": [
            [
                0,
                0.030400572246065807,
                0.029178674351585013,
                0.09443269908386187,
                0.36768018018018017,
                0.14084507042253522,
                0,
                0.0,
                0.5682993197278912,
                1
            ],
            [
                0,
                0.7725321888412017,
                0.7744956772334294,
                0.4498473103124266,
                0.4671546546546546,
                0.19718309859154928,
                0,
                0.0,
                0.12639455782312928,
                1
            ],
            [
                1,
                0.19706723891273248,
                0.1963256484149856,
                0.45571999060371154,
                0.16516516516516516,
                0.6619718309859155,
                0,
                1.0,
                0.3053061224489796,
                1
            ]
        ]
    },
    "To help first create Maps, Google acquired what company?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.13220518244315177,
                    0.3587180241105557,
                    0.5458231532983313,
                    0.10200665237429944,
                    0.3333333333333333,
                    0.0,
                    0.11538461538461539,
                    0.2353580380175633,
                    1
                ],
                [
                    0.007359421822668782,
                    0.02087621287856513,
                    0.003482871549617924,
                    0.7842139097653803,
                    0.0582010582010582,
                    0.0,
                    0.09615384615384616,
                    0.5118666624399094,
                    1
                ],
                [
                    0.8604353957341795,
                    0.6204057630108791,
                    0.45069397515205073,
                    0.1137794378603202,
                    0.6084656084656085,
                    1.0,
                    0.7884615384615384,
                    0.2527752995425272,
                    1
                ]
            ],
            "fraction_answers": {
                "mapquest": 0.22785362487023128,
                "waze": 0.586877127278388,
                "where 2 technologies": 0.18526924785138074
            },
            "question": "to help first create maps, google acquired what company?",
            "rate_limited": false,
            "answers": [
                "mapquest",
                "where 2 technologies",
                "waze"
            ],
            "ml_answers": {
                "mapquest": 0.11703557173224517,
                "waze": 0.5214195514788521,
                "where 2 technologies": 0.18469787495649054
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "mapquest": 1,
                "waze": 5,
                "where 2 technologies": 2
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    122000.0,
                    7100.0,
                    211000.0
                ],
                "wikipedia_search": [
                    0.6120399142457966,
                    4.705283458592282,
                    0.6826766271619212
                ],
                "result_count_noun_chunks": [
                    1050000.0,
                    6700.0,
                    867000.0
                ],
                "word_relation_to_question": [
                    1.4121482281053799,
                    3.0711999746394563,
                    1.5166517972551632
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    12.0
                ],
                "word_count_raw": [
                    6.0,
                    5.0,
                    41.0
                ],
                "result_count": [
                    120000.0,
                    6680.0,
                    781000.0
                ],
                "word_count_appended": [
                    378.0,
                    66.0,
                    690.0
                ]
            },
            "z-best_answer_by_ml": [
                "waze"
            ]
        },
        "lines": [
            [
                0,
                0.13220518244315177,
                0.3587180241105557,
                0.5458231532983313,
                0.10200665237429944,
                0.3333333333333333,
                0.0,
                0.11538461538461539,
                0.2353580380175633,
                1
            ],
            [
                1,
                0.007359421822668782,
                0.02087621287856513,
                0.003482871549617924,
                0.7842139097653803,
                0.0582010582010582,
                0.0,
                0.09615384615384616,
                0.5118666624399094,
                1
            ],
            [
                0,
                0.8604353957341795,
                0.6204057630108791,
                0.45069397515205073,
                0.1137794378603202,
                0.6084656084656085,
                1.0,
                0.7884615384615384,
                0.2527752995425272,
                1
            ]
        ]
    },
    "The lyrics to \u201cThe Star-Spangled Banner\u201d were written during what conflict?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.43223647518126046,
                    0.45228215767634855,
                    0.532648569332355,
                    0.19660919540229885,
                    0.3016759776536313,
                    0.10526315789473684,
                    0.038461538461538464,
                    0.15146133925268218,
                    1
                ],
                [
                    0.32069157836029,
                    0.3278008298755187,
                    0.32575201760821715,
                    0.0932758620689655,
                    0.20670391061452514,
                    0.0,
                    0.07692307692307693,
                    0.08851646318904921,
                    1
                ],
                [
                    0.24707194645844952,
                    0.21991701244813278,
                    0.14159941305942772,
                    0.7101149425287357,
                    0.49162011173184356,
                    0.8947368421052632,
                    0.8846153846153846,
                    0.7600221975582686,
                    1
                ]
            ],
            "fraction_answers": {
                "american revolution": 0.17995796732995534,
                "the war of 1812": 0.5437122313131882,
                "the civil war": 0.2763298013568565
            },
            "question": "the lyrics to \u201cthe star-spangled banner\u201d were written during what conflict?",
            "rate_limited": false,
            "answers": [
                "the civil war",
                "american revolution",
                "the war of 1812"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "american revolution": 0,
                "the war of 1812": 5,
                "the civil war": 3
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    218000.0,
                    158000.0,
                    106000.0
                ],
                "wikipedia_search": [
                    1.1796551724137931,
                    0.559655172413793,
                    4.260689655172414
                ],
                "result_count_noun_chunks": [
                    726000.0,
                    444000.0,
                    193000.0
                ],
                "word_relation_to_question": [
                    0.757306696263411,
                    0.44258231594524605,
                    3.800110987791343
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    17.0
                ],
                "word_count_raw": [
                    1.0,
                    2.0,
                    23.0
                ],
                "result_count": [
                    155000.0,
                    115000.0,
                    88600.0
                ],
                "word_count_appended": [
                    54.0,
                    37.0,
                    88.0
                ]
            },
            "z-best_answer_by_ml": [
                "the war of 1812"
            ],
            "ml_answers": {
                "american revolution": 0.2228339007349677,
                "the war of 1812": 0.8864035959478959,
                "the civil war": 0.12822841149471367
            }
        },
        "lines": [
            [
                0,
                0.43223647518126046,
                0.45228215767634855,
                0.532648569332355,
                0.19660919540229885,
                0.3016759776536313,
                0.10526315789473684,
                0.038461538461538464,
                0.15146133925268218,
                1
            ],
            [
                0,
                0.32069157836029,
                0.3278008298755187,
                0.32575201760821715,
                0.0932758620689655,
                0.20670391061452514,
                0.0,
                0.07692307692307693,
                0.08851646318904921,
                1
            ],
            [
                1,
                0.24707194645844952,
                0.21991701244813278,
                0.14159941305942772,
                0.7101149425287357,
                0.49162011173184356,
                0.8947368421052632,
                0.8846153846153846,
                0.7600221975582686,
                1
            ]
        ]
    },
    "Where do marsupials keep their undeveloped young?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.00024790678704806994,
                    0.00040771799138282516,
                    0.20243776877194206,
                    0,
                    0.07653061224489796,
                    0,
                    0.0,
                    1.0,
                    3
                ],
                [
                    0.00012795189008932642,
                    0.00013223286207010546,
                    0.00019740585482541195,
                    0,
                    0.04081632653061224,
                    0,
                    0.0,
                    0.0,
                    3
                ],
                [
                    0.9996241413228626,
                    0.9994600491465471,
                    0.7973648253732325,
                    0,
                    0.8826530612244898,
                    0,
                    1.0,
                    0.0,
                    3
                ]
            ],
            "fraction_answers": {
                "in their pouches": 0.21327066763254512,
                "underwater": 0.7798503461778553,
                "in a paper bag": 0.006878986189599514
            },
            "question": "where do marsupials keep their undeveloped young?",
            "rate_limited": false,
            "answers": [
                "in their pouches",
                "in a paper bag",
                "underwater"
            ],
            "integer_answers": {
                "in their pouches": 1,
                "underwater": 5,
                "in a paper bag": 0
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "in their pouches": 0.0865845322079359,
                "underwater": 0.7254589849209157,
                "in a paper bag": 0.06967982821667003
            },
            "question_type": 3,
            "data": {
                "result_count_important_words": [
                    37.0,
                    12.0,
                    90700.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    52300.0,
                    51.0,
                    206000.0
                ],
                "word_relation_to_question": [
                    2.0,
                    0.0,
                    0.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count": [
                    31.0,
                    16.0,
                    125000.0
                ],
                "word_count_appended": [
                    15.0,
                    8.0,
                    173.0
                ]
            },
            "z-best_answer_by_ml": [
                "underwater"
            ]
        },
        "lines": [
            [
                1,
                0.00024790678704806994,
                0.00040771799138282516,
                0.20243776877194206,
                0,
                0.07653061224489796,
                0,
                0.0,
                1.0,
                3
            ],
            [
                0,
                0.00012795189008932642,
                0.00013223286207010546,
                0.00019740585482541195,
                0,
                0.04081632653061224,
                0,
                0.0,
                0.0,
                3
            ],
            [
                0,
                0.9996241413228626,
                0.9994600491465471,
                0.7973648253732325,
                0,
                0.8826530612244898,
                0,
                1.0,
                0.0,
                3
            ]
        ]
    },
    "Mount Rushmore was named after a person with what profession?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.17424772568229532,
                    0.024808091457187098,
                    0.10596579057154777,
                    0.265625,
                    0.29473684210526313,
                    0.0,
                    0.0,
                    0.30219344608879495,
                    1
                ],
                [
                    0.37788663400979705,
                    0.5337198958916747,
                    0.5340008343763037,
                    0.6437190594059405,
                    0.3539473684210526,
                    0.25,
                    1.0,
                    0.36751233262861177,
                    1
                ],
                [
                    0.44786564030790765,
                    0.4414720126511383,
                    0.3600333750521485,
                    0.0906559405940594,
                    0.3513157894736842,
                    0.75,
                    0.0,
                    0.3302942212825934,
                    1
                ]
            ],
            "fraction_answers": {
                "architect": 0.3464546224201914,
                "prospector": 0.14594711198813604,
                "lawyer": 0.5075982655916725
            },
            "question": "mount rushmore was named after a person with what profession?",
            "rate_limited": false,
            "answers": [
                "prospector",
                "lawyer",
                "architect"
            ],
            "ml_answers": {
                "architect": 0.37761737743748586,
                "prospector": -0.003536403777590097,
                "lawyer": 0.7575932214470541
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "architect": 2,
                "prospector": 0,
                "lawyer": 6
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    753.0,
                    16200.0,
                    13400.0
                ],
                "wikipedia_search": [
                    1.0625,
                    2.574876237623762,
                    0.3626237623762376
                ],
                "result_count_noun_chunks": [
                    2540000.0,
                    12800000.0,
                    8630000.0
                ],
                "word_relation_to_question": [
                    1.2087737843551796,
                    1.4700493305144469,
                    1.3211768851303733
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    3.0
                ],
                "word_count_raw": [
                    0.0,
                    7.0,
                    0.0
                ],
                "word_count_appended": [
                    224.0,
                    269.0,
                    267.0
                ],
                "result_count": [
                    49800.0,
                    108000.0,
                    128000.0
                ]
            },
            "z-best_answer_by_ml": [
                "lawyer"
            ]
        },
        "lines": [
            [
                0,
                0.17424772568229532,
                0.024808091457187098,
                0.10596579057154777,
                0.265625,
                0.29473684210526313,
                0.0,
                0.0,
                0.30219344608879495,
                1
            ],
            [
                1,
                0.37788663400979705,
                0.5337198958916747,
                0.5340008343763037,
                0.6437190594059405,
                0.3539473684210526,
                0.25,
                1.0,
                0.36751233262861177,
                1
            ],
            [
                0,
                0.44786564030790765,
                0.4414720126511383,
                0.3600333750521485,
                0.0906559405940594,
                0.3513157894736842,
                0.75,
                0.0,
                0.3302942212825934,
                1
            ]
        ]
    },
    "Which verb describes the sound minerals make when they are heated?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.7241379310344828,
                    1.0,
                    0,
                    0.8741830065359477,
                    2
                ],
                [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.13793103448275862,
                    0.0,
                    0,
                    0.007352941176470588,
                    2
                ],
                [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.13793103448275862,
                    0.0,
                    0,
                    0.1184640522875817,
                    2
                ]
            ],
            "fraction_answers": {
                "frangelle": 0.020754853665604173,
                "decrepitate": 0.9426172767957758,
                "recleft": 0.03662786953862004
            },
            "question": "which verb describes the sound minerals make when they are heated?",
            "rate_limited": false,
            "answers": [
                "decrepitate",
                "frangelle",
                "recleft"
            ],
            "ml_answers": {
                "frangelle": 0.1334906788342909,
                "decrepitate": 0.5221530174453806,
                "recleft": 0.1334906788342909
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "frangelle": 0,
                "decrepitate": 7,
                "recleft": 0
            },
            "question_type": 2,
            "data": {
                "result_count_important_words": [
                    4460.0,
                    0,
                    0
                ],
                "wikipedia_search": [
                    2.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    4850.0,
                    0,
                    0
                ],
                "word_relation_to_question": [
                    3.496732026143791,
                    0.029411764705882353,
                    0.4738562091503268
                ],
                "word_count_noun_chunks": [
                    7.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    84.0,
                    16.0,
                    16.0
                ],
                "result_count": [
                    4470.0,
                    0,
                    0
                ]
            },
            "z-best_answer_by_ml": [
                "decrepitate"
            ]
        },
        "lines": [
            [
                1,
                1.0,
                1.0,
                1.0,
                1.0,
                0.7241379310344828,
                1.0,
                0,
                0.8741830065359477,
                2
            ],
            [
                0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.13793103448275862,
                0.0,
                0,
                0.007352941176470588,
                2
            ],
            [
                0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.13793103448275862,
                0.0,
                0,
                0.1184640522875817,
                2
            ]
        ]
    },
    "Which of these celebrities has NOT been a ProActiv spokesperson?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.22005044136191676,
                    0.25699558173784975,
                    0.21850542792224187,
                    0.4431818181818182,
                    0.2665198237885462,
                    0.06,
                    0.05555555555555558,
                    0.35714285714285715,
                    -1
                ],
                [
                    0.3436317780580076,
                    0.32547864506627394,
                    0.3434738702347892,
                    0.23958333333333331,
                    0.32599118942731276,
                    0.46,
                    0.4722222222222222,
                    0.47619047619047616,
                    -1
                ],
                [
                    0.43631778058007564,
                    0.4175257731958763,
                    0.43802070184296893,
                    0.3172348484848485,
                    0.40748898678414097,
                    0.48,
                    0.4722222222222222,
                    0.16666666666666669,
                    -1
                ]
            ],
            "fraction_answers": {
                "selena gomez": 0.2161307550558002,
                "katy perry": 0.5305121235773035,
                "lindsay lohan": 0.2533571213668962
            },
            "question": "which of these celebrities has not been a proactiv spokesperson?",
            "rate_limited": false,
            "answers": [
                "katy perry",
                "lindsay lohan",
                "selena gomez"
            ],
            "ml_answers": {
                "selena gomez": 0.4132478724743926,
                "katy perry": 0.24196760807231368,
                "lindsay lohan": 0.28797855107910764
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "selena gomez": 1,
                "katy perry": 6,
                "lindsay lohan": 1
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    330000.0,
                    237000.0,
                    112000.0
                ],
                "wikipedia_search": [
                    0.34090909090909094,
                    1.5625,
                    1.0965909090909092
                ],
                "result_count_noun_chunks": [
                    446000.0,
                    248000.0,
                    98200.0
                ],
                "word_relation_to_question": [
                    0.2857142857142857,
                    0.047619047619047616,
                    0.6666666666666666
                ],
                "word_count_noun_chunks": [
                    22.0,
                    2.0,
                    1.0
                ],
                "word_count_raw": [
                    16.0,
                    1.0,
                    1.0
                ],
                "word_count_appended": [
                    106.0,
                    79.0,
                    42.0
                ],
                "result_count": [
                    444000.0,
                    248000.0,
                    101000.0
                ]
            },
            "z-best_answer_by_ml": [
                "selena gomez"
            ]
        },
        "lines": [
            [
                0,
                0.22005044136191676,
                0.25699558173784975,
                0.21850542792224187,
                0.4431818181818182,
                0.2665198237885462,
                0.06,
                0.05555555555555558,
                0.35714285714285715,
                -1
            ],
            [
                0,
                0.3436317780580076,
                0.32547864506627394,
                0.3434738702347892,
                0.23958333333333331,
                0.32599118942731276,
                0.46,
                0.4722222222222222,
                0.47619047619047616,
                -1
            ],
            [
                1,
                0.43631778058007564,
                0.4175257731958763,
                0.43802070184296893,
                0.3172348484848485,
                0.40748898678414097,
                0.48,
                0.4722222222222222,
                0.16666666666666669,
                -1
            ]
        ]
    },
    "Which of these do NOT have flippers?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.21334128102308442,
                    0.28383641674780913,
                    0.28383641674780913,
                    0.5,
                    0.4422336328626444,
                    0.5,
                    0.5,
                    0.5,
                    -1
                ],
                [
                    0.29245691990896283,
                    0.3184031158714703,
                    0.3184031158714703,
                    0.4,
                    0.11681643132220798,
                    0.5,
                    0.0,
                    0.5,
                    -1
                ],
                [
                    0.49420179906795275,
                    0.3977604673807206,
                    0.3977604673807206,
                    0.09999999999999998,
                    0.44094993581514763,
                    0.0,
                    0.5,
                    0.0,
                    -1
                ]
            ],
            "fraction_answers": {
                "new yorkers": 0.19418806315466322,
                "pinball machines": 0.41733183258886464,
                "dolphins": 0.38848010425647217
            },
            "question": "which of these do not have flippers?",
            "rate_limited": false,
            "answers": [
                "new yorkers",
                "dolphins",
                "pinball machines"
            ],
            "integer_answers": {
                "new yorkers": 3,
                "pinball machines": 3,
                "dolphins": 2
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "new yorkers": 0.5666401036263058,
                "pinball machines": 0.19052309899252487,
                "dolphins": 0.4416991905533221
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    444000.0,
                    373000.0,
                    210000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.2,
                    0.8
                ],
                "result_count_noun_chunks": [
                    444000.0,
                    373000.0,
                    210000.0
                ],
                "word_relation_to_question": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    4.0
                ],
                "word_count_raw": [
                    0.0,
                    20.0,
                    0.0
                ],
                "result_count": [
                    5290000.0,
                    3830000.0,
                    107000.0
                ],
                "word_count_appended": [
                    90.0,
                    597.0,
                    92.0
                ]
            },
            "z-best_answer_by_ml": [
                "new yorkers"
            ]
        },
        "lines": [
            [
                1,
                0.21334128102308442,
                0.28383641674780913,
                0.28383641674780913,
                0.5,
                0.4422336328626444,
                0.5,
                0.5,
                0.5,
                -1
            ],
            [
                0,
                0.29245691990896283,
                0.3184031158714703,
                0.3184031158714703,
                0.4,
                0.11681643132220798,
                0.5,
                0.0,
                0.5,
                -1
            ],
            [
                0,
                0.49420179906795275,
                0.3977604673807206,
                0.3977604673807206,
                0.09999999999999998,
                0.44094993581514763,
                0.0,
                0.5,
                0.0,
                -1
            ]
        ]
    },
    "In \u201cPeanuts,\u201d what breed of dog is Snoopy?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.0938709608958065,
                    0.20209667762111097,
                    2.6046308293297776e-05,
                    0.3698524365133837,
                    0.08294392523364486,
                    0.0,
                    0.007518796992481203,
                    0.2921716434847453,
                    1
                ],
                [
                    7.019915336555964e-05,
                    7.390102390622715e-05,
                    0.6486041476958465,
                    0.31762611530542206,
                    0.19742990654205608,
                    0.0,
                    0.0,
                    0.1082028337061894,
                    1
                ],
                [
                    0.9060588399508279,
                    0.7978294213549828,
                    0.35136980599586015,
                    0.3125214481811942,
                    0.719626168224299,
                    1.0,
                    0.9924812030075187,
                    0.5996255228090653,
                    1
                ]
            ],
            "fraction_answers": {
                "beagle": 0.7099390511904685,
                "border collie": 0.13106006088118324,
                "pitbull": 0.15900088792834827
            },
            "question": "in \u201cpeanuts,\u201d what breed of dog is snoopy?",
            "rate_limited": false,
            "answers": [
                "border collie",
                "pitbull",
                "beagle"
            ],
            "ml_answers": {
                "beagle": 0.8502176121370388,
                "border collie": 0.24287070603094596,
                "pitbull": 0.13171728994214257
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "beagle": 6,
                "border collie": 1,
                "pitbull": 1
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    134000.0,
                    49.0,
                    529000.0
                ],
                "wikipedia_search": [
                    0.7397048730267674,
                    0.6352522306108441,
                    0.6250428963623884
                ],
                "result_count_noun_chunks": [
                    51.0,
                    1270000.0,
                    688000.0
                ],
                "word_relation_to_question": [
                    0.8765149304542359,
                    0.3246085011185682,
                    1.798876568427196
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    132.0
                ],
                "word_count_appended": [
                    71.0,
                    169.0,
                    616.0
                ],
                "result_count": [
                    115000.0,
                    86.0,
                    1110000.0
                ]
            },
            "z-best_answer_by_ml": [
                "beagle"
            ]
        },
        "lines": [
            [
                0,
                0.0938709608958065,
                0.20209667762111097,
                2.6046308293297776e-05,
                0.3698524365133837,
                0.08294392523364486,
                0.0,
                0.007518796992481203,
                0.2921716434847453,
                1
            ],
            [
                0,
                7.019915336555964e-05,
                7.390102390622715e-05,
                0.6486041476958465,
                0.31762611530542206,
                0.19742990654205608,
                0.0,
                0.0,
                0.1082028337061894,
                1
            ],
            [
                1,
                0.9060588399508279,
                0.7978294213549828,
                0.35136980599586015,
                0.3125214481811942,
                0.719626168224299,
                1.0,
                0.9924812030075187,
                0.5996255228090653,
                1
            ]
        ]
    },
    "The '70s sitcom \u201cThree's Company\u201d was about three people who were what?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.999875691130292,
                    0.8191343043772686,
                    0.9998906102580231,
                    0.28205128205128205,
                    0.9547511312217195,
                    1.0,
                    1.0,
                    0.17916666666666667,
                    1
                ],
                [
                    0.0,
                    4.206081152129749e-06,
                    0.0,
                    0.3333333333333333,
                    0.01809954751131222,
                    0.0,
                    0.0,
                    0.6708333333333334,
                    1
                ],
                [
                    0.00012430886970809035,
                    0.1808614895415792,
                    0.00010938974197694612,
                    0.3846153846153846,
                    0.027149321266968326,
                    0.0,
                    0.0,
                    0.15,
                    1
                ]
            ],
            "fraction_answers": {
                "roommates": 0.7793587107131564,
                "trivia show hosts": 0.12778380253239138,
                "panda bears": 0.09285748675445216
            },
            "question": "the '70s sitcom \u201cthree's company\u201d was about three people who were what?",
            "rate_limited": false,
            "answers": [
                "roommates",
                "trivia show hosts",
                "panda bears"
            ],
            "ml_answers": {
                "roommates": 0.8683863264219358,
                "trivia show hosts": 0.033997470516865094,
                "panda bears": 0.1076450892550797
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "roommates": 6,
                "trivia show hosts": 1,
                "panda bears": 1
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    779000.0,
                    4.0,
                    172000.0
                ],
                "wikipedia_search": [
                    0.8461538461538461,
                    1.0,
                    1.1538461538461537
                ],
                "result_count_noun_chunks": [
                    585000.0,
                    0,
                    64.0
                ],
                "word_relation_to_question": [
                    0.35833333333333334,
                    1.3416666666666668,
                    0.3
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    4.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    211.0,
                    4.0,
                    6.0
                ],
                "result_count": [
                    185000.0,
                    0,
                    23.0
                ]
            },
            "z-best_answer_by_ml": [
                "roommates"
            ]
        },
        "lines": [
            [
                1,
                0.999875691130292,
                0.8191343043772686,
                0.9998906102580231,
                0.28205128205128205,
                0.9547511312217195,
                1.0,
                1.0,
                0.17916666666666667,
                1
            ],
            [
                0,
                0.0,
                4.206081152129749e-06,
                0.0,
                0.3333333333333333,
                0.01809954751131222,
                0.0,
                0.0,
                0.6708333333333334,
                1
            ],
            [
                0,
                0.00012430886970809035,
                0.1808614895415792,
                0.00010938974197694612,
                0.3846153846153846,
                0.027149321266968326,
                0.0,
                0.0,
                0.15,
                1
            ]
        ]
    },
    "What was the first popular home video game?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.017394555521925686,
                    0.0065750788474902445,
                    0.2250124316260567,
                    0.09539914521327161,
                    0.14285714285714285,
                    0.0,
                    0.0,
                    0.33717116217116216,
                    1
                ],
                [
                    0.9560774120034896,
                    0.983589030844069,
                    0.7570860268523123,
                    0.7075650244795225,
                    0.7234600262123198,
                    1.0,
                    1.0,
                    0.41415084915084915,
                    1
                ],
                [
                    0.026528032474584722,
                    0.009835890308440691,
                    0.01790154152163103,
                    0.19703583030720578,
                    0.13368283093053734,
                    0.0,
                    0.0,
                    0.24867798867798868,
                    1
                ]
            ],
            "fraction_answers": {
                "tekken 2": 0.10305118952963116,
                "pong": 0.8177410461928204,
                "half-life 3": 0.07920776427754854
            },
            "question": "what was the first popular home video game?",
            "rate_limited": false,
            "answers": [
                "tekken 2",
                "pong",
                "half-life 3"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "tekken 2": 0.1836340667729954,
                "pong": 0.913882441902346,
                "half-life 3": 0.1928051738097714
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    123000.0,
                    18400000.0,
                    184000.0
                ],
                "wikipedia_search": [
                    0.38159658085308645,
                    2.83026009791809,
                    0.7881433212288231
                ],
                "result_count_noun_chunks": [
                    1810000.0,
                    6090000.0,
                    144000.0
                ],
                "word_relation_to_question": [
                    1.3486846486846487,
                    1.6566033966033966,
                    0.9947119547119547
                ],
                "word_count_noun_chunks": [
                    0.0,
                    18.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    20.0,
                    0.0
                ],
                "word_count_appended": [
                    109.0,
                    552.0,
                    102.0
                ],
                "result_count": [
                    97700.0,
                    5370000.0,
                    149000.0
                ]
            },
            "z-best_answer_by_ml": [
                "pong"
            ],
            "integer_answers": {
                "tekken 2": 0,
                "pong": 8,
                "half-life 3": 0
            }
        },
        "lines": [
            [
                0,
                0.017394555521925686,
                0.0065750788474902445,
                0.2250124316260567,
                0.09539914521327161,
                0.14285714285714285,
                0.0,
                0.0,
                0.33717116217116216,
                1
            ],
            [
                1,
                0.9560774120034896,
                0.983589030844069,
                0.7570860268523123,
                0.7075650244795225,
                0.7234600262123198,
                1.0,
                1.0,
                0.41415084915084915,
                1
            ],
            [
                0,
                0.026528032474584722,
                0.009835890308440691,
                0.01790154152163103,
                0.19703583030720578,
                0.13368283093053734,
                0.0,
                0.0,
                0.24867798867798868,
                1
            ]
        ]
    },
    "Featuring 20 scoops of ice cream, the Vermonster is found on what chain's menu?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.25263157894736843,
                    0.16428571428571428,
                    0.005812897366030881,
                    0.15921843066037847,
                    0.7204301075268817,
                    0.6666666666666666,
                    0.5,
                    0.6418096682058462,
                    1
                ],
                [
                    0.22631578947368422,
                    0.2,
                    0.009627611262488647,
                    0.36529698691253404,
                    0.27956989247311825,
                    0.3333333333333333,
                    0.5,
                    0.2376331385573368,
                    1
                ],
                [
                    0.5210526315789473,
                    0.6357142857142857,
                    0.9845594913714805,
                    0.47548458242708747,
                    0.0,
                    0.0,
                    0.0,
                    0.12055719323681699,
                    1
                ]
            ],
            "fraction_answers": {
                "ben & jerry's": 0.34217102304107727,
                "dairy queen": 0.26897209400156197,
                "baskin-robbins": 0.3888568829573608
            },
            "question": "featuring 20 scoops of ice cream, the vermonster is found on what chain's menu?",
            "rate_limited": false,
            "answers": [
                "baskin-robbins",
                "dairy queen",
                "ben & jerry's"
            ],
            "integer_answers": {
                "ben & jerry's": 4,
                "dairy queen": 0,
                "baskin-robbins": 4
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "ben & jerry's": 0.30195038370570754,
                "dairy queen": 0.298729624564943,
                "baskin-robbins": 0.4523479935005889
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    23.0,
                    28.0,
                    89.0
                ],
                "wikipedia_search": [
                    1.2737474452830277,
                    2.9223758953002723,
                    3.8038766594166997
                ],
                "result_count_noun_chunks": [
                    32.0,
                    53.0,
                    5420.0
                ],
                "word_relation_to_question": [
                    5.134477345646769,
                    1.9010651084586945,
                    0.9644575458945359
                ],
                "word_count_noun_chunks": [
                    2.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    0.0
                ],
                "word_count_appended": [
                    67.0,
                    26.0,
                    0.0
                ],
                "result_count": [
                    48.0,
                    43.0,
                    99.0
                ]
            },
            "z-best_answer_by_ml": [
                "baskin-robbins"
            ]
        },
        "lines": [
            [
                0,
                0.25263157894736843,
                0.16428571428571428,
                0.005812897366030881,
                0.15921843066037847,
                0.7204301075268817,
                0.6666666666666666,
                0.5,
                0.6418096682058462,
                1
            ],
            [
                0,
                0.22631578947368422,
                0.2,
                0.009627611262488647,
                0.36529698691253404,
                0.27956989247311825,
                0.3333333333333333,
                0.5,
                0.2376331385573368,
                1
            ],
            [
                1,
                0.5210526315789473,
                0.6357142857142857,
                0.9845594913714805,
                0.47548458242708747,
                0.0,
                0.0,
                0.0,
                0.12055719323681699,
                1
            ]
        ]
    },
    "The U.S. has never had a Miss America from what state?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.2049092849519744,
                    0.298841059602649,
                    0.41685885608856094,
                    0.318804923840329,
                    0.4249271137026239,
                    0.5,
                    0.5,
                    0.34930555555555554,
                    1
                ],
                [
                    0.4402347918890075,
                    0.39569536423841056,
                    0.16443726937269373,
                    0.2949478392633389,
                    0.3680758017492711,
                    0.011627906976744207,
                    0.0,
                    0.3342893217893218,
                    1
                ],
                [
                    0.35485592315901815,
                    0.3054635761589404,
                    0.4187038745387454,
                    0.3862472368963321,
                    0.20699708454810495,
                    0.4883720930232558,
                    0.5,
                    0.31640512265512266,
                    1
                ]
            ],
            "fraction_answers": {
                "nebraska": 0.2557387722551201,
                "new mexico": 0.2465883015645768,
                "north dakota": 0.49767292618030307
            },
            "question": "the u.s. has never had a miss america from what state?",
            "rate_limited": false,
            "answers": [
                "new mexico",
                "north dakota",
                "nebraska"
            ],
            "ml_answers": {
                "nebraska": 0.6043421770295646,
                "new mexico": 0.4091658953059731,
                "north dakota": 0.3485946594626119
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "nebraska": 2,
                "new mexico": 2,
                "north dakota": 4
            },
            "question_type": 1,
            "data": {
                "result_count_important_words": [
                    2430000.0,
                    1260000.0,
                    2350000.0
                ],
                "wikipedia_search": [
                    1.0871704569580258,
                    1.2303129644199668,
                    0.6825165786220073
                ],
                "result_count_noun_chunks": [
                    721000.0,
                    2910000.0,
                    705000.0
                ],
                "word_relation_to_question": [
                    1.2055555555555557,
                    1.3256854256854256,
                    1.4687590187590187
                ],
                "word_count_noun_chunks": [
                    0.0,
                    42.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    16.0,
                    0.0
                ],
                "result_count": [
                    5530000.0,
                    1120000.0,
                    2720000.0
                ],
                "word_count_appended": [
                    103.0,
                    181.0,
                    402.0
                ]
            },
            "z-best_answer_by_ml": [
                "nebraska"
            ]
        },
        "lines": [
            [
                1,
                0.2049092849519744,
                0.298841059602649,
                0.41685885608856094,
                0.318804923840329,
                0.4249271137026239,
                0.5,
                0.5,
                0.34930555555555554,
                1
            ],
            [
                0,
                0.4402347918890075,
                0.39569536423841056,
                0.16443726937269373,
                0.2949478392633389,
                0.3680758017492711,
                0.011627906976744207,
                0.0,
                0.3342893217893218,
                1
            ],
            [
                0,
                0.35485592315901815,
                0.3054635761589404,
                0.4187038745387454,
                0.3862472368963321,
                0.20699708454810495,
                0.4883720930232558,
                0.5,
                0.31640512265512266,
                1
            ]
        ]
    },
    "Which of these do adverbs NOT typically modify?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.07717823971693943,
                    0.3661473087818697,
                    0.31880189798339265,
                    0.5,
                    0.420231504781077,
                    0.4985521235521235,
                    0.4958828911253431,
                    0.5000000000000001,
                    -1
                ],
                [
                    0.4666076957098629,
                    0.38031161473087816,
                    0.3537959667852906,
                    0.1739750445632799,
                    0.21514846502264723,
                    0.041023166023166024,
                    0.10338517840805123,
                    0.1272793922660552,
                    -1
                ],
                [
                    0.4562140645731977,
                    0.2535410764872521,
                    0.3274021352313167,
                    0.32602495543672017,
                    0.36462003019627576,
                    0.46042471042471045,
                    0.40073193046660566,
                    0.37272060773394483,
                    -1
                ]
            ],
            "fraction_answers": {
                "adjective": 0.25958012236249417,
                "pronoun": 0.20580150851481366,
                "adverb": 0.5346183691226922
            },
            "question": "which of these do adverbs not typically modify?",
            "rate_limited": false,
            "answers": [
                "pronoun",
                "adverb",
                "adjective"
            ],
            "integer_answers": {
                "adjective": 1,
                "pronoun": 2,
                "adverb": 5
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "adjective": 0.3515383253955313,
                "pronoun": 0.5666401036263058,
                "adverb": 0.22241544518073755
            },
            "question_type": -1,
            "data": {
                "result_count_important_words": [
                    189000.0,
                    169000.0,
                    348000.0
                ],
                "wikipedia_search": [
                    0.0,
                    1.9561497326203208,
                    1.0438502673796792
                ],
                "result_count_noun_chunks": [
                    611000.0,
                    493000.0,
                    582000.0
                ],
                "word_relation_to_question": [
                    0.0,
                    2.236323646403669,
                    0.7636763535963311
                ],
                "word_count_noun_chunks": [
                    3.0,
                    951.0,
                    82.0
                ],
                "word_count_raw": [
                    9.0,
                    867.0,
                    217.0
                ],
                "word_count_appended": [
                    317.0,
                    1132.0,
                    538.0
                ],
                "result_count": [
                    9560000.0,
                    755000.0,
                    990000.0
                ]
            },
            "z-best_answer_by_ml": [
                "pronoun"
            ]
        },
        "lines": [
            [
                1,
                0.07717823971693943,
                0.3661473087818697,
                0.31880189798339265,
                0.5,
                0.420231504781077,
                0.4985521235521235,
                0.4958828911253431,
                0.5000000000000001,
                -1
            ],
            [
                0,
                0.4666076957098629,
                0.38031161473087816,
                0.3537959667852906,
                0.1739750445632799,
                0.21514846502264723,
                0.041023166023166024,
                0.10338517840805123,
                0.1272793922660552,
                -1
            ],
            [
                0,
                0.4562140645731977,
                0.2535410764872521,
                0.3274021352313167,
                0.32602495543672017,
                0.36462003019627576,
                0.46042471042471045,
                0.40073193046660566,
                0.37272060773394483,
                -1
            ]
        ]
    }
}