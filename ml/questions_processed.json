{
    "Which of these versions of the Old Testament typically contains the most books?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which of these versions of the Old Testament typically contains the most books?",
            "fraction_answers": {
                "catholic": 0.5670473914280697,
                "protestant": 0.2521096373818675,
                "eastern orthodox": 0.18084297119006273
            },
            "lines": [
                [
                    0.2826025048456836,
                    0.7134831460674157,
                    0.16428571428571428,
                    0.706766917293233,
                    0.5501730103806228,
                    0.9849730556957491
                ],
                [
                    0.226874977812804,
                    0.28651685393258425,
                    0.3095238095238095,
                    0.2857142857142857,
                    0.39100346020761245,
                    0.013024437100109079
                ],
                [
                    0.4905225173415125,
                    0.0,
                    0.5261904761904762,
                    0.007518796992481203,
                    0.058823529411764705,
                    0.002002507204141771
                ]
            ],
            "rate_limited": false,
            "answers": [
                "catholic",
                "protestant",
                "eastern orthodox"
            ],
            "ml_answers": {
                "catholic": 0.3351218909647482,
                "protestant": 0.22615200576522013,
                "eastern orthodox": 0.28027708626977677
            },
            "z-best_answer_by_ml": [
                "catholic"
            ],
            "data": {
                "wikipedia_search": [
                    1.413012524228418,
                    1.13437488906402,
                    2.4526125867075623
                ],
                "word_count_entities": [
                    127.0,
                    51.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.8214285714285714,
                    1.5476190476190474,
                    2.630952380952381
                ],
                "word_count_raw": [
                    94.0,
                    38.0,
                    1.0
                ],
                "word_count_appended": [
                    477.0,
                    339.0,
                    51.0
                ],
                "result_count": [
                    121000000.0,
                    1600000.0,
                    246000.0
                ]
            },
            "integer_answers": {
                "catholic": 4,
                "protestant": 0,
                "eastern orthodox": 2
            }
        },
        "lines": [
            [
                0,
                0.2826025048456836,
                0.7134831460674157,
                0.16428571428571428,
                0.706766917293233,
                0.5501730103806228,
                0.9849730556957491
            ],
            [
                0,
                0.226874977812804,
                0.28651685393258425,
                0.3095238095238095,
                0.2857142857142857,
                0.39100346020761245,
                0.013024437100109079
            ],
            [
                1,
                0.4905225173415125,
                0.0,
                0.5261904761904762,
                0.007518796992481203,
                0.058823529411764705,
                0.002002507204141771
            ]
        ]
    },
    "The material that forms images in an Etch A Sketch is also the main component in which item?": {
        "raw_data": {
            "negative_question": false,
            "question": "The material that forms images in an Etch A Sketch is also the main component in which item?",
            "fraction_answers": {
                "zinc supplement tablets": 0.5407004774492038,
                "soda cans": 0.24334598918133452,
                "u.s. nickels": 0.21595353336946171
            },
            "lines": [
                [
                    0.5265374064179195,
                    0,
                    0.3510338546144728,
                    0,
                    0.2857142857142857,
                    0.999516363050137
                ],
                [
                    0.2287847914025237,
                    0,
                    0.4443355149716944,
                    0,
                    0.19047619047619047,
                    0.0002176366274383363
                ],
                [
                    0.24467780217955684,
                    0,
                    0.2046306304138328,
                    0,
                    0.5238095238095238,
                    0.00026600032242463323
                ]
            ],
            "rate_limited": false,
            "answers": [
                "zinc supplement tablets",
                "u.s. nickels",
                "soda cans"
            ],
            "ml_answers": {
                "zinc supplement tablets": 0.15813973964325514,
                "soda cans": 0.39535605855638734,
                "u.s. nickels": 0.14503661547836452
            },
            "integer_answers": {
                "zinc supplement tablets": 2,
                "soda cans": 1,
                "u.s. nickels": 1
            },
            "data": {
                "wikipedia_search": [
                    4.212299251343356,
                    1.8302783312201896,
                    1.9574224174364547
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    2.4572369823013096,
                    3.1103486048018607,
                    1.4324144128968297
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    6.0,
                    4.0,
                    11.0
                ],
                "result_count": [
                    124000.0,
                    27.0,
                    33.0
                ]
            },
            "z-best_answer_by_ml": [
                "soda cans"
            ]
        },
        "lines": [
            [
                0,
                0.5265374064179195,
                0,
                0.3510338546144728,
                0,
                0.2857142857142857,
                0.999516363050137
            ],
            [
                0,
                0.2287847914025237,
                0,
                0.4443355149716944,
                0,
                0.19047619047619047,
                0.0002176366274383363
            ],
            [
                1,
                0.24467780217955684,
                0,
                0.2046306304138328,
                0,
                0.5238095238095238,
                0.00026600032242463323
            ]
        ]
    },
    "Which of these actresses is NOT mentioned in Madonna\u2019s song \u201cVogue\u201d?": {
        "raw_data": {
            "negative_question": true,
            "question": "Which of these actresses is NOT mentioned in Madonna\u2019s song \u201cVogue\u201d?",
            "fraction_answers": {
                "jean harlow": 0.20337814597198892,
                "audrey hepburn": 0.21809134275220213,
                "rita hayworth": 0.5785305112758089
            },
            "lines": [
                [
                    0.4688183807439825,
                    0.5,
                    0.3456910858226648,
                    0.5,
                    0.2701612903225806,
                    0.3051948051948052
                ],
                [
                    0.18347500420804574,
                    0.5,
                    0.33940848743480323,
                    0.5,
                    0.407258064516129,
                    0.4155844155844156
                ],
                [
                    0.34770661504797173,
                    0.0,
                    0.31490042674253205,
                    0.0,
                    0.3225806451612903,
                    0.27922077922077926
                ]
            ],
            "rate_limited": false,
            "answers": [
                "jean harlow",
                "audrey hepburn",
                "rita hayworth"
            ],
            "ml_answers": {
                "jean harlow": 0.41870868797702643,
                "audrey hepburn": 0.33309351448854463,
                "rita hayworth": 0.20767501167683133
            },
            "integer_answers": {
                "jean harlow": 1,
                "audrey hepburn": 1,
                "rita hayworth": 4
            },
            "data": {
                "wikipedia_search": [
                    0.24945295404814005,
                    2.532199966335634,
                    1.2183470796162261
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_relation_to_question": [
                    1.2344713134186818,
                    1.2847321005215742,
                    1.4807965860597438
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    4.0
                ],
                "word_count_appended": [
                    30.0,
                    13.0,
                    34.0
                ],
                "result_count": [
                    57.0,
                    23.0,
                    44.0
                ]
            },
            "z-best_answer_by_ml": [
                "jean harlow"
            ]
        },
        "lines": [
            [
                0,
                0.4688183807439825,
                0.5,
                0.3456910858226648,
                0.5,
                0.2701612903225806,
                0.3051948051948052
            ],
            [
                1,
                0.18347500420804574,
                0.5,
                0.33940848743480323,
                0.5,
                0.407258064516129,
                0.4155844155844156
            ],
            [
                0,
                0.34770661504797173,
                0.0,
                0.31490042674253205,
                0.0,
                0.3225806451612903,
                0.27922077922077926
            ]
        ]
    },
    "Wrestling legend Ric Flair entered the ring to the same music used in what classic film?": {
        "raw_data": {
            "negative_question": false,
            "question": "Wrestling legend Ric Flair entered the ring to the same music used in what classic film?",
            "fraction_answers": {
                "star wars: episode iv": 0.4438837797143363,
                "back to the future": 0.17905836709703277,
                "2001: a space odyssey": 0.377057853188631
            },
            "lines": [
                [
                    0.7327353950108441,
                    0,
                    0.2101429310141733,
                    0,
                    0.2,
                    0.6326567928323277
                ],
                [
                    0.04456875982111226,
                    0,
                    0.47796508482223965,
                    0,
                    0.62,
                    0.36569756811117204
                ],
                [
                    0.22269584516804364,
                    0,
                    0.31189198416358704,
                    0,
                    0.18,
                    0.0016456390565002743
                ]
            ],
            "rate_limited": false,
            "answers": [
                "star wars: episode iv",
                "2001: a space odyssey",
                "back to the future"
            ],
            "ml_answers": {
                "star wars: episode iv": 0.28892491906756657,
                "back to the future": 0.15108560815615696,
                "2001: a space odyssey": 0.330766471432584
            },
            "integer_answers": {
                "star wars: episode iv": 2,
                "back to the future": 0,
                "2001: a space odyssey": 2
            },
            "data": {
                "wikipedia_search": [
                    5.129147765075908,
                    0.3119813187477858,
                    1.5588709161763052
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.471000517099213,
                    3.3457555937556775,
                    2.183243889145109
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    10.0,
                    31.0,
                    9.0
                ],
                "result_count": [
                    17300.0,
                    10000.0,
                    45.0
                ]
            },
            "z-best_answer_by_ml": [
                "2001: a space odyssey"
            ]
        },
        "lines": [
            [
                0,
                0.7327353950108441,
                0,
                0.2101429310141733,
                0,
                0.2,
                0.6326567928323277
            ],
            [
                1,
                0.04456875982111226,
                0,
                0.47796508482223965,
                0,
                0.62,
                0.36569756811117204
            ],
            [
                0,
                0.22269584516804364,
                0,
                0.31189198416358704,
                0,
                0.18,
                0.0016456390565002743
            ]
        ]
    },
    "The man famously known as the Science Guy holds a patent for which of these items?": {
        "raw_data": {
            "negative_question": false,
            "question": "The man famously known as the Science Guy holds a patent for which of these items?",
            "fraction_answers": {
                "pulse rate monitor": 0.2408846748171922,
                "mechanical pencil": 0.6478318935642435,
                "ballet shoe": 0.11128343161856429
            },
            "lines": [
                [
                    0.23822222222222225,
                    0,
                    0.5361014687640074,
                    0,
                    0.004599623667154506,
                    0.18461538461538463
                ],
                [
                    0.7137777777777778,
                    0,
                    0.3187030261333667,
                    0,
                    0.9896160011150603,
                    0.5692307692307692
                ],
                [
                    0.048,
                    0,
                    0.14519550510262583,
                    0,
                    0.005784375217785211,
                    0.24615384615384617
                ]
            ],
            "rate_limited": false,
            "answers": [
                "pulse rate monitor",
                "mechanical pencil",
                "ballet shoe"
            ],
            "ml_answers": {
                "pulse rate monitor": 0.21294717231121374,
                "mechanical pencil": 0.2839717252888349,
                "ballet shoe": 0.16784009659227414
            },
            "integer_answers": {
                "pulse rate monitor": 1,
                "mechanical pencil": 3,
                "ballet shoe": 0
            },
            "data": {
                "wikipedia_search": [
                    1.1911111111111112,
                    3.568888888888889,
                    0.24
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    3.216608812584045,
                    1.9122181568002001,
                    0.871173030615755
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    12.0,
                    37.0,
                    16.0
                ],
                "result_count": [
                    66.0,
                    14200.0,
                    83.0
                ]
            },
            "z-best_answer_by_ml": [
                "mechanical pencil"
            ]
        },
        "lines": [
            [
                0,
                0.23822222222222225,
                0,
                0.5361014687640074,
                0,
                0.004599623667154506,
                0.18461538461538463
            ],
            [
                0,
                0.7137777777777778,
                0,
                0.3187030261333667,
                0,
                0.9896160011150603,
                0.5692307692307692
            ],
            [
                1,
                0.048,
                0,
                0.14519550510262583,
                0,
                0.005784375217785211,
                0.24615384615384617
            ]
        ]
    },
    "What advertising mascot wears epaulettes?": {
        "raw_data": {
            "negative_question": false,
            "question": "What advertising mascot wears epaulettes?",
            "fraction_answers": {
                "sun-maid raisin girl": 0.19686018346732634,
                "mr. peanut": 0.3771446807161093,
                "cap'n crunch": 0.42599513581656445
            },
            "lines": [
                [
                    0.08333333333333333,
                    0,
                    0.28914141414141414,
                    0,
                    0.08163265306122448,
                    0.3333333333333333
                ],
                [
                    0.21794871794871795,
                    0,
                    0.31313131313131315,
                    0,
                    0.3877551020408163,
                    0.5897435897435898
                ],
                [
                    0.6987179487179488,
                    0,
                    0.3977272727272727,
                    0,
                    0.5306122448979592,
                    0.07692307692307693
                ]
            ],
            "rate_limited": false,
            "answers": [
                "sun-maid raisin girl",
                "mr. peanut",
                "cap'n crunch"
            ],
            "ml_answers": {
                "sun-maid raisin girl": 0.18901868044413808,
                "mr. peanut": 0.13400438301937548,
                "cap'n crunch": 0.2822065533342457
            },
            "integer_answers": {
                "sun-maid raisin girl": 0,
                "mr. peanut": 1,
                "cap'n crunch": 3
            },
            "data": {
                "wikipedia_search": [
                    0.25,
                    0.6538461538461539,
                    2.0961538461538463
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.1565656565656566,
                    1.2525252525252526,
                    1.5909090909090908
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    13.0,
                    23.0,
                    3.0
                ],
                "result_count": [
                    8.0,
                    38.0,
                    52.0
                ]
            },
            "z-best_answer_by_ml": [
                "cap'n crunch"
            ]
        },
        "lines": [
            [
                0,
                0.08333333333333333,
                0,
                0.28914141414141414,
                0,
                0.08163265306122448,
                0.3333333333333333
            ],
            [
                0,
                0.21794871794871795,
                0,
                0.31313131313131315,
                0,
                0.3877551020408163,
                0.5897435897435898
            ],
            [
                1,
                0.6987179487179488,
                0,
                0.3977272727272727,
                0,
                0.5306122448979592,
                0.07692307692307693
            ]
        ]
    },
    "Which alum from \u201cThe Hills\u201d founded a wildly popular millennial skincare line?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which alum from \u201cThe Hills\u201d founded a wildly popular millennial skincare line?",
            "fraction_answers": {
                "emily weiss": 0.34089665978737743,
                "whitney port": 0.25170520593528084,
                "lauren conrad": 0.40739813427734173
            },
            "lines": [
                [
                    0.3932291666666667,
                    0,
                    0.5220050104317508,
                    0,
                    0.1780821917808219,
                    0.2702702702702703
                ],
                [
                    0.5166495901639344,
                    0,
                    0.18735982958149333,
                    0,
                    0.4931506849315068,
                    0.43243243243243246
                ],
                [
                    0.0901212431693989,
                    0,
                    0.2906351599867559,
                    0,
                    0.3287671232876712,
                    0.2972972972972973
                ]
            ],
            "rate_limited": false,
            "answers": [
                "emily weiss",
                "lauren conrad",
                "whitney port"
            ],
            "ml_answers": {
                "emily weiss": 0.32677107623509216,
                "whitney port": 0.2614728907036259,
                "lauren conrad": 0.2837451518042939
            },
            "integer_answers": {
                "emily weiss": 1,
                "whitney port": 0,
                "lauren conrad": 3
            },
            "data": {
                "wikipedia_search": [
                    1.5729166666666667,
                    2.0665983606557377,
                    0.3604849726775956
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    3.132030062590505,
                    1.12415897748896,
                    1.7438109599205354
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    10.0,
                    16.0,
                    11.0
                ],
                "result_count": [
                    13.0,
                    36.0,
                    24.0
                ]
            },
            "z-best_answer_by_ml": [
                "emily weiss"
            ]
        },
        "lines": [
            [
                1,
                0.3932291666666667,
                0,
                0.5220050104317508,
                0,
                0.1780821917808219,
                0.2702702702702703
            ],
            [
                0,
                0.5166495901639344,
                0,
                0.18735982958149333,
                0,
                0.4931506849315068,
                0.43243243243243246
            ],
            [
                0,
                0.0901212431693989,
                0,
                0.2906351599867559,
                0,
                0.3287671232876712,
                0.2972972972972973
            ]
        ]
    },
    "Which color is NOT represented in the original electronic Simon game?": {
        "raw_data": {
            "negative_question": true,
            "question": "Which color is NOT represented in the original electronic Simon game?",
            "fraction_answers": {
                "blue": 0.33029084748321824,
                "orange": 0.2781125137147933,
                "green": 0.3915966388019885
            },
            "lines": [
                [
                    0.4636430376008925,
                    0.25,
                    0.197707336523126,
                    0.5,
                    0.32170542635658916,
                    0.2760716570697377
                ],
                [
                    0.3968557101297111,
                    0.5,
                    0.36985645933014355,
                    0.25,
                    0.34728682170542635,
                    0.3016634676903391
                ],
                [
                    0.1395012522693965,
                    0.25,
                    0.43243620414673045,
                    0.25,
                    0.3310077519379845,
                    0.42226487523992323
                ]
            ],
            "rate_limited": false,
            "answers": [
                "blue",
                "orange",
                "green"
            ],
            "ml_answers": {
                "blue": 0.35777093099550333,
                "orange": 0.36108238678452487,
                "green": 0.21588749540741797
            },
            "integer_answers": {
                "blue": 4,
                "orange": 1,
                "green": 1
            },
            "data": {
                "wikipedia_search": [
                    0.3635696239910755,
                    1.0314428987028894,
                    3.604987477306035
                ],
                "word_count_entities": [
                    5.0,
                    0.0,
                    5.0
                ],
                "word_relation_to_question": [
                    3.0229266347687402,
                    1.3014354066985647,
                    0.6756379585326954
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    1.0
                ],
                "word_count_appended": [
                    230.0,
                    197.0,
                    218.0
                ],
                "result_count": [
                    2800000.0,
                    2480000.0,
                    972000.0
                ]
            },
            "z-best_answer_by_ml": [
                "orange"
            ]
        },
        "lines": [
            [
                0,
                0.4636430376008925,
                0.25,
                0.197707336523126,
                0.5,
                0.32170542635658916,
                0.2760716570697377
            ],
            [
                1,
                0.3968557101297111,
                0.5,
                0.36985645933014355,
                0.25,
                0.34728682170542635,
                0.3016634676903391
            ],
            [
                0,
                0.1395012522693965,
                0.25,
                0.43243620414673045,
                0.25,
                0.3310077519379845,
                0.42226487523992323
            ]
        ]
    },
    "Which person is most likely to use a Reuleaux triangle at work?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which person is most likely to use a Reuleaux triangle at work?",
            "fraction_answers": {
                "banksy": 0.6404085845792307,
                "adam levine": 0.16721724482726577,
                "greta gerwig": 0.19237417059350354
            },
            "lines": [
                [
                    0.028149123167838973,
                    0,
                    0.5139573783276222,
                    0,
                    0.1111111111111111,
                    0.11627906976744186
                ],
                [
                    0.1535139205740991,
                    0,
                    0.15295195795977024,
                    0,
                    0.08333333333333333,
                    0.27906976744186046
                ],
                [
                    0.8183369562580619,
                    0,
                    0.3330906637126075,
                    0,
                    0.8055555555555556,
                    0.6046511627906976
                ]
            ],
            "rate_limited": false,
            "answers": [
                "greta gerwig",
                "adam levine",
                "banksy"
            ],
            "ml_answers": {
                "banksy": 0.2839717252888349,
                "adam levine": 0.22751070731534595,
                "greta gerwig": 0.2032963719663892
            },
            "integer_answers": {
                "banksy": 3,
                "adam levine": 0,
                "greta gerwig": 1
            },
            "data": {
                "wikipedia_search": [
                    0.08444736950351692,
                    0.4605417617222973,
                    2.4550108687741856
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    2.055829513310489,
                    0.6118078318390809,
                    1.33236265485043
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    4.0,
                    3.0,
                    29.0
                ],
                "result_count": [
                    10.0,
                    24.0,
                    52.0
                ]
            },
            "z-best_answer_by_ml": [
                "banksy"
            ]
        },
        "lines": [
            [
                0,
                0.028149123167838973,
                0,
                0.5139573783276222,
                0,
                0.1111111111111111,
                0.11627906976744186
            ],
            [
                1,
                0.1535139205740991,
                0,
                0.15295195795977024,
                0,
                0.08333333333333333,
                0.27906976744186046
            ],
            [
                0,
                0.8183369562580619,
                0,
                0.3330906637126075,
                0,
                0.8055555555555556,
                0.6046511627906976
            ]
        ]
    },
    "Which of these songs was written by the man nicknamed \u201cSlowhand\u201d?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which of these songs was written by the man nicknamed \u201cSlowhand\u201d?",
            "fraction_answers": {
                "lay down sally": 0.6623088236195577,
                "lover lay down": 0.08617866357261998,
                "lay lady lay": 0.2515125128078224
            },
            "lines": [
                [
                    0.2289321789321789,
                    0,
                    0.3832537288419641,
                    1.0,
                    0.7,
                    0.9993582103236454
                ],
                [
                    0.6875901875901875,
                    0,
                    0.38388218535277363,
                    0.0,
                    0.18571428571428572,
                    0.00037590538186485744
                ],
                [
                    0.08347763347763348,
                    0,
                    0.23286408580526227,
                    0.0,
                    0.11428571428571428,
                    0.0002658842944897772
                ]
            ],
            "rate_limited": false,
            "answers": [
                "lay down sally",
                "lay lady lay",
                "lover lay down"
            ],
            "ml_answers": {
                "lay down sally": 0.5652397057899899,
                "lover lay down": 0.18517252515527247,
                "lay lady lay": 0.2186938877797599
            },
            "z-best_answer_by_ml": [
                "lay down sally"
            ],
            "data": {
                "wikipedia_search": [
                    0.6867965367965367,
                    2.0627705627705626,
                    0.25043290043290045
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.5330149153678565,
                    1.5355287414110945,
                    0.9314563432210491
                ],
                "word_count_raw": [
                    3.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    49.0,
                    13.0,
                    8.0
                ],
                "result_count": [
                    109000.0,
                    41.0,
                    29.0
                ]
            },
            "integer_answers": {
                "lay down sally": 3,
                "lover lay down": 0,
                "lay lady lay": 2
            }
        },
        "lines": [
            [
                1,
                0.2289321789321789,
                0,
                0.3832537288419641,
                1.0,
                0.7,
                0.9993582103236454
            ],
            [
                0,
                0.6875901875901875,
                0,
                0.38388218535277363,
                0.0,
                0.18571428571428572,
                0.00037590538186485744
            ],
            [
                0,
                0.08347763347763348,
                0,
                0.23286408580526227,
                0.0,
                0.11428571428571428,
                0.0002658842944897772
            ]
        ]
    },
    "Which of these quantities is the largest?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which of these quantities is the largest?",
            "fraction_answers": {
                "two half-dozens": 0.004341820980509864,
                "baker's dozen": 0.10464845536337537,
                "dozen": 0.8910097236561149
            },
            "lines": [
                [
                    0,
                    0,
                    1.0,
                    0,
                    0.9739583333333334,
                    0.699070837635011
                ],
                [
                    0,
                    0,
                    0.0,
                    0,
                    0.013020833333333334,
                    4.62960819625835e-06
                ],
                [
                    0,
                    0,
                    0.0,
                    0,
                    0.013020833333333334,
                    0.3009245327567928
                ]
            ],
            "rate_limited": false,
            "answers": [
                "dozen",
                "two half-dozens",
                "baker's dozen"
            ],
            "ml_answers": {
                "two half-dozens": 0.17612355908568428,
                "baker's dozen": 0.16032162935260066,
                "dozen": 0.30425750956182557
            },
            "z-best_answer_by_ml": [
                "dozen"
            ],
            "data": {
                "wikipedia_search": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    2.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    374.0,
                    5.0,
                    5.0
                ],
                "result_count": [
                    1510000.0,
                    10.0,
                    650000.0
                ]
            },
            "integer_answers": {
                "two half-dozens": 0,
                "baker's dozen": 0,
                "dozen": 3
            }
        },
        "lines": [
            [
                0,
                0,
                0,
                1.0,
                0,
                0.9739583333333334,
                0.699070837635011
            ],
            [
                0,
                0,
                0,
                0.0,
                0,
                0.013020833333333334,
                4.62960819625835e-06
            ],
            [
                1,
                0,
                0,
                0.0,
                0,
                0.013020833333333334,
                0.3009245327567928
            ]
        ]
    },
    "The actor who played Don Draper provides the voice for what car company\u2019s ads?": {
        "raw_data": {
            "negative_question": false,
            "question": "The actor who played Don Draper provides the voice for what car company\u2019s ads?",
            "fraction_answers": {
                "jaguar": 0.33496530996411145,
                "bmw": 0.24741378791023041,
                "mercedes-benz": 0.4176209021256582
            },
            "lines": [
                [
                    0.3687622707895024,
                    0.5135135135135135,
                    0.45383083644111194,
                    0.5909090909090909,
                    0.3250414593698176,
                    0.25366824173091274
                ],
                [
                    0.5397534328304646,
                    0.08108108108108109,
                    0.35474497065118904,
                    0.13636363636363635,
                    0.3150912106135987,
                    0.057448395921412584
                ],
                [
                    0.09148429638003297,
                    0.40540540540540543,
                    0.19142419290769896,
                    0.2727272727272727,
                    0.3598673300165838,
                    0.6888833623476747
                ]
            ],
            "rate_limited": false,
            "answers": [
                "mercedes-benz",
                "bmw",
                "jaguar"
            ],
            "ml_answers": {
                "jaguar": 0.3305600862018863,
                "bmw": 0.22348944491351255,
                "mercedes-benz": 0.37199380768588103
            },
            "z-best_answer_by_ml": [
                "mercedes-benz"
            ],
            "data": {
                "wikipedia_search": [
                    2.2125736247370145,
                    3.2385205969827875,
                    0.5489057782801978
                ],
                "word_count_entities": [
                    19.0,
                    3.0,
                    15.0
                ],
                "word_relation_to_question": [
                    3.6306466915288955,
                    2.8379597652095123,
                    1.5313935432615917
                ],
                "word_count_raw": [
                    13.0,
                    3.0,
                    6.0
                ],
                "word_count_appended": [
                    196.0,
                    190.0,
                    217.0
                ],
                "result_count": [
                    1020000.0,
                    231000.0,
                    2770000.0
                ]
            },
            "integer_answers": {
                "jaguar": 2,
                "bmw": 1,
                "mercedes-benz": 3
            }
        },
        "lines": [
            [
                1,
                0.3687622707895024,
                0.5135135135135135,
                0.45383083644111194,
                0.5909090909090909,
                0.3250414593698176,
                0.25366824173091274
            ],
            [
                0,
                0.5397534328304646,
                0.08108108108108109,
                0.35474497065118904,
                0.13636363636363635,
                0.3150912106135987,
                0.057448395921412584
            ],
            [
                0,
                0.09148429638003297,
                0.40540540540540543,
                0.19142419290769896,
                0.2727272727272727,
                0.3598673300165838,
                0.6888833623476747
            ]
        ]
    },
    "Which of these substances expands when it freezes?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which of these substances expands when it freezes?",
            "fraction_answers": {
                "sodium chloride": 0.3300552821488029,
                "carbon dioxide": 0.42065905829658884,
                "dihydrogen monoxide": 0.24928565955460819
            },
            "lines": [
                [
                    0.0,
                    0,
                    0.3333333333333333,
                    0,
                    0.2962962962962963,
                    0.6905914989655821
                ],
                [
                    1.0,
                    0,
                    0.0,
                    0,
                    0.37407407407407406,
                    0.30856215911228135
                ],
                [
                    0.0,
                    0,
                    0.6666666666666666,
                    0,
                    0.3296296296296296,
                    0.0008463419221365431
                ]
            ],
            "rate_limited": false,
            "answers": [
                "sodium chloride",
                "carbon dioxide",
                "dihydrogen monoxide"
            ],
            "ml_answers": {
                "sodium chloride": 0.19340958739271286,
                "carbon dioxide": 0.20722446340251613,
                "dihydrogen monoxide": 0.33471674928634215
            },
            "z-best_answer_by_ml": [
                "dihydrogen monoxide"
            ],
            "data": {
                "wikipedia_search": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.3333333333333333,
                    0.0,
                    0.6666666666666666
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    80.0,
                    101.0,
                    89.0
                ],
                "result_count": [
                    2350000.0,
                    1050000.0,
                    2880.0
                ]
            },
            "integer_answers": {
                "sodium chloride": 1,
                "carbon dioxide": 2,
                "dihydrogen monoxide": 1
            }
        },
        "lines": [
            [
                0,
                0.0,
                0,
                0.3333333333333333,
                0,
                0.2962962962962963,
                0.6905914989655821
            ],
            [
                0,
                1.0,
                0,
                0.0,
                0,
                0.37407407407407406,
                0.30856215911228135
            ],
            [
                1,
                0.0,
                0,
                0.6666666666666666,
                0,
                0.3296296296296296,
                0.0008463419221365431
            ]
        ]
    },
    "Which U.S. president's wife was NOT born in North America?": {
        "raw_data": {
            "negative_question": true,
            "question": "Which U.S. president's wife was NOT born in North America?",
            "fraction_answers": {
                "martin van buren": 0.22452812343852976,
                "john quincy adams": 0.5891022188568006,
                "rutherford b. hayes": 0.1863696577046696
            },
            "lines": [
                [
                    0.35336066652605147,
                    0.08333333333333331,
                    0.30122801014345135,
                    0.0,
                    0.1706507304116866,
                    0.3241206030150754
                ],
                [
                    0.3798067381558843,
                    0.4166666666666667,
                    0.3499606092436975,
                    0.5,
                    0.42762284196547146,
                    0.3668341708542714
                ],
                [
                    0.26683259531806425,
                    0.5,
                    0.3488113806128512,
                    0.5,
                    0.40172642762284194,
                    0.30904522613065327
                ]
            ],
            "rate_limited": false,
            "answers": [
                "john quincy adams",
                "rutherford b. hayes",
                "martin van buren"
            ],
            "ml_answers": {
                "martin van buren": 0.3522091001839567,
                "john quincy adams": 0.26585079851574367,
                "rutherford b. hayes": 0.3510801575634477
            },
            "integer_answers": {
                "martin van buren": 2,
                "john quincy adams": 4,
                "rutherford b. hayes": 0
            },
            "data": {
                "wikipedia_search": [
                    1.759672001687382,
                    1.4423191421293886,
                    2.7980088561832295
                ],
                "word_count_entities": [
                    5.0,
                    1.0,
                    0.0
                ],
                "word_relation_to_question": [
                    2.7828078579916817,
                    2.1005514705882353,
                    2.1166406714200834
                ],
                "word_count_raw": [
                    6.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    140.0,
                    106.0,
                    152.0
                ],
                "result_count": [
                    248000.0,
                    54500.0,
                    74000.0
                ]
            },
            "z-best_answer_by_ml": [
                "martin van buren"
            ]
        },
        "lines": [
            [
                1,
                0.35336066652605147,
                0.08333333333333331,
                0.30122801014345135,
                0.0,
                0.1706507304116866,
                0.3241206030150754
            ],
            [
                0,
                0.3798067381558843,
                0.4166666666666667,
                0.3499606092436975,
                0.5,
                0.42762284196547146,
                0.3668341708542714
            ],
            [
                0,
                0.26683259531806425,
                0.5,
                0.3488113806128512,
                0.5,
                0.40172642762284194,
                0.30904522613065327
            ]
        ]
    },
    "Who was the president of the Screen Actors Guild before its merger with AFTRA?": {
        "raw_data": {
            "negative_question": false,
            "question": "Who was the president of the Screen Actors Guild before its merger with AFTRA?",
            "fraction_answers": {
                "gabrielle carteris": 0.24841860071328872,
                "ken howard": 0.4612149670795224,
                "melissa gilbert": 0.2903664322071889
            },
            "lines": [
                [
                    0.3248780487804878,
                    0.23809523809523808,
                    0.3502429411610986,
                    0.2903225806451613,
                    0.2838983050847458,
                    0.0030744905130007027
                ],
                [
                    0.22235772357723577,
                    0.047619047619047616,
                    0.10152514821189519,
                    0.06451612903225806,
                    0.3135593220338983,
                    0.9926212227687983
                ],
                [
                    0.45276422764227636,
                    0.7142857142857143,
                    0.5482319106270062,
                    0.6451612903225806,
                    0.4025423728813559,
                    0.004304286718200984
                ]
            ],
            "rate_limited": false,
            "answers": [
                "gabrielle carteris",
                "melissa gilbert",
                "ken howard"
            ],
            "ml_answers": {
                "gabrielle carteris": 0.21635488801411573,
                "ken howard": 0.3660875575033991,
                "melissa gilbert": 0.22088005914166284
            },
            "integer_answers": {
                "gabrielle carteris": 0,
                "ken howard": 5,
                "melissa gilbert": 1
            },
            "data": {
                "wikipedia_search": [
                    2.274146341463415,
                    1.5565040650406503,
                    3.1693495934959346
                ],
                "word_count_entities": [
                    15.0,
                    3.0,
                    45.0
                ],
                "word_relation_to_question": [
                    2.4517005881276903,
                    0.7106760374832664,
                    3.837623374389043
                ],
                "word_count_raw": [
                    9.0,
                    2.0,
                    20.0
                ],
                "word_count_appended": [
                    67.0,
                    74.0,
                    95.0
                ],
                "result_count": [
                    35.0,
                    11300.0,
                    49.0
                ]
            },
            "z-best_answer_by_ml": [
                "ken howard"
            ]
        },
        "lines": [
            [
                0,
                0.3248780487804878,
                0.23809523809523808,
                0.3502429411610986,
                0.2903225806451613,
                0.2838983050847458,
                0.0030744905130007027
            ],
            [
                0,
                0.22235772357723577,
                0.047619047619047616,
                0.10152514821189519,
                0.06451612903225806,
                0.3135593220338983,
                0.9926212227687983
            ],
            [
                1,
                0.45276422764227636,
                0.7142857142857143,
                0.5482319106270062,
                0.6451612903225806,
                0.4025423728813559,
                0.004304286718200984
            ]
        ]
    },
    "Jean Valjean, the protagonist of \u201cLes Mis\u00e9rables,\u201d is identified by what prisoner number?": {
        "raw_data": {
            "data": {
                "wikipedia_search": [
                    0.5798381162619572,
                    4.802929961870359,
                    0.6172319218676834
                ],
                "word_count_entities": [
                    0.0,
                    32.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.11325262308313155,
                    5.65589991928975,
                    1.2308474576271187
                ],
                "word_count_raw": [
                    0.0,
                    16.0,
                    0.0
                ],
                "result_count": [
                    787.0,
                    205000.0,
                    47.0
                ],
                "word_count_appended": [
                    80.0,
                    257.0,
                    54.0
                ]
            },
            "ml_answers": {
                "y2k": 0.28635703550741365,
                "24601": 0.7289675859727426,
                "867-5309": 0.1404496724152193
            },
            "z-best_answer_by_ml": [
                "24601"
            ],
            "negative_question": false,
            "lines": [
                [
                    0.09663968604365954,
                    0.0,
                    0.01617894615473308,
                    0.0,
                    0.003823469397669967,
                    0.20460358056265984
                ],
                [
                    0.8004883269783932,
                    1.0,
                    0.8079857027556786,
                    1.0,
                    0.995948191260919,
                    0.6572890025575447
                ],
                [
                    0.10287198697794724,
                    0.0,
                    0.1758353510895884,
                    0.0,
                    0.00022833934141103997,
                    0.13810741687979539
                ]
            ],
            "fraction_answers": {
                "y2k": 0.053540947026453733,
                "24601": 0.8769518705920892,
                "867-5309": 0.069507182381457
            },
            "question": "Jean Valjean, the protagonist of \u201cLes Mis\u00e9rables,\u201d is identified by what prisoner number?",
            "rate_limited": false,
            "answers": [
                "y2k",
                "24601",
                "867-5309"
            ],
            "integer_answers": {
                "y2k": 0,
                "24601": 6,
                "867-5309": 0
            }
        },
        "lines": [
            [
                0,
                0.09663968604365954,
                0.0,
                0.01617894615473308,
                0.0,
                0.003823469397669967,
                0.20460358056265984
            ],
            [
                1,
                0.8004883269783932,
                1.0,
                0.8079857027556786,
                1.0,
                0.995948191260919,
                0.6572890025575447
            ],
            [
                0,
                0.10287198697794724,
                0.0,
                0.1758353510895884,
                0.0,
                0.00022833934141103997,
                0.13810741687979539
            ]
        ]
    },
    "What is the grammatically correct way to announce people have arrived?": {
        "raw_data": {
            "negative_question": false,
            "question": "What is the grammatically correct way to announce people have arrived?",
            "fraction_answers": {
                "they're here": 0.3098639180216931,
                "there here": 0.32191946340336003,
                "their here": 0.3682166185749469
            },
            "lines": [
                [
                    0.0,
                    0,
                    0.4126109191898666,
                    0,
                    0.15789473684210525,
                    0.6689500160548004
                ],
                [
                    0.5,
                    0,
                    0.18825357312199417,
                    0,
                    0.3157894736842105,
                    0.28363480680723535
                ],
                [
                    0.5,
                    0,
                    0.39913550768813927,
                    0,
                    0.5263157894736842,
                    0.047415177137964254
                ]
            ],
            "rate_limited": false,
            "answers": [
                "they're here",
                "there here",
                "their here"
            ],
            "ml_answers": {
                "they're here": 0.22535018715304778,
                "there here": 0.2609760763787055,
                "their here": 0.26952034197090635
            },
            "integer_answers": {
                "they're here": 2,
                "there here": 1,
                "their here": 1
            },
            "data": {
                "wikipedia_search": [
                    0.0,
                    2.5,
                    2.5
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    2.4756655151391995,
                    1.129521438731965,
                    2.3948130461288355
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    3.0,
                    6.0,
                    10.0
                ],
                "result_count": [
                    125000.0,
                    53000.0,
                    8860.0
                ]
            },
            "z-best_answer_by_ml": [
                "their here"
            ]
        },
        "lines": [
            [
                1,
                0.0,
                0,
                0.4126109191898666,
                0,
                0.15789473684210525,
                0.6689500160548004
            ],
            [
                0,
                0.5,
                0,
                0.18825357312199417,
                0,
                0.3157894736842105,
                0.28363480680723535
            ],
            [
                0,
                0.5,
                0,
                0.39913550768813927,
                0,
                0.5263157894736842,
                0.047415177137964254
            ]
        ]
    },
    "Basketball is NOT a major theme of which of these 90s movies?": {
        "raw_data": {
            "negative_question": true,
            "question": "Basketball is NOT a major theme of which of these 90s movies?",
            "fraction_answers": {
                "white men can't jump": 0.20680150704110095,
                "point break": 0.3115268609796965,
                "eddie": 0.48167163197920254
            },
            "lines": [
                [
                    0.3041666666666667,
                    0,
                    0.30833776663120027,
                    0,
                    0.49480968858131485,
                    0.4790828640386163
                ],
                [
                    0.2773809523809524,
                    0,
                    0.227260348583878,
                    0,
                    0.41695501730103807,
                    0.4553499597747385
                ],
                [
                    0.41845238095238096,
                    0,
                    0.4644018847849217,
                    0,
                    0.08823529411764708,
                    0.0655671761866452
                ]
            ],
            "rate_limited": false,
            "answers": [
                "white men can't jump",
                "point break",
                "eddie"
            ],
            "ml_answers": {
                "white men can't jump": 0.13607131557070778,
                "point break": 0.22479647814026096,
                "eddie": 0.3068841635911455
            },
            "z-best_answer_by_ml": [
                "eddie"
            ],
            "data": {
                "wikipedia_search": [
                    1.9583333333333333,
                    2.2261904761904763,
                    0.8154761904761905
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    2.2999468004255967,
                    3.272875816993464,
                    0.4271773825809393
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    3.0,
                    48.0,
                    238.0
                ],
                "result_count": [
                    52000.0,
                    111000.0,
                    1080000.0
                ]
            },
            "integer_answers": {
                "white men can't jump": 0,
                "point break": 2,
                "eddie": 2
            }
        },
        "lines": [
            [
                0,
                0.3041666666666667,
                0,
                0.30833776663120027,
                0,
                0.49480968858131485,
                0.4790828640386163
            ],
            [
                1,
                0.2773809523809524,
                0,
                0.227260348583878,
                0,
                0.41695501730103807,
                0.4553499597747385
            ],
            [
                0,
                0.41845238095238096,
                0,
                0.4644018847849217,
                0,
                0.08823529411764708,
                0.0655671761866452
            ]
        ]
    },
    "What soda is named for a medical condition?": {
        "raw_data": {
            "negative_question": false,
            "question": "What soda is named for a medical condition?",
            "fraction_answers": {
                "faygo": 0.14421999291344711,
                "pepsi": 0.5726665535345876,
                "fanta": 0.28311345355196527
            },
            "lines": [
                [
                    0.11149825783972127,
                    0.0,
                    0.4358381502890173,
                    0.0,
                    0.2870762711864407,
                    0.03090727816550349
                ],
                [
                    0.7746806039488967,
                    1.0,
                    0.43706647398843934,
                    0.6,
                    0.4427966101694915,
                    0.1814556331006979
                ],
                [
                    0.11382113821138214,
                    0.0,
                    0.12709537572254334,
                    0.4,
                    0.2701271186440678,
                    0.7876370887337986
                ]
            ],
            "rate_limited": false,
            "answers": [
                "faygo",
                "pepsi",
                "fanta"
            ],
            "ml_answers": {
                "faygo": 0.22069316619031581,
                "pepsi": 0.40899134550665994,
                "fanta": 0.2250045775249769
            },
            "integer_answers": {
                "faygo": 0,
                "pepsi": 5,
                "fanta": 1
            },
            "data": {
                "wikipedia_search": [
                    0.33449477351916374,
                    2.3240418118466897,
                    0.34146341463414637
                ],
                "word_count_entities": [
                    0.0,
                    2.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.7433526011560692,
                    1.7482658959537574,
                    0.5083815028901734
                ],
                "word_count_raw": [
                    0.0,
                    6.0,
                    4.0
                ],
                "word_count_appended": [
                    271.0,
                    418.0,
                    255.0
                ],
                "result_count": [
                    62000.0,
                    364000.0,
                    1580000.0
                ]
            },
            "z-best_answer_by_ml": [
                "pepsi"
            ]
        },
        "lines": [
            [
                0,
                0.11149825783972127,
                0.0,
                0.4358381502890173,
                0.0,
                0.2870762711864407,
                0.03090727816550349
            ],
            [
                1,
                0.7746806039488967,
                1.0,
                0.43706647398843934,
                0.6,
                0.4427966101694915,
                0.1814556331006979
            ],
            [
                0,
                0.11382113821138214,
                0.0,
                0.12709537572254334,
                0.4,
                0.2701271186440678,
                0.7876370887337986
            ]
        ]
    },
    "Who was NOT a wife of Henry VIII?": {
        "raw_data": {
            "negative_question": true,
            "question": "Who was NOT a wife of Henry VIII?",
            "fraction_answers": {
                "catherine of york": 0.09665340662199738,
                "catherine parr": 0.42467732474903164,
                "catherine howard": 0.478669268628971
            },
            "lines": [
                [
                    0.34954407294832823,
                    0.2857142857142857,
                    0.2984656185697864,
                    0.28448275862068967,
                    0.26322418136020154,
                    0.24453710853961355
                ],
                [
                    0.378972800249396,
                    0.5,
                    0.3625757526935734,
                    0.5,
                    0.46851385390428213,
                    0.4999773732867564
                ],
                [
                    0.27148312680227576,
                    0.2142857142857143,
                    0.3389586287366403,
                    0.21551724137931033,
                    0.26826196473551633,
                    0.25548551817363013
                ]
            ],
            "rate_limited": false,
            "answers": [
                "catherine parr",
                "catherine of york",
                "catherine howard"
            ],
            "ml_answers": {
                "catherine of york": 0.3798485233717196,
                "catherine parr": 0.23034152532323499,
                "catherine howard": 0.1567902978995085
            },
            "z-best_answer_by_ml": [
                "catherine of york"
            ],
            "data": {
                "wikipedia_search": [
                    1.5045592705167175,
                    1.21027199750604,
                    2.2851687319772425
                ],
                "word_count_entities": [
                    27.0,
                    0.0,
                    36.0
                ],
                "word_relation_to_question": [
                    2.0153438143021365,
                    1.374242473064266,
                    1.6104137126335973
                ],
                "word_count_raw": [
                    25.0,
                    0.0,
                    33.0
                ],
                "word_count_appended": [
                    188.0,
                    25.0,
                    184.0
                ],
                "result_count": [
                    350000.0,
                    31.0,
                    335000.0
                ]
            },
            "integer_answers": {
                "catherine of york": 0,
                "catherine parr": 3,
                "catherine howard": 3
            }
        },
        "lines": [
            [
                0,
                0.34954407294832823,
                0.2857142857142857,
                0.2984656185697864,
                0.28448275862068967,
                0.26322418136020154,
                0.24453710853961355
            ],
            [
                1,
                0.378972800249396,
                0.5,
                0.3625757526935734,
                0.5,
                0.46851385390428213,
                0.4999773732867564
            ],
            [
                0,
                0.27148312680227576,
                0.2142857142857143,
                0.3389586287366403,
                0.21551724137931033,
                0.26826196473551633,
                0.25548551817363013
            ]
        ]
    },
    "Which of these is NOT a name of one of the Florida Keys?": {
        "raw_data": {
            "negative_question": true,
            "question": "Which of these is NOT a name of one of the Florida Keys?",
            "fraction_answers": {
                "pigeon key": 0.5955913430437583,
                "turtle key": 0.19816819161896154,
                "fat deer key": 0.20624046533728016
            },
            "lines": [
                [
                    0.4117838541666667,
                    0,
                    0.2672032249429682,
                    0,
                    0.4085365853658537,
                    0.49999540484995114
                ],
                [
                    0.2705078125,
                    0,
                    0.4218432754425707,
                    0,
                    0.41158536585365857,
                    0.4997271629658477
                ],
                [
                    0.31770833333333337,
                    0,
                    0.31095349961446117,
                    0,
                    0.1798780487804878,
                    0.0002774321842011762
                ]
            ],
            "rate_limited": false,
            "answers": [
                "fat deer key",
                "turtle key",
                "pigeon key"
            ],
            "ml_answers": {
                "pigeon key": 0.1400079161291328,
                "turtle key": 0.1840708393135538,
                "fat deer key": 0.23261899282851808
            },
            "z-best_answer_by_ml": [
                "fat deer key"
            ],
            "data": {
                "wikipedia_search": [
                    0.3528645833333333,
                    0.91796875,
                    0.7291666666666666
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.3967806503421907,
                    0.46894034734457585,
                    1.1342790023132332
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    30.0,
                    29.0,
                    105.0
                ],
                "result_count": [
                    32.0,
                    1900.0,
                    3480000.0
                ]
            },
            "integer_answers": {
                "pigeon key": 2,
                "turtle key": 1,
                "fat deer key": 1
            }
        },
        "lines": [
            [
                0,
                0.4117838541666667,
                0,
                0.2672032249429682,
                0,
                0.4085365853658537,
                0.49999540484995114
            ],
            [
                1,
                0.2705078125,
                0,
                0.4218432754425707,
                0,
                0.41158536585365857,
                0.4997271629658477
            ],
            [
                0,
                0.31770833333333337,
                0,
                0.31095349961446117,
                0,
                0.1798780487804878,
                0.0002774321842011762
            ]
        ]
    },
    "Lonnie Lynn's only Academy Award win was in what category?": {
        "raw_data": {
            "negative_question": false,
            "question": "Lonnie Lynn's only Academy Award win was in what category?",
            "fraction_answers": {
                "best adapted screenplay": 0.20693329357793513,
                "best original song": 0.5978813663648563,
                "best cinematography": 0.19518534005720856
            },
            "lines": [
                [
                    0.24969888587774766,
                    0.0,
                    0.31293891028532433,
                    0.0,
                    0.41313918049441245,
                    0.26582278481012656
                ],
                [
                    0.2619424011700434,
                    0.0,
                    0.3677657032487885,
                    0.0,
                    0.3325431764307484,
                    0.2088607594936709
                ],
                [
                    0.4883587129522089,
                    1.0,
                    0.3192953864658871,
                    1.0,
                    0.2543176430748392,
                    0.5253164556962026
                ]
            ],
            "rate_limited": false,
            "answers": [
                "best adapted screenplay",
                "best cinematography",
                "best original song"
            ],
            "ml_answers": {
                "best adapted screenplay": 0.155159836559306,
                "best original song": 0.6916782533080683,
                "best cinematography": 0.12628910791948567
            },
            "z-best_answer_by_ml": [
                "best original song"
            ],
            "data": {
                "wikipedia_search": [
                    0.9987955435109906,
                    1.0477696046801737,
                    1.9534348518088356
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    28.0
                ],
                "word_relation_to_question": [
                    1.2517556411412973,
                    1.471062812995154,
                    1.2771815458635485
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    26.0
                ],
                "word_count_appended": [
                    42.0,
                    33.0,
                    83.0
                ],
                "result_count": [
                    12200.0,
                    9820.0,
                    7510.0
                ]
            },
            "integer_answers": {
                "best adapted screenplay": 1,
                "best original song": 4,
                "best cinematography": 1
            }
        },
        "lines": [
            [
                0,
                0.24969888587774766,
                0.0,
                0.31293891028532433,
                0.0,
                0.41313918049441245,
                0.26582278481012656
            ],
            [
                0,
                0.2619424011700434,
                0.0,
                0.3677657032487885,
                0.0,
                0.3325431764307484,
                0.2088607594936709
            ],
            [
                1,
                0.4883587129522089,
                1.0,
                0.3192953864658871,
                1.0,
                0.2543176430748392,
                0.5253164556962026
            ]
        ]
    },
    "Which of these products was featured on \u201cShark Tank\u201d?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which of these products was featured on \u201cShark Tank\u201d?",
            "fraction_answers": {
                "scrub daddy": 0.8146051570325915,
                "sticky buddy": 0.07099314716355916,
                "instant pot": 0.11440169580384936
            },
            "lines": [
                [
                    0.03125,
                    0.0,
                    0.009259259259259259,
                    0.0,
                    0.2271062271062271,
                    0.4187946884576098
                ],
                [
                    0.84375,
                    1.0,
                    0.8203703703703704,
                    1.0,
                    0.6446886446886447,
                    0.5788219271365339
                ],
                [
                    0.125,
                    0.0,
                    0.17037037037037037,
                    0.0,
                    0.1282051282051282,
                    0.002383384405856316
                ]
            ],
            "rate_limited": false,
            "answers": [
                "instant pot",
                "scrub daddy",
                "sticky buddy"
            ],
            "ml_answers": {
                "scrub daddy": 0.544953136048589,
                "sticky buddy": 0.2594654828311993,
                "instant pot": 0.1937950515534829
            },
            "z-best_answer_by_ml": [
                "scrub daddy"
            ],
            "data": {
                "wikipedia_search": [
                    0.125,
                    3.375,
                    0.5
                ],
                "word_count_entities": [
                    0.0,
                    3.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.037037037037037035,
                    3.2814814814814817,
                    0.6814814814814815
                ],
                "word_count_raw": [
                    0.0,
                    3.0,
                    0.0
                ],
                "word_count_appended": [
                    62.0,
                    176.0,
                    35.0
                ],
                "result_count": [
                    9840.0,
                    13600.0,
                    56.0
                ]
            },
            "integer_answers": {
                "scrub daddy": 6,
                "sticky buddy": 0,
                "instant pot": 0
            }
        },
        "lines": [
            [
                0,
                0.03125,
                0.0,
                0.009259259259259259,
                0.0,
                0.2271062271062271,
                0.4187946884576098
            ],
            [
                1,
                0.84375,
                1.0,
                0.8203703703703704,
                1.0,
                0.6446886446886447,
                0.5788219271365339
            ],
            [
                0,
                0.125,
                0.0,
                0.17037037037037037,
                0.0,
                0.1282051282051282,
                0.002383384405856316
            ]
        ]
    },
    "What are the first words spoken by God in the King James Bible?": {
        "raw_data": {
            "negative_question": false,
            "question": "What are the first words spoken by God in the King James Bible?",
            "fraction_answers": {
                "hello, my children": 0.14416543605181267,
                "let there be light": 0.7584329746425408,
                "this is my gift": 0.09740158930564656
            },
            "lines": [
                [
                    0.5000000000000001,
                    1.0,
                    0.22605820105820107,
                    1.0,
                    0.8260869565217391,
                    0.9984526902753044
                ],
                [
                    0.33333333333333337,
                    0.0,
                    0.4878306878306879,
                    0.0,
                    0.043478260869565216,
                    0.00035033427728958045
                ],
                [
                    0.16666666666666669,
                    0.0,
                    0.28611111111111115,
                    0.0,
                    0.13043478260869565,
                    0.0011969754474060667
                ]
            ],
            "rate_limited": false,
            "answers": [
                "let there be light",
                "hello, my children",
                "this is my gift"
            ],
            "ml_answers": {
                "hello, my children": 0.2499517265571196,
                "let there be light": 0.685280477012364,
                "this is my gift": 0.15750242073445164
            },
            "z-best_answer_by_ml": [
                "let there be light"
            ],
            "data": {
                "wikipedia_search": [
                    1.0,
                    0.6666666666666666,
                    0.3333333333333333
                ],
                "word_count_entities": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.3563492063492064,
                    2.9269841269841272,
                    1.7166666666666668
                ],
                "word_count_raw": [
                    6.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    95.0,
                    5.0,
                    15.0
                ],
                "result_count": [
                    34200.0,
                    12.0,
                    41.0
                ]
            },
            "integer_answers": {
                "hello, my children": 1,
                "let there be light": 5,
                "this is my gift": 0
            }
        },
        "lines": [
            [
                1,
                0.5000000000000001,
                1.0,
                0.22605820105820107,
                1.0,
                0.8260869565217391,
                0.9984526902753044
            ],
            [
                0,
                0.33333333333333337,
                0.0,
                0.4878306878306879,
                0.0,
                0.043478260869565216,
                0.00035033427728958045
            ],
            [
                0,
                0.16666666666666669,
                0.0,
                0.28611111111111115,
                0.0,
                0.13043478260869565,
                0.0011969754474060667
            ]
        ]
    },
    "The inventor of the Erector Set made another toy that contained what?": {
        "raw_data": {
            "negative_question": false,
            "question": "The inventor of the Erector Set made another toy that contained what?",
            "fraction_answers": {
                "uranium ore": 0.3691400559280053,
                "asbestos powder": 0.3072336402031893,
                "live ants": 0.32362630386880537
            },
            "lines": [
                [
                    0.2516666666666667,
                    0,
                    0.21030353661932608,
                    0,
                    0.3624161073825503,
                    0.6521739130434783
                ],
                [
                    0.12666666666666668,
                    0,
                    0.281203007518797,
                    0,
                    0.6040268456375839,
                    0.2826086956521739
                ],
                [
                    0.6216666666666667,
                    0,
                    0.5084934558618769,
                    0,
                    0.03355704697986577,
                    0.06521739130434782
                ]
            ],
            "rate_limited": false,
            "answers": [
                "uranium ore",
                "live ants",
                "asbestos powder"
            ],
            "ml_answers": {
                "uranium ore": 0.29190233845229957,
                "asbestos powder": 0.20942029321388028,
                "live ants": 0.20492743285873602
            },
            "integer_answers": {
                "uranium ore": 1,
                "asbestos powder": 2,
                "live ants": 1
            },
            "data": {
                "wikipedia_search": [
                    0.5033333333333334,
                    0.25333333333333335,
                    1.2433333333333334
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.6309106098579782,
                    0.843609022556391,
                    1.5254803675856308
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    30.0,
                    13.0,
                    3.0
                ],
                "result_count": [
                    54.0,
                    90.0,
                    5.0
                ]
            },
            "z-best_answer_by_ml": [
                "uranium ore"
            ]
        },
        "lines": [
            [
                1,
                0.2516666666666667,
                0,
                0.21030353661932608,
                0,
                0.3624161073825503,
                0.6521739130434783
            ],
            [
                0,
                0.12666666666666668,
                0,
                0.281203007518797,
                0,
                0.6040268456375839,
                0.2826086956521739
            ],
            [
                0,
                0.6216666666666667,
                0,
                0.5084934558618769,
                0,
                0.03355704697986577,
                0.06521739130434782
            ]
        ]
    },
    "Which of these creatures is most likely to bark?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which of these creatures is most likely to bark?",
            "fraction_answers": {
                "blue whale": 0.2200541235555494,
                "mime": 0.12932402586273933,
                "dog": 0.6506218505817113
            },
            "lines": [
                [
                    0.038461538461538464,
                    0.4,
                    0.0,
                    0.04669260700389105,
                    0.21188878235858102,
                    0.07890122735242548
                ],
                [
                    0.2692307692307692,
                    0.0,
                    0.5384615384615384,
                    0.0,
                    0.15436241610738255,
                    0.3582700175336061
                ],
                [
                    0.6923076923076923,
                    0.6,
                    0.46153846153846156,
                    0.953307392996109,
                    0.6337488015340365,
                    0.5628287551139685
                ]
            ],
            "rate_limited": false,
            "answers": [
                "mime",
                "blue whale",
                "dog"
            ],
            "ml_answers": {
                "blue whale": 0.25077590854878645,
                "mime": 0.21459905111312544,
                "dog": 0.5938449892300259
            },
            "z-best_answer_by_ml": [
                "dog"
            ],
            "data": {
                "wikipedia_search": [
                    0.07692307692307693,
                    0.5384615384615384,
                    1.3846153846153846
                ],
                "word_count_entities": [
                    2.0,
                    0.0,
                    3.0
                ],
                "word_relation_to_question": [
                    0.0,
                    1.0769230769230769,
                    0.9230769230769231
                ],
                "word_count_raw": [
                    12.0,
                    0.0,
                    245.0
                ],
                "word_count_appended": [
                    221.0,
                    161.0,
                    661.0
                ],
                "result_count": [
                    135000.0,
                    613000.0,
                    963000.0
                ]
            },
            "integer_answers": {
                "blue whale": 1,
                "mime": 0,
                "dog": 5
            }
        },
        "lines": [
            [
                0,
                0.038461538461538464,
                0.4,
                0.0,
                0.04669260700389105,
                0.21188878235858102,
                0.07890122735242548
            ],
            [
                0,
                0.2692307692307692,
                0.0,
                0.5384615384615384,
                0.0,
                0.15436241610738255,
                0.3582700175336061
            ],
            [
                1,
                0.6923076923076923,
                0.6,
                0.46153846153846156,
                0.953307392996109,
                0.6337488015340365,
                0.5628287551139685
            ]
        ]
    },
    "Which of these phrases, written backwards, is a hip hop group?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which of these phrases, written backwards, is a hip hop group?",
            "fraction_answers": {
                "beat chefs": 0.20765480074458947,
                "drummers ear": 0.2251206557984727,
                "blues rhythm": 0.5672245434569378
            },
            "lines": [
                [
                    0.03571428571428571,
                    0,
                    0.4198973301085977,
                    0,
                    0.4375,
                    0.007371007371007371
                ],
                [
                    0.0,
                    0,
                    0.5170662019253568,
                    0,
                    0.3125,
                    0.001053001053001053
                ],
                [
                    0.9642857142857143,
                    0,
                    0.06303646796604544,
                    0,
                    0.25,
                    0.9915759915759915
                ]
            ],
            "rate_limited": false,
            "answers": [
                "drummers ear",
                "beat chefs",
                "blues rhythm"
            ],
            "ml_answers": {
                "beat chefs": 0.28588755100303964,
                "drummers ear": 0.29476815601047196,
                "blues rhythm": 0.20222580809860213
            },
            "z-best_answer_by_ml": [
                "drummers ear"
            ],
            "data": {
                "wikipedia_search": [
                    0.14285714285714285,
                    0.0,
                    3.857142857142857
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    2.519383980651586,
                    3.102397211552141,
                    0.3782188077962726
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    14.0,
                    10.0,
                    8.0
                ],
                "result_count": [
                    42.0,
                    6.0,
                    5650.0
                ]
            },
            "integer_answers": {
                "beat chefs": 1,
                "drummers ear": 1,
                "blues rhythm": 2
            }
        },
        "lines": [
            [
                1,
                0.03571428571428571,
                0,
                0.4198973301085977,
                0,
                0.4375,
                0.007371007371007371
            ],
            [
                0,
                0.0,
                0,
                0.5170662019253568,
                0,
                0.3125,
                0.001053001053001053
            ],
            [
                0,
                0.9642857142857143,
                0,
                0.06303646796604544,
                0,
                0.25,
                0.9915759915759915
            ]
        ]
    },
    "Which of these is a French territory?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which of these is a French territory?",
            "fraction_answers": {
                "french guiana": 0.7716551968153876,
                "french stewart": 0.12936837902219842,
                "french cyprus": 0.09897642416241377
            },
            "lines": [
                [
                    0.48484848484848486,
                    1.0,
                    0.6986899563318778,
                    0,
                    0.9990618672209006,
                    0.6756756756756757
                ],
                [
                    0.18181818181818182,
                    0.0,
                    0.18995633187772928,
                    0,
                    0.0009361072838069808,
                    0.27413127413127414
                ],
                [
                    0.3333333333333333,
                    0.0,
                    0.11135371179039302,
                    0,
                    2.025495292447853e-06,
                    0.05019305019305019
                ]
            ],
            "rate_limited": false,
            "answers": [
                "french guiana",
                "french stewart",
                "french cyprus"
            ],
            "ml_answers": {
                "french guiana": 0.42344360589040386,
                "french stewart": 0.26792043597652554,
                "french cyprus": 0.18984807973333556
            },
            "z-best_answer_by_ml": [
                "french guiana"
            ],
            "data": {
                "wikipedia_search": [
                    0.9696969696969697,
                    0.36363636363636365,
                    0.6666666666666666
                ],
                "word_count_entities": [
                    8.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.3973799126637554,
                    0.3799126637554585,
                    0.22270742358078602
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    175.0,
                    71.0,
                    13.0
                ],
                "result_count": [
                    36500000.0,
                    34200.0,
                    74.0
                ]
            },
            "integer_answers": {
                "french guiana": 5,
                "french stewart": 0,
                "french cyprus": 0
            }
        },
        "lines": [
            [
                1,
                0.48484848484848486,
                1.0,
                0.6986899563318778,
                0,
                0.9990618672209006,
                0.6756756756756757
            ],
            [
                0,
                0.18181818181818182,
                0.0,
                0.18995633187772928,
                0,
                0.0009361072838069808,
                0.27413127413127414
            ],
            [
                0,
                0.3333333333333333,
                0.0,
                0.11135371179039302,
                0,
                2.025495292447853e-06,
                0.05019305019305019
            ]
        ]
    },
    "What makeup item often contains dried cochineal bugs as an ingredient?": {
        "raw_data": {
            "negative_question": false,
            "question": "What makeup item often contains dried cochineal bugs as an ingredient?",
            "fraction_answers": {
                "lipstick": 0.7502416548467844,
                "mascara": 0.1574022431465362,
                "eyeliner": 0.09235610200667928
            },
            "lines": [
                [
                    0.17410714285714285,
                    0.0,
                    0.3659990796372993,
                    0.022727272727272728,
                    0.2603305785123967,
                    0.12124938514510576
                ],
                [
                    0.06398809523809523,
                    0.0,
                    0.21452617524483591,
                    0.0,
                    0.20110192837465565,
                    0.07452041318248893
                ],
                [
                    0.761904761904762,
                    1.0,
                    0.41947474511786476,
                    0.9772727272727273,
                    0.5385674931129476,
                    0.8042302016724053
                ]
            ],
            "rate_limited": false,
            "answers": [
                "mascara",
                "eyeliner",
                "lipstick"
            ],
            "ml_answers": {
                "lipstick": 0.6932220528569221,
                "mascara": 0.2052306774901781,
                "eyeliner": 0.3028430346824463
            },
            "z-best_answer_by_ml": [
                "lipstick"
            ],
            "data": {
                "wikipedia_search": [
                    1.3928571428571428,
                    0.5119047619047619,
                    6.095238095238096
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    73.0
                ],
                "word_relation_to_question": [
                    1.8299953981864965,
                    1.0726308762241796,
                    2.097373725589324
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    43.0
                ],
                "word_count_appended": [
                    189.0,
                    146.0,
                    391.0
                ],
                "result_count": [
                    4930.0,
                    3030.0,
                    32700.0
                ]
            },
            "integer_answers": {
                "lipstick": 6,
                "mascara": 0,
                "eyeliner": 0
            }
        },
        "lines": [
            [
                0,
                0.17410714285714285,
                0.0,
                0.3659990796372993,
                0.022727272727272728,
                0.2603305785123967,
                0.12124938514510576
            ],
            [
                0,
                0.06398809523809523,
                0.0,
                0.21452617524483591,
                0.0,
                0.20110192837465565,
                0.07452041318248893
            ],
            [
                1,
                0.761904761904762,
                1.0,
                0.41947474511786476,
                0.9772727272727273,
                0.5385674931129476,
                0.8042302016724053
            ]
        ]
    },
    "What term describes a person from the state between New York and Rhode Island?": {
        "raw_data": {
            "negative_question": false,
            "question": "What term describes a person from the state between New York and Rhode Island?",
            "fraction_answers": {
                "hoosier": 0.4400240033872136,
                "nutmegger": 0.20761187571158624,
                "cheesehead": 0.3523641209012001
            },
            "lines": [
                [
                    0.23612410656270302,
                    0,
                    0.2721919542406358,
                    0,
                    0.26280623608017817,
                    0.6383341867212835
                ],
                [
                    0.05494476933073424,
                    0,
                    0.42509213797246254,
                    0,
                    0.3207126948775056,
                    0.029697900665642603
                ],
                [
                    0.7089311241065627,
                    0,
                    0.3027159077869018,
                    0,
                    0.41648106904231624,
                    0.3319679126130739
                ]
            ],
            "rate_limited": false,
            "answers": [
                "cheesehead",
                "nutmegger",
                "hoosier"
            ],
            "ml_answers": {
                "hoosier": 0.22340407080131314,
                "nutmegger": 0.25766770087968804,
                "cheesehead": 0.1673715682335545
            },
            "integer_answers": {
                "hoosier": 2,
                "nutmegger": 1,
                "cheesehead": 1
            },
            "data": {
                "wikipedia_search": [
                    1.4167446393762182,
                    0.32966861598440544,
                    4.253586744639376
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.9053436796844503,
                    2.9756449658072373,
                    2.119011354508312
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    118.0,
                    144.0,
                    187.0
                ],
                "result_count": [
                    74800.0,
                    3480.0,
                    38900.0
                ]
            },
            "z-best_answer_by_ml": [
                "nutmegger"
            ]
        },
        "lines": [
            [
                0,
                0.23612410656270302,
                0,
                0.2721919542406358,
                0,
                0.26280623608017817,
                0.6383341867212835
            ],
            [
                1,
                0.05494476933073424,
                0,
                0.42509213797246254,
                0,
                0.3207126948775056,
                0.029697900665642603
            ],
            [
                0,
                0.7089311241065627,
                0,
                0.3027159077869018,
                0,
                0.41648106904231624,
                0.3319679126130739
            ]
        ]
    },
    "Where in the home does the Maillard reaction typically occur?": {
        "raw_data": {
            "negative_question": false,
            "question": "Where in the home does the Maillard reaction typically occur?",
            "fraction_answers": {
                "bathroom": 0.08682834981592609,
                "kitchen": 0.749132121845942,
                "bedroom": 0.16403952833813185
            },
            "lines": [
                [
                    0.7149588595484118,
                    1.0,
                    0.566400304414003,
                    1.0,
                    0.6284533299590873,
                    0.5849802371541502
                ],
                [
                    0.1256816877152698,
                    0.0,
                    0.23611111111111108,
                    0.0,
                    0.3694799443249399,
                    0.25296442687747034
                ],
                [
                    0.1593594527363184,
                    0.0,
                    0.19748858447488585,
                    0.0,
                    0.002066725715972837,
                    0.16205533596837945
                ]
            ],
            "rate_limited": false,
            "answers": [
                "kitchen",
                "bedroom",
                "bathroom"
            ],
            "ml_answers": {
                "bathroom": 0.2879557471222827,
                "kitchen": 0.48373628733097684,
                "bedroom": 0.20712978072447072
            },
            "integer_answers": {
                "bathroom": 0,
                "kitchen": 6,
                "bedroom": 0
            },
            "data": {
                "wikipedia_search": [
                    2.859835438193647,
                    0.5027267508610792,
                    0.6374378109452736
                ],
                "word_count_entities": [
                    4.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.6992009132420092,
                    0.7083333333333333,
                    0.5924657534246576
                ],
                "word_count_raw": [
                    32.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    296.0,
                    128.0,
                    82.0
                ],
                "result_count": [
                    14900.0,
                    8760.0,
                    49.0
                ]
            },
            "z-best_answer_by_ml": [
                "kitchen"
            ]
        },
        "lines": [
            [
                1,
                0.7149588595484118,
                1.0,
                0.566400304414003,
                1.0,
                0.6284533299590873,
                0.5849802371541502
            ],
            [
                0,
                0.1256816877152698,
                0.0,
                0.23611111111111108,
                0.0,
                0.3694799443249399,
                0.25296442687747034
            ],
            [
                0,
                0.1593594527363184,
                0.0,
                0.19748858447488585,
                0.0,
                0.002066725715972837,
                0.16205533596837945
            ]
        ]
    },
    "Who is the director of \u201cTyler Perry\u2019s Madea\u2019s Family Reunion\u201d?": {
        "raw_data": {
            "negative_question": false,
            "question": "Who is the director of \u201cTyler Perry\u2019s Madea\u2019s Family Reunion\u201d?",
            "fraction_answers": {
                "tyler perry": 0.7595059670541805,
                "abraham lincoln": 0.12733105500495165,
                "george lucas": 0.11316297794086787
            },
            "lines": [
                [
                    0.5559806205654982,
                    1.0,
                    0.817016317016317,
                    0.9910714285714286,
                    0.41232227488151657,
                    0.7806451612903226
                ],
                [
                    0.15290426771153237,
                    0.0,
                    0.09742340992340992,
                    0.004464285714285714,
                    0.3080568720379147,
                    0.11612903225806452
                ],
                [
                    0.2911151117229694,
                    0.0,
                    0.08556027306027306,
                    0.004464285714285714,
                    0.2796208530805687,
                    0.1032258064516129
                ]
            ],
            "rate_limited": false,
            "answers": [
                "tyler perry",
                "george lucas",
                "abraham lincoln"
            ],
            "ml_answers": {
                "tyler perry": 0.7135617490757769,
                "abraham lincoln": 0.11903318813440313,
                "george lucas": 0.12423660966192739
            },
            "integer_answers": {
                "tyler perry": 6,
                "abraham lincoln": 0,
                "george lucas": 0
            },
            "data": {
                "wikipedia_search": [
                    3.8918643439584875,
                    1.0703298739807265,
                    2.0378057820607856
                ],
                "word_count_entities": [
                    229.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    5.7191142191142195,
                    0.6819638694638694,
                    0.5989219114219114
                ],
                "word_count_raw": [
                    222.0,
                    1.0,
                    1.0
                ],
                "word_count_appended": [
                    242.0,
                    36.0,
                    32.0
                ],
                "result_count": [
                    87.0,
                    65.0,
                    59.0
                ]
            },
            "z-best_answer_by_ml": [
                "tyler perry"
            ]
        },
        "lines": [
            [
                1,
                0.5559806205654982,
                1.0,
                0.817016317016317,
                0.9910714285714286,
                0.41232227488151657,
                0.7806451612903226
            ],
            [
                0,
                0.15290426771153237,
                0.0,
                0.09742340992340992,
                0.004464285714285714,
                0.3080568720379147,
                0.11612903225806452
            ],
            [
                0,
                0.2911151117229694,
                0.0,
                0.08556027306027306,
                0.004464285714285714,
                0.2796208530805687,
                0.1032258064516129
            ]
        ]
    },
    "The first person to lead an expedition to the South Pole came from what country?": {
        "raw_data": {
            "negative_question": false,
            "question": "The first person to lead an expedition to the South Pole came from what country?",
            "fraction_answers": {
                "canada": 0.3657191831467827,
                "iceland": 0.22082100389086082,
                "norway": 0.4134598129623564
            },
            "lines": [
                [
                    0.383931694648721,
                    0.75,
                    0.25720551378446116,
                    0.5,
                    0.29892761394101874,
                    0.29069405539993776
                ],
                [
                    0.38405329402673144,
                    0.0,
                    0.3906015037593985,
                    0.0,
                    0.38873994638069703,
                    0.161531279178338
                ],
                [
                    0.2320150113245475,
                    0.25,
                    0.35219298245614034,
                    0.5,
                    0.31233243967828417,
                    0.5477746654217243
                ]
            ],
            "rate_limited": false,
            "answers": [
                "norway",
                "iceland",
                "canada"
            ],
            "ml_answers": {
                "canada": 0.24006703516897082,
                "iceland": 0.28326648034326585,
                "norway": 0.44416105518138893
            },
            "integer_answers": {
                "canada": 1,
                "iceland": 3,
                "norway": 2
            },
            "data": {
                "wikipedia_search": [
                    2.6875218625410473,
                    2.6883730581871204,
                    1.6241050792718326
                ],
                "word_count_entities": [
                    6.0,
                    0.0,
                    2.0
                ],
                "word_relation_to_question": [
                    1.800438596491228,
                    2.734210526315789,
                    2.465350877192982
                ],
                "word_count_raw": [
                    6.0,
                    0.0,
                    6.0
                ],
                "word_count_appended": [
                    223.0,
                    290.0,
                    233.0
                ],
                "result_count": [
                    93400.0,
                    51900.0,
                    176000.0
                ]
            },
            "z-best_answer_by_ml": [
                "norway"
            ]
        },
        "lines": [
            [
                1,
                0.383931694648721,
                0.75,
                0.25720551378446116,
                0.5,
                0.29892761394101874,
                0.29069405539993776
            ],
            [
                0,
                0.38405329402673144,
                0.0,
                0.3906015037593985,
                0.0,
                0.38873994638069703,
                0.161531279178338
            ],
            [
                0,
                0.2320150113245475,
                0.25,
                0.35219298245614034,
                0.5,
                0.31233243967828417,
                0.5477746654217243
            ]
        ]
    },
    "What Japanese word means \u201cempty orchestra\u201d?": {
        "raw_data": {
            "negative_question": false,
            "question": "What Japanese word means \u201cempty orchestra\u201d?",
            "fraction_answers": {
                "sake": 0.12388788223515124,
                "anime": 0.17736251735668918,
                "karaoke": 0.6987496004081596
            },
            "lines": [
                [
                    0.1431818181818182,
                    0.0,
                    0.3617352115830062,
                    0.0,
                    0.050576752440106475,
                    0.18783351120597652
                ],
                [
                    0.5053030303030304,
                    0.017921146953405017,
                    0.252427238819026,
                    0.0165016501650165,
                    0.05323868677905945,
                    0.21878335112059766
                ],
                [
                    0.3515151515151515,
                    0.982078853046595,
                    0.3858375495979678,
                    0.9834983498349835,
                    0.8961845607808341,
                    0.5933831376734259
                ]
            ],
            "rate_limited": false,
            "answers": [
                "sake",
                "anime",
                "karaoke"
            ],
            "ml_answers": {
                "sake": 0.12497213893454004,
                "anime": 0.16105709575934668,
                "karaoke": 0.629830961552847
            },
            "z-best_answer_by_ml": [
                "karaoke"
            ],
            "data": {
                "wikipedia_search": [
                    0.7159090909090909,
                    2.526515151515152,
                    1.7575757575757576
                ],
                "word_count_entities": [
                    0.0,
                    5.0,
                    274.0
                ],
                "word_relation_to_question": [
                    1.808676057915031,
                    1.2621361940951301,
                    1.929187747989839
                ],
                "word_count_raw": [
                    0.0,
                    5.0,
                    298.0
                ],
                "word_count_appended": [
                    176.0,
                    205.0,
                    556.0
                ],
                "result_count": [
                    1140.0,
                    1200.0,
                    20200.0
                ]
            },
            "integer_answers": {
                "sake": 0,
                "anime": 1,
                "karaoke": 5
            }
        },
        "lines": [
            [
                0,
                0.1431818181818182,
                0.0,
                0.3617352115830062,
                0.0,
                0.050576752440106475,
                0.18783351120597652
            ],
            [
                0,
                0.5053030303030304,
                0.017921146953405017,
                0.252427238819026,
                0.0165016501650165,
                0.05323868677905945,
                0.21878335112059766
            ],
            [
                1,
                0.3515151515151515,
                0.982078853046595,
                0.3858375495979678,
                0.9834983498349835,
                0.8961845607808341,
                0.5933831376734259
            ]
        ]
    },
    "Which of these astronomical objects orbits the Earth?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which of these astronomical objects orbits the Earth?",
            "fraction_answers": {
                "sun": 0.4172087997689619,
                "milky way": 0.17306015567908117,
                "moon": 0.4097310445519569
            },
            "lines": [
                [
                    0.40514200711569137,
                    0.4166666666666667,
                    0.4642857142857143,
                    0.3795180722891566,
                    0.3170028818443804,
                    0.47577092511013214
                ],
                [
                    0.3276821862348178,
                    0.058333333333333334,
                    0.35714285714285715,
                    0.030120481927710843,
                    0.17146974063400577,
                    0.09361233480176212
                ],
                [
                    0.26717580664949087,
                    0.525,
                    0.17857142857142858,
                    0.5903614457831325,
                    0.5115273775216138,
                    0.43061674008810574
                ]
            ],
            "rate_limited": false,
            "answers": [
                "moon",
                "milky way",
                "sun"
            ],
            "ml_answers": {
                "sun": 0.3281523817774727,
                "milky way": 0.22707879034522874,
                "moon": 0.31351276387264276
            },
            "integer_answers": {
                "sun": 3,
                "milky way": 0,
                "moon": 3
            },
            "data": {
                "wikipedia_search": [
                    1.6205680284627655,
                    1.3107287449392713,
                    1.0687032265979635
                ],
                "word_count_entities": [
                    50.0,
                    7.0,
                    63.0
                ],
                "word_relation_to_question": [
                    1.8571428571428572,
                    1.4285714285714286,
                    0.7142857142857143
                ],
                "word_count_raw": [
                    63.0,
                    5.0,
                    98.0
                ],
                "word_count_appended": [
                    432.0,
                    85.0,
                    391.0
                ],
                "result_count": [
                    880000.0,
                    476000.0,
                    1420000.0
                ]
            },
            "z-best_answer_by_ml": [
                "sun"
            ]
        },
        "lines": [
            [
                1,
                0.40514200711569137,
                0.4166666666666667,
                0.4642857142857143,
                0.3795180722891566,
                0.3170028818443804,
                0.47577092511013214
            ],
            [
                0,
                0.3276821862348178,
                0.058333333333333334,
                0.35714285714285715,
                0.030120481927710843,
                0.17146974063400577,
                0.09361233480176212
            ],
            [
                0,
                0.26717580664949087,
                0.525,
                0.17857142857142858,
                0.5903614457831325,
                0.5115273775216138,
                0.43061674008810574
            ]
        ]
    },
    "Made famous in a documentary, where is the \u201cGrey Gardens\u201d home located?": {
        "raw_data": {
            "negative_question": false,
            "question": "Made famous in a documentary, where is the \u201cGrey Gardens\u201d home located?",
            "fraction_answers": {
                "east hampton, ny": 0.7145757957017498,
                "savannah, ga": 0.15143476019430216,
                "cape cod, ma": 0.13398944410394792
            },
            "lines": [
                [
                    0.34523809523809523,
                    0.0,
                    0.14546225614927907,
                    0.0,
                    0.10810810810810811,
                    0.20512820512820512
                ],
                [
                    0.38499999999999995,
                    1.0,
                    0.610008481764207,
                    1.0,
                    0.7027027027027027,
                    0.5897435897435898
                ],
                [
                    0.26976190476190476,
                    0.0,
                    0.24452926208651402,
                    0.0,
                    0.1891891891891892,
                    0.20512820512820512
                ]
            ],
            "rate_limited": false,
            "answers": [
                "cape cod, ma",
                "east hampton, ny",
                "savannah, ga"
            ],
            "ml_answers": {
                "east hampton, ny": 0.44977858057765047,
                "savannah, ga": 0.14267039514230717,
                "cape cod, ma": 0.1719843883233158
            },
            "z-best_answer_by_ml": [
                "east hampton, ny"
            ],
            "data": {
                "wikipedia_search": [
                    1.726190476190476,
                    1.9249999999999998,
                    1.3488095238095237
                ],
                "word_count_entities": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.8727735368956744,
                    3.6600508905852416,
                    1.467175572519084
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_appended": [
                    4.0,
                    26.0,
                    7.0
                ],
                "result_count": [
                    32.0,
                    92.0,
                    32.0
                ]
            },
            "integer_answers": {
                "east hampton, ny": 6,
                "savannah, ga": 0,
                "cape cod, ma": 0
            }
        },
        "lines": [
            [
                0,
                0.34523809523809523,
                0.0,
                0.14546225614927907,
                0.0,
                0.10810810810810811,
                0.20512820512820512
            ],
            [
                1,
                0.38499999999999995,
                1.0,
                0.610008481764207,
                1.0,
                0.7027027027027027,
                0.5897435897435898
            ],
            [
                0,
                0.26976190476190476,
                0.0,
                0.24452926208651402,
                0.0,
                0.1891891891891892,
                0.20512820512820512
            ]
        ]
    },
    "Which of these is NOT a machine used for printing?": {
        "raw_data": {
            "negative_question": true,
            "question": "Which of these is NOT a machine used for printing?",
            "fraction_answers": {
                "spirit duplicator": 0.4444847459317485,
                "hydraulophone": 0.20876152503209422,
                "hectograph": 0.34675372903615725
            },
            "lines": [
                [
                    0.3976897689768977,
                    0,
                    0.4385964912280702,
                    0,
                    0.4110429447852761,
                    0.33514774494556765
                ],
                [
                    0.3335844454010618,
                    0,
                    0.3893537261958315,
                    0,
                    0.30828220858895705,
                    0.2752721617418351
                ],
                [
                    0.2687257856220405,
                    0,
                    0.17204978257609838,
                    0,
                    0.28067484662576686,
                    0.3895800933125972
                ]
            ],
            "rate_limited": false,
            "answers": [
                "hydraulophone",
                "hectograph",
                "spirit duplicator"
            ],
            "ml_answers": {
                "spirit duplicator": 0.23530217326143257,
                "hydraulophone": 0.31547627735710215,
                "hectograph": 0.20745647845787585
            },
            "integer_answers": {
                "spirit duplicator": 3,
                "hydraulophone": 0,
                "hectograph": 1
            },
            "data": {
                "wikipedia_search": [
                    0.6138613861386139,
                    0.998493327593629,
                    1.387645286267757
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.3684210526315789,
                    0.6638776428250113,
                    1.9677013045434097
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    212.0,
                    289.0,
                    142.0
                ],
                "result_count": [
                    11600.0,
                    25000.0,
                    28600.0
                ]
            },
            "z-best_answer_by_ml": [
                "hydraulophone"
            ]
        },
        "lines": [
            [
                1,
                0.3976897689768977,
                0,
                0.4385964912280702,
                0,
                0.4110429447852761,
                0.33514774494556765
            ],
            [
                0,
                0.3335844454010618,
                0,
                0.3893537261958315,
                0,
                0.30828220858895705,
                0.2752721617418351
            ],
            [
                0,
                0.2687257856220405,
                0,
                0.17204978257609838,
                0,
                0.28067484662576686,
                0.3895800933125972
            ]
        ]
    },
    "How does the second verse of \u201cThe Star-Spangled Banner\u201d begin?": {
        "raw_data": {
            "negative_question": false,
            "question": "How does the second verse of \u201cThe Star-Spangled Banner\u201d begin?",
            "fraction_answers": {
                "on the shore": 0.5186657797481599,
                "travels far": 0.16469842342804236,
                "where the foe": 0.3166357968237978
            },
            "lines": [
                [
                    0.6444444444444445,
                    0.0,
                    0.2648556896313155,
                    0.0,
                    0.0009683285704163813,
                    0.07792207792207792
                ],
                [
                    0.07222222222222222,
                    0.8,
                    0.2892648735823039,
                    1.0,
                    0.44401407619092603,
                    0.5064935064935064
                ],
                [
                    0.2833333333333333,
                    0.2,
                    0.44587943678638053,
                    0.0,
                    0.5550175952386576,
                    0.4155844155844156
                ]
            ],
            "rate_limited": false,
            "answers": [
                "travels far",
                "on the shore",
                "where the foe"
            ],
            "ml_answers": {
                "on the shore": 0.5939614464167802,
                "travels far": 0.16042952539982477,
                "where the foe": 0.21543966718240382
            },
            "integer_answers": {
                "on the shore": 3,
                "travels far": 1,
                "where the foe": 2
            },
            "data": {
                "wikipedia_search": [
                    3.2222222222222223,
                    0.3611111111111111,
                    1.4166666666666665
                ],
                "word_count_entities": [
                    0.0,
                    4.0,
                    1.0
                ],
                "word_relation_to_question": [
                    1.5891341377878931,
                    1.7355892414938234,
                    2.675276620718283
                ],
                "word_count_raw": [
                    0.0,
                    2.0,
                    0.0
                ],
                "word_count_appended": [
                    6.0,
                    39.0,
                    32.0
                ],
                "result_count": [
                    41.0,
                    18800.0,
                    23500.0
                ]
            },
            "z-best_answer_by_ml": [
                "on the shore"
            ]
        },
        "lines": [
            [
                0,
                0.6444444444444445,
                0.0,
                0.2648556896313155,
                0.0,
                0.0009683285704163813,
                0.07792207792207792
            ],
            [
                1,
                0.07222222222222222,
                0.8,
                0.2892648735823039,
                1.0,
                0.44401407619092603,
                0.5064935064935064
            ],
            [
                0,
                0.2833333333333333,
                0.2,
                0.44587943678638053,
                0.0,
                0.5550175952386576,
                0.4155844155844156
            ]
        ]
    },
    "Which of these substances is both artificially made and found in nature?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which of these substances is both artificially made and found in nature?",
            "fraction_answers": {
                "latex": 0.3314619767276082,
                "teflon": 0.37193733561042935,
                "nylon": 0.2966006876619625
            },
            "lines": [
                [
                    0.2833333333333333,
                    0,
                    0.7142857142857143,
                    0.5,
                    0.28107074569789675,
                    0.08099688473520249
                ],
                [
                    0.09999999999999999,
                    0,
                    0.09523809523809523,
                    0.5,
                    0.29923518164435947,
                    0.48853016142735767
                ],
                [
                    0.6166666666666667,
                    0,
                    0.19047619047619047,
                    0.0,
                    0.4196940726577438,
                    0.43047295383743983
                ]
            ],
            "rate_limited": false,
            "answers": [
                "teflon",
                "nylon",
                "latex"
            ],
            "ml_answers": {
                "latex": 0.25411170587176074,
                "teflon": 0.3181126691584047,
                "nylon": 0.23027397254619278
            },
            "z-best_answer_by_ml": [
                "teflon"
            ],
            "data": {
                "wikipedia_search": [
                    0.85,
                    0.3,
                    1.85
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.7142857142857143,
                    0.09523809523809523,
                    0.19047619047619047
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    0.0
                ],
                "word_count_appended": [
                    294.0,
                    313.0,
                    439.0
                ],
                "result_count": [
                    572000.0,
                    3450000.0,
                    3040000.0
                ]
            },
            "integer_answers": {
                "latex": 2,
                "teflon": 2,
                "nylon": 1
            }
        },
        "lines": [
            [
                0,
                0.2833333333333333,
                0,
                0.7142857142857143,
                0.5,
                0.28107074569789675,
                0.08099688473520249
            ],
            [
                0,
                0.09999999999999999,
                0,
                0.09523809523809523,
                0.5,
                0.29923518164435947,
                0.48853016142735767
            ],
            [
                1,
                0.6166666666666667,
                0,
                0.19047619047619047,
                0.0,
                0.4196940726577438,
                0.43047295383743983
            ]
        ]
    },
    "Every U.S. state that starts with which of these letters has a Democratic governor?": {
        "raw_data": {
            "negative_question": false,
            "question": "Every U.S. state that starts with which of these letters has a Democratic governor?",
            "fraction_answers": {
                "c": 0.3850899631420675,
                "w": 0.2662580246267274,
                "v": 0.34865201223120507
            },
            "lines": [
                [
                    0.18421052631578946,
                    0.2372751300196095,
                    0.36666666666666664,
                    0.22455674451300475,
                    0.23474359965967978,
                    0.35009548058561424
                ],
                [
                    0.5,
                    0.45016625458265835,
                    0.09333333333333334,
                    0.4517691166132938,
                    0.48108902467321524,
                    0.3341820496499045
                ],
                [
                    0.3157894736842105,
                    0.3125586153977321,
                    0.54,
                    0.3236741388737015,
                    0.284167375667105,
                    0.3157224697644812
                ]
            ],
            "rate_limited": false,
            "answers": [
                "w",
                "c",
                "v"
            ],
            "ml_answers": {
                "c": 0.3694414034046611,
                "w": 0.1721201574208155,
                "v": 0.2546103390471252
            },
            "integer_answers": {
                "c": 4,
                "w": 1,
                "v": 1
            },
            "data": {
                "wikipedia_search": [
                    0.3684210526315789,
                    1.0,
                    0.631578947368421
                ],
                "word_count_entities": [
                    2783.0,
                    5280.0,
                    3666.0
                ],
                "word_relation_to_question": [
                    1.8333333333333333,
                    0.4666666666666667,
                    2.7
                ],
                "word_count_raw": [
                    2875.0,
                    5784.0,
                    4144.0
                ],
                "word_count_appended": [
                    3035.0,
                    6220.0,
                    3674.0
                ],
                "result_count": [
                    11000000.0,
                    10500000.0,
                    9920000.0
                ]
            },
            "z-best_answer_by_ml": [
                "c"
            ]
        },
        "lines": [
            [
                0,
                0.18421052631578946,
                0.2372751300196095,
                0.36666666666666664,
                0.22455674451300475,
                0.23474359965967978,
                0.35009548058561424
            ],
            [
                1,
                0.5,
                0.45016625458265835,
                0.09333333333333334,
                0.4517691166132938,
                0.48108902467321524,
                0.3341820496499045
            ],
            [
                0,
                0.3157894736842105,
                0.3125586153977321,
                0.54,
                0.3236741388737015,
                0.284167375667105,
                0.3157224697644812
            ]
        ]
    },
    "What does a rattlesnake typically do when it feels threatened?": {
        "raw_data": {
            "negative_question": false,
            "question": "What does a rattlesnake typically do when it feels threatened?",
            "fraction_answers": {
                "sends an angry email": 0.26171436202686205,
                "eats its feelings": 0.21989010989010987,
                "rattles its tail": 0.5183955280830281
            },
            "lines": [
                [
                    0.4333333333333334,
                    0,
                    0.5983440170940171,
                    0,
                    0.7333333333333333,
                    0.30857142857142855
                ],
                [
                    0.2,
                    0,
                    0.2205128205128205,
                    0,
                    0.13333333333333333,
                    0.32571428571428573
                ],
                [
                    0.3666666666666667,
                    0,
                    0.1811431623931624,
                    0,
                    0.13333333333333333,
                    0.3657142857142857
                ]
            ],
            "rate_limited": false,
            "answers": [
                "rattles its tail",
                "eats its feelings",
                "sends an angry email"
            ],
            "ml_answers": {
                "sends an angry email": 0.35920601267238444,
                "eats its feelings": 0.2609799929837579,
                "rattles its tail": 0.3740735310788371
            },
            "z-best_answer_by_ml": [
                "rattles its tail"
            ],
            "data": {
                "wikipedia_search": [
                    2.166666666666667,
                    1.0,
                    1.8333333333333335
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    2.3933760683760683,
                    0.882051282051282,
                    0.7245726495726496
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    33.0,
                    6.0,
                    6.0
                ],
                "result_count": [
                    54.0,
                    57.0,
                    64.0
                ]
            },
            "integer_answers": {
                "sends an angry email": 1,
                "eats its feelings": 0,
                "rattles its tail": 3
            }
        },
        "lines": [
            [
                0,
                0.4333333333333334,
                0,
                0.5983440170940171,
                0,
                0.7333333333333333,
                0.30857142857142855
            ],
            [
                1,
                0.2,
                0,
                0.2205128205128205,
                0,
                0.13333333333333333,
                0.32571428571428573
            ],
            [
                0,
                0.3666666666666667,
                0,
                0.1811431623931624,
                0,
                0.13333333333333333,
                0.3657142857142857
            ]
        ]
    },
    "Which of these was a name the ancient Greeks gave the planet Venus?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which of these was a name the ancient Greeks gave the planet Venus?",
            "fraction_answers": {
                "antimony": 0.3199512490969611,
                "flourine": 0.05156086083494765,
                "phosphorus": 0.6284878900680912
            },
            "lines": [
                [
                    0.14285714285714288,
                    0.0,
                    0.0,
                    0.0,
                    0.14903846153846154,
                    0.017469560614081524
                ],
                [
                    0.634920634920635,
                    1.0,
                    0.0,
                    1.0,
                    0.5833333333333334,
                    0.5526733721545791
                ],
                [
                    0.22222222222222224,
                    0.0,
                    1.0,
                    0.0,
                    0.2676282051282051,
                    0.4298570672313393
                ]
            ],
            "rate_limited": false,
            "answers": [
                "flourine",
                "phosphorus",
                "antimony"
            ],
            "ml_answers": {
                "antimony": 0.2484641897868101,
                "flourine": 0.12513385488844112,
                "phosphorus": 0.649196776506931
            },
            "integer_answers": {
                "antimony": 1,
                "flourine": 0,
                "phosphorus": 5
            },
            "data": {
                "wikipedia_search": [
                    0.42857142857142855,
                    1.9047619047619047,
                    0.6666666666666666
                ],
                "word_count_entities": [
                    0.0,
                    5.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_count_raw": [
                    0.0,
                    7.0,
                    0.0
                ],
                "word_count_appended": [
                    93.0,
                    364.0,
                    167.0
                ],
                "result_count": [
                    1650.0,
                    52200.0,
                    40600.0
                ]
            },
            "z-best_answer_by_ml": [
                "phosphorus"
            ]
        },
        "lines": [
            [
                0,
                0.14285714285714288,
                0.0,
                0.0,
                0.0,
                0.14903846153846154,
                0.017469560614081524
            ],
            [
                1,
                0.634920634920635,
                1.0,
                0.0,
                1.0,
                0.5833333333333334,
                0.5526733721545791
            ],
            [
                0,
                0.22222222222222224,
                0.0,
                1.0,
                0.0,
                0.2676282051282051,
                0.4298570672313393
            ]
        ]
    },
    "Which of these film composers most recently won an Oscar?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which of these film composers most recently won an Oscar?",
            "fraction_answers": {
                "ennio morricone": 0.3119609266441836,
                "danny elfman": 0.2502795762433751,
                "hans zimmer": 0.4377594971124412
            },
            "lines": [
                [
                    0.6345354645354646,
                    0.2,
                    0.240362700850811,
                    0.0,
                    0.3756345177664975,
                    0.4212328767123288
                ],
                [
                    0.23106617520410624,
                    0.4,
                    0.334879584328896,
                    1.0,
                    0.3147208121827411,
                    0.3458904109589041
                ],
                [
                    0.13439836026042923,
                    0.4,
                    0.42475771482029306,
                    0.0,
                    0.3096446700507614,
                    0.2328767123287671
                ]
            ],
            "rate_limited": false,
            "answers": [
                "ennio morricone",
                "hans zimmer",
                "danny elfman"
            ],
            "ml_answers": {
                "ennio morricone": 0.19271489524479082,
                "danny elfman": 0.24486932320150176,
                "hans zimmer": 0.4940406221390077
            },
            "integer_answers": {
                "ennio morricone": 3,
                "danny elfman": 1,
                "hans zimmer": 2
            },
            "data": {
                "wikipedia_search": [
                    3.172677322677323,
                    1.1553308760205312,
                    0.6719918013021462
                ],
                "word_count_entities": [
                    2.0,
                    4.0,
                    4.0
                ],
                "word_relation_to_question": [
                    1.201813504254055,
                    1.6743979216444798,
                    2.1237885741014653
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_appended": [
                    123.0,
                    101.0,
                    68.0
                ],
                "result_count": [
                    74.0,
                    62.0,
                    61.0
                ]
            },
            "z-best_answer_by_ml": [
                "hans zimmer"
            ]
        },
        "lines": [
            [
                1,
                0.6345354645354646,
                0.2,
                0.240362700850811,
                0.0,
                0.3756345177664975,
                0.4212328767123288
            ],
            [
                0,
                0.23106617520410624,
                0.4,
                0.334879584328896,
                1.0,
                0.3147208121827411,
                0.3458904109589041
            ],
            [
                0,
                0.13439836026042923,
                0.4,
                0.42475771482029306,
                0.0,
                0.3096446700507614,
                0.2328767123287671
            ]
        ]
    },
    "Which of these figure skating jumps was invented the most recently?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which of these figure skating jumps was invented the most recently?",
            "fraction_answers": {
                "salchow": 0.4375115183690274,
                "lutz": 0.18954674312999079,
                "axel": 0.3729417385009819
            },
            "lines": [
                [
                    0.013447286636385369,
                    0.26865671641791045,
                    0.06739388436912157,
                    0.2037037037037037,
                    0.2638459710007131,
                    0.32023289665211063
                ],
                [
                    0.9759144154912742,
                    0.26865671641791045,
                    0.817270159447896,
                    0.14814814814814814,
                    0.11813643926788686,
                    0.29694323144104806
                ],
                [
                    0.010638297872340425,
                    0.4626865671641791,
                    0.11533595618298231,
                    0.6481481481481481,
                    0.6180175897314001,
                    0.38282387190684136
                ]
            ],
            "rate_limited": false,
            "answers": [
                "lutz",
                "salchow",
                "axel"
            ],
            "ml_answers": {
                "salchow": 0.29648167561679145,
                "lutz": 0.20215431376836768,
                "axel": 0.3479633217247962
            },
            "z-best_answer_by_ml": [
                "axel"
            ],
            "data": {
                "wikipedia_search": [
                    0.053789146545541476,
                    3.903657661965097,
                    0.0425531914893617
                ],
                "word_count_entities": [
                    18.0,
                    18.0,
                    31.0
                ],
                "word_relation_to_question": [
                    0.3369694218456079,
                    4.086350797239481,
                    0.5766797809149117
                ],
                "word_count_raw": [
                    11.0,
                    8.0,
                    35.0
                ],
                "word_count_appended": [
                    220.0,
                    204.0,
                    263.0
                ],
                "result_count": [
                    111000.0,
                    49700.0,
                    260000.0
                ]
            },
            "integer_answers": {
                "salchow": 2,
                "lutz": 0,
                "axel": 4
            }
        },
        "lines": [
            [
                1,
                0.013447286636385369,
                0.26865671641791045,
                0.06739388436912157,
                0.2037037037037037,
                0.2638459710007131,
                0.32023289665211063
            ],
            [
                0,
                0.9759144154912742,
                0.26865671641791045,
                0.817270159447896,
                0.14814814814814814,
                0.11813643926788686,
                0.29694323144104806
            ],
            [
                0,
                0.010638297872340425,
                0.4626865671641791,
                0.11533595618298231,
                0.6481481481481481,
                0.6180175897314001,
                0.38282387190684136
            ]
        ]
    },
    "Romaine, Iceberg and Butterhead are all varieties of what?": {
        "raw_data": {
            "negative_question": false,
            "question": "Romaine, Iceberg and Butterhead are all varieties of what?",
            "fraction_answers": {
                "lettuce": 0.9937417919426434,
                "race cars": 0.004009591128327542,
                "disney dwarfs": 0.002248616929029026
            },
            "lines": [
                [
                    0.9895833333333334,
                    1.0,
                    1.0,
                    1.0,
                    0.9745011086474501,
                    0.9983663096750771
                ],
                [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.012195121951219513,
                    0.0012965796229546457
                ],
                [
                    0.010416666666666666,
                    0.0,
                    0.0,
                    0.0,
                    0.013303769401330377,
                    0.00033711070196820785
                ]
            ],
            "rate_limited": false,
            "answers": [
                "lettuce",
                "disney dwarfs",
                "race cars"
            ],
            "ml_answers": {
                "lettuce": 0.5796330537928813,
                "race cars": 0.17612355908568428,
                "disney dwarfs": 0.17612355908568428
            },
            "integer_answers": {
                "lettuce": 6,
                "race cars": 0,
                "disney dwarfs": 0
            },
            "data": {
                "wikipedia_search": [
                    3.9583333333333335,
                    0.0,
                    0.041666666666666664
                ],
                "word_count_entities": [
                    671.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    4.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    662.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    879.0,
                    11.0,
                    12.0
                ],
                "result_count": [
                    38500.0,
                    50.0,
                    13.0
                ]
            },
            "z-best_answer_by_ml": [
                "lettuce"
            ]
        },
        "lines": [
            [
                1,
                0.9895833333333334,
                1.0,
                1.0,
                1.0,
                0.9745011086474501,
                0.9983663096750771
            ],
            [
                0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.012195121951219513,
                0.0012965796229546457
            ],
            [
                0,
                0.010416666666666666,
                0.0,
                0.0,
                0.0,
                0.013303769401330377,
                0.00033711070196820785
            ]
        ]
    },
    "What is the correct pronunciation of the performer who sings \u201cSmooth Operator\u201d?": {
        "raw_data": {
            "negative_question": false,
            "question": "What is the correct pronunciation of the performer who sings \u201cSmooth Operator\u201d?",
            "fraction_answers": {
                "say-dee": 0.2489182286389733,
                "sayd": 0.05351865923674434,
                "shah-day": 0.6975631121242823
            },
            "lines": [
                [
                    0.0,
                    1.0,
                    0.6488095238095238,
                    1.0,
                    0.7553191489361702,
                    0.78125
                ],
                [
                    0.0,
                    0.0,
                    0.047619047619047616,
                    0.0,
                    0.1276595744680851,
                    0.14583333333333334
                ],
                [
                    1.0,
                    0.0,
                    0.30357142857142855,
                    0.0,
                    0.11702127659574468,
                    0.07291666666666667
                ]
            ],
            "rate_limited": false,
            "answers": [
                "shah-day",
                "sayd",
                "say-dee"
            ],
            "ml_answers": {
                "say-dee": 0.17592503298101145,
                "sayd": 0.14291509998279228,
                "shah-day": 0.5085023749109152
            },
            "z-best_answer_by_ml": [
                "shah-day"
            ],
            "data": {
                "wikipedia_search": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_count_entities": [
                    2.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    2.5952380952380953,
                    0.19047619047619047,
                    1.2142857142857142
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    142.0,
                    24.0,
                    22.0
                ],
                "result_count": [
                    75.0,
                    14.0,
                    7.0
                ]
            },
            "integer_answers": {
                "say-dee": 1,
                "sayd": 0,
                "shah-day": 5
            }
        },
        "lines": [
            [
                1,
                0.0,
                1.0,
                0.6488095238095238,
                1.0,
                0.7553191489361702,
                0.78125
            ],
            [
                0,
                0.0,
                0.0,
                0.047619047619047616,
                0.0,
                0.1276595744680851,
                0.14583333333333334
            ],
            [
                0,
                1.0,
                0.0,
                0.30357142857142855,
                0.0,
                0.11702127659574468,
                0.07291666666666667
            ]
        ]
    },
    "Which of these are you most likely to find in a toolbox?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which of these are you most likely to find in a toolbox?",
            "fraction_answers": {
                "mc hammer": 0.29877339849932405,
                "hammer": 0.48325497273097956,
                "hammerhead shark": 0.2179716287696963
            },
            "lines": [
                [
                    0.05555555555555555,
                    1.0,
                    0.10526315789473684,
                    0,
                    0.8422330097087378,
                    0.4132231404958678
                ],
                [
                    0.8888888888888888,
                    0.0,
                    0.15789473684210525,
                    0,
                    0.11650485436893204,
                    0.3305785123966942
                ],
                [
                    0.05555555555555555,
                    0.0,
                    0.7368421052631579,
                    0,
                    0.0412621359223301,
                    0.256198347107438
                ]
            ],
            "rate_limited": false,
            "answers": [
                "hammer",
                "mc hammer",
                "hammerhead shark"
            ],
            "ml_answers": {
                "mc hammer": 0.2365578235802886,
                "hammer": 0.4668328042376323,
                "hammerhead shark": 0.24317325874299173
            },
            "integer_answers": {
                "mc hammer": 1,
                "hammer": 3,
                "hammerhead shark": 1
            },
            "data": {
                "wikipedia_search": [
                    0.1111111111111111,
                    1.7777777777777777,
                    0.1111111111111111
                ],
                "word_count_entities": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.21052631578947367,
                    0.3157894736842105,
                    1.4736842105263157
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    347.0,
                    48.0,
                    17.0
                ],
                "result_count": [
                    100.0,
                    80.0,
                    62.0
                ]
            },
            "z-best_answer_by_ml": [
                "hammer"
            ]
        },
        "lines": [
            [
                1,
                0.05555555555555555,
                1.0,
                0.10526315789473684,
                0,
                0.8422330097087378,
                0.4132231404958678
            ],
            [
                0,
                0.8888888888888888,
                0.0,
                0.15789473684210525,
                0,
                0.11650485436893204,
                0.3305785123966942
            ],
            [
                0,
                0.05555555555555555,
                0.0,
                0.7368421052631579,
                0,
                0.0412621359223301,
                0.256198347107438
            ]
        ]
    },
    "Which of these is usually found on the ocean floor?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which of these is usually found on the ocean floor?",
            "fraction_answers": {
                "sweet potato": 0.3620928754927446,
                "cherry tomato": 0.07986953872915914,
                "sea cucumber": 0.5580375857780963
            },
            "lines": [
                [
                    0.28108974358974365,
                    0,
                    0.05238095238095238,
                    0,
                    0.3481675392670157,
                    0.7667332667332667
                ],
                [
                    0.03541666666666667,
                    0,
                    0.05494505494505495,
                    0,
                    0.15968586387434555,
                    0.06943056943056942
                ],
                [
                    0.6834935897435898,
                    0,
                    0.8926739926739927,
                    0,
                    0.49214659685863876,
                    0.16383616383616384
                ]
            ],
            "rate_limited": false,
            "answers": [
                "sweet potato",
                "cherry tomato",
                "sea cucumber"
            ],
            "ml_answers": {
                "sweet potato": 0.14569226312108313,
                "cherry tomato": 0.14344701711134225,
                "sea cucumber": 0.3176257479401914
            },
            "integer_answers": {
                "sweet potato": 1,
                "cherry tomato": 0,
                "sea cucumber": 3
            },
            "data": {
                "wikipedia_search": [
                    1.1243589743589744,
                    0.14166666666666666,
                    2.7339743589743586
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.2095238095238095,
                    0.21978021978021978,
                    3.5706959706959704
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    133.0,
                    61.0,
                    188.0
                ],
                "result_count": [
                    307000.0,
                    27800.0,
                    65600.0
                ]
            },
            "z-best_answer_by_ml": [
                "sea cucumber"
            ]
        },
        "lines": [
            [
                0,
                0.28108974358974365,
                0,
                0.05238095238095238,
                0,
                0.3481675392670157,
                0.7667332667332667
            ],
            [
                0,
                0.03541666666666667,
                0,
                0.05494505494505495,
                0,
                0.15968586387434555,
                0.06943056943056942
            ],
            [
                1,
                0.6834935897435898,
                0,
                0.8926739926739927,
                0,
                0.49214659685863876,
                0.16383616383616384
            ]
        ]
    },
    "The '90s band The Lightning Seeds took their name from which song?": {
        "raw_data": {
            "negative_question": false,
            "question": "The '90s band The Lightning Seeds took their name from which song?",
            "fraction_answers": {
                "when doves cry": 0.24653311210627518,
                "raspberry beret": 0.4675149689789008,
                "purple rain": 0.28595191891482397
            },
            "lines": [
                [
                    0.241156116068292,
                    1.0,
                    0.11941833137485311,
                    1.0,
                    0.3611111111111111,
                    0.08340425531914894
                ],
                [
                    0.4220824843673154,
                    0.0,
                    0.3061349392871132,
                    0.0,
                    0.3611111111111111,
                    0.6263829787234042
                ],
                [
                    0.33676139956439266,
                    0.0,
                    0.5744467293380336,
                    0.0,
                    0.2777777777777778,
                    0.2902127659574468
                ]
            ],
            "rate_limited": false,
            "answers": [
                "raspberry beret",
                "purple rain",
                "when doves cry"
            ],
            "ml_answers": {
                "when doves cry": 0.302748847209015,
                "raspberry beret": 0.6807769480775829,
                "purple rain": 0.24317043412664557
            },
            "integer_answers": {
                "when doves cry": 1,
                "raspberry beret": 3,
                "purple rain": 2
            },
            "data": {
                "wikipedia_search": [
                    0.964624464273168,
                    1.6883299374692615,
                    1.3470455982575706
                ],
                "word_count_entities": [
                    3.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.47767332549941244,
                    1.2245397571484529,
                    2.2977869173521346
                ],
                "word_count_raw": [
                    3.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    26.0,
                    26.0,
                    20.0
                ],
                "result_count": [
                    9800.0,
                    73600.0,
                    34100.0
                ]
            },
            "z-best_answer_by_ml": [
                "raspberry beret"
            ]
        },
        "lines": [
            [
                1,
                0.241156116068292,
                1.0,
                0.11941833137485311,
                1.0,
                0.3611111111111111,
                0.08340425531914894
            ],
            [
                0,
                0.4220824843673154,
                0.0,
                0.3061349392871132,
                0.0,
                0.3611111111111111,
                0.6263829787234042
            ],
            [
                0,
                0.33676139956439266,
                0.0,
                0.5744467293380336,
                0.0,
                0.2777777777777778,
                0.2902127659574468
            ]
        ]
    },
    "Which Las Vegas hotel features a replica of the Rialto Bridge?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which Las Vegas hotel features a replica of the Rialto Bridge?",
            "fraction_answers": {
                "caesars palace": 0.23223517884113035,
                "luxor": 0.21844961363087803,
                "the venetian": 0.5493152075279916
            },
            "lines": [
                [
                    0.1981892595339231,
                    0.02564102564102564,
                    0.23642551297301048,
                    0.0,
                    0.5182926829268293,
                    0.3321492007104796
                ],
                [
                    0.0324301175737756,
                    0.9102564102564102,
                    0.3813208524517198,
                    0.9746835443037974,
                    0.3932926829268293,
                    0.6039076376554174
                ],
                [
                    0.7693806228923012,
                    0.0641025641025641,
                    0.3822536345752698,
                    0.02531645569620253,
                    0.08841463414634146,
                    0.06394316163410302
                ]
            ],
            "rate_limited": false,
            "answers": [
                "luxor",
                "the venetian",
                "caesars palace"
            ],
            "ml_answers": {
                "caesars palace": 0.2754021026967471,
                "luxor": 0.2403870934091376,
                "the venetian": 0.5825719164167447
            },
            "integer_answers": {
                "caesars palace": 2,
                "luxor": 1,
                "the venetian": 3
            },
            "data": {
                "wikipedia_search": [
                    1.1891355572035387,
                    0.19458070544265357,
                    4.6162837373538075
                ],
                "word_count_entities": [
                    2.0,
                    71.0,
                    5.0
                ],
                "word_relation_to_question": [
                    0.9457020518920418,
                    1.525283409806879,
                    1.529014538301079
                ],
                "word_count_raw": [
                    0.0,
                    77.0,
                    2.0
                ],
                "word_count_appended": [
                    170.0,
                    129.0,
                    29.0
                ],
                "result_count": [
                    74800.0,
                    136000.0,
                    14400.0
                ]
            },
            "z-best_answer_by_ml": [
                "the venetian"
            ]
        },
        "lines": [
            [
                0,
                0.1981892595339231,
                0.02564102564102564,
                0.23642551297301048,
                0.0,
                0.5182926829268293,
                0.3321492007104796
            ],
            [
                1,
                0.0324301175737756,
                0.9102564102564102,
                0.3813208524517198,
                0.9746835443037974,
                0.3932926829268293,
                0.6039076376554174
            ],
            [
                0,
                0.7693806228923012,
                0.0641025641025641,
                0.3822536345752698,
                0.02531645569620253,
                0.08841463414634146,
                0.06394316163410302
            ]
        ]
    },
    "What TV series derived from a nearly 20-year-old Michael Crichton screenplay?": {
        "raw_data": {
            "negative_question": false,
            "question": "What TV series derived from a nearly 20-year-old Michael Crichton screenplay?",
            "fraction_answers": {
                "numb3rs": 0.09639244829103295,
                "the expanse": 0.045707355044831026,
                "er": 0.857900196664136
            },
            "lines": [
                [
                    0.01818181818181818,
                    0.0,
                    0.24509840806149397,
                    0.0,
                    0.01032258064516129,
                    0.0006413233805127089
                ],
                [
                    0.3020438754318636,
                    0.0,
                    0.2010231782705558,
                    0.0,
                    0.07483870967741936,
                    0.00044892636635889623
                ],
                [
                    0.6797743063863182,
                    1.0,
                    0.5538784136679502,
                    1.0,
                    0.9148387096774193,
                    0.9989097502531284
                ]
            ],
            "rate_limited": false,
            "answers": [
                "the expanse",
                "numb3rs",
                "er"
            ],
            "ml_answers": {
                "numb3rs": 0.2684820745236839,
                "the expanse": 0.21027604308656586,
                "er": 0.5095502975513679
            },
            "z-best_answer_by_ml": [
                "er"
            ],
            "data": {
                "wikipedia_search": [
                    0.09090909090909091,
                    1.5102193771593182,
                    3.398871531931591
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    1384.0
                ],
                "word_relation_to_question": [
                    1.4705904483689638,
                    1.2061390696233347,
                    3.3232704820077013
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1145.0
                ],
                "word_count_appended": [
                    16.0,
                    116.0,
                    1418.0
                ],
                "result_count": [
                    3300.0,
                    2310.0,
                    5140000.0
                ]
            },
            "integer_answers": {
                "numb3rs": 0,
                "the expanse": 0,
                "er": 6
            }
        },
        "lines": [
            [
                0,
                0.01818181818181818,
                0.0,
                0.24509840806149397,
                0.0,
                0.01032258064516129,
                0.0006413233805127089
            ],
            [
                0,
                0.3020438754318636,
                0.0,
                0.2010231782705558,
                0.0,
                0.07483870967741936,
                0.00044892636635889623
            ],
            [
                1,
                0.6797743063863182,
                1.0,
                0.5538784136679502,
                1.0,
                0.9148387096774193,
                0.9989097502531284
            ]
        ]
    },
    "Which of these video games was NOT produced by FromSoftware?": {
        "raw_data": {
            "negative_question": true,
            "question": "Which of these video games was NOT produced by FromSoftware?",
            "fraction_answers": {
                "demon's souls": 0.16350499669071294,
                "beyond: two souls": 0.13162469620246467,
                "dark souls": 0.7048703071068224
            },
            "lines": [
                [
                    0.352131988967241,
                    0.0,
                    0.32359350045428914,
                    0.0,
                    0.08947368421052632,
                    0.12018990504747629
                ],
                [
                    0.40359642970648607,
                    0.5,
                    0.3224506000742943,
                    0.5,
                    0.42280701754385963,
                    0.456271864067966
                ],
                [
                    0.24427158132627286,
                    0.5,
                    0.3539558994714166,
                    0.5,
                    0.48771929824561405,
                    0.4235382308845577
                ]
            ],
            "rate_limited": false,
            "answers": [
                "dark souls",
                "beyond: two souls",
                "demon's souls"
            ],
            "ml_answers": {
                "demon's souls": 0.32648108974807916,
                "beyond: two souls": 0.4061276082528663,
                "dark souls": 0.19631185208575633
            },
            "z-best_answer_by_ml": [
                "beyond: two souls"
            ],
            "data": {
                "wikipedia_search": [
                    1.1829440882620716,
                    0.7712285623481113,
                    2.0458273493898167
                ],
                "word_count_entities": [
                    64.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.4112519963656869,
                    1.4203951994056454,
                    1.1683528042286675
                ],
                "word_count_raw": [
                    55.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    234.0,
                    44.0,
                    7.0
                ],
                "result_count": [
                    304000.0,
                    35000.0,
                    61200.0
                ]
            },
            "integer_answers": {
                "demon's souls": 1,
                "beyond: two souls": 1,
                "dark souls": 4
            }
        },
        "lines": [
            [
                0,
                0.352131988967241,
                0.0,
                0.32359350045428914,
                0.0,
                0.08947368421052632,
                0.12018990504747629
            ],
            [
                1,
                0.40359642970648607,
                0.5,
                0.3224506000742943,
                0.5,
                0.42280701754385963,
                0.456271864067966
            ],
            [
                0,
                0.24427158132627286,
                0.5,
                0.3539558994714166,
                0.5,
                0.48771929824561405,
                0.4235382308845577
            ]
        ]
    },
    "Which of these is NOT the title of a current TV show?": {
        "raw_data": {
            "negative_question": true,
            "question": "Which of these is NOT the title of a current TV show?",
            "fraction_answers": {
                "chicago med": 0.3467350585492217,
                "chicago police": 0.3909065432131543,
                "chicage fire": 0.26235839823762397
            },
            "lines": [
                [
                    0.38209348572157187,
                    0,
                    0.26402600335527165,
                    0,
                    0.19383259911894274,
                    0.4665777947057703
                ],
                [
                    0.2905110587169161,
                    0,
                    0.3096306781520196,
                    0,
                    0.4955947136563877,
                    0.3795467529994287
                ],
                [
                    0.32739545556151195,
                    0,
                    0.42634331849270873,
                    0,
                    0.3105726872246696,
                    0.15387545229480099
                ]
            ],
            "rate_limited": false,
            "answers": [
                "chicago med",
                "chicage fire",
                "chicago police"
            ],
            "ml_answers": {
                "chicago med": 0.28601863255279825,
                "chicago police": 0.19196187979224458,
                "chicage fire": 0.12035733916036254
            },
            "z-best_answer_by_ml": [
                "chicago med"
            ],
            "data": {
                "wikipedia_search": [
                    0.9432521142274248,
                    1.675911530264671,
                    1.3808363555079042
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.8877919731578268,
                    1.522954574783843,
                    0.5892534520583301
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    139.0,
                    2.0,
                    86.0
                ],
                "result_count": [
                    70200.0,
                    253000.0,
                    727000.0
                ]
            },
            "integer_answers": {
                "chicago med": 2,
                "chicago police": 1,
                "chicage fire": 1
            }
        },
        "lines": [
            [
                0,
                0.38209348572157187,
                0,
                0.26402600335527165,
                0,
                0.19383259911894274,
                0.4665777947057703
            ],
            [
                0,
                0.2905110587169161,
                0,
                0.3096306781520196,
                0,
                0.4955947136563877,
                0.3795467529994287
            ],
            [
                1,
                0.32739545556151195,
                0,
                0.42634331849270873,
                0,
                0.3105726872246696,
                0.15387545229480099
            ]
        ]
    },
    "Which of these things is NOT found inside an atom?": {
        "raw_data": {
            "negative_question": true,
            "question": "Which of these things is NOT found inside an atom?",
            "fraction_answers": {
                "wonton": 0.1445413753635065,
                "proton": 0.40479876268851894,
                "neutron": 0.4506598619479745
            },
            "lines": [
                [
                    0.3238856589147287,
                    0.21999999999999997,
                    0.342330053667263,
                    0.1893939393939394,
                    0.316607460035524,
                    0.3933865999229881
                ],
                [
                    0.34294320137693635,
                    0.5,
                    0.3682692307692308,
                    0.5,
                    0.36589698046181174,
                    0.48926646130150175
                ],
                [
                    0.333171139708335,
                    0.28,
                    0.28940071556350627,
                    0.3106060606060606,
                    0.3174955595026643,
                    0.11734693877551022
                ]
            ],
            "rate_limited": false,
            "answers": [
                "proton",
                "wonton",
                "neutron"
            ],
            "ml_answers": {
                "wonton": 0.3386180131069936,
                "proton": 0.17477813296719455,
                "neutron": 0.22382966807336285
            },
            "integer_answers": {
                "wonton": 0,
                "proton": 4,
                "neutron": 2
            },
            "data": {
                "wikipedia_search": [
                    1.4089147286821704,
                    1.2564543889845095,
                    1.33463088233332
                ],
                "word_count_entities": [
                    42.0,
                    0.0,
                    33.0
                ],
                "word_relation_to_question": [
                    1.2613595706618963,
                    1.0538461538461539,
                    1.6847942754919498
                ],
                "word_count_raw": [
                    82.0,
                    0.0,
                    50.0
                ],
                "word_count_appended": [
                    413.0,
                    302.0,
                    411.0
                ],
                "result_count": [
                    886000.0,
                    89200.0,
                    3180000.0
                ]
            },
            "z-best_answer_by_ml": [
                "wonton"
            ]
        },
        "lines": [
            [
                0,
                0.3238856589147287,
                0.21999999999999997,
                0.342330053667263,
                0.1893939393939394,
                0.316607460035524,
                0.3933865999229881
            ],
            [
                1,
                0.34294320137693635,
                0.5,
                0.3682692307692308,
                0.5,
                0.36589698046181174,
                0.48926646130150175
            ],
            [
                0,
                0.333171139708335,
                0.28,
                0.28940071556350627,
                0.3106060606060606,
                0.3174955595026643,
                0.11734693877551022
            ]
        ]
    },
    "What actor famously yelled \"Not the bees! Not the bees!\" in a 2006 film?": {
        "raw_data": {
            "negative_question": false,
            "question": "What actor famously yelled \"Not the bees! Not the bees!\" in a 2006 film?",
            "fraction_answers": {
                "macauley culkin": 0.09161691244262204,
                "nicolas cage": 0.7253347719472001,
                "oprah winfrey": 0.18304831561017795
            },
            "lines": [
                [
                    0.32023099755262463,
                    1.0,
                    0.5856237879767291,
                    1.0,
                    0.8461538461538461,
                    0.6
                ],
                [
                    0.15433194222737637,
                    0.0,
                    0.24536953242835596,
                    0.0,
                    0.0,
                    0.15
                ],
                [
                    0.525437060219999,
                    0.0,
                    0.16900667959491486,
                    0.0,
                    0.15384615384615385,
                    0.25
                ]
            ],
            "rate_limited": false,
            "answers": [
                "nicolas cage",
                "macauley culkin",
                "oprah winfrey"
            ],
            "ml_answers": {
                "macauley culkin": 0.1812374449188749,
                "nicolas cage": 0.44977858057765047,
                "oprah winfrey": 0.187090508724793
            },
            "z-best_answer_by_ml": [
                "nicolas cage"
            ],
            "data": {
                "wikipedia_search": [
                    1.9213859853157478,
                    0.9259916533642583,
                    3.152622361319994
                ],
                "word_count_entities": [
                    18.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    4.099366515837104,
                    1.717586726998492,
                    1.183046757164404
                ],
                "word_count_raw": [
                    3.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    12.0,
                    3.0,
                    5.0
                ],
                "result_count": [
                    11.0,
                    0,
                    2.0
                ]
            },
            "integer_answers": {
                "macauley culkin": 0,
                "nicolas cage": 5,
                "oprah winfrey": 1
            }
        },
        "lines": [
            [
                1,
                0.32023099755262463,
                1.0,
                0.5856237879767291,
                1.0,
                0.8461538461538461,
                0.6
            ],
            [
                0,
                0.15433194222737637,
                0.0,
                0.24536953242835596,
                0.0,
                0.0,
                0.15
            ],
            [
                0,
                0.525437060219999,
                0.0,
                0.16900667959491486,
                0.0,
                0.15384615384615385,
                0.25
            ]
        ]
    },
    "What form of transportation counts Jay-Z as a prominent investor?": {
        "raw_data": {
            "negative_question": false,
            "question": "What form of transportation counts Jay-Z as a prominent investor?",
            "fraction_answers": {
                "boats": 0.3773907214374474,
                "aviation": 0.3254374367181149,
                "e-bikes": 0.2971718418444377
            },
            "lines": [
                [
                    0.18208815028901731,
                    0,
                    0.3586028242649635,
                    0,
                    0.41842105263157897,
                    0.3426377196869
                ],
                [
                    0.5325722543352601,
                    0,
                    0.3229395733958553,
                    0,
                    0.24736842105263157,
                    0.08580711859400383
                ],
                [
                    0.2853395953757225,
                    0,
                    0.31845760233918136,
                    0,
                    0.33421052631578946,
                    0.5715551617190961
                ]
            ],
            "rate_limited": false,
            "answers": [
                "aviation",
                "e-bikes",
                "boats"
            ],
            "ml_answers": {
                "boats": 0.2727457841335815,
                "aviation": 0.2736776307288564,
                "e-bikes": 0.1438896487703891
            },
            "integer_answers": {
                "boats": 1,
                "aviation": 2,
                "e-bikes": 1
            },
            "data": {
                "wikipedia_search": [
                    0.9104407514450866,
                    2.6628612716763005,
                    1.4266979768786126
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    2.151616945589781,
                    1.9376374403751315,
                    1.910745614035088
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    159.0,
                    94.0,
                    127.0
                ],
                "result_count": [
                    232000.0,
                    58100.0,
                    387000.0
                ]
            },
            "z-best_answer_by_ml": [
                "aviation"
            ]
        },
        "lines": [
            [
                1,
                0.18208815028901731,
                0,
                0.3586028242649635,
                0,
                0.41842105263157897,
                0.3426377196869
            ],
            [
                0,
                0.5325722543352601,
                0,
                0.3229395733958553,
                0,
                0.24736842105263157,
                0.08580711859400383
            ],
            [
                0,
                0.2853395953757225,
                0,
                0.31845760233918136,
                0,
                0.33421052631578946,
                0.5715551617190961
            ]
        ]
    },
    "Mardi Gras is celebrated right before what other observance?": {
        "raw_data": {
            "negative_question": false,
            "question": "Mardi Gras is celebrated right before what other observance?",
            "fraction_answers": {
                "kwanzaa": 0.23038569578425785,
                "ramadan": 0.14940647099761248,
                "lent": 0.6202078332181296
            },
            "lines": [
                [
                    0.3926007326007326,
                    0.9852941176470589,
                    0.508383017163505,
                    1.0,
                    0.4669987546699875,
                    0.36797037722749365
                ],
                [
                    0.2874725274725275,
                    0.0,
                    0.27604336043360433,
                    0.0,
                    0.2876712328767123,
                    0.5311270539227031
                ],
                [
                    0.31992673992673987,
                    0.014705882352941176,
                    0.21557362240289066,
                    0.0,
                    0.24533001245330013,
                    0.10090256884980328
                ]
            ],
            "rate_limited": false,
            "answers": [
                "lent",
                "kwanzaa",
                "ramadan"
            ],
            "ml_answers": {
                "kwanzaa": 0.15422972002214325,
                "ramadan": 0.3138863001311299,
                "lent": 0.6974042375834094
            },
            "integer_answers": {
                "kwanzaa": 1,
                "ramadan": 0,
                "lent": 5
            },
            "data": {
                "wikipedia_search": [
                    1.1778021978021977,
                    0.8624175824175825,
                    0.9597802197802197
                ],
                "word_count_entities": [
                    67.0,
                    0.0,
                    1.0
                ],
                "word_relation_to_question": [
                    2.541915085817525,
                    1.3802168021680217,
                    1.0778681120144533
                ],
                "word_count_raw": [
                    56.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    375.0,
                    231.0,
                    197.0
                ],
                "result_count": [
                    31800.0,
                    45900.0,
                    8720.0
                ]
            },
            "z-best_answer_by_ml": [
                "lent"
            ]
        },
        "lines": [
            [
                1,
                0.3926007326007326,
                0.9852941176470589,
                0.508383017163505,
                1.0,
                0.4669987546699875,
                0.36797037722749365
            ],
            [
                0,
                0.2874725274725275,
                0.0,
                0.27604336043360433,
                0.0,
                0.2876712328767123,
                0.5311270539227031
            ],
            [
                0,
                0.31992673992673987,
                0.014705882352941176,
                0.21557362240289066,
                0.0,
                0.24533001245330013,
                0.10090256884980328
            ]
        ]
    },
    "In baking, yeast helps bread rise, but scientifically yeast is what?": {
        "raw_data": {
            "negative_question": false,
            "question": "In baking, yeast helps bread rise, but scientifically yeast is what?",
            "fraction_answers": {
                "fungus": 0.5542879014378674,
                "plant": 0.20723414892807654,
                "bacteria": 0.238477949634056
            },
            "lines": [
                [
                    0.8807241145950824,
                    0.47619047619047616,
                    0.7333333333333333,
                    0.8571428571428571,
                    0.02902155887230514,
                    0.3493150684931507
                ],
                [
                    0.06468531468531469,
                    0.38095238095238093,
                    0.26666666666666666,
                    0.07142857142857142,
                    0.15008291873963517,
                    0.3095890410958904
                ],
                [
                    0.05459057071960298,
                    0.14285714285714285,
                    0.0,
                    0.07142857142857142,
                    0.8208955223880597,
                    0.3410958904109589
                ]
            ],
            "rate_limited": false,
            "answers": [
                "fungus",
                "plant",
                "bacteria"
            ],
            "ml_answers": {
                "fungus": 0.5369637555552721,
                "plant": 0.21830990399613465,
                "bacteria": 0.3549471765995724
            },
            "z-best_answer_by_ml": [
                "fungus"
            ],
            "data": {
                "wikipedia_search": [
                    3.5228964583803295,
                    0.25874125874125875,
                    0.21836228287841192
                ],
                "word_count_entities": [
                    10.0,
                    8.0,
                    3.0
                ],
                "word_relation_to_question": [
                    3.6666666666666665,
                    1.3333333333333333,
                    0.0
                ],
                "word_count_raw": [
                    12.0,
                    1.0,
                    1.0
                ],
                "word_count_appended": [
                    255.0,
                    226.0,
                    249.0
                ],
                "result_count": [
                    105000.0,
                    543000.0,
                    2970000.0
                ]
            },
            "integer_answers": {
                "fungus": 5,
                "plant": 0,
                "bacteria": 1
            }
        },
        "lines": [
            [
                1,
                0.8807241145950824,
                0.47619047619047616,
                0.7333333333333333,
                0.8571428571428571,
                0.02902155887230514,
                0.3493150684931507
            ],
            [
                0,
                0.06468531468531469,
                0.38095238095238093,
                0.26666666666666666,
                0.07142857142857142,
                0.15008291873963517,
                0.3095890410958904
            ],
            [
                0,
                0.05459057071960298,
                0.14285714285714285,
                0.0,
                0.07142857142857142,
                0.8208955223880597,
                0.3410958904109589
            ]
        ]
    },
    "Which of these is classified as a neurological condition or disorder?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which of these is classified as a neurological condition or disorder?",
            "fraction_answers": {
                "halitosis": 0.16166981705242464,
                "cystic fibrosis": 0.1775143848259948,
                "multiple sclerosis": 0.6608157981215806
            },
            "lines": [
                [
                    0.28756674294431733,
                    0.0,
                    0.16449221645663456,
                    0.0,
                    0.5140495867768595,
                    0.003910356136736324
                ],
                [
                    0.20404271548436306,
                    0.0,
                    0.2097356066221893,
                    0.0,
                    0.2396694214876033,
                    0.41163856536181304
                ],
                [
                    0.5083905415713196,
                    1.0,
                    0.6257721769211761,
                    1.0,
                    0.2462809917355372,
                    0.5844510785014506
                ]
            ],
            "rate_limited": false,
            "answers": [
                "halitosis",
                "cystic fibrosis",
                "multiple sclerosis"
            ],
            "ml_answers": {
                "halitosis": 0.32175931291310883,
                "cystic fibrosis": 0.3009759541974086,
                "multiple sclerosis": 0.5039371038144039
            },
            "integer_answers": {
                "halitosis": 1,
                "cystic fibrosis": 0,
                "multiple sclerosis": 5
            },
            "data": {
                "wikipedia_search": [
                    0.8627002288329519,
                    0.6121281464530892,
                    1.5251716247139586
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    3.0
                ],
                "word_relation_to_question": [
                    0.49347664936990365,
                    0.6292068198665679,
                    1.8773165307635284
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    8.0
                ],
                "word_count_appended": [
                    311.0,
                    145.0,
                    149.0
                ],
                "result_count": [
                    93000.0,
                    9790000.0,
                    13900000.0
                ]
            },
            "z-best_answer_by_ml": [
                "multiple sclerosis"
            ]
        },
        "lines": [
            [
                0,
                0.28756674294431733,
                0.0,
                0.16449221645663456,
                0.0,
                0.5140495867768595,
                0.003910356136736324
            ],
            [
                0,
                0.20404271548436306,
                0.0,
                0.2097356066221893,
                0.0,
                0.2396694214876033,
                0.41163856536181304
            ],
            [
                1,
                0.5083905415713196,
                1.0,
                0.6257721769211761,
                1.0,
                0.2462809917355372,
                0.5844510785014506
            ]
        ]
    },
    "Which of these Uranus moons is NOT named after a Shakespearean character?": {
        "raw_data": {
            "negative_question": true,
            "question": "Which of these Uranus moons is NOT named after a Shakespearean character?",
            "fraction_answers": {
                "oberon": 0.36162812200789113,
                "trinculo": 0.21105924837221146,
                "umbriel": 0.4273126296198974
            },
            "lines": [
                [
                    0.47252959927140253,
                    0.17857142857142855,
                    0.4631771089767614,
                    0.21052631578947367,
                    0.27834302325581395,
                    0.3119681581114466
                ],
                [
                    0.22851548269581057,
                    0.3660714285714286,
                    0.25097460336123334,
                    0.29824561403508776,
                    0.33502906976744184,
                    0.23922591270930554
                ],
                [
                    0.2989549180327869,
                    0.45535714285714285,
                    0.2858482876620052,
                    0.49122807017543857,
                    0.3866279069767442,
                    0.4488059291792479
                ]
            ],
            "rate_limited": false,
            "answers": [
                "oberon",
                "umbriel",
                "trinculo"
            ],
            "ml_answers": {
                "oberon": 0.3225572045367969,
                "trinculo": 0.361470572816902,
                "umbriel": 0.1830602045554194
            },
            "integer_answers": {
                "oberon": 3,
                "trinculo": 0,
                "umbriel": 3
            },
            "data": {
                "wikipedia_search": [
                    0.2747040072859745,
                    2.7148451730418945,
                    2.010450819672131
                ],
                "word_count_entities": [
                    36.0,
                    15.0,
                    5.0
                ],
                "word_relation_to_question": [
                    0.44187469227886333,
                    2.9883047596651995,
                    2.5698205480559375
                ],
                "word_count_raw": [
                    33.0,
                    23.0,
                    1.0
                ],
                "word_count_appended": [
                    305.0,
                    227.0,
                    156.0
                ],
                "result_count": [
                    13700.0,
                    19000.0,
                    3730.0
                ]
            },
            "z-best_answer_by_ml": [
                "trinculo"
            ]
        },
        "lines": [
            [
                0,
                0.47252959927140253,
                0.17857142857142855,
                0.4631771089767614,
                0.21052631578947367,
                0.27834302325581395,
                0.3119681581114466
            ],
            [
                1,
                0.22851548269581057,
                0.3660714285714286,
                0.25097460336123334,
                0.29824561403508776,
                0.33502906976744184,
                0.23922591270930554
            ],
            [
                0,
                0.2989549180327869,
                0.45535714285714285,
                0.2858482876620052,
                0.49122807017543857,
                0.3866279069767442,
                0.4488059291792479
            ]
        ]
    },
    "Which of these knots is typically used to add another line to a rope?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which of these knots is typically used to add another line to a rope?",
            "fraction_answers": {
                "grantchester": 0.07345803988003861,
                "rolling hitch": 0.19979412280964826,
                "bowline": 0.7267478373103132
            },
            "lines": [
                [
                    0.5531135737814656,
                    1.0,
                    0.5154571800489616,
                    0.9,
                    0.6091269841269841,
                    0.7827892859044677
                ],
                [
                    0.39039940475230894,
                    0.0,
                    0.36214061469496384,
                    0.1,
                    0.15079365079365079,
                    0.19543106661696577
                ],
                [
                    0.05648702146622541,
                    0.0,
                    0.12240220525607459,
                    0.0,
                    0.2400793650793651,
                    0.021779647478566484
                ]
            ],
            "rate_limited": false,
            "answers": [
                "bowline",
                "rolling hitch",
                "grantchester"
            ],
            "ml_answers": {
                "grantchester": 0.23125251157517038,
                "rolling hitch": 0.24076900896804884,
                "bowline": 0.6512092444744547
            },
            "z-best_answer_by_ml": [
                "bowline"
            ],
            "data": {
                "wikipedia_search": [
                    2.2124542951258626,
                    1.5615976190092358,
                    0.22594808586490164
                ],
                "word_count_entities": [
                    20.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    2.0618287201958463,
                    1.4485624587798553,
                    0.48960882102429837
                ],
                "word_count_raw": [
                    9.0,
                    1.0,
                    0.0
                ],
                "word_count_appended": [
                    307.0,
                    76.0,
                    121.0
                ],
                "result_count": [
                    147000.0,
                    36700.0,
                    4090.0
                ]
            },
            "integer_answers": {
                "grantchester": 0,
                "rolling hitch": 0,
                "bowline": 6
            }
        },
        "lines": [
            [
                0,
                0.5531135737814656,
                1.0,
                0.5154571800489616,
                0.9,
                0.6091269841269841,
                0.7827892859044677
            ],
            [
                1,
                0.39039940475230894,
                0.0,
                0.36214061469496384,
                0.1,
                0.15079365079365079,
                0.19543106661696577
            ],
            [
                0,
                0.05648702146622541,
                0.0,
                0.12240220525607459,
                0.0,
                0.2400793650793651,
                0.021779647478566484
            ]
        ]
    },
    "Which of these celebrities is known for having aviophobia?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which of these celebrities is known for having aviophobia?",
            "fraction_answers": {
                "angelina jolie": 0.4580085104221283,
                "john travolta": 0.2668581474983914,
                "john madden": 0.27513334207948026
            },
            "lines": [
                [
                    0.8344907407407408,
                    0.25,
                    0.2512820512820513,
                    0.5,
                    0.4634146341463415,
                    0.44886363636363635
                ],
                [
                    0.07423941798941798,
                    0.5,
                    0.4615384615384615,
                    0.0,
                    0.35365853658536583,
                    0.26136363636363635
                ],
                [
                    0.09126984126984126,
                    0.25,
                    0.2871794871794872,
                    0.5,
                    0.18292682926829268,
                    0.2897727272727273
                ]
            ],
            "rate_limited": false,
            "answers": [
                "angelina jolie",
                "john madden",
                "john travolta"
            ],
            "ml_answers": {
                "angelina jolie": 0.3004133770027518,
                "john travolta": 0.25467487690398943,
                "john madden": 0.326531964461274
            },
            "z-best_answer_by_ml": [
                "john madden"
            ],
            "data": {
                "wikipedia_search": [
                    2.5034722222222223,
                    0.22271825396825395,
                    0.2738095238095238
                ],
                "word_count_entities": [
                    1.0,
                    2.0,
                    1.0
                ],
                "word_relation_to_question": [
                    0.7538461538461538,
                    1.3846153846153846,
                    0.8615384615384616
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    1.0
                ],
                "word_count_appended": [
                    38.0,
                    29.0,
                    15.0
                ],
                "result_count": [
                    79.0,
                    46.0,
                    51.0
                ]
            },
            "integer_answers": {
                "angelina jolie": 4,
                "john travolta": 0,
                "john madden": 2
            }
        },
        "lines": [
            [
                0,
                0.8344907407407408,
                0.25,
                0.2512820512820513,
                0.5,
                0.4634146341463415,
                0.44886363636363635
            ],
            [
                1,
                0.07423941798941798,
                0.5,
                0.4615384615384615,
                0.0,
                0.35365853658536583,
                0.26136363636363635
            ],
            [
                0,
                0.09126984126984126,
                0.25,
                0.2871794871794872,
                0.5,
                0.18292682926829268,
                0.2897727272727273
            ]
        ]
    },
    "Who wrote a #1 hit song for the Monkees?": {
        "raw_data": {
            "negative_question": false,
            "question": "Who wrote a #1 hit song for the Monkees?",
            "fraction_answers": {
                "neil diamond": 0.3021789803494291,
                "james taylor": 0.3582254058075709,
                "jackson browne": 0.3395956138430001
            },
            "lines": [
                [
                    0.49659083138703847,
                    0,
                    0.19587384757413578,
                    0,
                    0,
                    0.38221153846153844
                ],
                [
                    0.32074195073600575,
                    0,
                    0.27650011851740963,
                    0,
                    0,
                    0.3092948717948718
                ],
                [
                    0.18266721787695572,
                    0,
                    0.5276260339084546,
                    0,
                    0,
                    0.30849358974358976
                ]
            ],
            "rate_limited": false,
            "answers": [
                "james taylor",
                "neil diamond",
                "jackson browne"
            ],
            "ml_answers": {
                "neil diamond": 0.17525967723165742,
                "james taylor": 0.31438406451972006,
                "jackson browne": 0.25077146987854665
            },
            "integer_answers": {
                "neil diamond": 0,
                "james taylor": 2,
                "jackson browne": 1
            },
            "data": {
                "wikipedia_search": [
                    2.4829541569351923,
                    1.6037097536800289,
                    0.9133360893847786
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.9793692378706789,
                    1.3825005925870482,
                    2.6381301695422734
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    47700000.0,
                    38600000.0,
                    38500000.0
                ]
            },
            "z-best_answer_by_ml": [
                "james taylor"
            ]
        },
        "lines": [
            [
                0,
                0.49659083138703847,
                0,
                0.19587384757413578,
                0,
                0,
                0.38221153846153844
            ],
            [
                1,
                0.32074195073600575,
                0,
                0.27650011851740963,
                0,
                0,
                0.3092948717948718
            ],
            [
                0,
                0.18266721787695572,
                0,
                0.5276260339084546,
                0,
                0,
                0.30849358974358976
            ]
        ]
    },
    "Which of these modes of transportation has only one wheel?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which of these modes of transportation has only one wheel?",
            "fraction_answers": {
                "unicycle": 0.29789965773140775,
                "bus": 0.6467471620554884,
                "monster truck": 0.05535318021310374
            },
            "lines": [
                [
                    0.09574468085106383,
                    0.0,
                    0.11666666666666668,
                    0.0,
                    0.10885167464114832,
                    0.01085605911974364
                ],
                [
                    0.5425531914893617,
                    0.8181818181818182,
                    0.46190476190476193,
                    0.6833333333333333,
                    0.39354066985645936,
                    0.9809691975671964
                ],
                [
                    0.3617021276595745,
                    0.18181818181818182,
                    0.42142857142857143,
                    0.31666666666666665,
                    0.49760765550239233,
                    0.00817474331305997
                ]
            ],
            "rate_limited": false,
            "answers": [
                "monster truck",
                "bus",
                "unicycle"
            ],
            "ml_answers": {
                "unicycle": 0.30783552593260644,
                "bus": 0.3750653733116785,
                "monster truck": 0.1580984414518603
            },
            "z-best_answer_by_ml": [
                "bus"
            ],
            "data": {
                "wikipedia_search": [
                    0.19148936170212766,
                    1.0851063829787233,
                    0.723404255319149
                ],
                "word_count_entities": [
                    0.0,
                    27.0,
                    6.0
                ],
                "word_relation_to_question": [
                    0.23333333333333334,
                    0.9238095238095237,
                    0.8428571428571427
                ],
                "word_count_raw": [
                    0.0,
                    41.0,
                    19.0
                ],
                "word_count_appended": [
                    91.0,
                    329.0,
                    416.0
                ],
                "result_count": [
                    1660000.0,
                    150000000.0,
                    1250000.0
                ]
            },
            "integer_answers": {
                "unicycle": 1,
                "bus": 5,
                "monster truck": 0
            }
        },
        "lines": [
            [
                0,
                0.09574468085106383,
                0.0,
                0.11666666666666668,
                0.0,
                0.10885167464114832,
                0.01085605911974364
            ],
            [
                0,
                0.5425531914893617,
                0.8181818181818182,
                0.46190476190476193,
                0.6833333333333333,
                0.39354066985645936,
                0.9809691975671964
            ],
            [
                1,
                0.3617021276595745,
                0.18181818181818182,
                0.42142857142857143,
                0.31666666666666665,
                0.49760765550239233,
                0.00817474331305997
            ]
        ]
    },
    "What is Telluride, Colorado named after?": {
        "raw_data": {
            "negative_question": false,
            "question": "What is Telluride, Colorado named after?",
            "fraction_answers": {
                "a european city": 0.10689242064608565,
                "an element": 0.5077554736347036,
                "a governor": 0.38535210571921075
            },
            "lines": [
                [
                    0.0,
                    0,
                    0.5095338983050848,
                    0,
                    0.5405405405405406,
                    0.9809474556931892
                ],
                [
                    1.0,
                    0,
                    0.1440677966101695,
                    0,
                    0.3783783783783784,
                    0.018962247888295123
                ],
                [
                    0.0,
                    0,
                    0.3463983050847458,
                    0,
                    0.08108108108108109,
                    9.029641851569106e-05
                ]
            ],
            "rate_limited": false,
            "answers": [
                "an element",
                "a governor",
                "a european city"
            ],
            "ml_answers": {
                "a european city": 0.16921050202770269,
                "an element": 0.24884720899815177,
                "a governor": 0.2733193201412854
            },
            "integer_answers": {
                "a european city": 0,
                "an element": 3,
                "a governor": 1
            },
            "data": {
                "wikipedia_search": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.0190677966101696,
                    0.288135593220339,
                    0.6927966101694916
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    60.0,
                    42.0,
                    9.0
                ],
                "result_count": [
                    239000.0,
                    4620.0,
                    22.0
                ]
            },
            "z-best_answer_by_ml": [
                "a governor"
            ]
        },
        "lines": [
            [
                1,
                0.0,
                0,
                0.5095338983050848,
                0,
                0.5405405405405406,
                0.9809474556931892
            ],
            [
                0,
                1.0,
                0,
                0.1440677966101695,
                0,
                0.3783783783783784,
                0.018962247888295123
            ],
            [
                0,
                0.0,
                0,
                0.3463983050847458,
                0,
                0.08108108108108109,
                9.029641851569106e-05
            ]
        ]
    },
    "Which of these is NOT a suit in a traditional Tarot deck?": {
        "raw_data": {
            "negative_question": true,
            "question": "Which of these is NOT a suit in a traditional Tarot deck?",
            "fraction_answers": {
                "gloves": 0.17240239844747948,
                "cups": 0.3471269624044591,
                "swords": 0.4804706391480614
            },
            "lines": [
                [
                    0.2818010255628205,
                    0.5,
                    0.4014423076923077,
                    0.5,
                    0.3974093264248705,
                    0.402140144977563
                ],
                [
                    0.28815042858867557,
                    0.18859649122807015,
                    0.23317307692307693,
                    0.24404761904761907,
                    0.29274611398963735,
                    0.31187435277873665
                ],
                [
                    0.43004854584850394,
                    0.3114035087719298,
                    0.3653846153846154,
                    0.25595238095238093,
                    0.30984455958549223,
                    0.28598550224370034
                ]
            ],
            "rate_limited": false,
            "answers": [
                "gloves",
                "swords",
                "cups"
            ],
            "ml_answers": {
                "gloves": 0.3015659949771282,
                "cups": 0.2775093746413746,
                "swords": 0.1861855267662014
            },
            "integer_answers": {
                "gloves": 1,
                "cups": 1,
                "swords": 4
            },
            "data": {
                "wikipedia_search": [
                    1.309193846623077,
                    1.2710974284679464,
                    0.41970872490897654
                ],
                "word_count_entities": [
                    0.0,
                    71.0,
                    43.0
                ],
                "word_relation_to_question": [
                    0.7884615384615384,
                    2.1346153846153846,
                    1.0769230769230769
                ],
                "word_count_raw": [
                    0.0,
                    43.0,
                    41.0
                ],
                "word_count_appended": [
                    198.0,
                    400.0,
                    367.0
                ],
                "result_count": [
                    567000.0,
                    1090000.0,
                    1240000.0
                ]
            },
            "z-best_answer_by_ml": [
                "gloves"
            ]
        },
        "lines": [
            [
                1,
                0.2818010255628205,
                0.5,
                0.4014423076923077,
                0.5,
                0.3974093264248705,
                0.402140144977563
            ],
            [
                0,
                0.28815042858867557,
                0.18859649122807015,
                0.23317307692307693,
                0.24404761904761907,
                0.29274611398963735,
                0.31187435277873665
            ],
            [
                0,
                0.43004854584850394,
                0.3114035087719298,
                0.3653846153846154,
                0.25595238095238093,
                0.30984455958549223,
                0.28598550224370034
            ]
        ]
    },
    "One of Tupac Shakur\u2019s biggest posthumous hits samples what singer?": {
        "raw_data": {
            "negative_question": false,
            "question": "One of Tupac Shakur\u2019s biggest posthumous hits samples what singer?",
            "fraction_answers": {
                "john mellencamp": 0.21150877184080422,
                "bruce hornsby": 0.6117926720675321,
                "christopher cross": 0.17669855609166377
            },
            "lines": [
                [
                    0.35713597406130965,
                    0.0,
                    0.5541255130524432,
                    0.0,
                    0.13333333333333333,
                    0.015596516102896496
                ],
                [
                    0.462362635286293,
                    0.0,
                    0.11836375175070028,
                    0.0,
                    0.13333333333333333,
                    0.5549929106744986
                ],
                [
                    0.18050139065239737,
                    1.0,
                    0.32751073519685653,
                    1.0,
                    0.7333333333333333,
                    0.42941057322260484
                ]
            ],
            "rate_limited": false,
            "answers": [
                "christopher cross",
                "john mellencamp",
                "bruce hornsby"
            ],
            "ml_answers": {
                "john mellencamp": 0.27621837798004684,
                "bruce hornsby": 0.6266184248408294,
                "christopher cross": 0.28003656492608864
            },
            "z-best_answer_by_ml": [
                "bruce hornsby"
            ],
            "data": {
                "wikipedia_search": [
                    1.4285438962452386,
                    1.849450541145172,
                    0.7220055626095895
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    7.0
                ],
                "word_relation_to_question": [
                    3.324753078314659,
                    0.7101825105042017,
                    1.965064411181139
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    5.0
                ],
                "word_count_appended": [
                    12.0,
                    12.0,
                    66.0
                ],
                "result_count": [
                    77.0,
                    2740.0,
                    2120.0
                ]
            },
            "integer_answers": {
                "john mellencamp": 2,
                "bruce hornsby": 3,
                "christopher cross": 1
            }
        },
        "lines": [
            [
                0,
                0.35713597406130965,
                0.0,
                0.5541255130524432,
                0.0,
                0.13333333333333333,
                0.015596516102896496
            ],
            [
                0,
                0.462362635286293,
                0.0,
                0.11836375175070028,
                0.0,
                0.13333333333333333,
                0.5549929106744986
            ],
            [
                1,
                0.18050139065239737,
                1.0,
                0.32751073519685653,
                1.0,
                0.7333333333333333,
                0.42941057322260484
            ]
        ]
    },
    "Which of these countries is closest to the International Date Line?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which of these countries is closest to the International Date Line?",
            "fraction_answers": {
                "brazil": 0.2660784757920751,
                "japan": 0.33475922732760804,
                "spain": 0.3991622968803168
            },
            "lines": [
                [
                    0.32236730863275115,
                    0.3333333333333333,
                    0.3041536171364497,
                    0.25,
                    0.310107948969578,
                    0.48859315589353614
                ],
                [
                    0.3777416673453997,
                    0.0,
                    0.32880038223385866,
                    0.25,
                    0.3395485770363101,
                    0.30038022813688214
                ],
                [
                    0.29989102402184914,
                    0.6666666666666666,
                    0.3670460006296916,
                    0.5,
                    0.35034347399411186,
                    0.21102661596958175
                ]
            ],
            "rate_limited": false,
            "answers": [
                "japan",
                "brazil",
                "spain"
            ],
            "ml_answers": {
                "brazil": 0.28059534110087464,
                "japan": 0.19049210937753977,
                "spain": 0.31158987351957373
            },
            "z-best_answer_by_ml": [
                "spain"
            ],
            "data": {
                "wikipedia_search": [
                    1.2894692345310046,
                    1.5109666693815988,
                    1.1995640960873966
                ],
                "word_count_entities": [
                    1.0,
                    0.0,
                    2.0
                ],
                "word_relation_to_question": [
                    1.216614468545799,
                    1.3152015289354346,
                    1.4681840025187665
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    2.0
                ],
                "word_count_appended": [
                    316.0,
                    346.0,
                    357.0
                ],
                "result_count": [
                    7710000.0,
                    4740000.0,
                    3330000.0
                ]
            },
            "integer_answers": {
                "brazil": 1,
                "japan": 1,
                "spain": 4
            }
        },
        "lines": [
            [
                1,
                0.32236730863275115,
                0.3333333333333333,
                0.3041536171364497,
                0.25,
                0.310107948969578,
                0.48859315589353614
            ],
            [
                0,
                0.3777416673453997,
                0.0,
                0.32880038223385866,
                0.25,
                0.3395485770363101,
                0.30038022813688214
            ],
            [
                0,
                0.29989102402184914,
                0.6666666666666666,
                0.3670460006296916,
                0.5,
                0.35034347399411186,
                0.21102661596958175
            ]
        ]
    },
    "By definition, an Anglophile would be most interested in which of these things?": {
        "raw_data": {
            "negative_question": false,
            "question": "By definition, an Anglophile would be most interested in which of these things?",
            "fraction_answers": {
                "geometry": 0.4473825926105486,
                "downton abbey": 0.5436135952443633,
                "trout fishing": 0.0090038121450882
            },
            "lines": [
                [
                    0.26666666666666666,
                    0,
                    0.39999999999999997,
                    0,
                    0.6946902654867256,
                    0.42817343828880206
                ],
                [
                    0.7333333333333334,
                    0,
                    0.6,
                    0,
                    0.26991150442477874,
                    0.5712095432193407
                ],
                [
                    0.0,
                    0,
                    0.0,
                    0,
                    0.035398230088495575,
                    0.0006170184918572256
                ]
            ],
            "rate_limited": false,
            "answers": [
                "geometry",
                "downton abbey",
                "trout fishing"
            ],
            "ml_answers": {
                "geometry": 0.24652257125733784,
                "downton abbey": 0.27741011218257916,
                "trout fishing": 0.1667231111061148
            },
            "z-best_answer_by_ml": [
                "downton abbey"
            ],
            "data": {
                "wikipedia_search": [
                    0.8,
                    2.2,
                    0.0
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.2,
                    1.8,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    157.0,
                    61.0,
                    8.0
                ],
                "result_count": [
                    45800.0,
                    61100.0,
                    66.0
                ]
            },
            "integer_answers": {
                "geometry": 1,
                "downton abbey": 3,
                "trout fishing": 0
            }
        },
        "lines": [
            [
                0,
                0.26666666666666666,
                0,
                0.39999999999999997,
                0,
                0.6946902654867256,
                0.42817343828880206
            ],
            [
                1,
                0.7333333333333334,
                0,
                0.6,
                0,
                0.26991150442477874,
                0.5712095432193407
            ],
            [
                0,
                0.0,
                0,
                0.0,
                0,
                0.035398230088495575,
                0.0006170184918572256
            ]
        ]
    },
    "Who is NOT considered an official member of the Eagles?": {
        "raw_data": {
            "negative_question": true,
            "question": "Who is NOT considered an official member of the Eagles?",
            "fraction_answers": {
                "j.d. souther": 0.163090156534774,
                "randy meisner": 0.4492724195370131,
                "bernie leadon": 0.387637423928213
            },
            "lines": [
                [
                    0.3856070087609512,
                    0.5,
                    0.3260053327823813,
                    0.5,
                    0.3679245283018868,
                    0.4311926605504587
                ],
                [
                    0.2327385422254008,
                    0.2,
                    0.36402616212011196,
                    0.25,
                    0.33018867924528306,
                    0.27522935779816515
                ],
                [
                    0.38165444901364803,
                    0.3,
                    0.30996850509750673,
                    0.25,
                    0.3018867924528302,
                    0.29357798165137616
                ]
            ],
            "rate_limited": false,
            "answers": [
                "j.d. souther",
                "randy meisner",
                "bernie leadon"
            ],
            "ml_answers": {
                "j.d. souther": 0.42010270121795984,
                "randy meisner": 0.1460436674530581,
                "bernie leadon": 0.29350443971809476
            },
            "integer_answers": {
                "j.d. souther": 0,
                "randy meisner": 4,
                "bernie leadon": 2
            },
            "data": {
                "wikipedia_search": [
                    1.143929912390488,
                    2.6726145777459918,
                    1.1834555098635198
                ],
                "word_count_entities": [
                    0.0,
                    3.0,
                    2.0
                ],
                "word_relation_to_question": [
                    2.087936006611424,
                    1.6316860545586567,
                    2.2803779388299192
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    1.0
                ],
                "word_count_appended": [
                    15.0,
                    49.0,
                    45.0
                ],
                "result_count": [
                    14.0,
                    18.0,
                    21.0
                ]
            },
            "z-best_answer_by_ml": [
                "j.d. souther"
            ]
        },
        "lines": [
            [
                1,
                0.3856070087609512,
                0.5,
                0.3260053327823813,
                0.5,
                0.3679245283018868,
                0.4311926605504587
            ],
            [
                0,
                0.2327385422254008,
                0.2,
                0.36402616212011196,
                0.25,
                0.33018867924528306,
                0.27522935779816515
            ],
            [
                0,
                0.38165444901364803,
                0.3,
                0.30996850509750673,
                0.25,
                0.3018867924528302,
                0.29357798165137616
            ]
        ]
    },
    "Queen Victoria is credited with starting what fashion trend?": {
        "raw_data": {
            "negative_question": false,
            "question": "Queen Victoria is credited with starting what fashion trend?",
            "fraction_answers": {
                "white wedding dress": 0.5764273803429882,
                "mini dress": 0.2603714635838048,
                "little black dress": 0.1632011560732069
            },
            "lines": [
                [
                    0.5936507936507937,
                    0,
                    0.13751686909581645,
                    0.0,
                    0.2,
                    0.3706896551724138
                ],
                [
                    0.11904761904761905,
                    0,
                    0.131769327164064,
                    0.0,
                    0.3238095238095238,
                    0.2413793103448276
                ],
                [
                    0.2873015873015873,
                    0,
                    0.7307138037401195,
                    1.0,
                    0.47619047619047616,
                    0.3879310344827586
                ]
            ],
            "rate_limited": false,
            "answers": [
                "mini dress",
                "little black dress",
                "white wedding dress"
            ],
            "ml_answers": {
                "white wedding dress": 0.5622267013828356,
                "mini dress": 0.18244710978971238,
                "little black dress": 0.17733910304162803
            },
            "z-best_answer_by_ml": [
                "white wedding dress"
            ],
            "data": {
                "wikipedia_search": [
                    2.3746031746031746,
                    0.4761904761904762,
                    1.1492063492063491
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.6875843454790822,
                    0.65884663582032,
                    3.6535690187005976
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    11.0
                ],
                "word_count_appended": [
                    21.0,
                    34.0,
                    50.0
                ],
                "result_count": [
                    43.0,
                    28.0,
                    45.0
                ]
            },
            "integer_answers": {
                "white wedding dress": 4,
                "mini dress": 1,
                "little black dress": 0
            }
        },
        "lines": [
            [
                0,
                0.5936507936507937,
                0,
                0.13751686909581645,
                0.0,
                0.2,
                0.3706896551724138
            ],
            [
                0,
                0.11904761904761905,
                0,
                0.131769327164064,
                0.0,
                0.3238095238095238,
                0.2413793103448276
            ],
            [
                1,
                0.2873015873015873,
                0,
                0.7307138037401195,
                1.0,
                0.47619047619047616,
                0.3879310344827586
            ]
        ]
    },
    "How do you let someone on Tinder know you're interested?": {
        "raw_data": {
            "negative_question": false,
            "question": "How do you let someone on Tinder know you're interested?",
            "fraction_answers": {
                "draw circle around face": 0.05627858543095552,
                "shake phone": 0.15040931553253215,
                "swipe right": 0.7933120990365122
            },
            "lines": [
                [
                    0.47840668750202936,
                    1.0,
                    0.4271121951860071,
                    1.0,
                    0.9985844807618067,
                    0.8557692307692307
                ],
                [
                    0.029962411777542358,
                    0.0,
                    0.27886294696203695,
                    0.0,
                    0.0,
                    0.028846153846153848
                ],
                [
                    0.4916309007204283,
                    0.0,
                    0.2940248578519559,
                    0.0,
                    0.0014155192381932827,
                    0.11538461538461539
                ]
            ],
            "rate_limited": false,
            "answers": [
                "swipe right",
                "draw circle around face",
                "shake phone"
            ],
            "ml_answers": {
                "draw circle around face": 0.17698952331005768,
                "shake phone": 0.22794909338374655,
                "swipe right": 0.6910064612877052
            },
            "z-best_answer_by_ml": [
                "swipe right"
            ],
            "data": {
                "wikipedia_search": [
                    2.870440125012176,
                    0.17977447066525415,
                    2.94978540432257
                ],
                "word_count_entities": [
                    9.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.7084487807440285,
                    1.1154517878481478,
                    1.1760994314078237
                ],
                "word_count_raw": [
                    8.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    89.0,
                    3.0,
                    12.0
                ],
                "result_count": [
                    38800.0,
                    0,
                    55.0
                ]
            },
            "integer_answers": {
                "draw circle around face": 0,
                "shake phone": 1,
                "swipe right": 5
            }
        },
        "lines": [
            [
                1,
                0.47840668750202936,
                1.0,
                0.4271121951860071,
                1.0,
                0.9985844807618067,
                0.8557692307692307
            ],
            [
                0,
                0.029962411777542358,
                0.0,
                0.27886294696203695,
                0.0,
                0.0,
                0.028846153846153848
            ],
            [
                0,
                0.4916309007204283,
                0.0,
                0.2940248578519559,
                0.0,
                0.0014155192381932827,
                0.11538461538461539
            ]
        ]
    },
    "Which of these sharks is NOT a Lamniforme?": {
        "raw_data": {
            "negative_question": true,
            "question": "Which of these sharks is NOT a Lamniforme?",
            "fraction_answers": {
                "goblin shark": 0.2403258883150422,
                "great white shark": 0.5986802889208328,
                "hammerhead shark": 0.16099382276412497
            },
            "lines": [
                [
                    0.4302948619210034,
                    0,
                    0.32740033072371655,
                    0,
                    0.2909090909090909,
                    0.47074393981610474
                ],
                [
                    0.16195951149741444,
                    0,
                    0.28518784073586023,
                    0,
                    0.3012987012987013,
                    0.05419336862635832
                ],
                [
                    0.40774562658158214,
                    0,
                    0.3874118285404232,
                    0,
                    0.4077922077922078,
                    0.47506269155753694
                ]
            ],
            "rate_limited": false,
            "answers": [
                "goblin shark",
                "great white shark",
                "hammerhead shark"
            ],
            "ml_answers": {
                "goblin shark": 0.27184805304819354,
                "great white shark": 0.23711057540790867,
                "hammerhead shark": 0.280618516571364
            },
            "z-best_answer_by_ml": [
                "hammerhead shark"
            ],
            "data": {
                "wikipedia_search": [
                    0.27882055231598635,
                    1.3521619540103422,
                    0.3690174936736715
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.035598015657701,
                    1.2888729555848384,
                    0.6755290287574607
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    161.0,
                    153.0,
                    71.0
                ],
                "result_count": [
                    105000.0,
                    1600000.0,
                    89500.0
                ]
            },
            "integer_answers": {
                "goblin shark": 1,
                "great white shark": 3,
                "hammerhead shark": 0
            }
        },
        "lines": [
            [
                0,
                0.4302948619210034,
                0,
                0.32740033072371655,
                0,
                0.2909090909090909,
                0.47074393981610474
            ],
            [
                0,
                0.16195951149741444,
                0,
                0.28518784073586023,
                0,
                0.3012987012987013,
                0.05419336862635832
            ],
            [
                1,
                0.40774562658158214,
                0,
                0.3874118285404232,
                0,
                0.4077922077922078,
                0.47506269155753694
            ]
        ]
    },
    "Which of these restaurant brands has its original location in Europe?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which of these restaurant brands has its original location in Europe?",
            "fraction_answers": {
                "mr. chow": 0.20136362328100949,
                "benihana": 0.5037141095027673,
                "p.f. chang's": 0.29492226721622317
            },
            "lines": [
                [
                    0.6653508771929825,
                    0,
                    0.19002525252525254,
                    0,
                    0.9106699751861043,
                    0.24881033310673012
                ],
                [
                    0.2655701754385965,
                    0,
                    0.39749729437229436,
                    0,
                    0.007444168734491315,
                    0.5091774303195106
                ],
                [
                    0.06907894736842105,
                    0,
                    0.4124774531024531,
                    0,
                    0.08188585607940446,
                    0.24201223657375934
                ]
            ],
            "rate_limited": false,
            "answers": [
                "benihana",
                "p.f. chang's",
                "mr. chow"
            ],
            "ml_answers": {
                "mr. chow": 0.19308944961380095,
                "benihana": 0.29447310318956915,
                "p.f. chang's": 0.21343074803005602
            },
            "integer_answers": {
                "mr. chow": 1,
                "benihana": 2,
                "p.f. chang's": 1
            },
            "data": {
                "wikipedia_search": [
                    2.66140350877193,
                    1.062280701754386,
                    0.2763157894736842
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.7601010101010102,
                    1.5899891774891775,
                    1.6499098124098124
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    367.0,
                    3.0,
                    33.0
                ],
                "result_count": [
                    36600.0,
                    74900.0,
                    35600.0
                ]
            },
            "z-best_answer_by_ml": [
                "benihana"
            ]
        },
        "lines": [
            [
                0,
                0.6653508771929825,
                0,
                0.19002525252525254,
                0,
                0.9106699751861043,
                0.24881033310673012
            ],
            [
                0,
                0.2655701754385965,
                0,
                0.39749729437229436,
                0,
                0.007444168734491315,
                0.5091774303195106
            ],
            [
                1,
                0.06907894736842105,
                0,
                0.4124774531024531,
                0,
                0.08188585607940446,
                0.24201223657375934
            ]
        ]
    },
    "In which of these movies is the title NOT spoken by any character?": {
        "raw_data": {
            "negative_question": true,
            "question": "In which of these movies is the title NOT spoken by any character?",
            "fraction_answers": {
                "inception": 0.42951478525663794,
                "speed": 0.447458024680889,
                "gravity": 0.12302719006247302
            },
            "lines": [
                [
                    0.1344883691284378,
                    0.5,
                    0.19879679197163858,
                    0.5,
                    0.07202932822625163,
                    0.30614115490375804
                ],
                [
                    0.39951875848944185,
                    0.5,
                    0.40280278087026555,
                    0.5,
                    0.4999992735876123,
                    0.32859761686526123
                ],
                [
                    0.4659928723821203,
                    0.0,
                    0.39840042715809587,
                    0.0,
                    0.427971398186136,
                    0.36526122823098073
                ]
            ],
            "rate_limited": false,
            "answers": [
                "inception",
                "gravity",
                "speed"
            ],
            "ml_answers": {
                "inception": 0.38437540403623277,
                "speed": 0.29669878705418185,
                "gravity": 0.37050427173893524
            },
            "z-best_answer_by_ml": [
                "inception"
            ],
            "data": {
                "wikipedia_search": [
                    2.924093046972497,
                    0.8038499320844649,
                    0.27205702094303785
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_relation_to_question": [
                    2.4096256642268914,
                    0.7775777530378758,
                    0.812796582735233
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_count_appended": [
                    423.0,
                    374.0,
                    294.0
                ],
                "result_count": [
                    48900000.0,
                    83.0,
                    8230000.0
                ]
            },
            "integer_answers": {
                "inception": 4,
                "speed": 2,
                "gravity": 0
            }
        },
        "lines": [
            [
                0,
                0.1344883691284378,
                0.5,
                0.19879679197163858,
                0.5,
                0.07202932822625163,
                0.30614115490375804
            ],
            [
                1,
                0.39951875848944185,
                0.5,
                0.40280278087026555,
                0.5,
                0.4999992735876123,
                0.32859761686526123
            ],
            [
                0,
                0.4659928723821203,
                0.0,
                0.39840042715809587,
                0.0,
                0.427971398186136,
                0.36526122823098073
            ]
        ]
    },
    "Tom from MySpace shares his name with a key character in what film franchise?": {
        "raw_data": {
            "negative_question": false,
            "question": "Tom from MySpace shares his name with a key character in what film franchise?",
            "fraction_answers": {
                "the godfather": 0.15944645943482602,
                "the matrix": 0.6217176550710356,
                "harry potter": 0.2188358854941383
            },
            "lines": [
                [
                    0.23463153875525009,
                    0,
                    0.3869722751549629,
                    0.16666666666666666,
                    0.3055555555555556,
                    0.00035339133825644026
                ],
                [
                    0.2946366297569047,
                    0,
                    0.3308972361498098,
                    0.0,
                    0.1712962962962963,
                    0.0004021349711193975
                ],
                [
                    0.4707318314878452,
                    0,
                    0.2821304886952272,
                    0.8333333333333334,
                    0.5231481481481481,
                    0.9992444736906242
                ]
            ],
            "rate_limited": false,
            "answers": [
                "harry potter",
                "the godfather",
                "the matrix"
            ],
            "ml_answers": {
                "the godfather": 0.1400079161291328,
                "the matrix": 0.5992545076552436,
                "harry potter": 0.20506445869695153
            },
            "z-best_answer_by_ml": [
                "the matrix"
            ],
            "data": {
                "wikipedia_search": [
                    1.1731576937762505,
                    1.4731831487845235,
                    2.353659157439226
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    2.3218336509297774,
                    1.9853834168988589,
                    1.6927829321713632
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    5.0
                ],
                "word_count_appended": [
                    66.0,
                    37.0,
                    113.0
                ],
                "result_count": [
                    87.0,
                    99.0,
                    246000.0
                ]
            },
            "integer_answers": {
                "the godfather": 0,
                "the matrix": 4,
                "harry potter": 1
            }
        },
        "lines": [
            [
                0,
                0.23463153875525009,
                0,
                0.3869722751549629,
                0.16666666666666666,
                0.3055555555555556,
                0.00035339133825644026
            ],
            [
                0,
                0.2946366297569047,
                0,
                0.3308972361498098,
                0.0,
                0.1712962962962963,
                0.0004021349711193975
            ],
            [
                1,
                0.4707318314878452,
                0,
                0.2821304886952272,
                0.8333333333333334,
                0.5231481481481481,
                0.9992444736906242
            ]
        ]
    },
    "Which player won Rookie of the Year in their sport most recently?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which player won Rookie of the Year in their sport most recently?",
            "fraction_answers": {
                "mike trout": 0.5149444873101591,
                "von miller": 0.14237354016971224,
                "blake griffin": 0.3426819725201288
            },
            "lines": [
                [
                    0.39216705261113155,
                    0.5,
                    0.49512027647620865,
                    0,
                    0.3361581920903955,
                    0.8512769153730596
                ],
                [
                    0.16188290419375945,
                    0.0,
                    0.17626461185783224,
                    0,
                    0.3163841807909605,
                    0.057336004006009016
                ],
                [
                    0.445950043195109,
                    0.5,
                    0.32861511166595914,
                    0,
                    0.3474576271186441,
                    0.0913870806209314
                ]
            ],
            "rate_limited": false,
            "answers": [
                "mike trout",
                "von miller",
                "blake griffin"
            ],
            "ml_answers": {
                "mike trout": 0.3963199131205005,
                "von miller": 0.36361647504794226,
                "blake griffin": 0.3789975894679836
            },
            "integer_answers": {
                "mike trout": 3,
                "von miller": 0,
                "blake griffin": 2
            },
            "data": {
                "wikipedia_search": [
                    1.9608352630556578,
                    0.8094145209687973,
                    2.229750215975545
                ],
                "word_count_entities": [
                    1.0,
                    0.0,
                    1.0
                ],
                "word_relation_to_question": [
                    2.970721658857252,
                    1.0575876711469934,
                    1.9716906699957548
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    119.0,
                    112.0,
                    123.0
                ],
                "result_count": [
                    3400000.0,
                    229000.0,
                    365000.0
                ]
            },
            "z-best_answer_by_ml": [
                "mike trout"
            ]
        },
        "lines": [
            [
                1,
                0.39216705261113155,
                0.5,
                0.49512027647620865,
                0,
                0.3361581920903955,
                0.8512769153730596
            ],
            [
                0,
                0.16188290419375945,
                0.0,
                0.17626461185783224,
                0,
                0.3163841807909605,
                0.057336004006009016
            ],
            [
                0,
                0.445950043195109,
                0.5,
                0.32861511166595914,
                0,
                0.3474576271186441,
                0.0913870806209314
            ]
        ]
    },
    "In computing, what unit is half a byte?": {
        "raw_data": {
            "negative_question": false,
            "question": "In computing, what unit is half a byte?",
            "fraction_answers": {
                "demibyte": 0.04543702387091347,
                "nibble": 0.6841411065064972,
                "octet": 0.27042186962258935
            },
            "lines": [
                [
                    1.0,
                    0.8379888268156425,
                    0.3145609471828984,
                    0.863905325443787,
                    0.4252336448598131,
                    0.6631578947368421
                ],
                [
                    0.0,
                    0.0,
                    0.25390869293308316,
                    0.0,
                    0.0,
                    0.01871345029239766
                ],
                [
                    0.0,
                    0.16201117318435754,
                    0.43153035988401844,
                    0.13609467455621302,
                    0.5747663551401869,
                    0.31812865497076026
                ]
            ],
            "rate_limited": false,
            "answers": [
                "nibble",
                "demibyte",
                "octet"
            ],
            "ml_answers": {
                "demibyte": 0.19721289739422124,
                "nibble": 0.5990721603608282,
                "octet": 0.2582259845320383
            },
            "integer_answers": {
                "demibyte": 0,
                "nibble": 4,
                "octet": 2
            },
            "data": {
                "wikipedia_search": [
                    4.0,
                    0.0,
                    0.0
                ],
                "word_count_entities": [
                    150.0,
                    0.0,
                    29.0
                ],
                "word_relation_to_question": [
                    1.2582437887315936,
                    1.0156347717323326,
                    1.7261214395360738
                ],
                "word_count_raw": [
                    146.0,
                    0.0,
                    23.0
                ],
                "word_count_appended": [
                    567.0,
                    16.0,
                    272.0
                ],
                "result_count": [
                    54600.0,
                    0,
                    73800.0
                ]
            },
            "z-best_answer_by_ml": [
                "nibble"
            ]
        },
        "lines": [
            [
                1,
                1.0,
                0.8379888268156425,
                0.3145609471828984,
                0.863905325443787,
                0.4252336448598131,
                0.6631578947368421
            ],
            [
                0,
                0.0,
                0.0,
                0.25390869293308316,
                0.0,
                0.0,
                0.01871345029239766
            ],
            [
                0,
                0.0,
                0.16201117318435754,
                0.43153035988401844,
                0.13609467455621302,
                0.5747663551401869,
                0.31812865497076026
            ]
        ]
    },
    "Which of these has NEVER been named Pantone\u2019s Color of the Year?": {
        "raw_data": {
            "negative_question": true,
            "question": "Which of these has NEVER been named Pantone\u2019s Color of the Year?",
            "fraction_answers": {
                "chili pepper": 0.24453020583834437,
                "cucumber": 0.48809978583191044,
                "sand dollar": 0.2673700083297453
            },
            "lines": [
                [
                    0.26118521293867863,
                    0,
                    0.44597750865051905,
                    0.33333333333333337,
                    0.17441860465116277,
                    0.06483587584653022
                ],
                [
                    0.4236090295991008,
                    0,
                    0.15415224913494807,
                    0.33333333333333337,
                    0.4205426356589147,
                    0.4999377314493399
                ],
                [
                    0.31520575746222057,
                    0,
                    0.3998702422145329,
                    0.33333333333333337,
                    0.4050387596899225,
                    0.4352263927041299
                ]
            ],
            "rate_limited": false,
            "answers": [
                "cucumber",
                "sand dollar",
                "chili pepper"
            ],
            "ml_answers": {
                "chili pepper": 0.3188406469869347,
                "cucumber": 0.22307511911707106,
                "sand dollar": 0.2662953688599972
            },
            "integer_answers": {
                "chili pepper": 0,
                "cucumber": 4,
                "sand dollar": 1
            },
            "data": {
                "wikipedia_search": [
                    2.3881478706132135,
                    0.763909704008992,
                    1.8479424253777945
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.4321799307958477,
                    2.766782006920415,
                    0.801038062283737
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    1.0
                ],
                "word_count_appended": [
                    168.0,
                    41.0,
                    49.0
                ],
                "result_count": [
                    608000.0,
                    87.0,
                    90500.0
                ]
            },
            "z-best_answer_by_ml": [
                "chili pepper"
            ]
        },
        "lines": [
            [
                1,
                0.26118521293867863,
                0,
                0.44597750865051905,
                0.33333333333333337,
                0.17441860465116277,
                0.06483587584653022
            ],
            [
                0,
                0.4236090295991008,
                0,
                0.15415224913494807,
                0.33333333333333337,
                0.4205426356589147,
                0.4999377314493399
            ],
            [
                0,
                0.31520575746222057,
                0,
                0.3998702422145329,
                0.33333333333333337,
                0.4050387596899225,
                0.4352263927041299
            ]
        ]
    },
    "What book that heavily influenced \u201cThe Matrix\u201d makes a cameo in the movie?": {
        "raw_data": {
            "negative_question": false,
            "question": "What book that heavily influenced \u201cThe Matrix\u201d makes a cameo in the movie?",
            "fraction_answers": {
                "simulacra & simulation": 0.056612392132487345,
                "neuromancer": 0.6905264891430062,
                "gravity's rainbow": 0.25286111872450645
            },
            "lines": [
                [
                    0.8084415584415584,
                    0,
                    0.6612429809059024,
                    1.0,
                    0.9787234042553191,
                    0.004224502112251056
                ],
                [
                    0.08311688311688312,
                    0,
                    0.1939100745280521,
                    0.0,
                    0.0,
                    0.006035003017501509
                ],
                [
                    0.10844155844155844,
                    0,
                    0.14484694456604572,
                    0.0,
                    0.02127659574468085,
                    0.9897404948702474
                ]
            ],
            "rate_limited": false,
            "answers": [
                "neuromancer",
                "simulacra & simulation",
                "gravity's rainbow"
            ],
            "ml_answers": {
                "simulacra & simulation": 0.3234650148478143,
                "neuromancer": 0.45418382655845285,
                "gravity's rainbow": 0.1920825470014288
            },
            "integer_answers": {
                "simulacra & simulation": 0,
                "neuromancer": 4,
                "gravity's rainbow": 1
            },
            "data": {
                "wikipedia_search": [
                    5.659090909090909,
                    0.5818181818181818,
                    0.759090909090909
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    3.9674578854354134,
                    1.1634604471683123,
                    0.8690816673962741
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    138.0,
                    0.0,
                    3.0
                ],
                "result_count": [
                    21.0,
                    30.0,
                    4920.0
                ]
            },
            "z-best_answer_by_ml": [
                "neuromancer"
            ]
        },
        "lines": [
            [
                0,
                0.8084415584415584,
                0,
                0.6612429809059024,
                1.0,
                0.9787234042553191,
                0.004224502112251056
            ],
            [
                1,
                0.08311688311688312,
                0,
                0.1939100745280521,
                0.0,
                0.0,
                0.006035003017501509
            ],
            [
                0,
                0.10844155844155844,
                0,
                0.14484694456604572,
                0.0,
                0.02127659574468085,
                0.9897404948702474
            ]
        ]
    },
    "Which of these things was created by a person who chose to remain anonymous?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which of these things was created by a person who chose to remain anonymous?",
            "fraction_answers": {
                "hoverboards": 0.09889546740377053,
                "fidget spinners": 0.10934571459817048,
                "bitcoin": 0.7917588179980591
            },
            "lines": [
                [
                    0.8944444444444445,
                    0,
                    0.3526785714285714,
                    1.0,
                    0.7799385875127943,
                    0.931732486604485
                ],
                [
                    0.09166666666666667,
                    0,
                    0.22321428571428573,
                    0.0,
                    0.15250767656090072,
                    0.027088708076999403
                ],
                [
                    0.013888888888888888,
                    0,
                    0.42410714285714285,
                    0.0,
                    0.06755373592630501,
                    0.04117880531851558
                ]
            ],
            "rate_limited": false,
            "answers": [
                "bitcoin",
                "hoverboards",
                "fidget spinners"
            ],
            "ml_answers": {
                "hoverboards": 0.25350634506916414,
                "fidget spinners": 0.1735978145186278,
                "bitcoin": 0.5769743338735984
            },
            "integer_answers": {
                "hoverboards": 0,
                "fidget spinners": 1,
                "bitcoin": 4
            },
            "data": {
                "wikipedia_search": [
                    3.577777777777778,
                    0.3666666666666667,
                    0.05555555555555555
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.4107142857142856,
                    0.8928571428571429,
                    1.6964285714285714
                ],
                "word_count_raw": [
                    21.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    762.0,
                    149.0,
                    66.0
                ],
                "result_count": [
                    939000.0,
                    27300.0,
                    41500.0
                ]
            },
            "z-best_answer_by_ml": [
                "bitcoin"
            ]
        },
        "lines": [
            [
                1,
                0.8944444444444445,
                0,
                0.3526785714285714,
                1.0,
                0.7799385875127943,
                0.931732486604485
            ],
            [
                0,
                0.09166666666666667,
                0,
                0.22321428571428573,
                0.0,
                0.15250767656090072,
                0.027088708076999403
            ],
            [
                0,
                0.013888888888888888,
                0,
                0.42410714285714285,
                0.0,
                0.06755373592630501,
                0.04117880531851558
            ]
        ]
    },
    "In which town were a President, Governor, Senator, NFL owner and late night host all born?": {
        "raw_data": {
            "negative_question": false,
            "question": "In which town were a President, Governor, Senator, NFL owner and late night host all born?",
            "fraction_answers": {
                "muncie, in": 0.3604394980027946,
                "hope, ar": 0.22818441397533226,
                "brookline, ma": 0.4113760880218731
            },
            "lines": [
                [
                    0.38285929032272115,
                    0,
                    0.2805478750640041,
                    0,
                    0.7647058823529411,
                    0.21739130434782608
                ],
                [
                    0.17637664562531583,
                    0,
                    0.27851254480286736,
                    0,
                    0.06654411764705882,
                    0.391304347826087
                ],
                [
                    0.44076406405196294,
                    0,
                    0.44093958013312856,
                    0,
                    0.16875,
                    0.391304347826087
                ]
            ],
            "rate_limited": false,
            "answers": [
                "brookline, ma",
                "hope, ar",
                "muncie, in"
            ],
            "ml_answers": {
                "muncie, in": 0.3283922182346341,
                "hope, ar": 0.1701033745676313,
                "brookline, ma": 0.31795401199198503
            },
            "integer_answers": {
                "muncie, in": 2,
                "hope, ar": 1,
                "brookline, ma": 1
            },
            "data": {
                "wikipedia_search": [
                    3.062874322581769,
                    1.4110131650025266,
                    3.5261125124157036
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    2.2443830005120327,
                    2.228100358422939,
                    3.5275166410650285
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    5.0,
                    9.0,
                    9.0
                ],
                "result_count": [
                    20800.0,
                    1810.0,
                    4590.0
                ]
            },
            "z-best_answer_by_ml": [
                "muncie, in"
            ]
        },
        "lines": [
            [
                1,
                0.38285929032272115,
                0,
                0.2805478750640041,
                0,
                0.7647058823529411,
                0.21739130434782608
            ],
            [
                0,
                0.17637664562531583,
                0,
                0.27851254480286736,
                0,
                0.06654411764705882,
                0.391304347826087
            ],
            [
                0,
                0.44076406405196294,
                0,
                0.44093958013312856,
                0,
                0.16875,
                0.391304347826087
            ]
        ]
    },
    "Though it now conveys something different, which of these words originally meant \u201cparrot\u201d?": {
        "raw_data": {
            "negative_question": false,
            "question": "Though it now conveys something different, which of these words originally meant \u201cparrot\u201d?",
            "fraction_answers": {
                "thespian": 0.1976025086216705,
                "popinjay": 0.6087153743642457,
                "warble": 0.19368211701408372
            },
            "lines": [
                [
                    0.0,
                    0,
                    0.15948275862068967,
                    0.0,
                    0.4755944931163955,
                    0.3333333333333333
                ],
                [
                    0.041666666666666664,
                    0,
                    0.2586206896551724,
                    0.0,
                    0.3316645807259074,
                    0.3560606060606061
                ],
                [
                    0.9583333333333334,
                    0,
                    0.5818965517241379,
                    1.0,
                    0.1927409261576971,
                    0.3106060606060606
                ]
            ],
            "rate_limited": false,
            "answers": [
                "warble",
                "thespian",
                "popinjay"
            ],
            "ml_answers": {
                "thespian": 0.2073797516351925,
                "popinjay": 0.48289046407148095,
                "warble": 0.2978415526099811
            },
            "integer_answers": {
                "thespian": 1,
                "popinjay": 3,
                "warble": 1
            },
            "data": {
                "wikipedia_search": [
                    0.0,
                    0.125,
                    2.875
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.6379310344827587,
                    1.0344827586206895,
                    2.3275862068965516
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_appended": [
                    132.0,
                    141.0,
                    123.0
                ],
                "result_count": [
                    15200.0,
                    10600.0,
                    6160.0
                ]
            },
            "z-best_answer_by_ml": [
                "popinjay"
            ]
        },
        "lines": [
            [
                0,
                0.0,
                0,
                0.15948275862068967,
                0.0,
                0.4755944931163955,
                0.3333333333333333
            ],
            [
                0,
                0.041666666666666664,
                0,
                0.2586206896551724,
                0.0,
                0.3316645807259074,
                0.3560606060606061
            ],
            [
                1,
                0.9583333333333334,
                0,
                0.5818965517241379,
                1.0,
                0.1927409261576971,
                0.3106060606060606
            ]
        ]
    },
    "What gargantuan fruit is the subject of a Roald Dahl children's book?": {
        "raw_data": {
            "negative_question": false,
            "question": "What gargantuan fruit is the subject of a Roald Dahl children's book?",
            "fraction_answers": {
                "loquat": 0.14915698948173545,
                "dragonfruit": 0.14328058079633657,
                "peach": 0.707562429721928
            },
            "lines": [
                [
                    0.34331205622893207,
                    0.0,
                    0.2623373631075813,
                    0.0,
                    0.25,
                    0.004034065441506051
                ],
                [
                    0.2671089435559062,
                    0.09090909090909091,
                    0.2560075140948055,
                    0.16666666666666666,
                    0.10887096774193548,
                    0.005378753922008068
                ],
                [
                    0.38957900021516173,
                    0.9090909090909091,
                    0.4816551227976132,
                    0.8333333333333334,
                    0.6411290322580645,
                    0.9905871806364859
                ]
            ],
            "rate_limited": false,
            "answers": [
                "dragonfruit",
                "loquat",
                "peach"
            ],
            "ml_answers": {
                "loquat": 0.18511357003637202,
                "dragonfruit": 0.24100891576041109,
                "peach": 0.631201878595311
            },
            "z-best_answer_by_ml": [
                "peach"
            ],
            "data": {
                "wikipedia_search": [
                    1.7165602811446603,
                    1.335544717779531,
                    1.9478950010758087
                ],
                "word_count_entities": [
                    0.0,
                    1.0,
                    10.0
                ],
                "word_relation_to_question": [
                    1.3116868155379067,
                    1.2800375704740274,
                    2.408275613988066
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    5.0
                ],
                "word_count_appended": [
                    62.0,
                    27.0,
                    159.0
                ],
                "result_count": [
                    9.0,
                    12.0,
                    2210.0
                ]
            },
            "integer_answers": {
                "loquat": 0,
                "dragonfruit": 0,
                "peach": 6
            }
        },
        "lines": [
            [
                0,
                0.34331205622893207,
                0.0,
                0.2623373631075813,
                0.0,
                0.25,
                0.004034065441506051
            ],
            [
                0,
                0.2671089435559062,
                0.09090909090909091,
                0.2560075140948055,
                0.16666666666666666,
                0.10887096774193548,
                0.005378753922008068
            ],
            [
                1,
                0.38957900021516173,
                0.9090909090909091,
                0.4816551227976132,
                0.8333333333333334,
                0.6411290322580645,
                0.9905871806364859
            ]
        ]
    },
    "The only person who owns more U.S. land than Ted Turner made his fortune in what business?": {
        "raw_data": {
            "negative_question": false,
            "question": "The only person who owns more U.S. land than Ted Turner made his fortune in what business?",
            "fraction_answers": {
                "pharmaceuticals": 0.379954584755536,
                "cable tv": 0.25914423088832084,
                "fast food": 0.36090118435614316
            },
            "lines": [
                [
                    0.33905134292231065,
                    0,
                    0.2306418219461698,
                    0,
                    0.865979381443299,
                    0.08414579271036449
                ],
                [
                    0.3952115626309175,
                    0,
                    0.4919944789510007,
                    0,
                    0.08247422680412371,
                    0.06689665516724164
                ],
                [
                    0.26573709444677185,
                    0,
                    0.27736369910282954,
                    0,
                    0.05154639175257732,
                    0.8489575521223939
                ]
            ],
            "rate_limited": false,
            "answers": [
                "pharmaceuticals",
                "cable tv",
                "fast food"
            ],
            "ml_answers": {
                "pharmaceuticals": 0.3610936707393102,
                "cable tv": 0.3043315266520145,
                "fast food": 0.15526089247199762
            },
            "z-best_answer_by_ml": [
                "pharmaceuticals"
            ],
            "data": {
                "wikipedia_search": [
                    1.6952567146115534,
                    1.9760578131545874,
                    1.3286854722338592
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.6144927536231886,
                    3.4439613526570048,
                    1.9415458937198067
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    168.0,
                    16.0,
                    10.0
                ],
                "result_count": [
                    561000.0,
                    446000.0,
                    5660000.0
                ]
            },
            "integer_answers": {
                "pharmaceuticals": 1,
                "cable tv": 2,
                "fast food": 1
            }
        },
        "lines": [
            [
                0,
                0.33905134292231065,
                0,
                0.2306418219461698,
                0,
                0.865979381443299,
                0.08414579271036449
            ],
            [
                1,
                0.3952115626309175,
                0,
                0.4919944789510007,
                0,
                0.08247422680412371,
                0.06689665516724164
            ],
            [
                0,
                0.26573709444677185,
                0,
                0.27736369910282954,
                0,
                0.05154639175257732,
                0.8489575521223939
            ]
        ]
    },
    "Which game is an example of combinatorics?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which game is an example of combinatorics?",
            "fraction_answers": {
                "sudoku": 0.38608316267510984,
                "risk": 0.43057957980017836,
                "crossword puzzles": 0.1833372575247118
            },
            "lines": [
                [
                    0.14516129032258066,
                    0,
                    0.3546511627906977,
                    0,
                    0.3107287449392713,
                    0.9117771211481638
                ],
                [
                    0.3544142614601019,
                    0,
                    0.27267441860465114,
                    0,
                    0.07692307692307693,
                    0.029337273111017307
                ],
                [
                    0.5004244482173175,
                    0,
                    0.37267441860465117,
                    0,
                    0.6123481781376519,
                    0.058885605740818914
                ]
            ],
            "rate_limited": false,
            "answers": [
                "risk",
                "crossword puzzles",
                "sudoku"
            ],
            "ml_answers": {
                "sudoku": 0.2668462470441413,
                "risk": 0.15176110049744035,
                "crossword puzzles": 0.2037866321450035
            },
            "integer_answers": {
                "sudoku": 3,
                "risk": 1,
                "crossword puzzles": 0
            },
            "data": {
                "wikipedia_search": [
                    0.2903225806451613,
                    0.7088285229202038,
                    1.000848896434635
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.7093023255813954,
                    0.5453488372093023,
                    0.7453488372093023
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    307.0,
                    76.0,
                    605.0
                ],
                "result_count": [
                    432000.0,
                    13900.0,
                    27900.0
                ]
            },
            "z-best_answer_by_ml": [
                "sudoku"
            ]
        },
        "lines": [
            [
                0,
                0.14516129032258066,
                0,
                0.3546511627906977,
                0,
                0.3107287449392713,
                0.9117771211481638
            ],
            [
                0,
                0.3544142614601019,
                0,
                0.27267441860465114,
                0,
                0.07692307692307693,
                0.029337273111017307
            ],
            [
                1,
                0.5004244482173175,
                0,
                0.37267441860465117,
                0,
                0.6123481781376519,
                0.058885605740818914
            ]
        ]
    },
    "What SportsCenter anchor shares their last name with Linus & Lucy from \u201cPeanuts\u201d?": {
        "raw_data": {
            "negative_question": false,
            "question": "What SportsCenter anchor shares their last name with Linus & Lucy from \u201cPeanuts\u201d?",
            "fraction_answers": {
                "jemele hill": 0.42552691791783237,
                "kenny mayne": 0.1830250697285963,
                "scott van pelt": 0.3914480123535712
            },
            "lines": [
                [
                    0.5222222222222223,
                    0,
                    0.27207903403360667,
                    0.5,
                    0.3333333333333333,
                    0.5
                ],
                [
                    0.13888888888888887,
                    0,
                    0.48501783954563393,
                    0.5,
                    0.3333333333333333,
                    0.5
                ],
                [
                    0.33888888888888885,
                    0,
                    0.24290312642075942,
                    0.0,
                    0.3333333333333333,
                    0.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "jemele hill",
                "scott van pelt",
                "kenny mayne"
            ],
            "ml_answers": {
                "jemele hill": 0.2268441016835667,
                "kenny mayne": 0.2664014824219558,
                "scott van pelt": 0.24324688320634777
            },
            "integer_answers": {
                "jemele hill": 4,
                "kenny mayne": 0,
                "scott van pelt": 1
            },
            "data": {
                "wikipedia_search": [
                    1.5666666666666667,
                    0.41666666666666663,
                    1.0166666666666666
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.0883161361344267,
                    1.9400713581825357,
                    0.9716125056830377
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    0.0
                ],
                "word_count_appended": [
                    1.0,
                    1.0,
                    0.0
                ],
                "result_count": [
                    11.0,
                    11.0,
                    11.0
                ]
            },
            "z-best_answer_by_ml": [
                "kenny mayne"
            ]
        },
        "lines": [
            [
                0,
                0.5222222222222223,
                0,
                0.27207903403360667,
                0.5,
                0.3333333333333333,
                0.5
            ],
            [
                1,
                0.13888888888888887,
                0,
                0.48501783954563393,
                0.5,
                0.3333333333333333,
                0.5
            ],
            [
                0,
                0.33888888888888885,
                0,
                0.24290312642075942,
                0.0,
                0.3333333333333333,
                0.0
            ]
        ]
    },
    "According to the old saying, what kind of animal can NOT change its spots?": {
        "raw_data": {
            "negative_question": true,
            "question": "According to the old saying, what kind of animal can NOT change its spots?",
            "fraction_answers": {
                "tiger": 0.3965508632343957,
                "leopard": 0.4691469019146263,
                "zebra": 0.134302234850978
            },
            "lines": [
                [
                    0.4819903327066832,
                    0.46946564885496184,
                    0.36795634920634923,
                    0.4318181818181818,
                    0.34586815227483747,
                    0.4999946305860525
                ],
                [
                    0.34403857088142176,
                    0.04580152671755727,
                    0.283234126984127,
                    0.08522727272727271,
                    0.3342618384401114,
                    0.4999959585056309
                ],
                [
                    0.17397109641189512,
                    0.4847328244274809,
                    0.3488095238095238,
                    0.48295454545454547,
                    0.31987000928505105,
                    9.410908316631605e-06
                ]
            ],
            "rate_limited": false,
            "answers": [
                "zebra",
                "leopard",
                "tiger"
            ],
            "ml_answers": {
                "tiger": 0.344818776117753,
                "leopard": 0.21459774843304907,
                "zebra": 0.3731169384689152
            },
            "integer_answers": {
                "tiger": 3,
                "leopard": 3,
                "zebra": 0
            },
            "data": {
                "wikipedia_search": [
                    0.25213534210643584,
                    2.183460007660096,
                    4.5644046502334685
                ],
                "word_count_entities": [
                    8.0,
                    119.0,
                    4.0
                ],
                "word_relation_to_question": [
                    1.848611111111111,
                    3.0347222222222223,
                    2.1166666666666667
                ],
                "word_count_raw": [
                    12.0,
                    73.0,
                    3.0
                ],
                "word_count_appended": [
                    332.0,
                    357.0,
                    388.0
                ],
                "result_count": [
                    93.0,
                    70.0,
                    8660000.0
                ]
            },
            "z-best_answer_by_ml": [
                "zebra"
            ]
        },
        "lines": [
            [
                0,
                0.4819903327066832,
                0.46946564885496184,
                0.36795634920634923,
                0.4318181818181818,
                0.34586815227483747,
                0.4999946305860525
            ],
            [
                1,
                0.34403857088142176,
                0.04580152671755727,
                0.283234126984127,
                0.08522727272727271,
                0.3342618384401114,
                0.4999959585056309
            ],
            [
                0,
                0.17397109641189512,
                0.4847328244274809,
                0.3488095238095238,
                0.48295454545454547,
                0.31987000928505105,
                9.410908316631605e-06
            ]
        ]
    },
    "Who defeated Napoleon at the Battle of Waterloo?": {
        "raw_data": {
            "negative_question": false,
            "question": "Who defeated Napoleon at the Battle of Waterloo?",
            "fraction_answers": {
                "jack skellington": 0.05999541664838487,
                "the duke of wellington": 0.8274208782480844,
                "beef wellington": 0.11258370510353062
            },
            "lines": [
                [
                    0.06636363636363636,
                    0.0,
                    0.045,
                    0.0,
                    0.13135593220338984,
                    0.11725293132328309
                ],
                [
                    0.9245454545454544,
                    1.0,
                    0.8298188405797102,
                    1.0,
                    0.423728813559322,
                    0.7864321608040201
                ],
                [
                    0.009090909090909089,
                    0.0,
                    0.12518115942028984,
                    0.0,
                    0.4449152542372881,
                    0.09631490787269682
                ]
            ],
            "rate_limited": false,
            "answers": [
                "jack skellington",
                "the duke of wellington",
                "beef wellington"
            ],
            "ml_answers": {
                "jack skellington": 0.13380749696499303,
                "the duke of wellington": 0.5918236578217569,
                "beef wellington": 0.27201234294603027
            },
            "integer_answers": {
                "jack skellington": 0,
                "the duke of wellington": 5,
                "beef wellington": 1
            },
            "data": {
                "wikipedia_search": [
                    0.33181818181818185,
                    4.622727272727273,
                    0.045454545454545456
                ],
                "word_count_entities": [
                    0.0,
                    14.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.225,
                    4.149094202898551,
                    0.6259057971014492
                ],
                "word_count_raw": [
                    0.0,
                    14.0,
                    0.0
                ],
                "word_count_appended": [
                    31.0,
                    100.0,
                    105.0
                ],
                "result_count": [
                    14000.0,
                    93900.0,
                    11500.0
                ]
            },
            "z-best_answer_by_ml": [
                "the duke of wellington"
            ]
        },
        "lines": [
            [
                0,
                0.06636363636363636,
                0.0,
                0.045,
                0.0,
                0.13135593220338984,
                0.11725293132328309
            ],
            [
                1,
                0.9245454545454544,
                1.0,
                0.8298188405797102,
                1.0,
                0.423728813559322,
                0.7864321608040201
            ],
            [
                0,
                0.009090909090909089,
                0.0,
                0.12518115942028984,
                0.0,
                0.4449152542372881,
                0.09631490787269682
            ]
        ]
    },
    "What generation of the iPod was the first to offer video?": {
        "raw_data": {
            "negative_question": false,
            "question": "What generation of the iPod was the first to offer video?",
            "fraction_answers": {
                "third generation": 0.23339133176854565,
                "u2 special edition": 0.6375096903926918,
                "fifth generation": 0.12909897783876242
            },
            "lines": [
                [
                    0.375,
                    0.0,
                    0.23177392489470466,
                    0.0,
                    0.5377601122281973,
                    0.2558139534883721
                ],
                [
                    0.625,
                    1.0,
                    0.4706768111687835,
                    1.0,
                    0.18868365676876314,
                    0.5406976744186046
                ],
                [
                    0.0,
                    0.0,
                    0.29754926393651177,
                    0.0,
                    0.2735562310030395,
                    0.20348837209302326
                ]
            ],
            "rate_limited": false,
            "answers": [
                "third generation",
                "u2 special edition",
                "fifth generation"
            ],
            "ml_answers": {
                "third generation": 0.3125110305906573,
                "u2 special edition": 0.6549227607822721,
                "fifth generation": 0.20761479035985064
            },
            "z-best_answer_by_ml": [
                "u2 special edition"
            ],
            "data": {
                "wikipedia_search": [
                    1.5,
                    2.5,
                    0.0
                ],
                "word_count_entities": [
                    0.0,
                    2.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.1588696244735233,
                    2.3533840558439176,
                    1.4877463196825589
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_appended": [
                    44.0,
                    93.0,
                    35.0
                ],
                "result_count": [
                    230000.0,
                    80700.0,
                    117000.0
                ]
            },
            "integer_answers": {
                "third generation": 1,
                "u2 special edition": 5,
                "fifth generation": 0
            }
        },
        "lines": [
            [
                0,
                0.375,
                0.0,
                0.23177392489470466,
                0.0,
                0.5377601122281973,
                0.2558139534883721
            ],
            [
                0,
                0.625,
                1.0,
                0.4706768111687835,
                1.0,
                0.18868365676876314,
                0.5406976744186046
            ],
            [
                1,
                0.0,
                0.0,
                0.29754926393651177,
                0.0,
                0.2735562310030395,
                0.20348837209302326
            ]
        ]
    },
    "Which of these is NOT a geometric shape?": {
        "raw_data": {
            "negative_question": true,
            "question": "Which of these is NOT a geometric shape?",
            "fraction_answers": {
                "hexagon": 0.5912216430212389,
                "octagon": 0.3172360675144659,
                "tarragon": 0.09154228946429509
            },
            "lines": [
                [
                    0.32568493150684935,
                    0.3,
                    0.2630952380952381,
                    0.4166666666666667,
                    0.32706766917293234,
                    0.41577729201491576
                ],
                [
                    0.4009132420091325,
                    0.5,
                    0.45238095238095244,
                    0.5,
                    0.3834586466165414,
                    0.4886202906004886
                ],
                [
                    0.2734018264840183,
                    0.2,
                    0.28452380952380957,
                    0.08333333333333331,
                    0.2894736842105263,
                    0.09560241738459563
                ]
            ],
            "rate_limited": false,
            "answers": [
                "octagon",
                "tarragon",
                "hexagon"
            ],
            "ml_answers": {
                "hexagon": 0.21572615373313647,
                "octagon": 0.2381059122411984,
                "tarragon": 0.3814933804920651
            },
            "integer_answers": {
                "hexagon": 5,
                "octagon": 1,
                "tarragon": 0
            },
            "data": {
                "wikipedia_search": [
                    0.6972602739726027,
                    0.3963470319634703,
                    0.9063926940639269
                ],
                "word_count_entities": [
                    2.0,
                    0.0,
                    3.0
                ],
                "word_relation_to_question": [
                    1.4214285714285713,
                    0.2857142857142857,
                    1.2928571428571427
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    5.0
                ],
                "word_count_appended": [
                    368.0,
                    248.0,
                    448.0
                ],
                "result_count": [
                    1310000.0,
                    177000.0,
                    6290000.0
                ]
            },
            "z-best_answer_by_ml": [
                "tarragon"
            ]
        },
        "lines": [
            [
                0,
                0.32568493150684935,
                0.3,
                0.2630952380952381,
                0.4166666666666667,
                0.32706766917293234,
                0.41577729201491576
            ],
            [
                1,
                0.4009132420091325,
                0.5,
                0.45238095238095244,
                0.5,
                0.3834586466165414,
                0.4886202906004886
            ],
            [
                0,
                0.2734018264840183,
                0.2,
                0.28452380952380957,
                0.08333333333333331,
                0.2894736842105263,
                0.09560241738459563
            ]
        ]
    },
    "What tech mogul became a billionaire the youngest?": {
        "raw_data": {
            "negative_question": false,
            "question": "What tech mogul became a billionaire the youngest?",
            "fraction_answers": {
                "mark zuckerberg": 0.5479768176709945,
                "evan spiegel": 0.28737238210400123,
                "larry page": 0.16465080022500422
            },
            "lines": [
                [
                    0.3331207482993197,
                    0.0,
                    0.5165312107247592,
                    0.375,
                    0.4049586776859504,
                    0.09462365591397849
                ],
                [
                    0.41326530612244894,
                    1.0,
                    0.2758441558441559,
                    0.5833333333333334,
                    0.359504132231405,
                    0.6559139784946236
                ],
                [
                    0.2536139455782313,
                    0.0,
                    0.2076246334310851,
                    0.041666666666666664,
                    0.23553719008264462,
                    0.24946236559139784
                ]
            ],
            "rate_limited": false,
            "answers": [
                "evan spiegel",
                "mark zuckerberg",
                "larry page"
            ],
            "ml_answers": {
                "mark zuckerberg": 0.47325538124713806,
                "evan spiegel": 0.34796196949856056,
                "larry page": 0.28984273931269455
            },
            "integer_answers": {
                "mark zuckerberg": 4,
                "evan spiegel": 2,
                "larry page": 0
            },
            "data": {
                "wikipedia_search": [
                    1.3324829931972788,
                    1.6530612244897958,
                    1.0144557823129252
                ],
                "word_count_entities": [
                    0.0,
                    12.0,
                    0.0
                ],
                "word_relation_to_question": [
                    2.5826560536237957,
                    1.3792207792207793,
                    1.0381231671554254
                ],
                "word_count_raw": [
                    9.0,
                    14.0,
                    1.0
                ],
                "word_count_appended": [
                    98.0,
                    87.0,
                    57.0
                ],
                "result_count": [
                    17600.0,
                    122000.0,
                    46400.0
                ]
            },
            "z-best_answer_by_ml": [
                "mark zuckerberg"
            ]
        },
        "lines": [
            [
                0,
                0.3331207482993197,
                0.0,
                0.5165312107247592,
                0.375,
                0.4049586776859504,
                0.09462365591397849
            ],
            [
                1,
                0.41326530612244894,
                1.0,
                0.2758441558441559,
                0.5833333333333334,
                0.359504132231405,
                0.6559139784946236
            ],
            [
                0,
                0.2536139455782313,
                0.0,
                0.2076246334310851,
                0.041666666666666664,
                0.23553719008264462,
                0.24946236559139784
            ]
        ]
    },
    "Which of these is NOT a marsupial?": {
        "raw_data": {
            "negative_question": true,
            "question": "Which of these is NOT a marsupial?",
            "fraction_answers": {
                "cuscus": 0.1937842770616005,
                "quintana roo": 0.09232021627375152,
                "wombat": 0.713895506664648
            },
            "lines": [
                [
                    0.3670212765957447,
                    0.5,
                    0.42,
                    0.5,
                    0.4669545294324991,
                    0.4690635451505017
                ],
                [
                    0.4925531914893617,
                    0.5,
                    0.2862264150943396,
                    0.38235294117647056,
                    0.473651744800846,
                    0.2838628762541806
                ],
                [
                    0.1404255319148936,
                    0.0,
                    0.2937735849056604,
                    0.11764705882352944,
                    0.059393725766654915,
                    0.2470735785953177
                ]
            ],
            "rate_limited": false,
            "answers": [
                "quintana roo",
                "cuscus",
                "wombat"
            ],
            "ml_answers": {
                "cuscus": 0.3907860887138839,
                "quintana roo": 0.3798485233717196,
                "wombat": 0.1390563030262045
            },
            "integer_answers": {
                "cuscus": 1,
                "quintana roo": 0,
                "wombat": 5
            },
            "data": {
                "wikipedia_search": [
                    0.5319148936170213,
                    0.029787234042553193,
                    1.4382978723404256
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    15.0
                ],
                "word_relation_to_question": [
                    0.32,
                    0.8550943396226416,
                    0.8249056603773585
                ],
                "word_count_raw": [
                    0.0,
                    4.0,
                    13.0
                ],
                "word_count_appended": [
                    74.0,
                    517.0,
                    605.0
                ],
                "result_count": [
                    37500.0,
                    29900.0,
                    500000.0
                ]
            },
            "z-best_answer_by_ml": [
                "cuscus"
            ]
        },
        "lines": [
            [
                1,
                0.3670212765957447,
                0.5,
                0.42,
                0.5,
                0.4669545294324991,
                0.4690635451505017
            ],
            [
                0,
                0.4925531914893617,
                0.5,
                0.2862264150943396,
                0.38235294117647056,
                0.473651744800846,
                0.2838628762541806
            ],
            [
                0,
                0.1404255319148936,
                0.0,
                0.2937735849056604,
                0.11764705882352944,
                0.059393725766654915,
                0.2470735785953177
            ]
        ]
    },
    "Catching a catfish with your bare hands is called what?": {
        "raw_data": {
            "negative_question": false,
            "question": "Catching a catfish with your bare hands is called what?",
            "fraction_answers": {
                "noodling": 0.8945317433087943,
                "whiskering": 0.04490826168049921,
                "strumming": 0.06055999501070649
            },
            "lines": [
                [
                    0.06666666666666667,
                    0.0,
                    0.046236559139784944,
                    0.0,
                    0.07478106858211159,
                    0.17567567567567569
                ],
                [
                    0.06666666666666667,
                    0.0,
                    0.16363636363636364,
                    0.0,
                    0.00029518842861359833,
                    0.03885135135135135
                ],
                [
                    0.8666666666666666,
                    1.0,
                    0.7901270772238514,
                    1.0,
                    0.9249237429892748,
                    0.785472972972973
                ]
            ],
            "rate_limited": false,
            "answers": [
                "strumming",
                "whiskering",
                "noodling"
            ],
            "ml_answers": {
                "noodling": 0.5796330537928813,
                "whiskering": 0.27445628130396604,
                "strumming": 0.13508217669402856
            },
            "integer_answers": {
                "noodling": 6,
                "whiskering": 0,
                "strumming": 0
            },
            "data": {
                "wikipedia_search": [
                    0.3333333333333333,
                    0.3333333333333333,
                    4.333333333333333
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    143.0
                ],
                "word_relation_to_question": [
                    0.23118279569892472,
                    0.8181818181818181,
                    3.950635386119257
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    215.0
                ],
                "word_count_appended": [
                    104.0,
                    23.0,
                    465.0
                ],
                "result_count": [
                    1520.0,
                    6.0,
                    18800.0
                ]
            },
            "z-best_answer_by_ml": [
                "noodling"
            ]
        },
        "lines": [
            [
                0,
                0.06666666666666667,
                0.0,
                0.046236559139784944,
                0.0,
                0.07478106858211159,
                0.17567567567567569
            ],
            [
                0,
                0.06666666666666667,
                0.0,
                0.16363636363636364,
                0.0,
                0.00029518842861359833,
                0.03885135135135135
            ],
            [
                1,
                0.8666666666666666,
                1.0,
                0.7901270772238514,
                1.0,
                0.9249237429892748,
                0.785472972972973
            ]
        ]
    },
    "What is the total cost of all the vowels on \u201cWheel of Fortune\u201d?": {
        "raw_data": {
            "negative_question": false,
            "question": "What is the total cost of all the vowels on \u201cWheel of Fortune\u201d?",
            "fraction_answers": {
                "$2,500": 0.5741185593779388,
                "$1,250": 0.15227207561621642,
                "$2,900": 0.2736093650058449
            },
            "lines": [
                [
                    0.0,
                    0,
                    0.20877896613190733,
                    0,
                    0.375,
                    0.02530933633295838
                ],
                [
                    0.5,
                    0,
                    0.43190730837789665,
                    0,
                    0.4,
                    0.9645669291338582
                ],
                [
                    0.5,
                    0,
                    0.359313725490196,
                    0,
                    0.225,
                    0.010123734533183352
                ]
            ],
            "rate_limited": false,
            "answers": [
                "$1,250",
                "$2,500",
                "$2,900"
            ],
            "ml_answers": {
                "$2,500": 0.21606072801869436,
                "$1,250": 0.40134052004670995,
                "$2,900": 0.22425307193357377
            },
            "integer_answers": {
                "$2,500": 4,
                "$1,250": 0,
                "$2,900": 0
            },
            "data": {
                "wikipedia_search": [
                    0.0,
                    1.0,
                    1.0
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.0438948306595366,
                    2.159536541889483,
                    1.7965686274509802
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    15.0,
                    16.0,
                    9.0
                ],
                "result_count": [
                    90.0,
                    3430.0,
                    36.0
                ]
            },
            "z-best_answer_by_ml": [
                "$1,250"
            ]
        },
        "lines": [
            [
                1,
                0.0,
                0,
                0.20877896613190733,
                0,
                0.375,
                0.02530933633295838
            ],
            [
                0,
                0.5,
                0,
                0.43190730837789665,
                0,
                0.4,
                0.9645669291338582
            ],
            [
                0,
                0.5,
                0,
                0.359313725490196,
                0,
                0.225,
                0.010123734533183352
            ]
        ]
    },
    "Which brand mascot was NOT a real person?": {
        "raw_data": {
            "negative_question": true,
            "question": "Which brand mascot was NOT a real person?",
            "fraction_answers": {
                "betty crocker": 0.6611895955378045,
                "sara lee": 0.13129466208939858,
                "little debbie": 0.2075157423727969
            },
            "lines": [
                [
                    0.3121871871871872,
                    0.5,
                    0.29260611704261774,
                    0.5,
                    0.4848687709773857,
                    0.2877906976744186
                ],
                [
                    0.4007632632632633,
                    0.5,
                    0.36543838106864185,
                    0.5,
                    0.4910771600975735,
                    0.3488372093023256
                ],
                [
                    0.28704954954954953,
                    0.0,
                    0.3419555018887404,
                    0.0,
                    0.024054068925040806,
                    0.3633720930232558
                ]
            ],
            "rate_limited": false,
            "answers": [
                "little debbie",
                "sara lee",
                "betty crocker"
            ],
            "ml_answers": {
                "betty crocker": 0.14047603290753635,
                "sara lee": 0.3517267814360149,
                "little debbie": 0.3233321025469769
            },
            "z-best_answer_by_ml": [
                "sara lee"
            ],
            "data": {
                "wikipedia_search": [
                    1.878128128128128,
                    0.9923673673673674,
                    2.1295045045045047
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_relation_to_question": [
                    2.0739388295738226,
                    1.3456161893135816,
                    1.5804449811125962
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_count_appended": [
                    146.0,
                    104.0,
                    94.0
                ],
                "result_count": [
                    165000.0,
                    97300.0,
                    5190000.0
                ]
            },
            "integer_answers": {
                "betty crocker": 4,
                "sara lee": 0,
                "little debbie": 2
            }
        },
        "lines": [
            [
                0,
                0.3121871871871872,
                0.5,
                0.29260611704261774,
                0.5,
                0.4848687709773857,
                0.2877906976744186
            ],
            [
                0,
                0.4007632632632633,
                0.5,
                0.36543838106864185,
                0.5,
                0.4910771600975735,
                0.3488372093023256
            ],
            [
                1,
                0.28704954954954953,
                0.0,
                0.3419555018887404,
                0.0,
                0.024054068925040806,
                0.3633720930232558
            ]
        ]
    },
    "What iconic painting once hung in Napoleon's bedroom?": {
        "raw_data": {
            "negative_question": false,
            "question": "What iconic painting once hung in Napoleon's bedroom?",
            "fraction_answers": {
                "the birth of venus": 0.18714865670582426,
                "mona lisa": 0.5055075719410045,
                "the starry night": 0.30734377135317126
            },
            "lines": [
                [
                    0.6898445529290853,
                    0.0,
                    0.330672268907563,
                    0.0,
                    0.1323529411764706,
                    0.6911928651059086
                ],
                [
                    0.19106500513874614,
                    0.9642857142857143,
                    0.1712418300653595,
                    1.0,
                    0.6529411764705882,
                    0.05351170568561873
                ],
                [
                    0.11909044193216856,
                    0.03571428571428571,
                    0.4980859010270775,
                    0.0,
                    0.21470588235294116,
                    0.2552954292084727
                ]
            ],
            "rate_limited": false,
            "answers": [
                "the starry night",
                "mona lisa",
                "the birth of venus"
            ],
            "ml_answers": {
                "the birth of venus": 0.26362277847145127,
                "mona lisa": 0.6294811307279601,
                "the starry night": 0.2063589574788174
            },
            "z-best_answer_by_ml": [
                "mona lisa"
            ],
            "data": {
                "wikipedia_search": [
                    2.7593782117163412,
                    0.7642600205549845,
                    0.4763617677286742
                ],
                "word_count_entities": [
                    0.0,
                    27.0,
                    1.0
                ],
                "word_relation_to_question": [
                    1.9840336134453782,
                    1.027450980392157,
                    2.988515406162465
                ],
                "word_count_raw": [
                    0.0,
                    49.0,
                    0.0
                ],
                "word_count_appended": [
                    45.0,
                    222.0,
                    73.0
                ],
                "result_count": [
                    2480000.0,
                    192000.0,
                    916000.0
                ]
            },
            "integer_answers": {
                "the birth of venus": 1,
                "mona lisa": 3,
                "the starry night": 2
            }
        },
        "lines": [
            [
                0,
                0.6898445529290853,
                0.0,
                0.330672268907563,
                0.0,
                0.1323529411764706,
                0.6911928651059086
            ],
            [
                1,
                0.19106500513874614,
                0.9642857142857143,
                0.1712418300653595,
                1.0,
                0.6529411764705882,
                0.05351170568561873
            ],
            [
                0,
                0.11909044193216856,
                0.03571428571428571,
                0.4980859010270775,
                0.0,
                0.21470588235294116,
                0.2552954292084727
            ]
        ]
    },
    "Which of these is NOT among the four \u201cC\u2019s\u201d of diamond buying?": {
        "raw_data": {
            "negative_question": true,
            "question": "Which of these is NOT among the four \u201cC\u2019s\u201d of diamond buying?",
            "fraction_answers": {
                "color": 0.43755524391345907,
                "core": 0.11076462752672661,
                "cut": 0.4516801285598142
            },
            "lines": [
                [
                    0.14864525977212018,
                    0.21250000000000002,
                    0.4184200858450771,
                    0.20943396226415095,
                    0.27330264672036825,
                    0.42503231365790606
                ],
                [
                    0.4619854985553072,
                    0.2875,
                    0.19209223588366509,
                    0.29056603773584905,
                    0.31242807825086305,
                    0.10038776389487292
                ],
                [
                    0.38936924167257264,
                    0.5,
                    0.38948767827125774,
                    0.5,
                    0.4142692750287687,
                    0.474579922447221
                ]
            ],
            "rate_limited": false,
            "answers": [
                "color",
                "cut",
                "core"
            ],
            "ml_answers": {
                "color": 0.24608517850939005,
                "core": 0.6196120716126767,
                "cut": 0.18248703482609
            },
            "integer_answers": {
                "color": 4,
                "core": 0,
                "cut": 2
            },
            "data": {
                "wikipedia_search": [
                    1.4054189609115193,
                    0.1520580057787712,
                    0.44252303330970943
                ],
                "word_count_entities": [
                    138.0,
                    102.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.6526393132393831,
                    2.4632621129306793,
                    0.8840985738299378
                ],
                "word_count_raw": [
                    154.0,
                    111.0,
                    0.0
                ],
                "word_count_appended": [
                    394.0,
                    326.0,
                    149.0
                ],
                "result_count": [
                    696000.0,
                    3710000.0,
                    236000.0
                ]
            },
            "z-best_answer_by_ml": [
                "core"
            ]
        },
        "lines": [
            [
                0,
                0.14864525977212018,
                0.21250000000000002,
                0.4184200858450771,
                0.20943396226415095,
                0.27330264672036825,
                0.42503231365790606
            ],
            [
                0,
                0.4619854985553072,
                0.2875,
                0.19209223588366509,
                0.29056603773584905,
                0.31242807825086305,
                0.10038776389487292
            ],
            [
                1,
                0.38936924167257264,
                0.5,
                0.38948767827125774,
                0.5,
                0.4142692750287687,
                0.474579922447221
            ]
        ]
    },
    "Rejected in the late 1700s, what was the name of the proposed 14th U.S. colony?": {
        "raw_data": {
            "negative_question": false,
            "question": "Rejected in the late 1700s, what was the name of the proposed 14th U.S. colony?",
            "fraction_answers": {
                "roanoke": 0.2632530814573219,
                "new albion": 0.1543079320331136,
                "vandalia": 0.5824389865095646
            },
            "lines": [
                [
                    0.008620689655172414,
                    0.0,
                    0.2386994254981054,
                    0.0,
                    0.6014234875444839,
                    0.7307748860461697
                ],
                [
                    0.24233716475095785,
                    0.9333333333333333,
                    0.7108055249969443,
                    1.0,
                    0.35231316725978645,
                    0.25584472871636527
                ],
                [
                    0.7490421455938697,
                    0.06666666666666667,
                    0.050495049504950505,
                    0.0,
                    0.046263345195729534,
                    0.013380385237465079
                ]
            ],
            "rate_limited": false,
            "answers": [
                "roanoke",
                "vandalia",
                "new albion"
            ],
            "ml_answers": {
                "roanoke": 0.19514025115149924,
                "new albion": 0.23556676179298963,
                "vandalia": 0.5045285762697449
            },
            "z-best_answer_by_ml": [
                "vandalia"
            ],
            "data": {
                "wikipedia_search": [
                    0.017241379310344827,
                    0.4846743295019157,
                    1.4980842911877394
                ],
                "word_count_entities": [
                    0.0,
                    14.0,
                    1.0
                ],
                "word_relation_to_question": [
                    1.1934971274905268,
                    3.5540276249847205,
                    0.2524752475247525
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_appended": [
                    169.0,
                    99.0,
                    13.0
                ],
                "result_count": [
                    4970.0,
                    1740.0,
                    91.0
                ]
            },
            "integer_answers": {
                "roanoke": 2,
                "new albion": 1,
                "vandalia": 3
            }
        },
        "lines": [
            [
                0,
                0.008620689655172414,
                0.0,
                0.2386994254981054,
                0.0,
                0.6014234875444839,
                0.7307748860461697
            ],
            [
                1,
                0.24233716475095785,
                0.9333333333333333,
                0.7108055249969443,
                1.0,
                0.35231316725978645,
                0.25584472871636527
            ],
            [
                0,
                0.7490421455938697,
                0.06666666666666667,
                0.050495049504950505,
                0.0,
                0.046263345195729534,
                0.013380385237465079
            ]
        ]
    },
    "In which version of \u201cDragnet\u201d is the line \u201cJust the facts, ma\u2019am\u201d first said?": {
        "raw_data": {
            "negative_question": false,
            "question": "In which version of \u201cDragnet\u201d is the line \u201cJust the facts, ma\u2019am\u201d first said?",
            "fraction_answers": {
                "'80s movie": 0.297236350577394,
                "'50s movie": 0.4194551965031187,
                "'50s tv show": 0.28330845291948725
            },
            "lines": [
                [
                    0.009057971014492754,
                    0,
                    0.4005986048910985,
                    0,
                    0.3333333333333333,
                    0.3902439024390244
                ],
                [
                    0.8049516908212561,
                    0,
                    0.29563332283349536,
                    0,
                    0.3333333333333333,
                    0.24390243902439024
                ],
                [
                    0.1859903381642512,
                    0,
                    0.3037680722754062,
                    0,
                    0.3333333333333333,
                    0.36585365853658536
                ]
            ],
            "rate_limited": false,
            "answers": [
                "'50s tv show",
                "'50s movie",
                "'80s movie"
            ],
            "ml_answers": {
                "'80s movie": 0.17239856009275767,
                "'50s movie": 0.18943024674652048,
                "'50s tv show": 0.23697540576104487
            },
            "integer_answers": {
                "'80s movie": 0,
                "'50s movie": 1,
                "'50s tv show": 3
            },
            "data": {
                "wikipedia_search": [
                    0.036231884057971016,
                    3.2198067632850242,
                    0.7439613526570048
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    2.403591629346591,
                    1.7737999370009723,
                    1.822608433652437
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    2.0,
                    2.0,
                    2.0
                ],
                "result_count": [
                    16.0,
                    10.0,
                    15.0
                ]
            },
            "z-best_answer_by_ml": [
                "'50s tv show"
            ]
        },
        "lines": [
            [
                0,
                0.009057971014492754,
                0,
                0.4005986048910985,
                0,
                0.3333333333333333,
                0.3902439024390244
            ],
            [
                0,
                0.8049516908212561,
                0,
                0.29563332283349536,
                0,
                0.3333333333333333,
                0.24390243902439024
            ],
            [
                1,
                0.1859903381642512,
                0,
                0.3037680722754062,
                0,
                0.3333333333333333,
                0.36585365853658536
            ]
        ]
    },
    "Which of these companies went public first?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which of these companies went public first?",
            "fraction_answers": {
                "facebook": 0.6520656612218365,
                "ferrari": 0.10777534928212218,
                "alibaba": 0.24015898949604128
            },
            "lines": [
                [
                    0.5554804804804805,
                    0,
                    0.5481481481481482,
                    1.0,
                    0.284,
                    0.872699677480554
                ],
                [
                    0.12832475332475332,
                    0,
                    0.059259259259259255,
                    0.0,
                    0.34114285714285714,
                    0.010149876683741225
                ],
                [
                    0.31619476619476616,
                    0,
                    0.39259259259259255,
                    0.0,
                    0.37485714285714283,
                    0.1171504458357048
                ]
            ],
            "rate_limited": false,
            "answers": [
                "facebook",
                "ferrari",
                "alibaba"
            ],
            "ml_answers": {
                "facebook": 0.5542449493440696,
                "ferrari": 0.22496580005393624,
                "alibaba": 0.2247850753454923
            },
            "integer_answers": {
                "facebook": 4,
                "ferrari": 0,
                "alibaba": 1
            },
            "data": {
                "wikipedia_search": [
                    1.6664414414414415,
                    0.38497425997425994,
                    0.9485842985842985
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.6444444444444444,
                    0.17777777777777776,
                    1.1777777777777776
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    497.0,
                    597.0,
                    656.0
                ],
                "result_count": [
                    184000000.0,
                    2140000.0,
                    24700000.0
                ]
            },
            "z-best_answer_by_ml": [
                "facebook"
            ]
        },
        "lines": [
            [
                1,
                0.5554804804804805,
                0,
                0.5481481481481482,
                1.0,
                0.284,
                0.872699677480554
            ],
            [
                0,
                0.12832475332475332,
                0,
                0.059259259259259255,
                0.0,
                0.34114285714285714,
                0.010149876683741225
            ],
            [
                0,
                0.31619476619476616,
                0,
                0.39259259259259255,
                0.0,
                0.37485714285714283,
                0.1171504458357048
            ]
        ]
    },
    "What topic would a herpetologist study?": {
        "raw_data": {
            "negative_question": false,
            "question": "What topic would a herpetologist study?",
            "fraction_answers": {
                "venereal disease": 0.5368847692032572,
                "mushroom farming": 0.1431032722433184,
                "crocodile teeth": 0.32001195855342435
            },
            "lines": [
                [
                    0.6317460317460317,
                    0,
                    0.1651174412793603,
                    0,
                    0.9660602191722524,
                    0.38461538461538464
                ],
                [
                    0.09523809523809523,
                    0,
                    0.1196901549225387,
                    0,
                    0.02415150547930631,
                    0.3333333333333333
                ],
                [
                    0.273015873015873,
                    0,
                    0.715192403798101,
                    0,
                    0.009788275348441323,
                    0.28205128205128205
                ]
            ],
            "rate_limited": false,
            "answers": [
                "venereal disease",
                "mushroom farming",
                "crocodile teeth"
            ],
            "ml_answers": {
                "venereal disease": 0.27824039218570157,
                "mushroom farming": 0.19164504601084548,
                "crocodile teeth": 0.2552797629746157
            },
            "z-best_answer_by_ml": [
                "venereal disease"
            ],
            "data": {
                "wikipedia_search": [
                    1.8952380952380952,
                    0.2857142857142857,
                    0.819047619047619
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.495352323838081,
                    0.35907046476761617,
                    2.1455772113943032
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    30.0,
                    26.0,
                    22.0
                ],
                "result_count": [
                    9080.0,
                    227.0,
                    92.0
                ]
            },
            "integer_answers": {
                "venereal disease": 3,
                "mushroom farming": 0,
                "crocodile teeth": 1
            }
        },
        "lines": [
            [
                0,
                0.6317460317460317,
                0,
                0.1651174412793603,
                0,
                0.9660602191722524,
                0.38461538461538464
            ],
            [
                0,
                0.09523809523809523,
                0,
                0.1196901549225387,
                0,
                0.02415150547930631,
                0.3333333333333333
            ],
            [
                1,
                0.273015873015873,
                0,
                0.715192403798101,
                0,
                0.009788275348441323,
                0.28205128205128205
            ]
        ]
    },
    "In the U.K., who appoints the Prime Minister?": {
        "raw_data": {
            "negative_question": false,
            "question": "In the U.K., who appoints the Prime Minister?",
            "fraction_answers": {
                "the parliament": 0.20647543984003822,
                "the people": 0.3126638111052688,
                "the queen": 0.480860749054693
            },
            "lines": [
                [
                    0.0,
                    0.2,
                    0.4444444444444444,
                    0.047619047619047616,
                    0.9602906825849987,
                    0.22362869198312235
                ],
                [
                    0.393939393939394,
                    0.0,
                    0.5222222222222223,
                    0.047619047619047616,
                    0.026126827580240507,
                    0.2489451476793249
                ],
                [
                    0.6060606060606061,
                    0.8,
                    0.03333333333333333,
                    0.9047619047619048,
                    0.013582489834760793,
                    0.5274261603375527
                ]
            ],
            "rate_limited": false,
            "answers": [
                "the people",
                "the parliament",
                "the queen"
            ],
            "ml_answers": {
                "the parliament": 0.3278286710861477,
                "the people": 0.3299818693528681,
                "the queen": 0.5869346540117579
            },
            "z-best_answer_by_ml": [
                "the queen"
            ],
            "data": {
                "wikipedia_search": [
                    0.0,
                    0.7878787878787878,
                    1.212121212121212
                ],
                "word_count_entities": [
                    1.0,
                    0.0,
                    4.0
                ],
                "word_relation_to_question": [
                    1.3333333333333333,
                    1.5666666666666667,
                    0.1
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    19.0
                ],
                "word_count_appended": [
                    53.0,
                    59.0,
                    125.0
                ],
                "result_count": [
                    22200000.0,
                    604000.0,
                    314000.0
                ]
            },
            "integer_answers": {
                "the parliament": 1,
                "the people": 1,
                "the queen": 4
            }
        },
        "lines": [
            [
                0,
                0.0,
                0.2,
                0.4444444444444444,
                0.047619047619047616,
                0.9602906825849987,
                0.22362869198312235
            ],
            [
                0,
                0.393939393939394,
                0.0,
                0.5222222222222223,
                0.047619047619047616,
                0.026126827580240507,
                0.2489451476793249
            ],
            [
                1,
                0.6060606060606061,
                0.8,
                0.03333333333333333,
                0.9047619047619048,
                0.013582489834760793,
                0.5274261603375527
            ]
        ]
    },
    "Which writer has stated that his/her trademark series of books would never be adapted for film?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which writer has stated that his/her trademark series of books would never be adapted for film?",
            "fraction_answers": {
                "jeff kinney": 0.3143103205230033,
                "james patterson": 0.39625018611373397,
                "sue grafton": 0.2894394933632628
            },
            "lines": [
                [
                    0.6122675675442902,
                    0.5,
                    0.2783102861013026,
                    0,
                    0.19692307692307692,
                    0.39375
                ],
                [
                    0.3877324324557097,
                    0.0,
                    0.40821503436060413,
                    0,
                    0.32,
                    0.33125
                ],
                [
                    0.0,
                    0.5,
                    0.3134746795380932,
                    0,
                    0.48307692307692307,
                    0.275
                ]
            ],
            "rate_limited": false,
            "answers": [
                "james patterson",
                "sue grafton",
                "jeff kinney"
            ],
            "ml_answers": {
                "jeff kinney": 0.31335428031140317,
                "james patterson": 0.2949945944312708,
                "sue grafton": 0.33834318190790197
            },
            "integer_answers": {
                "jeff kinney": 1,
                "james patterson": 3,
                "sue grafton": 1
            },
            "data": {
                "wikipedia_search": [
                    3.6736054052657416,
                    2.3263945947342584,
                    0.0
                ],
                "word_count_entities": [
                    1.0,
                    0.0,
                    1.0
                ],
                "word_relation_to_question": [
                    1.6698617166078156,
                    2.449290206163625,
                    1.8808480772285594
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    63.0,
                    53.0,
                    44.0
                ],
                "result_count": [
                    12800.0,
                    20800.0,
                    31400.0
                ]
            },
            "z-best_answer_by_ml": [
                "sue grafton"
            ]
        },
        "lines": [
            [
                0,
                0.6122675675442902,
                0.5,
                0.2783102861013026,
                0,
                0.19692307692307692,
                0.39375
            ],
            [
                1,
                0.3877324324557097,
                0.0,
                0.40821503436060413,
                0,
                0.32,
                0.33125
            ],
            [
                0,
                0.0,
                0.5,
                0.3134746795380932,
                0,
                0.48307692307692307,
                0.275
            ]
        ]
    },
    "In Harry Potter's Quidditch, what ALWAYS happens when one team catches the snitch?": {
        "raw_data": {
            "negative_question": false,
            "question": "In Harry Potter's Quidditch, what ALWAYS happens when one team catches the snitch?",
            "fraction_answers": {
                "the game ends": 0.7105639221802625,
                "that team loses": 0.2198883011468662,
                "that team wins": 0.0695477766728713
            },
            "lines": [
                [
                    0.0625,
                    0.0,
                    0.18760179153094464,
                    0.0,
                    0.12903225806451613,
                    0.03815261044176707
                ],
                [
                    0.9375,
                    0.0,
                    0.17662187839305105,
                    0.0,
                    0.0967741935483871,
                    0.10843373493975904
                ],
                [
                    0.0,
                    1.0,
                    0.6357763300760043,
                    1.0,
                    0.7741935483870968,
                    0.8534136546184738
                ]
            ],
            "rate_limited": false,
            "answers": [
                "that team wins",
                "that team loses",
                "the game ends"
            ],
            "ml_answers": {
                "the game ends": 0.5386170215586963,
                "that team loses": 0.26261549194758527,
                "that team wins": 0.290243619009459
            },
            "z-best_answer_by_ml": [
                "the game ends"
            ],
            "data": {
                "wikipedia_search": [
                    0.0625,
                    0.9375,
                    0.0
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    5.0
                ],
                "word_relation_to_question": [
                    1.5008143322475571,
                    1.4129750271444084,
                    5.0862106406080345
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    7.0
                ],
                "word_count_appended": [
                    8.0,
                    6.0,
                    48.0
                ],
                "result_count": [
                    19.0,
                    54.0,
                    425.0
                ]
            },
            "integer_answers": {
                "the game ends": 5,
                "that team loses": 1,
                "that team wins": 0
            }
        },
        "lines": [
            [
                0,
                0.0625,
                0.0,
                0.18760179153094464,
                0.0,
                0.12903225806451613,
                0.03815261044176707
            ],
            [
                0,
                0.9375,
                0.0,
                0.17662187839305105,
                0.0,
                0.0967741935483871,
                0.10843373493975904
            ],
            [
                1,
                0.0,
                1.0,
                0.6357763300760043,
                1.0,
                0.7741935483870968,
                0.8534136546184738
            ]
        ]
    },
    "Laurie Metcalf, Amy Morton and Tracy Letts are members of a theatre company from what city?": {
        "raw_data": {
            "negative_question": false,
            "question": "Laurie Metcalf, Amy Morton and Tracy Letts are members of a theatre company from what city?",
            "fraction_answers": {
                "new york": 0.23905571000303527,
                "los angeles": 0.14203789074982462,
                "chicago": 0.6189063992471402
            },
            "lines": [
                [
                    0.2384799271652288,
                    0.0743801652892562,
                    0.5072261072261073,
                    0.08695652173913043,
                    0.1378299120234604,
                    0.38946162657502864
                ],
                [
                    0.2840352524392133,
                    0.0,
                    0.2423698523698524,
                    0.0,
                    0.07038123167155426,
                    0.2554410080183276
                ],
                [
                    0.47748482039555795,
                    0.9256198347107438,
                    0.25040404040404035,
                    0.9130434782608695,
                    0.7917888563049853,
                    0.3550973654066438
                ]
            ],
            "rate_limited": false,
            "answers": [
                "new york",
                "los angeles",
                "chicago"
            ],
            "ml_answers": {
                "new york": 0.2724439681060517,
                "los angeles": 0.12260181995647382,
                "chicago": 0.5745536490406634
            },
            "z-best_answer_by_ml": [
                "chicago"
            ],
            "data": {
                "wikipedia_search": [
                    0.9539197086609152,
                    1.1361410097568532,
                    1.9099392815822318
                ],
                "word_count_entities": [
                    9.0,
                    0.0,
                    112.0
                ],
                "word_relation_to_question": [
                    2.5361305361305364,
                    1.211849261849262,
                    1.2520202020202018
                ],
                "word_count_raw": [
                    10.0,
                    0.0,
                    105.0
                ],
                "word_count_appended": [
                    47.0,
                    24.0,
                    270.0
                ],
                "result_count": [
                    1360.0,
                    892.0,
                    1240.0
                ]
            },
            "integer_answers": {
                "new york": 2,
                "los angeles": 0,
                "chicago": 4
            }
        },
        "lines": [
            [
                0,
                0.2384799271652288,
                0.0743801652892562,
                0.5072261072261073,
                0.08695652173913043,
                0.1378299120234604,
                0.38946162657502864
            ],
            [
                0,
                0.2840352524392133,
                0.0,
                0.2423698523698524,
                0.0,
                0.07038123167155426,
                0.2554410080183276
            ],
            [
                1,
                0.47748482039555795,
                0.9256198347107438,
                0.25040404040404035,
                0.9130434782608695,
                0.7917888563049853,
                0.3550973654066438
            ]
        ]
    },
    "Which of these two U.S. cities are in the same time zone?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which of these two U.S. cities are in the same time zone?",
            "fraction_answers": {
                "el paso / pierre": 0.16744329519637344,
                "pensacola / sioux falls": 0.24122526502706093,
                "bismarck / cheyenne": 0.5913314397765655
            },
            "lines": [
                [
                    0.014492753623188406,
                    0,
                    0.21849881796690307,
                    0,
                    0.10344827586206896,
                    0.3333333333333333
                ],
                [
                    0.7671497584541062,
                    0,
                    0.5234633569739953,
                    0,
                    0.7413793103448276,
                    0.3333333333333333
                ],
                [
                    0.2183574879227053,
                    0,
                    0.25803782505910167,
                    0,
                    0.15517241379310345,
                    0.3333333333333333
                ]
            ],
            "rate_limited": false,
            "answers": [
                "el paso / pierre",
                "bismarck / cheyenne",
                "pensacola / sioux falls"
            ],
            "ml_answers": {
                "el paso / pierre": 0.29486050389817636,
                "pensacola / sioux falls": 0.18195669183826488,
                "bismarck / cheyenne": 0.35604207019349654
            },
            "integer_answers": {
                "el paso / pierre": 1,
                "pensacola / sioux falls": 0,
                "bismarck / cheyenne": 3
            },
            "data": {
                "wikipedia_search": [
                    0.043478260869565216,
                    2.3014492753623186,
                    0.6550724637681159
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.8739952718676123,
                    2.093853427895981,
                    1.0321513002364067
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    3.0,
                    3.0,
                    3.0
                ],
                "result_count": [
                    6.0,
                    43.0,
                    9.0
                ]
            },
            "z-best_answer_by_ml": [
                "bismarck / cheyenne"
            ]
        },
        "lines": [
            [
                0,
                0.014492753623188406,
                0,
                0.21849881796690307,
                0,
                0.10344827586206896,
                0.3333333333333333
            ],
            [
                0,
                0.7671497584541062,
                0,
                0.5234633569739953,
                0,
                0.7413793103448276,
                0.3333333333333333
            ],
            [
                1,
                0.2183574879227053,
                0,
                0.25803782505910167,
                0,
                0.15517241379310345,
                0.3333333333333333
            ]
        ]
    },
    "Until it was banned, lithium was a key ingredient in which of these brands?": {
        "raw_data": {
            "negative_question": false,
            "question": "Until it was banned, lithium was a key ingredient in which of these brands?",
            "fraction_answers": {
                "cracker jack": 0.17227682823694762,
                "7up": 0.8008108212985316,
                "good and plenty": 0.026912350464520576
            },
            "lines": [
                [
                    0.13513513513513514,
                    0.0,
                    0.23700980392156865,
                    0.0,
                    0.04878048780487805,
                    0.612735542560104
                ],
                [
                    0.8648648648648649,
                    1.0,
                    0.6272058823529412,
                    1.0,
                    0.926829268292683,
                    0.38596491228070173
                ],
                [
                    0.0,
                    0.0,
                    0.13578431372549019,
                    0.0,
                    0.024390243902439025,
                    0.001299545159194282
                ]
            ],
            "rate_limited": false,
            "answers": [
                "cracker jack",
                "7up",
                "good and plenty"
            ],
            "ml_answers": {
                "cracker jack": 0.17440264798707095,
                "7up": 0.44571397096482496,
                "good and plenty": 0.20339745916729846
            },
            "z-best_answer_by_ml": [
                "7up"
            ],
            "data": {
                "wikipedia_search": [
                    0.40540540540540543,
                    2.5945945945945947,
                    0.0
                ],
                "word_count_entities": [
                    0.0,
                    5.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.9480392156862746,
                    2.5088235294117647,
                    0.5431372549019607
                ],
                "word_count_raw": [
                    0.0,
                    6.0,
                    0.0
                ],
                "word_count_appended": [
                    8.0,
                    152.0,
                    4.0
                ],
                "result_count": [
                    943.0,
                    594.0,
                    2.0
                ]
            },
            "integer_answers": {
                "cracker jack": 1,
                "7up": 5,
                "good and plenty": 0
            }
        },
        "lines": [
            [
                0,
                0.13513513513513514,
                0.0,
                0.23700980392156865,
                0.0,
                0.04878048780487805,
                0.612735542560104
            ],
            [
                1,
                0.8648648648648649,
                1.0,
                0.6272058823529412,
                1.0,
                0.926829268292683,
                0.38596491228070173
            ],
            [
                0,
                0.0,
                0.0,
                0.13578431372549019,
                0.0,
                0.024390243902439025,
                0.001299545159194282
            ]
        ]
    },
    "Aside from blood cells, what would you also find inside your blood vessels?": {
        "raw_data": {
            "negative_question": false,
            "question": "Aside from blood cells, what would you also find inside your blood vessels?",
            "fraction_answers": {
                "marrow": 0.6444903781154626,
                "plasma": 0.287098617526732,
                "plastids": 0.06841100435780538
            },
            "lines": [
                [
                    0.08711650922177237,
                    0.6296296296296297,
                    0.31781835421215954,
                    0.37142857142857144,
                    0.31635802469135804,
                    0.00024061597690086623
                ],
                [
                    0.8813045434098065,
                    0.37037037037037035,
                    0.5666546948626594,
                    0.6285714285714286,
                    0.4876543209876543,
                    0.9323869104908566
                ],
                [
                    0.031578947368421054,
                    0.0,
                    0.11552695092518102,
                    0.0,
                    0.19598765432098766,
                    0.06737247353224254
                ]
            ],
            "rate_limited": false,
            "answers": [
                "plasma",
                "marrow",
                "plastids"
            ],
            "ml_answers": {
                "marrow": 0.2889894060841226,
                "plasma": 0.36239454165014406,
                "plastids": 0.1868654397487893
            },
            "integer_answers": {
                "marrow": 5,
                "plasma": 1,
                "plastids": 0
            },
            "data": {
                "wikipedia_search": [
                    0.4355825461088619,
                    4.406522717049032,
                    0.15789473684210525
                ],
                "word_count_entities": [
                    17.0,
                    10.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.5890917710607977,
                    2.8332734743132972,
                    0.5776347546259051
                ],
                "word_count_raw": [
                    13.0,
                    22.0,
                    0.0
                ],
                "word_count_appended": [
                    205.0,
                    316.0,
                    127.0
                ],
                "result_count": [
                    80.0,
                    310000.0,
                    22400.0
                ]
            },
            "z-best_answer_by_ml": [
                "plasma"
            ]
        },
        "lines": [
            [
                1,
                0.08711650922177237,
                0.6296296296296297,
                0.31781835421215954,
                0.37142857142857144,
                0.31635802469135804,
                0.00024061597690086623
            ],
            [
                0,
                0.8813045434098065,
                0.37037037037037035,
                0.5666546948626594,
                0.6285714285714286,
                0.4876543209876543,
                0.9323869104908566
            ],
            [
                0,
                0.031578947368421054,
                0.0,
                0.11552695092518102,
                0.0,
                0.19598765432098766,
                0.06737247353224254
            ]
        ]
    },
    "Which of these Kentucky Derby winners was named for its trainer?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which of these Kentucky Derby winners was named for its trainer?",
            "fraction_answers": {
                "lieut. gibson": 0.13146469576533862,
                "paul jones": 0.24057774100307924,
                "clyde van dusen": 0.6279575632315821
            },
            "lines": [
                [
                    0.7333333333333334,
                    0,
                    0.8715135830353222,
                    1.0,
                    0.5258620689655172,
                    0.00907883082373782
                ],
                [
                    0.0,
                    0,
                    0.0018181818181818184,
                    0.0,
                    0.3706896551724138,
                    0.8303808680248007
                ],
                [
                    0.26666666666666666,
                    0,
                    0.12666823514649603,
                    0.0,
                    0.10344827586206896,
                    0.16054030115146148
                ]
            ],
            "rate_limited": false,
            "answers": [
                "clyde van dusen",
                "paul jones",
                "lieut. gibson"
            ],
            "ml_answers": {
                "lieut. gibson": 0.14351157479431342,
                "paul jones": 0.16808173900409684,
                "clyde van dusen": 0.5743420943830014
            },
            "integer_answers": {
                "lieut. gibson": 0,
                "paul jones": 1,
                "clyde van dusen": 4
            },
            "data": {
                "wikipedia_search": [
                    2.2,
                    0.0,
                    0.8
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    4.3575679151766105,
                    0.00909090909090909,
                    0.63334117573248
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    61.0,
                    43.0,
                    12.0
                ],
                "result_count": [
                    82.0,
                    7500.0,
                    1450.0
                ]
            },
            "z-best_answer_by_ml": [
                "clyde van dusen"
            ]
        },
        "lines": [
            [
                1,
                0.7333333333333334,
                0,
                0.8715135830353222,
                1.0,
                0.5258620689655172,
                0.00907883082373782
            ],
            [
                0,
                0.0,
                0,
                0.0018181818181818184,
                0.0,
                0.3706896551724138,
                0.8303808680248007
            ],
            [
                0,
                0.26666666666666666,
                0,
                0.12666823514649603,
                0.0,
                0.10344827586206896,
                0.16054030115146148
            ]
        ]
    },
    "Which of these Hebrew texts does NOT form a significant part of the Christian Old Testament?": {
        "raw_data": {
            "negative_question": true,
            "question": "Which of these Hebrew texts does NOT form a significant part of the Christian Old Testament?",
            "fraction_answers": {
                "talmud": 0.24256570548737696,
                "ketuvim": 0.13031151409634648,
                "torah": 0.6271227804162766
            },
            "lines": [
                [
                    0.34888075894904846,
                    0.020833333333333315,
                    0.37245193955962597,
                    0.019230769230769218,
                    0.30091012514220705,
                    0.056324732536186306
                ],
                [
                    0.44733373213524646,
                    0.5,
                    0.3096872016122595,
                    0.5,
                    0.36746302616609783,
                    0.4845814977973568
                ],
                [
                    0.20378550891570507,
                    0.4791666666666667,
                    0.31786085882811455,
                    0.4807692307692308,
                    0.3316268486916951,
                    0.4590937696664569
                ]
            ],
            "rate_limited": false,
            "answers": [
                "torah",
                "ketuvim",
                "talmud"
            ],
            "ml_answers": {
                "talmud": 0.35503540981597104,
                "ketuvim": 0.42010270121795984,
                "torah": 0.28523740228437444
            },
            "z-best_answer_by_ml": [
                "ketuvim"
            ],
            "data": {
                "wikipedia_search": [
                    2.4179078568152246,
                    0.8426602858360566,
                    4.739431857348719
                ],
                "word_count_entities": [
                    23.0,
                    0.0,
                    1.0
                ],
                "word_relation_to_question": [
                    2.040768967045985,
                    3.045004774203848,
                    2.914226258750167
                ],
                "word_count_raw": [
                    25.0,
                    0.0,
                    1.0
                ],
                "word_count_appended": [
                    350.0,
                    233.0,
                    296.0
                ],
                "result_count": [
                    4230000.0,
                    147000.0,
                    390000.0
                ]
            },
            "integer_answers": {
                "talmud": 1,
                "ketuvim": 1,
                "torah": 4
            }
        },
        "lines": [
            [
                0,
                0.34888075894904846,
                0.020833333333333315,
                0.37245193955962597,
                0.019230769230769218,
                0.30091012514220705,
                0.056324732536186306
            ],
            [
                0,
                0.44733373213524646,
                0.5,
                0.3096872016122595,
                0.5,
                0.36746302616609783,
                0.4845814977973568
            ],
            [
                1,
                0.20378550891570507,
                0.4791666666666667,
                0.31786085882811455,
                0.4807692307692308,
                0.3316268486916951,
                0.4590937696664569
            ]
        ]
    },
    "How many of the three Baltic countries border Russia?": {
        "raw_data": {
            "negative_question": false,
            "question": "How many of the three Baltic countries border Russia?",
            "fraction_answers": {
                "none": 0.3895879415538704,
                "three": 0.38733258735391213,
                "two": 0.22307947109221749
            },
            "lines": [
                [
                    0.23643892339544514,
                    0.6957605985037406,
                    0.2839068825910931,
                    0.676056338028169,
                    0.4318181818181818,
                    1.459978684311209e-05
                ],
                [
                    0.17163561076604553,
                    0.014962593516209476,
                    0.19939271255060728,
                    0.008450704225352112,
                    0.24204545454545454,
                    0.7019897509496361
                ],
                [
                    0.5919254658385094,
                    0.2892768079800499,
                    0.5167004048582996,
                    0.3154929577464789,
                    0.3261363636363636,
                    0.29799564926352073
                ]
            ],
            "rate_limited": false,
            "answers": [
                "three",
                "two",
                "none"
            ],
            "ml_answers": {
                "none": 0.2529762047790761,
                "three": 0.3739348764729545,
                "two": 0.3334355119506104
            },
            "integer_answers": {
                "none": 2,
                "three": 3,
                "two": 1
            },
            "data": {
                "wikipedia_search": [
                    0.7093167701863354,
                    0.5149068322981366,
                    1.775776397515528
                ],
                "word_count_entities": [
                    279.0,
                    6.0,
                    116.0
                ],
                "word_relation_to_question": [
                    0.5678137651821862,
                    0.39878542510121456,
                    1.0334008097165992
                ],
                "word_count_raw": [
                    240.0,
                    3.0,
                    112.0
                ],
                "word_count_appended": [
                    380.0,
                    213.0,
                    287.0
                ],
                "result_count": [
                    73.0,
                    3510000.0,
                    1490000.0
                ]
            },
            "z-best_answer_by_ml": [
                "three"
            ]
        },
        "lines": [
            [
                1,
                0.23643892339544514,
                0.6957605985037406,
                0.2839068825910931,
                0.676056338028169,
                0.4318181818181818,
                1.459978684311209e-05
            ],
            [
                0,
                0.17163561076604553,
                0.014962593516209476,
                0.19939271255060728,
                0.008450704225352112,
                0.24204545454545454,
                0.7019897509496361
            ],
            [
                0,
                0.5919254658385094,
                0.2892768079800499,
                0.5167004048582996,
                0.3154929577464789,
                0.3261363636363636,
                0.29799564926352073
            ]
        ]
    },
    "Which of these actors was a high school cheerleader?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which of these actors was a high school cheerleader?",
            "fraction_answers": {
                "john travolta": 0.3196482828811019,
                "george clooney": 0.3092025784452152,
                "michael douglas": 0.3711491386736829
            },
            "lines": [
                [
                    0.36481481481481487,
                    0.0,
                    0.42540792540792544,
                    0,
                    0.33783783783783783,
                    0.4179523141654979
                ],
                [
                    0.10555555555555556,
                    1.0,
                    0.1185897435897436,
                    0,
                    0.3581081081081081,
                    0.27349228611500703
                ],
                [
                    0.5296296296296297,
                    0.0,
                    0.45600233100233095,
                    0,
                    0.30405405405405406,
                    0.3085553997194951
                ]
            ],
            "rate_limited": false,
            "answers": [
                "george clooney",
                "michael douglas",
                "john travolta"
            ],
            "ml_answers": {
                "john travolta": 0.2305200462494937,
                "george clooney": 0.331741311539268,
                "michael douglas": 0.41703153195417647
            },
            "integer_answers": {
                "john travolta": 2,
                "george clooney": 1,
                "michael douglas": 2
            },
            "data": {
                "wikipedia_search": [
                    1.0944444444444446,
                    0.31666666666666665,
                    1.588888888888889
                ],
                "word_count_entities": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.2762237762237763,
                    0.3557692307692308,
                    1.368006993006993
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    50.0,
                    53.0,
                    45.0
                ],
                "result_count": [
                    149000.0,
                    97500.0,
                    110000.0
                ]
            },
            "z-best_answer_by_ml": [
                "michael douglas"
            ]
        },
        "lines": [
            [
                0,
                0.36481481481481487,
                0.0,
                0.42540792540792544,
                0,
                0.33783783783783783,
                0.4179523141654979
            ],
            [
                1,
                0.10555555555555556,
                1.0,
                0.1185897435897436,
                0,
                0.3581081081081081,
                0.27349228611500703
            ],
            [
                0,
                0.5296296296296297,
                0.0,
                0.45600233100233095,
                0,
                0.30405405405405406,
                0.3085553997194951
            ]
        ]
    },
    "Which of these phrases appears in a Shakespeare play?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which of these phrases appears in a Shakespeare play?",
            "fraction_answers": {
                "at a loss": 0.43059972584011086,
                "in such a pickle": 0.42432710222935804,
                "up a dark creek": 0.14507317193053126
            },
            "lines": [
                [
                    0.813953488372093,
                    0.3333333333333333,
                    0.7826981707317073,
                    0.3333333333333333,
                    3.559195350673959e-05,
                    0.2826086956521739
                ],
                [
                    0.13953488372093023,
                    0.3333333333333333,
                    0.1361280487804878,
                    0.3333333333333333,
                    0.9999644080464932,
                    0.6413043478260869
                ],
                [
                    0.046511627906976744,
                    0.3333333333333333,
                    0.08117378048780488,
                    0.3333333333333333,
                    0.0,
                    0.07608695652173914
                ]
            ],
            "rate_limited": false,
            "answers": [
                "in such a pickle",
                "at a loss",
                "up a dark creek"
            ],
            "ml_answers": {
                "at a loss": 0.2583373551882723,
                "in such a pickle": 0.30511792706661217,
                "up a dark creek": 0.12689328228251137
            },
            "z-best_answer_by_ml": [
                "in such a pickle"
            ],
            "data": {
                "wikipedia_search": [
                    0.813953488372093,
                    0.13953488372093023,
                    0.046511627906976744
                ],
                "word_count_entities": [
                    1.0,
                    1.0,
                    1.0
                ],
                "word_relation_to_question": [
                    3.1307926829268293,
                    0.5445121951219511,
                    0.3246951219512195
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    1.0
                ],
                "word_count_appended": [
                    26.0,
                    59.0,
                    7.0
                ],
                "result_count": [
                    42.0,
                    1180000.0,
                    0
                ]
            },
            "integer_answers": {
                "at a loss": 2,
                "in such a pickle": 4,
                "up a dark creek": 0
            }
        },
        "lines": [
            [
                1,
                0.813953488372093,
                0.3333333333333333,
                0.7826981707317073,
                0.3333333333333333,
                3.559195350673959e-05,
                0.2826086956521739
            ],
            [
                0,
                0.13953488372093023,
                0.3333333333333333,
                0.1361280487804878,
                0.3333333333333333,
                0.9999644080464932,
                0.6413043478260869
            ],
            [
                0,
                0.046511627906976744,
                0.3333333333333333,
                0.08117378048780488,
                0.3333333333333333,
                0.0,
                0.07608695652173914
            ]
        ]
    },
    "J.K. Rowling\u2019s first book published in England was titled \u201cHarry Potter and the\u201d what?": {
        "raw_data": {
            "negative_question": false,
            "question": "J.K. Rowling\u2019s first book published in England was titled \u201cHarry Potter and the\u201d what?",
            "fraction_answers": {
                "magician's stone": 0.10799400891350225,
                "philosopher's stone": 0.2832417424205087,
                "sorcerer's stone": 0.608764248665989
            },
            "lines": [
                [
                    0.0700354609929078,
                    0,
                    0.2374380006885599,
                    0,
                    0.42549350800056734,
                    0.4
                ],
                [
                    0.874113475177305,
                    0,
                    0.6864556517821813,
                    0,
                    0.5744878677044697,
                    0.3
                ],
                [
                    0.05585106382978724,
                    0,
                    0.07610634752925877,
                    0,
                    1.8624294962987794e-05,
                    0.3
                ]
            ],
            "rate_limited": false,
            "answers": [
                "philosopher's stone",
                "sorcerer's stone",
                "magician's stone"
            ],
            "ml_answers": {
                "magician's stone": 0.14594254224161793,
                "philosopher's stone": 0.19703414353344098,
                "sorcerer's stone": 0.32530436629952675
            },
            "z-best_answer_by_ml": [
                "sorcerer's stone"
            ],
            "data": {
                "wikipedia_search": [
                    0.42021276595744683,
                    5.24468085106383,
                    0.3351063829787234
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.8995040055084793,
                    5.49164521425745,
                    0.6088507802340701
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    4.0,
                    3.0,
                    3.0
                ],
                "result_count": [
                    297000.0,
                    401000.0,
                    13.0
                ]
            },
            "integer_answers": {
                "magician's stone": 0,
                "philosopher's stone": 1,
                "sorcerer's stone": 3
            }
        },
        "lines": [
            [
                1,
                0.0700354609929078,
                0,
                0.2374380006885599,
                0,
                0.42549350800056734,
                0.4
            ],
            [
                0,
                0.874113475177305,
                0,
                0.6864556517821813,
                0,
                0.5744878677044697,
                0.3
            ],
            [
                0,
                0.05585106382978724,
                0,
                0.07610634752925877,
                0,
                1.8624294962987794e-05,
                0.3
            ]
        ]
    },
    "Which former NFL star does NOT have a football video game named after him?": {
        "raw_data": {
            "negative_question": true,
            "question": "Which former NFL star does NOT have a football video game named after him?",
            "fraction_answers": {
                "kurt warner": 0.3269985774232046,
                "brett favre": 0.3946793012197595,
                "emmitt smith": 0.2783221213570358
            },
            "lines": [
                [
                    0.41392722148689676,
                    0,
                    0.33306673414574295,
                    0.25,
                    0.4569591950810509,
                    0.3502415458937198
                ],
                [
                    0.2819775742620657,
                    0,
                    0.3354457773417706,
                    0.375,
                    0.19116825041922864,
                    0.32971014492753625
                ],
                [
                    0.30409520425103753,
                    0,
                    0.33148748851248644,
                    0.375,
                    0.35187255449972055,
                    0.32004830917874394
                ]
            ],
            "rate_limited": false,
            "answers": [
                "emmitt smith",
                "brett favre",
                "kurt warner"
            ],
            "ml_answers": {
                "kurt warner": 0.19483540892469403,
                "brett favre": 0.19682341774979445,
                "emmitt smith": 0.24869926331133585
            },
            "integer_answers": {
                "kurt warner": 2,
                "brett favre": 2,
                "emmitt smith": 1
            },
            "data": {
                "wikipedia_search": [
                    1.2050188991834456,
                    3.05231396033108,
                    2.7426671404854748
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    2.3370657219595987,
                    2.303759117215211,
                    2.35917516082519
                ],
                "word_count_raw": [
                    2.0,
                    1.0,
                    1.0
                ],
                "word_count_appended": [
                    124.0,
                    141.0,
                    149.0
                ],
                "result_count": [
                    30800.0,
                    221000.0,
                    106000.0
                ]
            },
            "z-best_answer_by_ml": [
                "emmitt smith"
            ]
        },
        "lines": [
            [
                0,
                0.41392722148689676,
                0,
                0.33306673414574295,
                0.25,
                0.4569591950810509,
                0.3502415458937198
            ],
            [
                1,
                0.2819775742620657,
                0,
                0.3354457773417706,
                0.375,
                0.19116825041922864,
                0.32971014492753625
            ],
            [
                0,
                0.30409520425103753,
                0,
                0.33148748851248644,
                0.375,
                0.35187255449972055,
                0.32004830917874394
            ]
        ]
    },
    "How do you spell the last name of Duke University\u2019s men's basketball coach?": {
        "raw_data": {
            "negative_question": false,
            "question": "How do you spell the last name of Duke University\u2019s men's basketball coach?",
            "fraction_answers": {
                "khzyrweski": 0.06668908049753482,
                "crzyzewski": 0.07428918111801648,
                "krzyzewski": 0.8590217383844487
            },
            "lines": [
                [
                    0.0,
                    0.0,
                    0.3435395606103358,
                    0.0,
                    0.06690140845070422,
                    0.03529411764705882
                ],
                [
                    1.0,
                    1.0,
                    0.3126639845734694,
                    1.0,
                    0.8767605633802817,
                    0.9647058823529412
                ],
                [
                    0.0,
                    0.0,
                    0.34379645481619475,
                    0.0,
                    0.056338028169014086,
                    0.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "crzyzewski",
                "krzyzewski",
                "khzyrweski"
            ],
            "ml_answers": {
                "khzyrweski": 0.1750319970242504,
                "crzyzewski": 0.16196885133190578,
                "krzyzewski": 0.6753054645527053
            },
            "integer_answers": {
                "khzyrweski": 1,
                "crzyzewski": 0,
                "krzyzewski": 5
            },
            "data": {
                "wikipedia_search": [
                    0.0,
                    6.0,
                    0.0
                ],
                "word_count_entities": [
                    0.0,
                    127.0,
                    0.0
                ],
                "word_relation_to_question": [
                    2.4047769242723507,
                    2.1886478920142856,
                    2.4065751837133633
                ],
                "word_count_raw": [
                    0.0,
                    29.0,
                    0.0
                ],
                "word_count_appended": [
                    19.0,
                    249.0,
                    16.0
                ],
                "result_count": [
                    3.0,
                    82.0,
                    0
                ]
            },
            "z-best_answer_by_ml": [
                "krzyzewski"
            ]
        },
        "lines": [
            [
                0,
                0.0,
                0.0,
                0.3435395606103358,
                0.0,
                0.06690140845070422,
                0.03529411764705882
            ],
            [
                1,
                1.0,
                1.0,
                0.3126639845734694,
                1.0,
                0.8767605633802817,
                0.9647058823529412
            ],
            [
                0,
                0.0,
                0.0,
                0.34379645481619475,
                0.0,
                0.056338028169014086,
                0.0
            ]
        ]
    },
    "Which of these consists of frozen water?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which of these consists of frozen water?",
            "fraction_answers": {
                "garden rake": 0.14950412772647545,
                "snowflake": 0.5204339664076125,
                "drake": 0.3300619058659121
            },
            "lines": [
                [
                    0.6818181818181819,
                    0,
                    0.5833333333333334,
                    0,
                    0.5279429250891795,
                    0.288641425389755
                ],
                [
                    0.0,
                    0,
                    0.08333333333333333,
                    0,
                    0.0558858501783591,
                    0.45879732739420936
                ],
                [
                    0.3181818181818182,
                    0,
                    0.3333333333333333,
                    0,
                    0.4161712247324614,
                    0.2525612472160356
                ]
            ],
            "rate_limited": false,
            "answers": [
                "snowflake",
                "garden rake",
                "drake"
            ],
            "ml_answers": {
                "garden rake": 0.16484476326681718,
                "snowflake": 0.3104852406274795,
                "drake": 0.1434660330980408
            },
            "z-best_answer_by_ml": [
                "snowflake"
            ],
            "data": {
                "wikipedia_search": [
                    1.3636363636363638,
                    0.0,
                    0.6363636363636364
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.1666666666666667,
                    0.16666666666666666,
                    0.6666666666666666
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    444.0,
                    47.0,
                    350.0
                ],
                "result_count": [
                    6480000.0,
                    10300000.0,
                    5670000.0
                ]
            },
            "integer_answers": {
                "garden rake": 1,
                "snowflake": 3,
                "drake": 0
            }
        },
        "lines": [
            [
                1,
                0.6818181818181819,
                0,
                0.5833333333333334,
                0,
                0.5279429250891795,
                0.288641425389755
            ],
            [
                0,
                0.0,
                0,
                0.08333333333333333,
                0,
                0.0558858501783591,
                0.45879732739420936
            ],
            [
                0,
                0.3181818181818182,
                0,
                0.3333333333333333,
                0,
                0.4161712247324614,
                0.2525612472160356
            ]
        ]
    },
    "What word describes joining a cause just to feel good about it?": {
        "raw_data": {
            "negative_question": false,
            "question": "What word describes joining a cause just to feel good about it?",
            "fraction_answers": {
                "slacktivism": 0.7968952722013801,
                "gung-faux": 0.08966532421027512,
                "joinerism": 0.11343940358834469
            },
            "lines": [
                [
                    0.07491289198606273,
                    0,
                    0.2901370354843823,
                    0,
                    0.08541666666666667,
                    0.003291020216267043
                ],
                [
                    0.2667426405735593,
                    0,
                    0.05025198960087451,
                    0,
                    0.041666666666666664,
                    0.0
                ],
                [
                    0.6583444674403779,
                    0,
                    0.6596109749147432,
                    0,
                    0.8729166666666667,
                    0.996708979783733
                ]
            ],
            "rate_limited": false,
            "answers": [
                "joinerism",
                "gung-faux",
                "slacktivism"
            ],
            "ml_answers": {
                "slacktivism": 0.271362926813908,
                "gung-faux": 0.14301517215170373,
                "joinerism": 0.17483072602628513
            },
            "z-best_answer_by_ml": [
                "slacktivism"
            ],
            "data": {
                "wikipedia_search": [
                    0.524390243902439,
                    1.8671984840149152,
                    4.608411272082646
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    2.030959248390676,
                    0.35176392720612154,
                    4.617276824403202
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    41.0,
                    20.0,
                    419.0
                ],
                "result_count": [
                    35.0,
                    0,
                    10600.0
                ]
            },
            "integer_answers": {
                "slacktivism": 4,
                "gung-faux": 0,
                "joinerism": 0
            }
        },
        "lines": [
            [
                0,
                0.07491289198606273,
                0,
                0.2901370354843823,
                0,
                0.08541666666666667,
                0.003291020216267043
            ],
            [
                0,
                0.2667426405735593,
                0,
                0.05025198960087451,
                0,
                0.041666666666666664,
                0.0
            ],
            [
                1,
                0.6583444674403779,
                0,
                0.6596109749147432,
                0,
                0.8729166666666667,
                0.996708979783733
            ]
        ]
    },
    "One of Apple\u2019s biggest flops was a product named after a man who did what?": {
        "raw_data": {
            "negative_question": false,
            "question": "One of Apple\u2019s biggest flops was a product named after a man who did what?",
            "fraction_answers": {
                "invented the transistor": 0.36848718288527965,
                "developed calculus": 0.25943246733885994,
                "discovered saturn": 0.37208034977586035
            },
            "lines": [
                [
                    0.39820885250651983,
                    0,
                    0.32435978084415584,
                    0,
                    0.36363636363636365,
                    0.4021164021164021
                ],
                [
                    0.2520847419427541,
                    0,
                    0.47414874188311684,
                    0,
                    0.4090909090909091,
                    0.3386243386243386
                ],
                [
                    0.349706405550726,
                    0,
                    0.20149147727272726,
                    0,
                    0.22727272727272727,
                    0.25925925925925924
                ]
            ],
            "rate_limited": false,
            "answers": [
                "discovered saturn",
                "invented the transistor",
                "developed calculus"
            ],
            "ml_answers": {
                "invented the transistor": 0.29465559216103954,
                "developed calculus": 0.31004203168892136,
                "discovered saturn": 0.2349791490648214
            },
            "z-best_answer_by_ml": [
                "developed calculus"
            ],
            "data": {
                "wikipedia_search": [
                    1.5928354100260793,
                    1.0083389677710164,
                    1.398825622202904
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.6217989042207792,
                    2.370743709415584,
                    1.0074573863636362
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    8.0,
                    9.0,
                    5.0
                ],
                "result_count": [
                    76.0,
                    64.0,
                    49.0
                ]
            },
            "integer_answers": {
                "invented the transistor": 2,
                "developed calculus": 0,
                "discovered saturn": 2
            }
        },
        "lines": [
            [
                0,
                0.39820885250651983,
                0,
                0.32435978084415584,
                0,
                0.36363636363636365,
                0.4021164021164021
            ],
            [
                0,
                0.2520847419427541,
                0,
                0.47414874188311684,
                0,
                0.4090909090909091,
                0.3386243386243386
            ],
            [
                1,
                0.349706405550726,
                0,
                0.20149147727272726,
                0,
                0.22727272727272727,
                0.25925925925925924
            ]
        ]
    },
    "What dish is made with ham, poached eggs and Hollandaise sauce?": {
        "raw_data": {
            "negative_question": false,
            "question": "What dish is made with ham, poached eggs and Hollandaise sauce?",
            "fraction_answers": {
                "eggs benedict": 0.862408962429114,
                "benedict cumberbatch": 0.0961644694581776,
                "pope benedict": 0.04142656811270845
            },
            "lines": [
                [
                    0.0,
                    0.0,
                    0.09169600625911307,
                    0.0,
                    0.00093119902730713,
                    0.15593220338983052
                ],
                [
                    0.7856215213358071,
                    1.0,
                    0.807057034778102,
                    1.0,
                    0.8733006421895882,
                    0.7084745762711865
                ],
                [
                    0.21437847866419293,
                    0.0,
                    0.10124695896278495,
                    0.0,
                    0.1257681587831046,
                    0.13559322033898305
                ]
            ],
            "rate_limited": false,
            "answers": [
                "pope benedict",
                "eggs benedict",
                "benedict cumberbatch"
            ],
            "ml_answers": {
                "eggs benedict": 0.4907946128118355,
                "benedict cumberbatch": 0.15234782150870685,
                "pope benedict": 0.17020556054122035
            },
            "z-best_answer_by_ml": [
                "eggs benedict"
            ],
            "data": {
                "wikipedia_search": [
                    0.0,
                    5.499350649350649,
                    1.5006493506493506
                ],
                "word_count_entities": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.6418720438137915,
                    5.649399243446714,
                    0.7087287127394947
                ],
                "word_count_raw": [
                    0.0,
                    154.0,
                    0.0
                ],
                "word_count_appended": [
                    46.0,
                    209.0,
                    40.0
                ],
                "result_count": [
                    579.0,
                    543000.0,
                    78200.0
                ]
            },
            "integer_answers": {
                "eggs benedict": 6,
                "benedict cumberbatch": 0,
                "pope benedict": 0
            }
        },
        "lines": [
            [
                0,
                0.0,
                0.0,
                0.09169600625911307,
                0.0,
                0.00093119902730713,
                0.15593220338983052
            ],
            [
                1,
                0.7856215213358071,
                1.0,
                0.807057034778102,
                1.0,
                0.8733006421895882,
                0.7084745762711865
            ],
            [
                0,
                0.21437847866419293,
                0.0,
                0.10124695896278495,
                0.0,
                0.1257681587831046,
                0.13559322033898305
            ]
        ]
    },
    "Which Oscar-winning actress has NOT won the award for playing a real person?": {
        "raw_data": {
            "negative_question": true,
            "question": "Which Oscar-winning actress has NOT won the award for playing a real person?",
            "fraction_answers": {
                "hilary swank": 0.39158464177582136,
                "emma thompson": 0.22827235244962144,
                "susan sarandon": 0.3801430057745572
            },
            "lines": [
                [
                    0.1600394205661591,
                    0.5,
                    0.3939167754643945,
                    0.5,
                    0.47095829024474317,
                    0.2902684563758389
                ],
                [
                    0.3952511703368479,
                    0.25,
                    0.31009215652072797,
                    0.5,
                    0.05187866253016199,
                    0.3523489932885906
                ],
                [
                    0.44470940909699297,
                    0.25,
                    0.2959910680148775,
                    0.0,
                    0.4771630472250948,
                    0.3573825503355705
                ]
            ],
            "rate_limited": false,
            "answers": [
                "emma thompson",
                "susan sarandon",
                "hilary swank"
            ],
            "ml_answers": {
                "hilary swank": 0.3152528107479911,
                "emma thompson": 0.31635932426103247,
                "susan sarandon": 0.34261090066404887
            },
            "z-best_answer_by_ml": [
                "susan sarandon"
            ],
            "data": {
                "wikipedia_search": [
                    6.119290429809136,
                    1.8854789339367375,
                    0.9952306362541261
                ],
                "word_count_entities": [
                    0.0,
                    1.0,
                    1.0
                ],
                "word_relation_to_question": [
                    1.9094980416408984,
                    3.4183411826268966,
                    3.672160775732204
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_count_appended": [
                    125.0,
                    88.0,
                    85.0
                ],
                "result_count": [
                    337000.0,
                    5200000.0,
                    265000.0
                ]
            },
            "integer_answers": {
                "hilary swank": 2,
                "emma thompson": 2,
                "susan sarandon": 2
            }
        },
        "lines": [
            [
                1,
                0.1600394205661591,
                0.5,
                0.3939167754643945,
                0.5,
                0.47095829024474317,
                0.2902684563758389
            ],
            [
                0,
                0.3952511703368479,
                0.25,
                0.31009215652072797,
                0.5,
                0.05187866253016199,
                0.3523489932885906
            ],
            [
                0,
                0.44470940909699297,
                0.25,
                0.2959910680148775,
                0.0,
                0.4771630472250948,
                0.3573825503355705
            ]
        ]
    },
    "The best-selling book \u201cThe Chocolate War\u201d is about what?": {
        "raw_data": {
            "negative_question": false,
            "question": "The best-selling book \u201cThe Chocolate War\u201d is about what?",
            "fraction_answers": {
                "high school conformity": 0.18615123522161162,
                "the rise of hershey's": 0.34910119088344155,
                "sugar addiction": 0.4647475738949469
            },
            "lines": [
                [
                    0.061224489795918366,
                    0,
                    0.2396551724137931,
                    0,
                    0.18446601941747573,
                    0.25925925925925924
                ],
                [
                    0.2571428571428571,
                    0,
                    0.11964679118773947,
                    0,
                    0.8155339805825242,
                    0.6666666666666666
                ],
                [
                    0.6816326530612244,
                    0,
                    0.6406980363984675,
                    0,
                    0.0,
                    0.07407407407407407
                ]
            ],
            "rate_limited": false,
            "answers": [
                "high school conformity",
                "sugar addiction",
                "the rise of hershey's"
            ],
            "ml_answers": {
                "high school conformity": 0.1635357980098004,
                "the rise of hershey's": 0.27956865641542133,
                "sugar addiction": 0.2683065957716286
            },
            "z-best_answer_by_ml": [
                "the rise of hershey's"
            ],
            "data": {
                "wikipedia_search": [
                    0.30612244897959184,
                    1.2857142857142856,
                    3.4081632653061225
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.1982758620689655,
                    0.5982339559386973,
                    3.203490181992337
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    7.0,
                    18.0,
                    2.0
                ],
                "result_count": [
                    19.0,
                    84.0,
                    0
                ]
            },
            "integer_answers": {
                "high school conformity": 0,
                "the rise of hershey's": 2,
                "sugar addiction": 2
            }
        },
        "lines": [
            [
                1,
                0.061224489795918366,
                0,
                0.2396551724137931,
                0,
                0.18446601941747573,
                0.25925925925925924
            ],
            [
                0,
                0.2571428571428571,
                0,
                0.11964679118773947,
                0,
                0.8155339805825242,
                0.6666666666666666
            ],
            [
                0,
                0.6816326530612244,
                0,
                0.6406980363984675,
                0,
                0.0,
                0.07407407407407407
            ]
        ]
    },
    "The \u201cAmerican Craftsman\u201d style of house was an architectural reaction to what?": {
        "raw_data": {
            "negative_question": false,
            "question": "The \u201cAmerican Craftsman\u201d style of house was an architectural reaction to what?",
            "fraction_answers": {
                "world war i": 0.14129714674185,
                "industrial revolution": 0.7112422137078044,
                "the great depression": 0.14746063955034563
            },
            "lines": [
                [
                    0.45732900136798904,
                    0.0,
                    0.15736720834343926,
                    0.0,
                    0.23214285714285715,
                    0.0009438135968146291
                ],
                [
                    0.3689329685362517,
                    1.0,
                    0.2929996361872423,
                    1.0,
                    0.6071428571428571,
                    0.9983778203804748
                ],
                [
                    0.17373803009575922,
                    0.0,
                    0.5496331554693185,
                    0.0,
                    0.16071428571428573,
                    0.0006783660227105147
                ]
            ],
            "rate_limited": false,
            "answers": [
                "world war i",
                "industrial revolution",
                "the great depression"
            ],
            "ml_answers": {
                "world war i": 0.34753064011632384,
                "industrial revolution": 0.6974710810412397,
                "the great depression": 0.2037054274973088
            },
            "z-best_answer_by_ml": [
                "industrial revolution"
            ],
            "data": {
                "wikipedia_search": [
                    2.286645006839945,
                    1.8446648426812584,
                    0.868690150478796
                ],
                "word_count_entities": [
                    0.0,
                    11.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.7868360417171962,
                    1.4649981809362114,
                    2.748165777346592
                ],
                "word_count_raw": [
                    0.0,
                    11.0,
                    0.0
                ],
                "word_count_appended": [
                    26.0,
                    68.0,
                    18.0
                ],
                "result_count": [
                    64.0,
                    67700.0,
                    46.0
                ]
            },
            "integer_answers": {
                "world war i": 1,
                "industrial revolution": 4,
                "the great depression": 1
            }
        },
        "lines": [
            [
                0,
                0.45732900136798904,
                0.0,
                0.15736720834343926,
                0.0,
                0.23214285714285715,
                0.0009438135968146291
            ],
            [
                1,
                0.3689329685362517,
                1.0,
                0.2929996361872423,
                1.0,
                0.6071428571428571,
                0.9983778203804748
            ],
            [
                0,
                0.17373803009575922,
                0.0,
                0.5496331554693185,
                0.0,
                0.16071428571428573,
                0.0006783660227105147
            ]
        ]
    },
    "Which three-letter-titled movie grossed the most worldwide?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which three-letter-titled movie grossed the most worldwide?",
            "fraction_answers": {
                "big": 0.4270115069203378,
                "saw": 0.26990173927384925,
                "ray": 0.30308675380581296
            },
            "lines": [
                [
                    0.0,
                    0.2,
                    0.6848739495798319,
                    0.125,
                    0.3313106796116505,
                    0.2782258064516129
                ],
                [
                    0.0,
                    0.3,
                    0.3088235294117647,
                    0.625,
                    0.38106796116504854,
                    0.20362903225806453
                ],
                [
                    1.0,
                    0.5,
                    0.0063025210084033615,
                    0.25,
                    0.28762135922330095,
                    0.5181451612903226
                ]
            ],
            "rate_limited": false,
            "answers": [
                "saw",
                "ray",
                "big"
            ],
            "ml_answers": {
                "big": 0.30702099980201125,
                "saw": 0.27217902569104424,
                "ray": 0.30528240000679246
            },
            "z-best_answer_by_ml": [
                "big"
            ],
            "data": {
                "wikipedia_search": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_entities": [
                    2.0,
                    3.0,
                    5.0
                ],
                "word_relation_to_question": [
                    2.7394957983193278,
                    1.2352941176470589,
                    0.025210084033613446
                ],
                "word_count_raw": [
                    1.0,
                    5.0,
                    2.0
                ],
                "word_count_appended": [
                    273.0,
                    314.0,
                    237.0
                ],
                "result_count": [
                    13800000.0,
                    10100000.0,
                    25700000.0
                ]
            },
            "integer_answers": {
                "big": 3,
                "saw": 1,
                "ray": 2
            }
        },
        "lines": [
            [
                0,
                0.0,
                0.2,
                0.6848739495798319,
                0.125,
                0.3313106796116505,
                0.2782258064516129
            ],
            [
                0,
                0.0,
                0.3,
                0.3088235294117647,
                0.625,
                0.38106796116504854,
                0.20362903225806453
            ],
            [
                1,
                1.0,
                0.5,
                0.0063025210084033615,
                0.25,
                0.28762135922330095,
                0.5181451612903226
            ]
        ]
    },
    "Talking is discouraged on what Amtrak car?": {
        "raw_data": {
            "negative_question": false,
            "question": "Talking is discouraged on what Amtrak car?",
            "fraction_answers": {
                "sports argument car": 0.28438100925780463,
                "quiet car": 0.6675358265211793,
                "meet & greet car": 0.04808316422101615
            },
            "lines": [
                [
                    0.20454545454545456,
                    0,
                    0.13555383423702558,
                    0.0,
                    0.9893687827166268,
                    0.09243697478991597
                ],
                [
                    0.20454545454545456,
                    0,
                    0.025616908266017483,
                    0.0,
                    0.010253458293608677,
                    0.0
                ],
                [
                    0.5909090909090909,
                    0,
                    0.838829257496957,
                    1.0,
                    0.0003777589897645302,
                    0.907563025210084
                ]
            ],
            "rate_limited": false,
            "answers": [
                "sports argument car",
                "meet & greet car",
                "quiet car"
            ],
            "ml_answers": {
                "sports argument car": 0.3427119020555119,
                "quiet car": 0.5500332345130354,
                "meet & greet car": 0.15835525072679643
            },
            "integer_answers": {
                "sports argument car": 1,
                "quiet car": 4,
                "meet & greet car": 0
            },
            "data": {
                "wikipedia_search": [
                    0.4090909090909091,
                    0.4090909090909091,
                    1.1818181818181819
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.4066615027110767,
                    0.07685072479805245,
                    2.516487772490871
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    3.0
                ],
                "word_count_appended": [
                    11.0,
                    0.0,
                    108.0
                ],
                "result_count": [
                    2750000.0,
                    28500.0,
                    1050.0
                ]
            },
            "z-best_answer_by_ml": [
                "quiet car"
            ]
        },
        "lines": [
            [
                0,
                0.20454545454545456,
                0,
                0.13555383423702558,
                0.0,
                0.9893687827166268,
                0.09243697478991597
            ],
            [
                0,
                0.20454545454545456,
                0,
                0.025616908266017483,
                0.0,
                0.010253458293608677,
                0.0
            ],
            [
                1,
                0.5909090909090909,
                0,
                0.838829257496957,
                1.0,
                0.0003777589897645302,
                0.907563025210084
            ]
        ]
    },
    "The creator of Wonder Woman also created an early version of what device?": {
        "raw_data": {
            "negative_question": false,
            "question": "The creator of Wonder Woman also created an early version of what device?",
            "fraction_answers": {
                "lie detector": 0.6611808826844076,
                "hearing aid": 0.21202464760118112,
                "magic marker": 0.12679446971441136
            },
            "lines": [
                [
                    0.036281179138321996,
                    0.0,
                    0.213953488372093,
                    0.0,
                    0.15447154471544716,
                    0.3560606060606061
                ],
                [
                    0.34076886568580916,
                    0.0,
                    0.4627906976744186,
                    0.0,
                    0.3170731707317073,
                    0.15151515151515152
                ],
                [
                    0.6229499551758689,
                    1.0,
                    0.32325581395348835,
                    1.0,
                    0.5284552845528455,
                    0.49242424242424243
                ]
            ],
            "rate_limited": false,
            "answers": [
                "magic marker",
                "hearing aid",
                "lie detector"
            ],
            "ml_answers": {
                "lie detector": 0.6626115293007399,
                "hearing aid": 0.22166231992606952,
                "magic marker": 0.29388694720553604
            },
            "z-best_answer_by_ml": [
                "lie detector"
            ],
            "data": {
                "wikipedia_search": [
                    0.25396825396825395,
                    2.3853820598006643,
                    4.360649686231082
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    12.0
                ],
                "word_relation_to_question": [
                    1.069767441860465,
                    2.313953488372093,
                    1.6162790697674418
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    26.0
                ],
                "word_count_appended": [
                    19.0,
                    39.0,
                    65.0
                ],
                "result_count": [
                    47.0,
                    20.0,
                    65.0
                ]
            },
            "integer_answers": {
                "lie detector": 5,
                "hearing aid": 1,
                "magic marker": 0
            }
        },
        "lines": [
            [
                0,
                0.036281179138321996,
                0.0,
                0.213953488372093,
                0.0,
                0.15447154471544716,
                0.3560606060606061
            ],
            [
                0,
                0.34076886568580916,
                0.0,
                0.4627906976744186,
                0.0,
                0.3170731707317073,
                0.15151515151515152
            ],
            [
                1,
                0.6229499551758689,
                1.0,
                0.32325581395348835,
                1.0,
                0.5284552845528455,
                0.49242424242424243
            ]
        ]
    },
    "Which of these verbs has two meanings that are opposites of each other?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which of these verbs has two meanings that are opposites of each other?",
            "fraction_answers": {
                "cleave": 0.5695035858726979,
                "jut": 0.11694659645013208,
                "branch": 0.31354981767717
            },
            "lines": [
                [
                    0,
                    0.0,
                    0.0,
                    0,
                    0.9299524988291965,
                    0.3242467718794835
                ],
                [
                    0,
                    0.0,
                    0.16666666666666669,
                    0,
                    0.03713119689569813,
                    0.26398852223816355
                ],
                [
                    0,
                    1.0,
                    0.8333333333333334,
                    0,
                    0.03291630427510537,
                    0.4117647058823529
                ]
            ],
            "rate_limited": false,
            "answers": [
                "branch",
                "jut",
                "cleave"
            ],
            "ml_answers": {
                "cleave": 0.41157184476645353,
                "jut": 0.24112065656179138,
                "branch": 0.30375363902072383
            },
            "z-best_answer_by_ml": [
                "cleave"
            ],
            "data": {
                "wikipedia_search": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_relation_to_question": [
                    0.0,
                    0.3333333333333333,
                    1.6666666666666665
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    226.0,
                    184.0,
                    287.0
                ],
                "result_count": [
                    2780000.0,
                    111000.0,
                    98400.0
                ]
            },
            "integer_answers": {
                "cleave": 3,
                "jut": 0,
                "branch": 1
            }
        },
        "lines": [
            [
                0,
                0,
                0.0,
                0.0,
                0,
                0.9299524988291965,
                0.3242467718794835
            ],
            [
                0,
                0,
                0.0,
                0.16666666666666669,
                0,
                0.03713119689569813,
                0.26398852223816355
            ],
            [
                1,
                0,
                1.0,
                0.8333333333333334,
                0,
                0.03291630427510537,
                0.4117647058823529
            ]
        ]
    },
    "Which of these is NOT a real animal?": {
        "raw_data": {
            "negative_question": true,
            "question": "Which of these is NOT a real animal?",
            "fraction_answers": {
                "liger": 0.6123071484181161,
                "wholphin": 0.1074369453051574,
                "jackalope": 0.28025590627672653
            },
            "lines": [
                [
                    0.17317644495287904,
                    0.5,
                    0.3775401069518717,
                    0.5,
                    0.3233512786002692,
                    0.28516445066480056
                ],
                [
                    0.38127144958326525,
                    0.0,
                    0.21969696969696972,
                    0.0,
                    0.2849932705248991,
                    0.27711686494051785
                ],
                [
                    0.44555210546385576,
                    0.5,
                    0.40276292335115865,
                    0.5,
                    0.3916554508748318,
                    0.4377186843946816
                ]
            ],
            "rate_limited": false,
            "answers": [
                "jackalope",
                "liger",
                "wholphin"
            ],
            "ml_answers": {
                "liger": 0.342518442454169,
                "wholphin": 0.3938236163368131,
                "jackalope": 0.30876077107866445
            },
            "z-best_answer_by_ml": [
                "wholphin"
            ],
            "data": {
                "wikipedia_search": [
                    1.9609413302827259,
                    0.7123713025004086,
                    0.32668736721686553
                ],
                "word_count_entities": [
                    0.0,
                    9.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.73475935828877,
                    1.6818181818181817,
                    0.5834224598930482
                ],
                "word_count_raw": [
                    0.0,
                    2.0,
                    0.0
                ],
                "word_count_appended": [
                    525.0,
                    639.0,
                    322.0
                ],
                "result_count": [
                    614000.0,
                    637000.0,
                    178000.0
                ]
            },
            "integer_answers": {
                "liger": 5,
                "wholphin": 0,
                "jackalope": 1
            }
        },
        "lines": [
            [
                1,
                0.17317644495287904,
                0.5,
                0.3775401069518717,
                0.5,
                0.3233512786002692,
                0.28516445066480056
            ],
            [
                0,
                0.38127144958326525,
                0.0,
                0.21969696969696972,
                0.0,
                0.2849932705248991,
                0.27711686494051785
            ],
            [
                0,
                0.44555210546385576,
                0.5,
                0.40276292335115865,
                0.5,
                0.3916554508748318,
                0.4377186843946816
            ]
        ]
    },
    "Pixies, Bon Iver, Iron & Wine and Bauhaus were all once signed to which record label?": {
        "raw_data": {
            "negative_question": false,
            "question": "Pixies, Bon Iver, Iron & Wine and Bauhaus were all once signed to which record label?",
            "fraction_answers": {
                "4ad": 0.740410467795532,
                "geffen": 0.09393758217627057,
                "subpop": 0.16565195002819746
            },
            "lines": [
                [
                    0.3267619047619048,
                    0.0,
                    0.30881646207394653,
                    0.0,
                    0.3333333333333333,
                    0.025
                ],
                [
                    0.6703809523809524,
                    1.0,
                    0.4887485210589063,
                    1.0,
                    0.3333333333333333,
                    0.95
                ],
                [
                    0.002857142857142858,
                    0.0,
                    0.2024350168671472,
                    0.0,
                    0.3333333333333333,
                    0.025
                ]
            ],
            "rate_limited": false,
            "answers": [
                "subpop",
                "4ad",
                "geffen"
            ],
            "ml_answers": {
                "4ad": 0.6741303413092523,
                "geffen": 0.38559528099128254,
                "subpop": 0.22643281256576542
            },
            "z-best_answer_by_ml": [
                "4ad"
            ],
            "data": {
                "wikipedia_search": [
                    2.287333333333333,
                    4.692666666666666,
                    0.02
                ],
                "word_count_entities": [
                    0.0,
                    118.0,
                    0.0
                ],
                "word_relation_to_question": [
                    2.7793481586655187,
                    4.398736689530157,
                    1.8219151518043248
                ],
                "word_count_raw": [
                    0.0,
                    37.0,
                    0.0
                ],
                "word_count_appended": [
                    1.0,
                    38.0,
                    1.0
                ],
                "result_count": [
                    231000.0,
                    231000.0,
                    231000.0
                ]
            },
            "integer_answers": {
                "4ad": 5,
                "geffen": 0,
                "subpop": 1
            }
        },
        "lines": [
            [
                0,
                0.3267619047619048,
                0.0,
                0.30881646207394653,
                0.0,
                0.3333333333333333,
                0.025
            ],
            [
                1,
                0.6703809523809524,
                1.0,
                0.4887485210589063,
                1.0,
                0.3333333333333333,
                0.95
            ],
            [
                0,
                0.002857142857142858,
                0.0,
                0.2024350168671472,
                0.0,
                0.3333333333333333,
                0.025
            ]
        ]
    },
    "Which of these would an oologist study?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which of these would an oologist study?",
            "fraction_answers": {
                "ice cave": 0.08621056533318691,
                "human liver": 0.34585529868299425,
                "ostrich egg": 0.5679341359838189
            },
            "lines": [
                [
                    0.0,
                    0,
                    0.0,
                    0,
                    0.18604651162790697,
                    0.15879574970484062
                ],
                [
                    0.08333333333333333,
                    0,
                    1.0,
                    0,
                    0.2558139534883721,
                    0.04427390791027155
                ],
                [
                    0.9166666666666666,
                    0,
                    0.0,
                    0,
                    0.5581395348837209,
                    0.7969303423848878
                ]
            ],
            "rate_limited": false,
            "answers": [
                "ice cave",
                "human liver",
                "ostrich egg"
            ],
            "ml_answers": {
                "ice cave": 0.1633633598737923,
                "human liver": 0.3091917814551401,
                "ostrich egg": 0.19379003420387866
            },
            "z-best_answer_by_ml": [
                "human liver"
            ],
            "data": {
                "wikipedia_search": [
                    0.0,
                    0.08333333333333333,
                    0.9166666666666666
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    16.0,
                    22.0,
                    48.0
                ],
                "result_count": [
                    269.0,
                    75.0,
                    1350.0
                ]
            },
            "integer_answers": {
                "ice cave": 0,
                "human liver": 1,
                "ostrich egg": 3
            }
        },
        "lines": [
            [
                0,
                0.0,
                0,
                0.0,
                0,
                0.18604651162790697,
                0.15879574970484062
            ],
            [
                0,
                0.08333333333333333,
                0,
                1.0,
                0,
                0.2558139534883721,
                0.04427390791027155
            ],
            [
                1,
                0.9166666666666666,
                0,
                0.0,
                0,
                0.5581395348837209,
                0.7969303423848878
            ]
        ]
    },
    "Who holds the record as the youngest solo artist with a Billboard #1 hit?": {
        "raw_data": {
            "negative_question": false,
            "question": "Who holds the record as the youngest solo artist with a Billboard #1 hit?",
            "fraction_answers": {
                "justin bieber": 0.35741539515079684,
                "stevie wonder": 0.3002756286969435,
                "michael jackson": 0.3423089761522596
            },
            "lines": [
                [
                    0.31720101247269517,
                    0.4482758620689655,
                    0.3190695017636604,
                    0.36363636363636365,
                    0.36363636363636365,
                    0.3326732673267327
                ],
                [
                    0.5286376111353471,
                    0.20689655172413793,
                    0.3492827903637037,
                    0.3181818181818182,
                    0.3181818181818182,
                    0.3326732673267327
                ],
                [
                    0.1541613763919577,
                    0.3448275862068966,
                    0.331647707872636,
                    0.3181818181818182,
                    0.3181818181818182,
                    0.3346534653465347
                ]
            ],
            "rate_limited": false,
            "answers": [
                "justin bieber",
                "michael jackson",
                "stevie wonder"
            ],
            "ml_answers": {
                "justin bieber": 0.3661841931490502,
                "stevie wonder": 0.21705482940764387,
                "michael jackson": 0.21505140920037064
            },
            "integer_answers": {
                "justin bieber": 3,
                "stevie wonder": 1,
                "michael jackson": 2
            },
            "data": {
                "wikipedia_search": [
                    2.8548091122542565,
                    4.7577385002181245,
                    1.3874523875276192
                ],
                "word_count_entities": [
                    13.0,
                    6.0,
                    10.0
                ],
                "word_relation_to_question": [
                    2.552556014109283,
                    2.7942623229096295,
                    2.653181662981088
                ],
                "word_count_raw": [
                    8.0,
                    7.0,
                    7.0
                ],
                "word_count_appended": [
                    8.0,
                    7.0,
                    7.0
                ],
                "result_count": [
                    1680000.0,
                    1680000.0,
                    1690000.0
                ]
            },
            "z-best_answer_by_ml": [
                "justin bieber"
            ]
        },
        "lines": [
            [
                0,
                0.31720101247269517,
                0.4482758620689655,
                0.3190695017636604,
                0.36363636363636365,
                0.36363636363636365,
                0.3326732673267327
            ],
            [
                0,
                0.5286376111353471,
                0.20689655172413793,
                0.3492827903637037,
                0.3181818181818182,
                0.3181818181818182,
                0.3326732673267327
            ],
            [
                1,
                0.1541613763919577,
                0.3448275862068966,
                0.331647707872636,
                0.3181818181818182,
                0.3181818181818182,
                0.3346534653465347
            ]
        ]
    },
    "Which of these states does NOT touch the Mason-Dixon Line?": {
        "raw_data": {
            "negative_question": true,
            "question": "Which of these states does NOT touch the Mason-Dixon Line?",
            "fraction_answers": {
                "tennessee": 0.21653242618530086,
                "delaware": 0.47380831350908753,
                "west virginia": 0.3096592603056117
            },
            "lines": [
                [
                    0.321084960584732,
                    0.3833333333333333,
                    0.42063781637441516,
                    0.375,
                    0.4427860696517413,
                    0.12818003913894327
                ],
                [
                    0.3378531653578983,
                    0.15000000000000002,
                    0.25137837199938917,
                    0.1875,
                    0.23631840796019898,
                    0.4155251141552512
                ],
                [
                    0.34106187405736976,
                    0.4666666666666667,
                    0.3279838116261957,
                    0.4375,
                    0.32089552238805974,
                    0.4562948467058056
                ]
            ],
            "rate_limited": false,
            "answers": [
                "west virginia",
                "delaware",
                "tennessee"
            ],
            "ml_answers": {
                "tennessee": 0.3685140269882145,
                "delaware": 0.18935423616184024,
                "west virginia": 0.2573756387805523
            },
            "z-best_answer_by_ml": [
                "tennessee"
            ],
            "data": {
                "wikipedia_search": [
                    1.0734902364916081,
                    0.9728810078526103,
                    0.9536287556557816
                ],
                "word_count_entities": [
                    7.0,
                    21.0,
                    2.0
                ],
                "word_relation_to_question": [
                    0.9523462035070183,
                    2.9834595360073304,
                    2.0641942604856514
                ],
                "word_count_raw": [
                    4.0,
                    10.0,
                    2.0
                ],
                "word_count_appended": [
                    69.0,
                    318.0,
                    216.0
                ],
                "result_count": [
                    1140000.0,
                    259000.0,
                    134000.0
                ]
            },
            "integer_answers": {
                "tennessee": 0,
                "delaware": 4,
                "west virginia": 2
            }
        },
        "lines": [
            [
                0,
                0.321084960584732,
                0.3833333333333333,
                0.42063781637441516,
                0.375,
                0.4427860696517413,
                0.12818003913894327
            ],
            [
                0,
                0.3378531653578983,
                0.15000000000000002,
                0.25137837199938917,
                0.1875,
                0.23631840796019898,
                0.4155251141552512
            ],
            [
                1,
                0.34106187405736976,
                0.4666666666666667,
                0.3279838116261957,
                0.4375,
                0.32089552238805974,
                0.4562948467058056
            ]
        ]
    },
    "To help first create Maps, Google acquired what company?": {
        "raw_data": {
            "negative_question": false,
            "question": "To help first create Maps, Google acquired what company?",
            "fraction_answers": {
                "mapquest": 0.19104645251877914,
                "waze": 0.5513771467427756,
                "where 2 technologies": 0.2575764007384452
            },
            "lines": [
                [
                    0.10200665237429944,
                    0.21739130434782608,
                    0.2883971342813841,
                    0.10204081632653061,
                    0.35278969957081546,
                    0.08365310821181889
                ],
                [
                    0.7842139097653803,
                    0.043478260869565216,
                    0.5379876711629941,
                    0.08163265306122448,
                    0.06437768240343347,
                    0.03376822716807368
                ],
                [
                    0.1137794378603202,
                    0.7391304347826086,
                    0.17361519455562177,
                    0.8163265306122449,
                    0.582832618025751,
                    0.8825786646201075
                ]
            ],
            "rate_limited": false,
            "answers": [
                "mapquest",
                "where 2 technologies",
                "waze"
            ],
            "ml_answers": {
                "mapquest": 0.24747457234351977,
                "waze": 0.6091256168685527,
                "where 2 technologies": 0.283848400594656
            },
            "integer_answers": {
                "mapquest": 0,
                "waze": 4,
                "where 2 technologies": 2
            },
            "data": {
                "wikipedia_search": [
                    0.6120399142457966,
                    4.705283458592282,
                    0.6826766271619212
                ],
                "word_count_entities": [
                    5.0,
                    1.0,
                    17.0
                ],
                "word_relation_to_question": [
                    1.7303828056883048,
                    3.227926026977965,
                    1.0416911673337308
                ],
                "word_count_raw": [
                    5.0,
                    4.0,
                    40.0
                ],
                "word_count_appended": [
                    411.0,
                    75.0,
                    679.0
                ],
                "result_count": [
                    109000.0,
                    44000.0,
                    1150000.0
                ]
            },
            "z-best_answer_by_ml": [
                "waze"
            ]
        },
        "lines": [
            [
                0,
                0.10200665237429944,
                0.21739130434782608,
                0.2883971342813841,
                0.10204081632653061,
                0.35278969957081546,
                0.08365310821181889
            ],
            [
                1,
                0.7842139097653803,
                0.043478260869565216,
                0.5379876711629941,
                0.08163265306122448,
                0.06437768240343347,
                0.03376822716807368
            ],
            [
                0,
                0.1137794378603202,
                0.7391304347826086,
                0.17361519455562177,
                0.8163265306122449,
                0.582832618025751,
                0.8825786646201075
            ]
        ]
    },
    "The lyrics to \u201cThe Star-Spangled Banner\u201d were written during what conflict?": {
        "raw_data": {
            "negative_question": false,
            "question": "The lyrics to \u201cThe Star-Spangled Banner\u201d were written during what conflict?",
            "fraction_answers": {
                "american revolution": 0.14689565727894285,
                "the war of 1812": 0.6349142001699736,
                "the civil war": 0.21819014255108357
            },
            "lines": [
                [
                    0.19883040935672514,
                    0.14285714285714285,
                    0.13081632653061223,
                    0.07407407407407407,
                    0.292817679558011,
                    0.4697452229299363
                ],
                [
                    0.09466374269005846,
                    0.0,
                    0.16127602556173984,
                    0.07407407407407407,
                    0.2265193370165746,
                    0.3248407643312102
                ],
                [
                    0.7065058479532164,
                    0.8571428571428571,
                    0.7079076479076478,
                    0.8518518518518519,
                    0.48066298342541436,
                    0.2054140127388535
                ]
            ],
            "rate_limited": false,
            "answers": [
                "the civil war",
                "american revolution",
                "the war of 1812"
            ],
            "ml_answers": {
                "american revolution": 0.2771486682697412,
                "the war of 1812": 0.4946959458533706,
                "the civil war": 0.21742402595176646
            },
            "integer_answers": {
                "american revolution": 0,
                "the war of 1812": 5,
                "the civil war": 1
            },
            "data": {
                "wikipedia_search": [
                    1.1929824561403508,
                    0.5679824561403508,
                    4.239035087719298
                ],
                "word_count_entities": [
                    1.0,
                    0.0,
                    6.0
                ],
                "word_relation_to_question": [
                    0.6540816326530612,
                    0.8063801278086992,
                    3.5395382395382393
                ],
                "word_count_raw": [
                    2.0,
                    2.0,
                    23.0
                ],
                "word_count_appended": [
                    53.0,
                    41.0,
                    87.0
                ],
                "result_count": [
                    29500.0,
                    20400.0,
                    12900.0
                ]
            },
            "z-best_answer_by_ml": [
                "the war of 1812"
            ]
        },
        "lines": [
            [
                0,
                0.19883040935672514,
                0.14285714285714285,
                0.13081632653061223,
                0.07407407407407407,
                0.292817679558011,
                0.4697452229299363
            ],
            [
                0,
                0.09466374269005846,
                0.0,
                0.16127602556173984,
                0.07407407407407407,
                0.2265193370165746,
                0.3248407643312102
            ],
            [
                1,
                0.7065058479532164,
                0.8571428571428571,
                0.7079076479076478,
                0.8518518518518519,
                0.48066298342541436,
                0.2054140127388535
            ]
        ]
    },
    "Where do marsupials keep their undeveloped young?": {
        "raw_data": {
            "negative_question": false,
            "question": "Where do marsupials keep their undeveloped young?",
            "fraction_answers": {
                "in their pouches": 0.3430349717469664,
                "underwater": 0.6435440674433,
                "in a paper bag": 0.013420960809733535
            },
            "lines": [
                [
                    0.75,
                    0,
                    0.8383838383838383,
                    0.0,
                    0.12244897959183673,
                    0.0043420407591568035
                ],
                [
                    0.0,
                    0,
                    0.04040404040404041,
                    0.0,
                    0.025510204081632654,
                    0.0011905595629946074
                ],
                [
                    0.25,
                    0,
                    0.12121212121212122,
                    1.0,
                    0.8520408163265306,
                    0.9944673996778486
                ]
            ],
            "rate_limited": false,
            "answers": [
                "in their pouches",
                "in a paper bag",
                "underwater"
            ],
            "ml_answers": {
                "in their pouches": 0.28673527433510143,
                "underwater": 0.5602770593343848,
                "in a paper bag": 0.17612355908568428
            },
            "z-best_answer_by_ml": [
                "underwater"
            ],
            "data": {
                "wikipedia_search": [
                    0.75,
                    0.0,
                    0.25
                ],
                "word_count_entities": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    2.515151515151515,
                    0.12121212121212122,
                    0.36363636363636365
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_appended": [
                    24.0,
                    5.0,
                    167.0
                ],
                "result_count": [
                    62.0,
                    17.0,
                    14200.0
                ]
            },
            "integer_answers": {
                "in their pouches": 2,
                "underwater": 3,
                "in a paper bag": 0
            }
        },
        "lines": [
            [
                1,
                0.75,
                0,
                0.8383838383838383,
                0.0,
                0.12244897959183673,
                0.0043420407591568035
            ],
            [
                0,
                0.0,
                0,
                0.04040404040404041,
                0.0,
                0.025510204081632654,
                0.0011905595629946074
            ],
            [
                0,
                0.25,
                0,
                0.12121212121212122,
                1.0,
                0.8520408163265306,
                0.9944673996778486
            ]
        ]
    },
    "Mount Rushmore was named after a person with what profession?": {
        "raw_data": {
            "negative_question": false,
            "question": "Mount Rushmore was named after a person with what profession?",
            "fraction_answers": {
                "architect": 0.22709189451797576,
                "prospector": 0.16485543965918523,
                "lawyer": 0.608052665822839
            },
            "lines": [
                [
                    0.265625,
                    0.0,
                    0.26315789473684215,
                    0.0,
                    0.2855245683930943,
                    0.17482517482517482
                ],
                [
                    0.6437190594059405,
                    1.0,
                    0.26973684210526316,
                    1.0,
                    0.35723771580345287,
                    0.3776223776223776
                ],
                [
                    0.0906559405940594,
                    0.0,
                    0.4671052631578947,
                    0.0,
                    0.35723771580345287,
                    0.44755244755244755
                ]
            ],
            "rate_limited": false,
            "answers": [
                "prospector",
                "lawyer",
                "architect"
            ],
            "ml_answers": {
                "architect": 0.18940020440691968,
                "prospector": 0.16795171531683245,
                "lawyer": 0.6626115293007399
            },
            "integer_answers": {
                "architect": 2,
                "prospector": 0,
                "lawyer": 4
            },
            "data": {
                "wikipedia_search": [
                    1.0625,
                    2.574876237623762,
                    0.3626237623762376
                ],
                "word_count_entities": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.0526315789473686,
                    1.0789473684210527,
                    1.8684210526315788
                ],
                "word_count_raw": [
                    0.0,
                    7.0,
                    0.0
                ],
                "word_count_appended": [
                    215.0,
                    269.0,
                    269.0
                ],
                "result_count": [
                    50000.0,
                    108000.0,
                    128000.0
                ]
            },
            "z-best_answer_by_ml": [
                "lawyer"
            ]
        },
        "lines": [
            [
                0,
                0.265625,
                0.0,
                0.26315789473684215,
                0.0,
                0.2855245683930943,
                0.17482517482517482
            ],
            [
                1,
                0.6437190594059405,
                1.0,
                0.26973684210526316,
                1.0,
                0.35723771580345287,
                0.3776223776223776
            ],
            [
                0,
                0.0906559405940594,
                0.0,
                0.4671052631578947,
                0.0,
                0.35723771580345287,
                0.44755244755244755
            ]
        ]
    },
    "Which verb describes the sound minerals make when they are heated?": {
        "raw_data": {
            "negative_question": false,
            "question": "Which verb describes the sound minerals make when they are heated?",
            "fraction_answers": {
                "frangelle": 0.022725979052823315,
                "decrepitate": 0.9360295233758348,
                "recleft": 0.041244497571341826
            },
            "lines": [
                [
                    1.0,
                    1.0,
                    0.8784722222222222,
                    1.0,
                    0.7377049180327869,
                    1.0
                ],
                [
                    0.0,
                    0.0,
                    0.005208333333333333,
                    0.0,
                    0.13114754098360656,
                    0.0
                ],
                [
                    0.0,
                    0.0,
                    0.11631944444444443,
                    0.0,
                    0.13114754098360656,
                    0.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "decrepitate",
                "frangelle",
                "recleft"
            ],
            "ml_answers": {
                "frangelle": 0.16090161610956707,
                "decrepitate": 0.5796330537928813,
                "recleft": 0.17586674981074818
            },
            "integer_answers": {
                "frangelle": 0,
                "decrepitate": 6,
                "recleft": 0
            },
            "data": {
                "wikipedia_search": [
                    2.0,
                    0.0,
                    0.0
                ],
                "word_count_entities": [
                    10.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    3.513888888888889,
                    0.020833333333333332,
                    0.46527777777777773
                ],
                "word_count_raw": [
                    6.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    90.0,
                    16.0,
                    16.0
                ],
                "result_count": [
                    4360.0,
                    0,
                    0
                ]
            },
            "z-best_answer_by_ml": [
                "decrepitate"
            ]
        },
        "lines": [
            [
                1,
                1.0,
                1.0,
                0.8784722222222222,
                1.0,
                0.7377049180327869,
                1.0
            ],
            [
                0,
                0.0,
                0.0,
                0.005208333333333333,
                0.0,
                0.13114754098360656,
                0.0
            ],
            [
                0,
                0.0,
                0.0,
                0.11631944444444443,
                0.0,
                0.13114754098360656,
                0.0
            ]
        ]
    },
    "Which of these celebrities has NOT been a ProActiv spokesperson?": {
        "raw_data": {
            "negative_question": true,
            "question": "Which of these celebrities has NOT been a ProActiv spokesperson?",
            "fraction_answers": {
                "selena gomez": 0.16990719164331156,
                "katy perry": 0.38687567521026495,
                "lindsay lohan": 0.44321713314642347
            },
            "lines": [
                [
                    0.41156311908430554,
                    0.09375,
                    0.4496692551049756,
                    0.05882352941176472,
                    0.32666666666666666,
                    0.4989004041014927
                ],
                [
                    0.26316207627118643,
                    0.40625,
                    0.2551049755536382,
                    0.4411764705882353,
                    0.3022222222222222,
                    0.002432855925447397
                ],
                [
                    0.32527480464450803,
                    0.5,
                    0.2952257693413862,
                    0.5,
                    0.3711111111111111,
                    0.4986667399730599
                ]
            ],
            "rate_limited": false,
            "answers": [
                "katy perry",
                "lindsay lohan",
                "selena gomez"
            ],
            "ml_answers": {
                "selena gomez": 0.3723718780473874,
                "katy perry": 0.351876424231821,
                "lindsay lohan": 0.2818914538802343
            },
            "integer_answers": {
                "selena gomez": 0,
                "katy perry": 2,
                "lindsay lohan": 4
            },
            "data": {
                "wikipedia_search": [
                    0.7074950473255558,
                    1.8947033898305086,
                    1.3978015628439358
                ],
                "word_count_entities": [
                    13.0,
                    3.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.30198446937014667,
                    1.4693701466781708,
                    1.2286453839516824
                ],
                "word_count_raw": [
                    15.0,
                    2.0,
                    0.0
                ],
                "word_count_appended": [
                    78.0,
                    89.0,
                    58.0
                ],
                "result_count": [
                    80.0,
                    36200.0,
                    97.0
                ]
            },
            "z-best_answer_by_ml": [
                "selena gomez"
            ]
        },
        "lines": [
            [
                0,
                0.41156311908430554,
                0.09375,
                0.4496692551049756,
                0.05882352941176472,
                0.32666666666666666,
                0.4989004041014927
            ],
            [
                0,
                0.26316207627118643,
                0.40625,
                0.2551049755536382,
                0.4411764705882353,
                0.3022222222222222,
                0.002432855925447397
            ],
            [
                1,
                0.32527480464450803,
                0.5,
                0.2952257693413862,
                0.5,
                0.3711111111111111,
                0.4986667399730599
            ]
        ]
    },
    "Which of these do NOT have flippers?": {
        "raw_data": {
            "negative_question": true,
            "question": "Which of these do NOT have flippers?",
            "fraction_answers": {
                "new yorkers": 0.0984732118464426,
                "pinball machines": 0.3439209615659691,
                "dolphins": 0.5576058265875884
            },
            "lines": [
                [
                    0.4520420792079208,
                    0.5,
                    0.4019607843137255,
                    0.5,
                    0.44260204081632654,
                    0.40797546012269936
                ],
                [
                    0.30426980198019804,
                    0.33333333333333337,
                    0.45588235294117646,
                    0.0,
                    0.117984693877551,
                    0.11571233810497616
                ],
                [
                    0.24368811881188118,
                    0.16666666666666669,
                    0.14215686274509803,
                    0.5,
                    0.43941326530612246,
                    0.4763122017723245
                ]
            ],
            "rate_limited": false,
            "answers": [
                "new yorkers",
                "dolphins",
                "pinball machines"
            ],
            "ml_answers": {
                "new yorkers": 0.3676729034374332,
                "pinball machines": 0.2737599642480353,
                "dolphins": 0.20082239803204546
            },
            "integer_answers": {
                "new yorkers": 0,
                "pinball machines": 3,
                "dolphins": 3
            },
            "data": {
                "wikipedia_search": [
                    0.19183168316831684,
                    0.7829207920792078,
                    1.0252475247524753
                ],
                "word_count_entities": [
                    0.0,
                    2.0,
                    4.0
                ],
                "word_relation_to_question": [
                    0.39215686274509803,
                    0.17647058823529413,
                    1.4313725490196079
                ],
                "word_count_raw": [
                    0.0,
                    20.0,
                    0.0
                ],
                "word_count_appended": [
                    90.0,
                    599.0,
                    95.0
                ],
                "result_count": [
                    108000.0,
                    451000.0,
                    27800.0
                ]
            },
            "z-best_answer_by_ml": [
                "new yorkers"
            ]
        },
        "lines": [
            [
                1,
                0.4520420792079208,
                0.5,
                0.4019607843137255,
                0.5,
                0.44260204081632654,
                0.40797546012269936
            ],
            [
                0,
                0.30426980198019804,
                0.33333333333333337,
                0.45588235294117646,
                0.0,
                0.117984693877551,
                0.11571233810497616
            ],
            [
                0,
                0.24368811881188118,
                0.16666666666666669,
                0.14215686274509803,
                0.5,
                0.43941326530612246,
                0.4763122017723245
            ]
        ]
    },
    "In \u201cPeanuts,\u201d what breed of dog is Snoopy?": {
        "raw_data": {
            "negative_question": false,
            "question": "In \u201cPeanuts,\u201d what breed of dog is Snoopy?",
            "fraction_answers": {
                "beagle": 0.6330799402035566,
                "pitbull": 0.1079634328497691,
                "border collie": 0.2589566269466743
            },
            "lines": [
                [
                    0.3698524365133837,
                    0.0,
                    0.3093799702426761,
                    0.0,
                    0.08853575482406356,
                    0.7859716000999225
                ],
                [
                    0.31762611530542206,
                    0.008264462809917356,
                    0.10393899748601919,
                    0.0,
                    0.21793416572077184,
                    1.685577648407063e-05
                ],
                [
                    0.3125214481811942,
                    0.9917355371900827,
                    0.5866810322713047,
                    1.0,
                    0.6935300794551645,
                    0.21401154412359338
                ]
            ],
            "rate_limited": false,
            "answers": [
                "border collie",
                "pitbull",
                "beagle"
            ],
            "ml_answers": {
                "beagle": 0.46319333337145924,
                "pitbull": 0.18604112235449977,
                "border collie": 0.2797516682352696
            },
            "z-best_answer_by_ml": [
                "beagle"
            ],
            "data": {
                "wikipedia_search": [
                    0.7397048730267674,
                    0.6352522306108441,
                    0.6250428963623884
                ],
                "word_count_entities": [
                    0.0,
                    1.0,
                    120.0
                ],
                "word_relation_to_question": [
                    0.9281399107280284,
                    0.31181699245805755,
                    1.7600430968139142
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    126.0
                ],
                "word_count_appended": [
                    78.0,
                    192.0,
                    611.0
                ],
                "result_count": [
                    4150000.0,
                    89.0,
                    1130000.0
                ]
            },
            "integer_answers": {
                "beagle": 4,
                "pitbull": 0,
                "border collie": 2
            }
        },
        "lines": [
            [
                0,
                0.3698524365133837,
                0.0,
                0.3093799702426761,
                0.0,
                0.08853575482406356,
                0.7859716000999225
            ],
            [
                0,
                0.31762611530542206,
                0.008264462809917356,
                0.10393899748601919,
                0.0,
                0.21793416572077184,
                1.685577648407063e-05
            ],
            [
                1,
                0.3125214481811942,
                0.9917355371900827,
                0.5866810322713047,
                1.0,
                0.6935300794551645,
                0.21401154412359338
            ]
        ]
    },
    "The '70s sitcom \u201cThree's Company\u201d was about three people who were what?": {
        "raw_data": {
            "negative_question": false,
            "question": "The '70s sitcom \u201cThree's Company\u201d was about three people who were what?",
            "fraction_answers": {
                "trivia show hosts": 0.13136644293990643,
                "roommates": 0.7652682051485821,
                "panda bears": 0.10336535191151153
            },
            "lines": [
                [
                    0.44871794871794873,
                    1.0,
                    0.18786764705882356,
                    1.0,
                    0.9551569506726457,
                    0.9998666844420744
                ],
                [
                    0.05555555555555555,
                    0.0,
                    0.7147058823529412,
                    0.0,
                    0.017937219730941704,
                    0.0
                ],
                [
                    0.49572649572649574,
                    0.0,
                    0.09742647058823531,
                    0.0,
                    0.026905829596412557,
                    0.0001333155579256099
                ]
            ],
            "rate_limited": false,
            "answers": [
                "roommates",
                "trivia show hosts",
                "panda bears"
            ],
            "ml_answers": {
                "trivia show hosts": 0.27658466940904736,
                "roommates": 0.685280477012364,
                "panda bears": 0.2426390969513081
            },
            "z-best_answer_by_ml": [
                "roommates"
            ],
            "data": {
                "wikipedia_search": [
                    1.3461538461538463,
                    0.16666666666666666,
                    1.4871794871794872
                ],
                "word_count_entities": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_relation_to_question": [
                    0.7514705882352941,
                    2.8588235294117643,
                    0.3897058823529412
                ],
                "word_count_raw": [
                    5.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    213.0,
                    4.0,
                    6.0
                ],
                "result_count": [
                    180000.0,
                    0,
                    24.0
                ]
            },
            "integer_answers": {
                "trivia show hosts": 1,
                "roommates": 4,
                "panda bears": 1
            }
        },
        "lines": [
            [
                1,
                0.44871794871794873,
                1.0,
                0.18786764705882356,
                1.0,
                0.9551569506726457,
                0.9998666844420744
            ],
            [
                0,
                0.05555555555555555,
                0.0,
                0.7147058823529412,
                0.0,
                0.017937219730941704,
                0.0
            ],
            [
                0,
                0.49572649572649574,
                0.0,
                0.09742647058823531,
                0.0,
                0.026905829596412557,
                0.0001333155579256099
            ]
        ]
    },
    "What was the first popular home video game?": {
        "raw_data": {
            "negative_question": false,
            "question": "What was the first popular home video game?",
            "fraction_answers": {
                "tekken 2": 0.09596939174315428,
                "pong": 0.7657773296740685,
                "half-life 3": 0.1382532785827771
            },
            "lines": [
                [
                    0.0763193161706173,
                    0.0,
                    0.3450876921356626,
                    0.0,
                    0.1544011544011544,
                    8.187751491382225e-06
                ],
                [
                    0.5660520195836181,
                    1.0,
                    0.3443271416979904,
                    1.0,
                    0.6984126984126984,
                    0.9858721183501046
                ],
                [
                    0.35762866424576456,
                    0.0,
                    0.3105851661663469,
                    0.0,
                    0.1471861471861472,
                    0.01411969389840404
                ]
            ],
            "rate_limited": false,
            "answers": [
                "tekken 2",
                "pong",
                "half-life 3"
            ],
            "ml_answers": {
                "tekken 2": 0.15932892707252647,
                "pong": 0.6562137530050356,
                "half-life 3": 0.24937540114114184
            },
            "integer_answers": {
                "tekken 2": 1,
                "pong": 5,
                "half-life 3": 0
            },
            "data": {
                "wikipedia_search": [
                    0.38159658085308645,
                    2.83026009791809,
                    1.788143321228823
                ],
                "word_count_entities": [
                    0.0,
                    28.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.725438460678313,
                    1.721635708489952,
                    1.5529258308317346
                ],
                "word_count_raw": [
                    0.0,
                    28.0,
                    0.0
                ],
                "word_count_appended": [
                    107.0,
                    484.0,
                    102.0
                ],
                "result_count": [
                    98.0,
                    11800000.0,
                    169000.0
                ]
            },
            "z-best_answer_by_ml": [
                "pong"
            ]
        },
        "lines": [
            [
                0,
                0.0763193161706173,
                0.0,
                0.3450876921356626,
                0.0,
                0.1544011544011544,
                8.187751491382225e-06
            ],
            [
                1,
                0.5660520195836181,
                1.0,
                0.3443271416979904,
                1.0,
                0.6984126984126984,
                0.9858721183501046
            ],
            [
                0,
                0.35762866424576456,
                0.0,
                0.3105851661663469,
                0.0,
                0.1471861471861472,
                0.01411969389840404
            ]
        ]
    },
    "Featuring 20 scoops of ice cream, the Vermonster is found on what chain's menu?": {
        "raw_data": {
            "negative_question": false,
            "question": "Featuring 20 scoops of ice cream, the Vermonster is found on what chain's menu?",
            "fraction_answers": {
                "ben & jerry's": 0.2584936758221549,
                "dairy queen": 0.30157028968428146,
                "baskin-robbins": 0.4399360344935636
            },
            "lines": [
                [
                    0.15921843066037847,
                    0.6666666666666666,
                    0.5774962667736584,
                    0.5,
                    0.0556792873051225,
                    0.6805555555555556
                ],
                [
                    0.36529698691253404,
                    0.3333333333333333,
                    0.22675900014143507,
                    0.5,
                    0.0645879732739421,
                    0.3194444444444444
                ],
                [
                    0.47548458242708747,
                    0.0,
                    0.19574473308490647,
                    0.0,
                    0.8797327394209354,
                    0.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "baskin-robbins",
                "dairy queen",
                "ben & jerry's"
            ],
            "ml_answers": {
                "ben & jerry's": 0.4203273668918247,
                "dairy queen": 0.3631741424862343,
                "baskin-robbins": 0.30433034414409627
            },
            "integer_answers": {
                "ben & jerry's": 2,
                "dairy queen": 0,
                "baskin-robbins": 4
            },
            "data": {
                "wikipedia_search": [
                    1.2737474452830277,
                    2.9223758953002723,
                    3.8038766594166997
                ],
                "word_count_entities": [
                    2.0,
                    1.0,
                    0.0
                ],
                "word_relation_to_question": [
                    4.619970134189267,
                    1.8140720011314806,
                    1.5659578646792518
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    0.0
                ],
                "word_count_appended": [
                    49.0,
                    23.0,
                    0.0
                ],
                "result_count": [
                    25.0,
                    29.0,
                    395.0
                ]
            },
            "z-best_answer_by_ml": [
                "ben & jerry's"
            ]
        },
        "lines": [
            [
                0,
                0.15921843066037847,
                0.6666666666666666,
                0.5774962667736584,
                0.5,
                0.0556792873051225,
                0.6805555555555556
            ],
            [
                0,
                0.36529698691253404,
                0.3333333333333333,
                0.22675900014143507,
                0.5,
                0.0645879732739421,
                0.3194444444444444
            ],
            [
                1,
                0.47548458242708747,
                0.0,
                0.19574473308490647,
                0.0,
                0.8797327394209354,
                0.0
            ]
        ]
    },
    "The U.S. has never had a Miss America from what state?": {
        "raw_data": {
            "negative_question": false,
            "question": "The U.S. has never had a Miss America from what state?",
            "fraction_answers": {
                "nebraska": 0.3186949692680649,
                "new mexico": 0.2205798428331981,
                "north dakota": 0.460725187898737
            },
            "lines": [
                [
                    0.36239015231934196,
                    0.0,
                    0.33219564180826733,
                    0.10714285714285714,
                    0.1360153256704981,
                    0.38573508005822416
                ],
                [
                    0.41010432147332226,
                    1.0,
                    0.3353388914005844,
                    0.5357142857142857,
                    0.26436781609195403,
                    0.2188258127122756
                ],
                [
                    0.22750552620733577,
                    0.0,
                    0.3324654667911483,
                    0.35714285714285715,
                    0.5996168582375478,
                    0.39543910722950026
                ]
            ],
            "rate_limited": false,
            "answers": [
                "new mexico",
                "north dakota",
                "nebraska"
            ],
            "ml_answers": {
                "nebraska": 0.2122875607726803,
                "new mexico": 0.2678562037896774,
                "north dakota": 0.47325538124713806
            },
            "z-best_answer_by_ml": [
                "north dakota"
            ],
            "data": {
                "wikipedia_search": [
                    1.0871704569580258,
                    1.2303129644199668,
                    0.6825165786220073
                ],
                "word_count_entities": [
                    0.0,
                    46.0,
                    0.0
                ],
                "word_relation_to_question": [
                    1.3287825672330693,
                    1.3413555656023377,
                    1.3298618671645932
                ],
                "word_count_raw": [
                    3.0,
                    15.0,
                    10.0
                ],
                "word_count_appended": [
                    71.0,
                    138.0,
                    313.0
                ],
                "result_count": [
                    1590000.0,
                    902000.0,
                    1630000.0
                ]
            },
            "integer_answers": {
                "nebraska": 2,
                "new mexico": 0,
                "north dakota": 4
            }
        },
        "lines": [
            [
                1,
                0.36239015231934196,
                0.0,
                0.33219564180826733,
                0.10714285714285714,
                0.1360153256704981,
                0.38573508005822416
            ],
            [
                0,
                0.41010432147332226,
                1.0,
                0.3353388914005844,
                0.5357142857142857,
                0.26436781609195403,
                0.2188258127122756
            ],
            [
                0,
                0.22750552620733577,
                0.0,
                0.3324654667911483,
                0.35714285714285715,
                0.5996168582375478,
                0.39543910722950026
            ]
        ]
    }
}