{
    "How many U.S. state names are only four letters long?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "50 states"
            ],
            "lines": [
                [
                    0.40708525578759913,
                    0.18672014260249556,
                    0.23181936492583202,
                    0.3242400805892452,
                    0.10464135021097046,
                    0.27068832173240526,
                    0.1233362910381544,
                    0.20202020202020202,
                    0.3599468488990129,
                    0.2109375,
                    0.2222222222222222,
                    0.3105236460902757,
                    0.0,
                    0,
                    5.0
                ],
                [
                    0.4245628946090335,
                    0.6115453174276704,
                    0.386373473493258,
                    0.3526356132102328,
                    0.6860759493670886,
                    0.2008249548852797,
                    0.6699201419698314,
                    0.5367965367965368,
                    0.5162743609212858,
                    0.6015625,
                    0.5555555555555556,
                    0.3646824987772076,
                    1.0,
                    0,
                    5.0
                ],
                [
                    0.16835184960336733,
                    0.20173453996983412,
                    0.38180716158091,
                    0.323124306200522,
                    0.20928270042194091,
                    0.5284867233823151,
                    0.2067435669920142,
                    0.2611832611832612,
                    0.12377879017970134,
                    0.1875,
                    0.2222222222222222,
                    0.32479385513251663,
                    0.0,
                    0,
                    5.0
                ]
            ],
            "fraction_answers": {
                "12 states": 0.22724470970141655,
                "50 states": 0.5312930613086908,
                "3 states": 0.24146222898989264
            },
            "question": "how many u.s. state names are only four letters long?",
            "rate_limited": false,
            "answers": [
                "12 states",
                "50 states",
                "3 states"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "12 states": 0.1100019832461876,
                "50 states": 0.4797004894037442,
                "3 states": 0.09746506547939993
            },
            "integer_answers": {
                "12 states": 0,
                "50 states": 12,
                "3 states": 1
            },
            "categorical_data": {
                "question_type": 5
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.5526182304513787,
                    1.823412493886038,
                    1.6239692756625832
                ],
                "result_count_important_words": [
                    1390000.0,
                    7550000.0,
                    2330000.0
                ],
                "wikipedia_search": [
                    1.0798405466970387,
                    1.5488230827638572,
                    0.371336370539104
                ],
                "word_count_appended_bing": [
                    2.0,
                    5.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.5601604278074866,
                    1.8346359522830111,
                    0.6052036199095023
                ],
                "cosine_similarity_raw": [
                    0.08051872253417969,
                    0.13420060276985168,
                    0.13261456787586212
                ],
                "result_count_noun_chunks": [
                    1400000.0,
                    3720000.0,
                    1810000.0
                ],
                "question_answer_similarity": [
                    6.924035631120205,
                    7.53041248396039,
                    6.900208652019501
                ],
                "word_count_noun_chunks": [
                    0.0,
                    5.0,
                    0.0
                ],
                "result_count_bing": [
                    105000000.0,
                    77900000.0,
                    205000000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    1240000.0,
                    8130000.0,
                    2480000.0
                ],
                "answer_relation_to_question": [
                    1.6283410231503965,
                    1.698251578436134,
                    0.6734073984134693
                ],
                "word_count_appended": [
                    27.0,
                    77.0,
                    24.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What play has the stage direction, \u201cEnter a Messenger, with two heads and a hand\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "titus andronicus"
            ],
            "lines": [
                [
                    0.27561885041723755,
                    0.37450578885262714,
                    0.05003639455789304,
                    0.11433539847364375,
                    0.02702702702702703,
                    0.0963572267920094,
                    0.14213197969543148,
                    0.1618320610687023,
                    0.22264552781439853,
                    0.034482758620689655,
                    0.08771929824561403,
                    0.11281885813827591,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.5460914008149031,
                    0.2544561443576968,
                    0.8856872225180772,
                    0.5014528563136913,
                    0.7972972972972973,
                    0.010575793184488837,
                    0.19923857868020303,
                    0.2916030534351145,
                    0.7545288609212119,
                    0.6206896551724138,
                    0.24561403508771928,
                    0.6828696198863226,
                    0.8888888888888888,
                    1.0,
                    1.0
                ],
                [
                    0.17828974876785939,
                    0.371038066789676,
                    0.0642763829240298,
                    0.38421174521266493,
                    0.17567567567567569,
                    0.8930669800235017,
                    0.6586294416243654,
                    0.5465648854961832,
                    0.022825611264389683,
                    0.3448275862068966,
                    0.6666666666666666,
                    0.20431152197540148,
                    0.1111111111111111,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "oedipus rex": 0.12139365497882497,
                "agamemnon": 0.3301068159813158,
                "titus andronicus": 0.5484995290398592
            },
            "question": "what play has the stage direction, \u201center a messenger, with two heads and a hand\u201d?",
            "rate_limited": false,
            "answers": [
                "oedipus rex",
                "titus andronicus",
                "agamemnon"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "oedipus rex": 0.16653385161899864,
                "agamemnon": 0.1305101182285861,
                "titus andronicus": 0.7906221929744528
            },
            "integer_answers": {
                "oedipus rex": 2,
                "agamemnon": 4,
                "titus andronicus": 8
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.7897320069679314,
                    4.780087339204258,
                    1.4301806538278106
                ],
                "result_count_important_words": [
                    11200.0,
                    15700.0,
                    51900.0
                ],
                "wikipedia_search": [
                    1.5585186947007896,
                    5.281702026448483,
                    0.1597792788507278
                ],
                "word_count_appended_bing": [
                    5.0,
                    14.0,
                    38.0
                ],
                "answer_relation_to_question_bing": [
                    2.247034733115763,
                    1.5267368661461809,
                    2.226228400738056
                ],
                "cosine_similarity_raw": [
                    0.02357955276966095,
                    0.41737836599349976,
                    0.030290119349956512
                ],
                "result_count_noun_chunks": [
                    10600.0,
                    19100.0,
                    35800.0
                ],
                "question_answer_similarity": [
                    -1.0870871188817546,
                    -4.767753015272319,
                    -3.6530387327075005
                ],
                "word_count_noun_chunks": [
                    0.0,
                    16.0,
                    2.0
                ],
                "result_count_bing": [
                    164000.0,
                    18000.0,
                    1520000.0
                ],
                "word_count_raw": [
                    0.0,
                    10.0,
                    0.0
                ],
                "result_count": [
                    2.0,
                    59.0,
                    13.0
                ],
                "answer_relation_to_question": [
                    1.9293319529206627,
                    3.822639805704322,
                    1.2480282413750157
                ],
                "word_count_appended": [
                    3.0,
                    54.0,
                    30.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Wrestling legend Ric Flair entered the ring to the same music used in what classic film?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "2001: a space odyssey"
            ],
            "lines": [
                [
                    0.16209301593752226,
                    0.19445294893056086,
                    0.46911630422421624,
                    0.2685614919917738,
                    0.6457964166174444,
                    0.06811182184316512,
                    0.6468708596368171,
                    0.3382076947137942,
                    0.732760827637417,
                    0.2222222222222222,
                    0.2857142857142857,
                    0.27603922855535024,
                    0,
                    0,
                    1.0
                ],
                [
                    0.4784342043176911,
                    0.45434047710167114,
                    0.1376273995809698,
                    0.2846153253030925,
                    0.3516440244142548,
                    0.8884150675195451,
                    0.34993375418907335,
                    0.6607757272442916,
                    0.0445646010268299,
                    0.5925925925925926,
                    0.42857142857142855,
                    0.42312651639710924,
                    0,
                    0,
                    1.0
                ],
                [
                    0.35947277974478664,
                    0.351206573967768,
                    0.39325629619481395,
                    0.44682318270513366,
                    0.0025595589683008466,
                    0.043473110637289744,
                    0.0031953861741095785,
                    0.0010165780419142947,
                    0.22267457133575316,
                    0.18518518518518517,
                    0.2857142857142857,
                    0.3008342550475405,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "star wars: episode iv": 0.35916225983538075,
                "back to the future": 0.21628431364307343,
                "2001: a space odyssey": 0.42455342652154576
            },
            "question": "wrestling legend ric flair entered the ring to the same music used in what classic film?",
            "rate_limited": false,
            "answers": [
                "star wars: episode iv",
                "2001: a space odyssey",
                "back to the future"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "star wars: episode iv": 0.0757528235662117,
                "back to the future": 0.10564275655914847,
                "2001: a space odyssey": 0.5968649866727558
            },
            "integer_answers": {
                "star wars: episode iv": 4,
                "back to the future": 1,
                "2001: a space odyssey": 7
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.484353056998152,
                    3.8081386475739833,
                    2.7075082954278646
                ],
                "result_count_important_words": [
                    16600.0,
                    8980.0,
                    82.0
                ],
                "wikipedia_search": [
                    5.129325793461918,
                    0.31195220718780925,
                    1.5587219993502717
                ],
                "word_count_appended_bing": [
                    2.0,
                    3.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.9722647446528043,
                    2.271702385508356,
                    1.75603286983884
                ],
                "cosine_similarity_raw": [
                    0.09000672399997711,
                    0.026405800133943558,
                    0.07545188814401627
                ],
                "result_count_noun_chunks": [
                    17300.0,
                    33800.0,
                    52.0
                ],
                "question_answer_similarity": [
                    15.269633424468338,
                    16.18240817822516,
                    25.405080061405897
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    1150.0,
                    15000.0,
                    734.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    16400.0,
                    8930.0,
                    65.0
                ],
                "answer_relation_to_question": [
                    1.1346511115626559,
                    3.3490394302238378,
                    2.5163094582135064
                ],
                "word_count_appended": [
                    12.0,
                    32.0,
                    10.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Yellow and blue mix to form what color?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "green"
            ],
            "question": "yellow and blue mix to form what color?",
            "lines": [
                [
                    0.34855612927902085,
                    0.38417021271815516,
                    0.570252563256666,
                    0.3504476153953488,
                    0.21354933726067746,
                    0.4289940828402367,
                    0.6892523364485982,
                    0.291497975708502,
                    0.31567261146800824,
                    0.3607112616426757,
                    0.40217391304347827,
                    0.3460102995641351,
                    0.7242990654205608,
                    0.8604651162790697,
                    1.0
                ],
                [
                    0.4282370752852681,
                    0.3256745922976916,
                    0.222001152552386,
                    0.31872289327726966,
                    0.6126656848306333,
                    0.24334319526627218,
                    0.10319314641744548,
                    0.5700404858299595,
                    0.40445411424634736,
                    0.3310753598645216,
                    0.266304347826087,
                    0.3261373519027365,
                    0.1308411214953271,
                    0.046511627906976744,
                    1.0
                ],
                [
                    0.2232067954357111,
                    0.2901551949841532,
                    0.20774628419094798,
                    0.3308294913273816,
                    0.17378497790868924,
                    0.3276627218934911,
                    0.2075545171339564,
                    0.13846153846153847,
                    0.2798732742856444,
                    0.3082133784928027,
                    0.33152173913043476,
                    0.32785234853312845,
                    0.14485981308411214,
                    0.09302325581395349,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "green",
                "purple",
                "orange"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "purple": 0.10111410098766807,
                "orange": 0.155223699259861,
                "green": 0.6542426232908667
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7300514978206756,
                    1.6306867595136825,
                    1.6392617426656422
                ],
                "result_count_important_words": [
                    177000000.0,
                    26500000.0,
                    53300000.0
                ],
                "wikipedia_search": [
                    1.5783630573400411,
                    2.022270571231737,
                    1.399366371428222
                ],
                "word_count_appended_bing": [
                    74.0,
                    49.0,
                    61.0
                ],
                "answer_relation_to_question_bing": [
                    1.5366808508726206,
                    1.3026983691907663,
                    1.1606207799366128
                ],
                "cosine_similarity_raw": [
                    0.15311259031295776,
                    0.05960722267627716,
                    0.055779796093702316
                ],
                "result_count_noun_chunks": [
                    36000000.0,
                    70400000.0,
                    17100000.0
                ],
                "question_answer_similarity": [
                    3.91780623793602,
                    3.563141778111458,
                    3.69848670065403
                ],
                "word_count_noun_chunks": [
                    155.0,
                    28.0,
                    31.0
                ],
                "result_count_bing": [
                    116000000.0,
                    65800000.0,
                    88600000.0
                ],
                "word_count_raw": [
                    37.0,
                    2.0,
                    4.0
                ],
                "result_count": [
                    14500000.0,
                    41600000.0,
                    11800000.0
                ],
                "answer_relation_to_question": [
                    1.7427806463951043,
                    2.1411853764263404,
                    1.1160339771785555
                ],
                "word_count_appended": [
                    426.0,
                    391.0,
                    364.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "\u201cGinevra de\u2019 Benci\u201d is the only work by what artist on permanent display in the U.S.?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "michelangelo"
            ],
            "lines": [
                [
                    0.27995730226797116,
                    0.3783421944829712,
                    0.17950328414083574,
                    -0.6423444691094917,
                    0.44790313838979845,
                    0.019997515836542044,
                    0.3613933236574746,
                    0.43271139341008336,
                    0.25134760035092263,
                    0.38461538461538464,
                    0.5342465753424658,
                    0.33698566295063165,
                    0.02702702702702703,
                    0.0,
                    1.0
                ],
                [
                    0.20524972080747284,
                    0.32531495607951916,
                    0.6318225618560575,
                    6.2929301845616035,
                    0.551186738794752,
                    0.1565022978511986,
                    0.4288824383164006,
                    0.5289797538705836,
                    0.49388165207426815,
                    0.2913752913752914,
                    0.0958904109589041,
                    0.38248987921989536,
                    0.9594594594594594,
                    1.0,
                    1.0
                ],
                [
                    0.514792976924556,
                    0.2963428494375096,
                    0.18867415400310678,
                    -4.650585715452111,
                    0.0009101228154495904,
                    0.8235001863122593,
                    0.20972423802612483,
                    0.03830885271933307,
                    0.2547707475748092,
                    0.32400932400932403,
                    0.3698630136986301,
                    0.2805244578294729,
                    0.013513513513513514,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "michelangelo": 0.21369185238304403,
                "botticelli": -0.09540366275628732,
                "leonardo da vinci": 0.8817118103732433
            },
            "question": "\u201cginevra de\u2019 benci\u201d is the only work by what artist on permanent display in the u.s.?",
            "rate_limited": false,
            "answers": [
                "michelangelo",
                "leonardo da vinci",
                "botticelli"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "michelangelo": 0.5377014037721425,
                "botticelli": 0.21714473912902957,
                "leonardo da vinci": 0.16078826456898168
            },
            "integer_answers": {
                "michelangelo": 3,
                "botticelli": 2,
                "leonardo da vinci": 9
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.3588996406544216,
                    2.6774291545392677,
                    1.9636712048063103
                ],
                "result_count_important_words": [
                    49800.0,
                    59100.0,
                    28900.0
                ],
                "wikipedia_search": [
                    1.0053904014036905,
                    1.9755266082970726,
                    1.0190829902992369
                ],
                "word_count_appended_bing": [
                    39.0,
                    7.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    1.5133687779318847,
                    1.3012598243180766,
                    1.1853713977500384
                ],
                "cosine_similarity_raw": [
                    0.048628970980644226,
                    0.17116612195968628,
                    0.05111343786120415
                ],
                "result_count_noun_chunks": [
                    43600.0,
                    53300.0,
                    3860.0
                ],
                "question_answer_similarity": [
                    -0.29872069600969553,
                    2.926511513796868,
                    -2.1627433076500893
                ],
                "word_count_noun_chunks": [
                    2.0,
                    71.0,
                    1.0
                ],
                "result_count_bing": [
                    16100.0,
                    126000.0,
                    663000.0
                ],
                "word_count_raw": [
                    0.0,
                    18.0,
                    0.0
                ],
                "result_count": [
                    43800.0,
                    53900.0,
                    89.0
                ],
                "answer_relation_to_question": [
                    1.1198292090718847,
                    0.8209988832298913,
                    2.059171907698224
                ],
                "word_count_appended": [
                    165.0,
                    125.0,
                    139.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "The top prize in architecture is named for the creator of what hotel chain?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "sheraton"
            ],
            "question": "the top prize in architecture is named for the creator of what hotel chain?",
            "lines": [
                [
                    0.5534075104311544,
                    0.18091998704243603,
                    0.46515455666434297,
                    0.030740286244520895,
                    0.42445054945054944,
                    0.37637362637362637,
                    0.3823047515019115,
                    0.48231511254019294,
                    0.5879600963192294,
                    0.4642857142857143,
                    0.5196078431372549,
                    0.3261835580579126,
                    0.8333333333333334,
                    0.75,
                    1.0
                ],
                [
                    0.20959666203059807,
                    0.6833495302883058,
                    0.13225379555568323,
                    0.04451064417717268,
                    0.28434065934065933,
                    0.32074175824175827,
                    0.24631348989623156,
                    0.08360128617363344,
                    0.41203990368077054,
                    0.3842857142857143,
                    0.38235294117647056,
                    0.3471431651858039,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.23699582753824755,
                    0.1357304826692582,
                    0.4025916477799738,
                    0.9247490695783064,
                    0.29120879120879123,
                    0.30288461538461536,
                    0.3713817586018569,
                    0.4340836012861736,
                    0.0,
                    0.15142857142857144,
                    0.09803921568627451,
                    0.3266732767562835,
                    0.16666666666666666,
                    0.25,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "hyatt",
                "sheraton",
                "four seasons"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sheraton": 0.5490007732947253,
                "four seasons": 0.10784013542039285,
                "hyatt": 0.3192437821120937
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9571013483474755,
                    2.0828589911148234,
                    1.960039660537701
                ],
                "result_count_important_words": [
                    1400000.0,
                    902000.0,
                    1360000.0
                ],
                "wikipedia_search": [
                    1.7638802889576881,
                    1.2361197110423117,
                    0.0
                ],
                "word_count_appended_bing": [
                    53.0,
                    39.0,
                    10.0
                ],
                "answer_relation_to_question_bing": [
                    0.36183997408487206,
                    1.3666990605766116,
                    0.2714609653385164
                ],
                "cosine_similarity_raw": [
                    0.18529634177684784,
                    0.05268387496471405,
                    0.16037413477897644
                ],
                "result_count_noun_chunks": [
                    1500000.0,
                    260000.0,
                    1350000.0
                ],
                "question_answer_similarity": [
                    0.2678688345476985,
                    0.38786282879300416,
                    8.058202631771564
                ],
                "word_count_noun_chunks": [
                    5.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    15.0,
                    0.0,
                    5.0
                ],
                "result_count_bing": [
                    548000.0,
                    467000.0,
                    441000.0
                ],
                "word_count_appended": [
                    325.0,
                    269.0,
                    106.0
                ],
                "answer_relation_to_question": [
                    2.767037552155772,
                    1.0479833101529903,
                    1.1849791376912377
                ],
                "result_count": [
                    3090000.0,
                    2070000.0,
                    2120000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "In which profession would you be most likely to use a psychrometer?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "meteorology"
            ],
            "question": "in which profession would you be most likely to use a psychrometer?",
            "lines": [
                [
                    0.28361344537815125,
                    0.15868263473053892,
                    0.8062155894795536,
                    0.08024635247023797,
                    0.8700362232980128,
                    0.015019210618232623,
                    0.013821138211382113,
                    0.7322580645161291,
                    0.48040918741555677,
                    0.6572237960339944,
                    0.5463917525773195,
                    0.4300167529445285,
                    1.0,
                    0,
                    -1.0
                ],
                [
                    0.22622733303847856,
                    0.12874251497005987,
                    0.08248202273532397,
                    0.051241211477662786,
                    0.1279664172788517,
                    0.4924903946908837,
                    0.9776422764227642,
                    0.11193548387096774,
                    0.09129511677282377,
                    0.24929178470254956,
                    0.3402061855670103,
                    0.2958082054269803,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.4901592215833702,
                    0.7125748502994012,
                    0.11130238778512234,
                    0.8685124360520993,
                    0.001997359423135516,
                    0.4924903946908837,
                    0.00853658536585366,
                    0.1558064516129032,
                    0.42829569581161936,
                    0.09348441926345609,
                    0.1134020618556701,
                    0.27417504162849127,
                    0.0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "meteorology",
                "neurosurgery",
                "electrical engineering"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "electrical engineering": 0.03800266093032326,
                "neurosurgery": -0.004058325659898268,
                "meteorology": 0.7413817392403487
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.720067011778114,
                    1.183232821707921,
                    1.096700166513965
                ],
                "result_count_important_words": [
                    68.0,
                    4810.0,
                    42.0
                ],
                "wikipedia_search": [
                    1.4412275622466704,
                    0.27388535031847133,
                    1.2848870874348581
                ],
                "word_count_appended_bing": [
                    53.0,
                    33.0,
                    11.0
                ],
                "answer_relation_to_question_bing": [
                    0.31736526946107785,
                    0.25748502994011974,
                    1.4251497005988023
                ],
                "cosine_similarity_raw": [
                    0.05702652037143707,
                    0.005834249313920736,
                    0.007872817106544971
                ],
                "result_count_noun_chunks": [
                    22700.0,
                    3470.0,
                    4830.0
                ],
                "question_answer_similarity": [
                    0.5130442450754344,
                    0.32760378322564065,
                    5.5527172684669495
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    129000.0,
                    4230000.0,
                    4230000.0
                ],
                "word_count_appended": [
                    232.0,
                    88.0,
                    33.0
                ],
                "answer_relation_to_question": [
                    0.5672268907563025,
                    0.4524546660769571,
                    0.9803184431667404
                ],
                "result_count": [
                    25700.0,
                    3780.0,
                    59.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which U.S. president's wife was NOT born in North America?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "martin van buren"
            ],
            "lines": [
                [
                    0.2530783241312107,
                    0.23306196503127447,
                    0.07525737448809561,
                    0.278352176574437,
                    0.17150395778364114,
                    0.4024822695035461,
                    0.17532467532467533,
                    0.17577197149643703,
                    0.34878386668759187,
                    0.31155778894472363,
                    0.4270833333333333,
                    0.33276247336560744,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.37045488951212613,
                    0.4144180414640773,
                    0.4842463262880563,
                    0.37061459986955764,
                    0.4270448548812665,
                    0.1453900709219858,
                    0.42514089683901,
                    0.42304038004750594,
                    0.39652199759360457,
                    0.3756281407035176,
                    0.3854166666666667,
                    0.337679348623596,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.37646678635666325,
                    0.35251999350464824,
                    0.4404962992238481,
                    0.35103322355600536,
                    0.40145118733509233,
                    0.4521276595744681,
                    0.39953442783631465,
                    0.40118764845605703,
                    0.25469413571880367,
                    0.3128140703517588,
                    0.1875,
                    0.3295581780107966,
                    0.5,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "martin van buren": 0.24865948429650628,
                "john quincy adams": 0.5449971176193465,
                "rutherford b. hayes": 0.20634339808414714
            },
            "question": "which u.s. president's wife was not born in north america?",
            "rate_limited": false,
            "answers": [
                "john quincy adams",
                "rutherford b. hayes",
                "martin van buren"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "martin van buren": 0.4027208663980872,
                "john quincy adams": 0.07687381715380737,
                "rutherford b. hayes": 0.3683872075766367
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.0068503196127114,
                    1.9478478165168482,
                    2.0453018638704408
                ],
                "result_count_important_words": [
                    265000.0,
                    61100.0,
                    82000.0
                ],
                "wikipedia_search": [
                    1.5121613331240817,
                    1.034780024063955,
                    2.4530586428119636
                ],
                "word_count_appended_bing": [
                    7.0,
                    11.0,
                    30.0
                ],
                "answer_relation_to_question_bing": [
                    3.203256419624706,
                    1.0269835024310727,
                    1.769760077944221
                ],
                "cosine_similarity_raw": [
                    0.4483479857444763,
                    0.01662919484078884,
                    0.06281065940856934
                ],
                "result_count_noun_chunks": [
                    273000.0,
                    64800.0,
                    83200.0
                ],
                "question_answer_similarity": [
                    5.790412285365164,
                    3.380113546270877,
                    3.8916648902813904
                ],
                "word_count_noun_chunks": [
                    5.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    6.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    3850000.0,
                    14000000.0,
                    1890000.0
                ],
                "result_count": [
                    249000.0,
                    55300.0,
                    74700.0
                ],
                "answer_relation_to_question": [
                    2.9630601104254723,
                    1.5545413258544867,
                    1.4823985637200412
                ],
                "word_count_appended": [
                    150.0,
                    99.0,
                    149.0
                ]
            },
            "integer_answers": {
                "martin van buren": 3,
                "john quincy adams": 10,
                "rutherford b. hayes": 1
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Who was the president of the Screen Actors Guild before its merger with AFTRA?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "gabrielle carteris"
            ],
            "lines": [
                [
                    0.42054615654423294,
                    0.5453968253968254,
                    0.6219188481405568,
                    -0.17112530697925354,
                    0.0028997100289971,
                    0.07977917298116476,
                    0.21305841924398625,
                    0.14945054945054945,
                    0.3790243902439025,
                    0.24782608695652175,
                    0.4050632911392405,
                    0.1805547670081146,
                    0.1016949152542373,
                    0.6923076923076923,
                    0.0
                ],
                [
                    0.07500520644942071,
                    0.15158730158730158,
                    0.037733299716115706,
                    0.29934179764972413,
                    0.9919008099190081,
                    0.03691275167785235,
                    0.20587316463605124,
                    0.7076923076923077,
                    0.10941734417344173,
                    0.2956521739130435,
                    0.08860759493670886,
                    0.5035735620189074,
                    0.05084745762711865,
                    0.0,
                    0.0
                ],
                [
                    0.5044486370063465,
                    0.303015873015873,
                    0.3403478521433276,
                    0.8717835093295294,
                    0.0051994800519948,
                    0.8833080753409829,
                    0.5810684161199625,
                    0.14285714285714285,
                    0.5115582655826558,
                    0.45652173913043476,
                    0.5063291139240507,
                    0.31587167097297814,
                    0.847457627118644,
                    0.3076923076923077,
                    0.0
                ]
            ],
            "fraction_answers": {
                "gabrielle carteris": 0.27631396555119775,
                "ken howard": 0.4698185507347307,
                "melissa gilbert": 0.25386748371407153
            },
            "question": "who was the president of the screen actors guild before its merger with aftra?",
            "rate_limited": false,
            "answers": [
                "gabrielle carteris",
                "melissa gilbert",
                "ken howard"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "gabrielle carteris": 0.614437924037881,
                "ken howard": 0.2968112698996444,
                "melissa gilbert": 0.22911291777945456
            },
            "integer_answers": {
                "gabrielle carteris": 3,
                "ken howard": 8,
                "melissa gilbert": 3
            },
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.0833286020486876,
                    3.021441372113444,
                    1.8952300258378687
                ],
                "result_count_important_words": [
                    6820.0,
                    6590.0,
                    18600.0
                ],
                "wikipedia_search": [
                    2.274146341463415,
                    0.6565040650406504,
                    3.069349593495935
                ],
                "word_count_appended_bing": [
                    32.0,
                    7.0,
                    40.0
                ],
                "answer_relation_to_question_bing": [
                    2.726984126984127,
                    0.7579365079365079,
                    1.515079365079365
                ],
                "cosine_similarity_raw": [
                    0.42212677001953125,
                    0.02561143785715103,
                    0.23101074993610382
                ],
                "result_count_noun_chunks": [
                    13600.0,
                    64400.0,
                    13000.0
                ],
                "question_answer_similarity": [
                    -0.4703610949218273,
                    0.8227814937708899,
                    2.3962151082232594
                ],
                "word_count_noun_chunks": [
                    6.0,
                    3.0,
                    50.0
                ],
                "result_count_bing": [
                    73700.0,
                    34100.0,
                    816000.0
                ],
                "word_count_raw": [
                    9.0,
                    0.0,
                    4.0
                ],
                "result_count": [
                    29.0,
                    9920.0,
                    52.0
                ],
                "answer_relation_to_question": [
                    2.5232769392653975,
                    0.4500312386965243,
                    3.0266918220380785
                ],
                "word_count_appended": [
                    57.0,
                    68.0,
                    105.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Jean Valjean, the protagonist of \u201cLes Mis\u00e9rables,\u201d is identified by what prisoner number?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "24601"
            ],
            "lines": [
                [
                    0.0027472527472527475,
                    0.025,
                    0.022622645358009074,
                    0.4163131932104924,
                    0.005112268930082507,
                    0.2989821882951654,
                    0.03311513284559107,
                    0.002796537054115967,
                    0.06878980891719745,
                    0.20551378446115287,
                    0.2621359223300971,
                    0.2586525636272882,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.804029304029304,
                    0.9083333333333334,
                    0.9602745559466378,
                    0.0845695215090429,
                    0.994624902727746,
                    0.40458015267175573,
                    0.9626492106276473,
                    0.9966382054562223,
                    0.8203821656050956,
                    0.6591478696741855,
                    0.46601941747572817,
                    0.579253800814999,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.1932234432234432,
                    0.06666666666666667,
                    0.0171027986953531,
                    0.4991172852804647,
                    0.0002628283421715805,
                    0.2964376590330789,
                    0.004235656526761648,
                    0.000565257489661738,
                    0.11082802547770701,
                    0.13533834586466165,
                    0.27184466019417475,
                    0.1620936355577128,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "y2k": 0.1144129498411746,
                "24601": 0.7600358885622641,
                "867-5309": 0.12555116159656124
            },
            "question": "jean valjean, the protagonist of \u201cles mis\u00e9rables,\u201d is identified by what prisoner number?",
            "rate_limited": false,
            "answers": [
                "y2k",
                "24601",
                "867-5309"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "y2k": 0.13928949556759776,
                "24601": 0.7906221929744528,
                "867-5309": 0.11671855008043819
            },
            "integer_answers": {
                "y2k": 0,
                "24601": 14,
                "867-5309": 0
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.5519153817637292,
                    3.475522804889994,
                    0.9725618133462768
                ],
                "result_count_important_words": [
                    86.0,
                    2500.0,
                    11.0
                ],
                "wikipedia_search": [
                    0.34394904458598724,
                    4.101910828025478,
                    0.554140127388535
                ],
                "word_count_appended_bing": [
                    27.0,
                    48.0,
                    28.0
                ],
                "answer_relation_to_question_bing": [
                    0.125,
                    4.541666666666667,
                    0.3333333333333333
                ],
                "cosine_similarity_raw": [
                    0.010702831670641899,
                    0.4543083608150482,
                    0.008091378025710583
                ],
                "result_count_noun_chunks": [
                    94.0,
                    33500.0,
                    19.0
                ],
                "question_answer_similarity": [
                    -1.5125535689294338,
                    -0.3072588946670294,
                    -1.8133982863801066
                ],
                "word_count_noun_chunks": [
                    0.0,
                    38.0,
                    0.0
                ],
                "result_count_bing": [
                    23500.0,
                    31800.0,
                    23300.0
                ],
                "word_count_raw": [
                    0.0,
                    12.0,
                    0.0
                ],
                "word_count_appended": [
                    82.0,
                    263.0,
                    54.0
                ],
                "answer_relation_to_question": [
                    0.016483516483516484,
                    4.824175824175824,
                    1.1593406593406592
                ],
                "result_count": [
                    992.0,
                    193000.0,
                    51.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these swimmers has NOT been featured on a Wheaties box?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "amy van dyken"
            ],
            "lines": [
                [
                    0.40705128205128205,
                    0.3675736961451247,
                    0.4288912686319266,
                    0.14035321572949488,
                    0.49918060262952063,
                    0.3247863247863248,
                    0.4989417805842759,
                    0.49287593160894344,
                    0.3380711318795431,
                    0.38031914893617025,
                    0.3,
                    0.39037211255622317,
                    0.2,
                    0.5,
                    -1.0
                ],
                [
                    0.24786324786324787,
                    0.4009070294784581,
                    0.42229759428307095,
                    0.33153579148140055,
                    0.49432008640917724,
                    0.300976800976801,
                    0.49200263687460966,
                    0.464160455940377,
                    0.4842289719626168,
                    0.29787234042553196,
                    0.44,
                    0.26776051054930744,
                    0.4,
                    0.5,
                    -1.0
                ],
                [
                    0.3450854700854701,
                    0.23151927437641723,
                    0.14881113708500243,
                    0.5281109927891046,
                    0.006499310961302074,
                    0.37423687423687424,
                    0.009055582541114438,
                    0.042963612450679534,
                    0.1776998961578401,
                    0.3218085106382979,
                    0.26,
                    0.34186737689446944,
                    0.4,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "amy van dyken": 0.24736907206588146,
                "norman ross": 0.20801064767934307,
                "ryan lochte": 0.5446202802547755
            },
            "question": "which of these swimmers has not been featured on a wheaties box?",
            "rate_limited": false,
            "answers": [
                "amy van dyken",
                "norman ross",
                "ryan lochte"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "amy van dyken": 0.4556145011180031,
                "norman ross": 0.21834245662727622,
                "ryan lochte": 0.4214740476190364
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.8770230995502146,
                    1.8579159156055405,
                    1.2650609848442447
                ],
                "result_count_important_words": [
                    61.0,
                    461.0,
                    28300.0
                ],
                "wikipedia_search": [
                    0.9715732087227413,
                    0.09462616822429906,
                    1.9338006230529594
                ],
                "word_count_appended_bing": [
                    10.0,
                    3.0,
                    12.0
                ],
                "answer_relation_to_question_bing": [
                    0.7945578231292517,
                    0.5945578231292517,
                    1.6108843537414965
                ],
                "cosine_similarity_raw": [
                    0.018851783126592636,
                    0.0205998457968235,
                    0.09310440719127655
                ],
                "result_count_noun_chunks": [
                    65.0,
                    327.0,
                    4170.0
                ],
                "question_answer_similarity": [
                    2.776972336694598,
                    1.3007775051519275,
                    -0.2170558802317828
                ],
                "word_count_noun_chunks": [
                    3.0,
                    1.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    28700.0,
                    32600.0,
                    20600.0
                ],
                "word_count_appended": [
                    45.0,
                    76.0,
                    67.0
                ],
                "answer_relation_to_question": [
                    0.5576923076923077,
                    1.5128205128205128,
                    0.9294871794871795
                ],
                "result_count": [
                    44.0,
                    305.0,
                    26500.0
                ]
            },
            "integer_answers": {
                "amy van dyken": 2,
                "norman ross": 4,
                "ryan lochte": 8
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What was the first registered trademark for a breakfast cereal?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "quaker oats"
            ],
            "lines": [
                [
                    0.6029516164263872,
                    0.3086297760210803,
                    0.9678192454021143,
                    0.26757112384816656,
                    0.8014735165272799,
                    0.47330222913426645,
                    0.2917744438696327,
                    0.5828002842928216,
                    0.7261904761904762,
                    0.2742200328407225,
                    0.21518987341772153,
                    0.33867956194028614,
                    0.9354838709677419,
                    1.0,
                    1.0
                ],
                [
                    0.22516655744866754,
                    0.4334650856389987,
                    0.020598658147501604,
                    0.7605214761464008,
                    0.0730784547988849,
                    0.37636080870917576,
                    0.22038282462493533,
                    0.00023691068467187872,
                    0.10833333333333334,
                    0.0,
                    0.0,
                    0.327912578508771,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.1718818261249454,
                    0.25790513833992096,
                    0.01158209645038408,
                    -0.028092599994567334,
                    0.12544802867383512,
                    0.1503369621565578,
                    0.48784273150543195,
                    0.4169628050225065,
                    0.1654761904761905,
                    0.7257799671592775,
                    0.7848101265822784,
                    0.3334078595509429,
                    0.06451612903225806,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "quaker oats": 0.5561490036341927,
                "wheaties": 0.2619898043628544,
                "kellogg\u2019s corn flakes": 0.18186119200295292
            },
            "question": "what was the first registered trademark for a breakfast cereal?",
            "rate_limited": false,
            "answers": [
                "quaker oats",
                "kellogg\u2019s corn flakes",
                "wheaties"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "quaker oats": 0.8271532799205323,
                "wheaties": 0.2500713443875775,
                "kellogg\u2019s corn flakes": 0.14612362962884304
            },
            "integer_answers": {
                "quaker oats": 9,
                "wheaties": 3,
                "kellogg\u2019s corn flakes": 2
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3547182477611446,
                    1.311650314035084,
                    1.3336314382037715
                ],
                "result_count_important_words": [
                    56400.0,
                    42600.0,
                    94300.0
                ],
                "wikipedia_search": [
                    2.9047619047619047,
                    0.43333333333333335,
                    0.661904761904762
                ],
                "word_count_appended_bing": [
                    17.0,
                    0.0,
                    62.0
                ],
                "answer_relation_to_question_bing": [
                    1.2345191040843213,
                    1.7338603425559949,
                    1.0316205533596838
                ],
                "cosine_similarity_raw": [
                    0.5257837176322937,
                    0.011190559715032578,
                    0.006292164325714111
                ],
                "result_count_noun_chunks": [
                    246000.0,
                    100.0,
                    176000.0
                ],
                "question_answer_similarity": [
                    2.40065147227142,
                    6.823408203199506,
                    -0.2520471587777138
                ],
                "word_count_noun_chunks": [
                    29.0,
                    0.0,
                    2.0
                ],
                "result_count_bing": [
                    913000.0,
                    726000.0,
                    290000.0
                ],
                "word_count_raw": [
                    14.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    167.0,
                    0.0,
                    442.0
                ],
                "answer_relation_to_question": [
                    2.4118064657055487,
                    0.9006662297946701,
                    0.6875273044997816
                ],
                "result_count": [
                    80500.0,
                    7340.0,
                    12600.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Jacksonville State University is in the same state as which school?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "emory university"
            ],
            "question": "jacksonville state university is in the same state as which school?",
            "lines": [
                [
                    0.4023637948409792,
                    0.3535253227408143,
                    0.1955770541746827,
                    0.27360402849560617,
                    9.186880054472794e-05,
                    0.2674512987012987,
                    0.17647058823529413,
                    9.668195806931102e-06,
                    0.26517364492048034,
                    0.359375,
                    0.26666666666666666,
                    0.24846582581304447,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.2844844421910503,
                    0.25399977932252016,
                    0.6159362097383232,
                    0.517861999154838,
                    0.00015941938918055733,
                    0.17248376623376624,
                    0.4068627450980392,
                    2.265120160481001e-05,
                    0.234031158714703,
                    0.2109375,
                    0.2,
                    0.3064313859284151,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.31315176296797054,
                    0.39247489793666557,
                    0.18848673608699407,
                    0.20853397234955579,
                    0.9997487118102747,
                    0.560064935064935,
                    0.4166666666666667,
                    0.9999676806025882,
                    0.5007951963648166,
                    0.4296875,
                    0.5333333333333333,
                    0.4451027882585404,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "auburn university",
                "university of miami",
                "emory university"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "emory university": 0.4545208843500559,
                "university of miami": 0.19525428515137744,
                "auburn university": 0.14267653577755923
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.9938633032521779,
                    1.2257255437136605,
                    1.7804111530341615
                ],
                "result_count_important_words": [
                    1080000.0,
                    2490000.0,
                    2550000.0
                ],
                "wikipedia_search": [
                    1.0606945796819214,
                    0.936124634858812,
                    2.0031807854592665
                ],
                "word_count_appended_bing": [
                    4.0,
                    3.0,
                    8.0
                ],
                "answer_relation_to_question_bing": [
                    1.414101290963257,
                    1.0159991172900806,
                    1.5698995917466623
                ],
                "cosine_similarity_raw": [
                    0.019825462251901627,
                    0.06243687495589256,
                    0.019106723368167877
                ],
                "result_count_noun_chunks": [
                    35.0,
                    82.0,
                    3620000.0
                ],
                "question_answer_similarity": [
                    6.697232287377119,
                    12.676136825233698,
                    5.104458659887314
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    659000.0,
                    425000.0,
                    1380000.0
                ],
                "word_count_appended": [
                    46.0,
                    27.0,
                    55.0
                ],
                "answer_relation_to_question": [
                    1.609455179363917,
                    1.1379377687642012,
                    1.2526070518718821
                ],
                "result_count": [
                    34.0,
                    59.0,
                    370000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these is a system widely used to teach reading?": {
        "raw_data": {
            "data": {
                "word_count_appended_relation_to_question": [
                    0.3591943572329436,
                    1.437174974213194,
                    2.2036306685538625
                ],
                "result_count_important_words": [
                    41.0,
                    151000.0,
                    343000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.09106182795698925,
                    3.908938172043011
                ],
                "word_count_appended_bing": [
                    2.0,
                    38.0,
                    69.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.6,
                    2.4
                ],
                "cosine_similarity_raw": [
                    0.025095365941524506,
                    0.015229221433401108,
                    0.4973677396774292
                ],
                "result_count_noun_chunks": [
                    82.0,
                    6970000.0,
                    9650000.0
                ],
                "question_answer_similarity": [
                    2.4823012524284422,
                    -0.1309175444766879,
                    1.282925121486187
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    17.0
                ],
                "result_count_bing": [
                    11400000.0,
                    15500000.0,
                    15500000.0
                ],
                "word_count_appended": [
                    13.0,
                    333.0,
                    446.0
                ],
                "answer_relation_to_question": [
                    0.5153078775369797,
                    1.2887719298245615,
                    2.195920192638459
                ],
                "result_count": [
                    24.0,
                    197000.0,
                    320000.0
                ]
            },
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "z-best_answer_by_ml": [
                "phonics",
                "bottom"
            ],
            "lines": [
                [
                    0.12882696938424493,
                    0.0,
                    0.046672352717209195,
                    0.6830187991515126,
                    4.641950857213592e-05,
                    0.2688679245283019,
                    8.298906366070832e-05,
                    4.9337903386998936e-06,
                    0.0,
                    0.016414141414141416,
                    0.01834862385321101,
                    0.0897985893082359,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.32219298245614036,
                    0.19999999999999998,
                    0.02832330064460497,
                    -0.03602268013556095,
                    0.38102679952961566,
                    0.36556603773584906,
                    0.305642649091877,
                    0.41937217878949096,
                    0.022765456989247312,
                    0.42045454545454547,
                    0.3486238532110092,
                    0.3592937435532985,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.5489800481596148,
                    0.7999999999999999,
                    0.9250043466381859,
                    0.35300388098404833,
                    0.6189267809618122,
                    0.36556603773584906,
                    0.6942743618444623,
                    0.5806228874201703,
                    0.9772345430107527,
                    0.5631313131313131,
                    0.6330275229357798,
                    0.5509076671384656,
                    0,
                    1.0,
                    -1.0
                ]
            ],
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "question": "which of these is a system widely used to teach reading?",
            "rate_limited": false,
            "answers": [
                "wild histrionics",
                "microelectronics",
                "phonics"
            ],
            "ml_answers": {
                "wild histrionics": 0.0319208849861696,
                "microelectronics": 0.09821699583262891,
                "phonics": 0.9688512645932614
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these famous fictional animals is a ruminant?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "bambi"
            ],
            "question": "which of these famous fictional animals is a ruminant?",
            "lines": [
                [
                    0.28174603174603174,
                    0.5,
                    0.47394566932414944,
                    0.022271896373905094,
                    0.9651076466221232,
                    0.42735042735042733,
                    0.4187844512849882,
                    0.9653647752394989,
                    0.3333333333333333,
                    0.5661016949152542,
                    0.37623762376237624,
                    0.4727225002107549,
                    1.0,
                    0,
                    -1.0
                ],
                [
                    0.35714285714285715,
                    0.0,
                    0.2658534546588037,
                    1.4944997655711405,
                    0.028062360801781736,
                    0.10945685139233527,
                    0.015677571766053403,
                    0.02800294767870302,
                    0.5333333333333333,
                    0.06779661016949153,
                    0.13861386138613863,
                    0.2230409356662501,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.3611111111111111,
                    0.5,
                    0.26020087601704683,
                    -0.5167716619450456,
                    0.006829992576095026,
                    0.46319272125723737,
                    0.5655379769489584,
                    0.006632277081798084,
                    0.13333333333333333,
                    0.36610169491525424,
                    0.48514851485148514,
                    0.30423656412299505,
                    0.0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "bambi",
                "mister ed",
                "eeyore"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "mister ed": 0.12789234060698845,
                "bambi": 0.6459955209948428,
                "eeyore": 0.2559255276754089
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.8908900008430196,
                    0.8921637426650004,
                    1.2169462564919802
                ],
                "result_count_important_words": [
                    11700.0,
                    438.0,
                    15800.0
                ],
                "wikipedia_search": [
                    0.6666666666666666,
                    1.0666666666666667,
                    0.26666666666666666
                ],
                "word_count_appended_bing": [
                    38.0,
                    14.0,
                    49.0
                ],
                "answer_relation_to_question_bing": [
                    1.0,
                    0.0,
                    1.0
                ],
                "cosine_similarity_raw": [
                    0.05643796920776367,
                    0.0316581204533577,
                    0.03098500519990921
                ],
                "result_count_noun_chunks": [
                    13100.0,
                    380.0,
                    90.0
                ],
                "question_answer_similarity": [
                    0.028029044391587377,
                    1.8808187488466501,
                    -0.6503539532423019
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    1550000.0,
                    397000.0,
                    1680000.0
                ],
                "word_count_appended": [
                    167.0,
                    20.0,
                    108.0
                ],
                "answer_relation_to_question": [
                    0.8452380952380952,
                    1.0714285714285714,
                    1.0833333333333333
                ],
                "result_count": [
                    13000.0,
                    378.0,
                    92.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these actresses is NOT mentioned in Madonna\u2019s song \u201cVogue\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "audrey hepburn"
            ],
            "lines": [
                [
                    0.3792673992673993,
                    0.5,
                    0.1927205580097775,
                    3.5234309945219278,
                    0.035348986909685465,
                    0.4660347551342812,
                    0.39348591549295775,
                    0.4082244008714597,
                    0.5,
                    0.3064516129032258,
                    0.35714285714285715,
                    0.30207569780386595,
                    0.5,
                    0.0,
                    -1.0
                ],
                [
                    0.27010989010989017,
                    0.16666666666666669,
                    0.39846338613765797,
                    1.9884020602008625,
                    0.4999720453862043,
                    0.2367035281727225,
                    0.3362676056338028,
                    0.13235294117647056,
                    0.06666666666666665,
                    0.4153225806451613,
                    0.38095238095238093,
                    0.3852049572818859,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.35062271062271066,
                    0.33333333333333337,
                    0.40881605585256453,
                    -4.51183305472279,
                    0.4646789677041102,
                    0.2972617166929963,
                    0.27024647887323944,
                    0.4594226579520697,
                    0.43333333333333335,
                    0.2782258064516129,
                    0.2619047619047619,
                    0.31271934491424813,
                    0.0,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "audrey hepburn": 0.10327361299566107,
                "jean harlow": -0.1234547397224911,
                "rita hayworth": 1.0201811267268301
            },
            "question": "which of these actresses is not mentioned in madonna\u2019s song \u201cvogue\u201d?",
            "rate_limited": false,
            "answers": [
                "jean harlow",
                "audrey hepburn",
                "rita hayworth"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "audrey hepburn": 0.46013602695950284,
                "jean harlow": 0.19317945683304036,
                "rita hayworth": 0.11364186956046968
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9792430219613404,
                    1.147950427181141,
                    1.872806550857519
                ],
                "result_count_important_words": [
                    121000.0,
                    186000.0,
                    261000.0
                ],
                "wikipedia_search": [
                    0.0,
                    2.6,
                    0.4
                ],
                "word_count_appended_bing": [
                    6.0,
                    5.0,
                    10.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    2.0,
                    1.0
                ],
                "cosine_similarity_raw": [
                    0.07597820460796356,
                    0.0251060388982296,
                    0.022546228021383286
                ],
                "result_count_noun_chunks": [
                    337000.0,
                    1350000.0,
                    149000.0
                ],
                "question_answer_similarity": [
                    0.8370858241105452,
                    0.41208820953033864,
                    -1.3876071292907
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    12900.0,
                    100000.0,
                    77000.0
                ],
                "result_count": [
                    1230000.0,
                    74.0,
                    93500.0
                ],
                "answer_relation_to_question": [
                    0.7243956043956045,
                    1.3793406593406594,
                    0.8962637362637363
                ],
                "word_count_appended": [
                    48.0,
                    21.0,
                    55.0
                ]
            },
            "integer_answers": {
                "audrey hepburn": 5,
                "jean harlow": 5,
                "rita hayworth": 4
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Lonnie Lynn's only Academy Award win was in what category?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "best original song"
            ],
            "lines": [
                [
                    0.3242604855925482,
                    0.17758118306743909,
                    0.22759196403908546,
                    0.31892247574764543,
                    0.05231004200076365,
                    0.11740890688259109,
                    0.0671892497200448,
                    0.05420331356105543,
                    0.3192177423386492,
                    0.2582781456953642,
                    0.2857142857142857,
                    0.3223458832957832,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.3417405858219797,
                    0.4073828315686249,
                    0.2858051931779614,
                    0.2546514258871344,
                    0.046582665139366174,
                    0.23481781376518218,
                    0.05655095184770437,
                    0.0478625485784414,
                    0.26135276137150854,
                    0.17880794701986755,
                    0.14285714285714285,
                    0.3253283650798625,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.3339989285854721,
                    0.415035985363936,
                    0.4866028427829532,
                    0.4264260983652201,
                    0.9011072928598702,
                    0.6477732793522267,
                    0.8762597984322509,
                    0.8979341378605031,
                    0.4194294962898423,
                    0.5629139072847682,
                    0.5714285714285714,
                    0.3523257516243543,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "best adapted screenplay": 0.18035883411823248,
                "best original song": 0.6350882921592835,
                "best cinematography": 0.18455287372248397
            },
            "question": "lonnie lynn's only academy award win was in what category?",
            "rate_limited": false,
            "answers": [
                "best adapted screenplay",
                "best cinematography",
                "best original song"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "best adapted screenplay": 0.12583740508419558,
                "best original song": 0.508863522024177,
                "best cinematography": 0.04799501952180854
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.934075299774699,
                    1.951970190479175,
                    2.113954509746126
                ],
                "result_count_important_words": [
                    2400.0,
                    2020.0,
                    31300.0
                ],
                "wikipedia_search": [
                    1.2768709693545968,
                    1.0454110454860341,
                    1.677717985159369
                ],
                "word_count_appended_bing": [
                    4.0,
                    2.0,
                    8.0
                ],
                "answer_relation_to_question_bing": [
                    0.7103247322697563,
                    1.6295313262744997,
                    1.660143941455744
                ],
                "cosine_similarity_raw": [
                    0.05464781075716019,
                    0.0686255693435669,
                    0.11683971434831619
                ],
                "result_count_noun_chunks": [
                    2650.0,
                    2340.0,
                    43900.0
                ],
                "question_answer_similarity": [
                    6.861212283256464,
                    5.478502219542861,
                    9.174016278237104
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    16.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    4.0
                ],
                "result_count_bing": [
                    20300.0,
                    40600.0,
                    112000.0
                ],
                "word_count_appended": [
                    39.0,
                    27.0,
                    85.0
                ],
                "answer_relation_to_question": [
                    1.2970419423701929,
                    1.3669623432879188,
                    1.3359957143418884
                ],
                "result_count": [
                    2740.0,
                    2440.0,
                    47200.0
                ]
            },
            "integer_answers": {
                "best adapted screenplay": 0,
                "best original song": 13,
                "best cinematography": 1
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these products was featured on \u201cShark Tank\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "scrub daddy"
            ],
            "lines": [
                [
                    0.005555555555555556,
                    0.09821428571428573,
                    0.038109894076648484,
                    0.3683393131998138,
                    0.4501851228542578,
                    0.7504288164665524,
                    0.9774117370087895,
                    0.3508887425938117,
                    0.03125,
                    0.17829457364341086,
                    0.08333333333333333,
                    0.28948285021393894,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.8104166666666667,
                    0.7678571428571429,
                    0.9450204982722142,
                    0.2753979538597314,
                    0.5469538875799395,
                    0.12735849056603774,
                    0.022484437793165658,
                    0.27518104015799866,
                    0.76875,
                    0.6705426356589147,
                    0.8333333333333334,
                    0.5165360463329981,
                    1.0,
                    0,
                    -1.0
                ],
                [
                    0.1840277777777778,
                    0.13392857142857145,
                    0.016869607651137264,
                    0.35626273294045474,
                    0.00286098956580276,
                    0.12221269296740996,
                    0.000103825198044912,
                    0.3739302172481896,
                    0.2,
                    0.1511627906976744,
                    0.08333333333333333,
                    0.1939811034530629,
                    0.0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "scrub daddy": 0.5815255486983187,
                "sticky buddy": 0.1398979724816507,
                "instant pot": 0.2785764788200306
            },
            "question": "which of these products was featured on \u201cshark tank\u201d?",
            "rate_limited": false,
            "answers": [
                "instant pot",
                "scrub daddy",
                "sticky buddy"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "scrub daddy": 0.6733220496632761,
                "sticky buddy": 0.08670646316010823,
                "instant pot": 0.0704715938999442
            },
            "integer_answers": {
                "scrub daddy": 9,
                "sticky buddy": 1,
                "instant pot": 3
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1579314008557557,
                    2.0661441853319924,
                    0.7759244138122516
                ],
                "result_count_important_words": [
                    7390000.0,
                    170000.0,
                    785.0
                ],
                "wikipedia_search": [
                    0.125,
                    3.075,
                    0.8
                ],
                "word_count_appended_bing": [
                    2.0,
                    20.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.39285714285714285,
                    3.071428571428571,
                    0.5357142857142857
                ],
                "cosine_similarity_raw": [
                    0.00944620929658413,
                    0.23423999547958374,
                    0.004181429743766785
                ],
                "result_count_noun_chunks": [
                    53300.0,
                    41800.0,
                    56800.0
                ],
                "question_answer_similarity": [
                    4.791550762951374,
                    3.5825208676978946,
                    4.634452279889956
                ],
                "word_count_noun_chunks": [
                    0.0,
                    4.0,
                    0.0
                ],
                "result_count_bing": [
                    1750000.0,
                    297000.0,
                    285000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    10700.0,
                    13000.0,
                    68.0
                ],
                "answer_relation_to_question": [
                    0.022222222222222223,
                    3.2416666666666667,
                    0.7361111111111112
                ],
                "word_count_appended": [
                    46.0,
                    173.0,
                    39.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What are the first words spoken by God in the King James Bible?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "let there be light"
            ],
            "lines": [
                [
                    0.46868173258003765,
                    0.5514705882352942,
                    0.374939533513722,
                    0.3809094603511877,
                    0.9983455987221176,
                    0.40141378439787934,
                    0.9993512184572658,
                    0.5830703800246948,
                    0.5000000000000001,
                    0.8095238095238095,
                    0.75,
                    0.6579292088249895,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.38002690341673395,
                    0.2647058823529412,
                    0.5085845423171628,
                    0.27323773210923086,
                    0.00031376575959837983,
                    0.2375662711436506,
                    0.000125203455615375,
                    0.00020578954589106874,
                    0.33333333333333337,
                    0.047619047619047616,
                    0.125,
                    0.09527312440451795,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.1512913640032284,
                    0.18382352941176472,
                    0.11647592416911523,
                    0.3458528075395815,
                    0.0013406355182839864,
                    0.3610199444584701,
                    0.0005235780871188408,
                    0.4167238304294142,
                    0.16666666666666669,
                    0.14285714285714285,
                    0.125,
                    0.24679766677049245,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "hello, my children": 0.16185654253269452,
                "let there be light": 0.6768310939022141,
                "this is my gift": 0.16131236356509135
            },
            "question": "what are the first words spoken by god in the king james bible?",
            "rate_limited": false,
            "answers": [
                "let there be light",
                "hello, my children",
                "this is my gift"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "hello, my children": 0.2558424637303858,
                "let there be light": 0.6580514912745106,
                "this is my gift": 0.07256102755443461
            },
            "integer_answers": {
                "hello, my children": 1,
                "let there be light": 13,
                "this is my gift": 0
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.947575252949937,
                    0.5716387464271077,
                    1.4807860006229547
                ],
                "result_count_important_words": [
                    87800.0,
                    11.0,
                    46.0
                ],
                "wikipedia_search": [
                    1.0,
                    0.6666666666666666,
                    0.3333333333333333
                ],
                "word_count_appended_bing": [
                    12.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    2.2058823529411766,
                    1.0588235294117647,
                    0.7352941176470589
                ],
                "cosine_similarity_raw": [
                    0.09085695445537567,
                    0.12324238568544388,
                    0.028224945068359375
                ],
                "result_count_noun_chunks": [
                    102000.0,
                    36.0,
                    72900.0
                ],
                "question_answer_similarity": [
                    21.874170124530792,
                    15.690995521843433,
                    19.861000940203667
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    1590000.0,
                    941000.0,
                    1430000.0
                ],
                "word_count_raw": [
                    4.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    35000.0,
                    11.0,
                    47.0
                ],
                "answer_relation_to_question": [
                    2.3434086629001882,
                    1.9001345170836696,
                    0.756456820016142
                ],
                "word_count_appended": [
                    85.0,
                    5.0,
                    15.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What weather term describes ice crystals that seem to fall from a cloudless sky?": {
        "raw_data": {
            "data": {
                "word_count_appended_relation_to_question": [
                    0.403751917595364,
                    2.9409810833977543,
                    4.655266999006881
                ],
                "result_count_important_words": [
                    0,
                    68.0,
                    6780.0
                ],
                "wikipedia_search": [
                    0.0,
                    2.6857793622499506,
                    3.3142206377500494
                ],
                "word_count_appended_bing": [
                    27.0,
                    2.0,
                    10.0
                ],
                "answer_relation_to_question_bing": [
                    2.2942176870748296,
                    1.6700680272108843,
                    2.0357142857142856
                ],
                "cosine_similarity_raw": [
                    0.01482301577925682,
                    0.0961662158370018,
                    0.333956241607666
                ],
                "result_count_noun_chunks": [
                    0,
                    1390.0,
                    9360.0
                ],
                "question_answer_similarity": [
                    0.0,
                    5.871638938784599,
                    6.879055388271809
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    15.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    15.0
                ],
                "result_count_bing": [
                    28400.0,
                    47400.0,
                    20200.0
                ],
                "word_count_appended": [
                    16.0,
                    19.0,
                    70.0
                ],
                "answer_relation_to_question": [
                    0.6828606442577031,
                    2.876968155683326,
                    3.440171200058971
                ],
                "result_count": [
                    0,
                    396.0,
                    6680.0
                ]
            },
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "z-best_answer_by_ml": [
                "diamond dust",
                "bottom"
            ],
            "lines": [
                [
                    0.0975515206082433,
                    0.3823696145124716,
                    0.03331422988047109,
                    0.0,
                    0.0,
                    0.29583333333333334,
                    0.0,
                    0.0,
                    0.0,
                    0.1523809523809524,
                    0.6923076923076923,
                    0.050468989699420506,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.4109954508119037,
                    0.27834467120181405,
                    0.21613033871367177,
                    0.4604956238598899,
                    0.05596382136800452,
                    0.49375,
                    0.009929906542056074,
                    0.12930232558139534,
                    0.4476298937083251,
                    0.18095238095238095,
                    0.05128205128205128,
                    0.36762263542471935,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.491453028579853,
                    0.33928571428571425,
                    0.7505554314058571,
                    0.5395043761401102,
                    0.9440361786319955,
                    0.21041666666666667,
                    0.990070093457944,
                    0.8706976744186047,
                    0.5523701062916749,
                    0.6666666666666666,
                    0.2564102564102564,
                    0.5819083748758602,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "question": "what weather term describes ice crystals that seem to fall from a cloudless sky?",
            "rate_limited": false,
            "answers": [
                "flenches",
                "mares' tails",
                "diamond dust"
            ],
            "ml_answers": {
                "diamond dust": 1.0041128804751562,
                "mares' tails": 0.1478480786382689,
                "flenches": 0.021666051995611273
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "The shore of Utah's Great Salt Lake is home to a famous piece of what kind of art?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "earthwork sculpture",
                "middle"
            ],
            "question": "the shore of utah's great salt lake is home to a famous piece of what kind of art?",
            "lines": [
                [
                    0.1630712820769578,
                    0.3039480299542219,
                    0.08411733804258678,
                    0.39845657918072797,
                    0.9998454615022259,
                    0.3156297420333839,
                    0.9999926812129853,
                    0.999869457502679,
                    0.6288305798509881,
                    0.1276595744680851,
                    0.25,
                    0.48576810468342363,
                    0.0,
                    0,
                    1.0
                ],
                [
                    0.6475167402384152,
                    0.49805509248233704,
                    0.8622649088598074,
                    0.12874226467866812,
                    0.0001154383959276313,
                    0.11229135053110774,
                    5.652132545986439e-06,
                    0.0001047209044443265,
                    0.12322643343051506,
                    0.6595744680851063,
                    0.5,
                    0.3735330489160977,
                    1.0,
                    0,
                    1.0
                ],
                [
                    0.1894119776846271,
                    0.197996877563441,
                    0.05361775309760575,
                    0.4728011561406039,
                    3.9100101846455763e-05,
                    0.5720789074355084,
                    1.6666544686883088e-06,
                    2.5821592876683246e-05,
                    0.24794298671849693,
                    0.2127659574468085,
                    0.25,
                    0.14069884640047875,
                    0.0,
                    0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "replica cave painting",
                "earthwork sculpture",
                "large-scale watercolor"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "replica cave painting": 0.11057652215471295,
                "large-scale watercolor": 0.002610472570025696,
                "earthwork sculpture": 0.761157478155356
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    4.857681046834236,
                    3.735330489160977,
                    1.4069884640047874
                ],
                "result_count_important_words": [
                    13800000.0,
                    78.0,
                    23.0
                ],
                "wikipedia_search": [
                    4.401814058956917,
                    0.8625850340136054,
                    1.7356009070294784
                ],
                "word_count_appended_bing": [
                    2.0,
                    4.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.8236881797253313,
                    2.988330554894022,
                    1.187981265380646
                ],
                "cosine_similarity_raw": [
                    0.04207490757107735,
                    0.4312989115715027,
                    0.026819227263331413
                ],
                "result_count_noun_chunks": [
                    697000.0,
                    73.0,
                    18.0
                ],
                "question_answer_similarity": [
                    14.63142841309309,
                    4.727449182188138,
                    17.36138046439737
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    41600.0,
                    14800.0,
                    75400.0
                ],
                "word_count_appended": [
                    6.0,
                    31.0,
                    10.0
                ],
                "answer_relation_to_question": [
                    1.6307128207695782,
                    6.475167402384152,
                    1.8941197768462708
                ],
                "result_count": [
                    537000.0,
                    62.0,
                    21.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these creatures is most likely to bark?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "dog"
            ],
            "lines": [
                [
                    0.0,
                    0.0,
                    0.02378844171978863,
                    0.07340714686114677,
                    0.1388888888888889,
                    0.1868941450174486,
                    0.012319339298004129,
                    0.10802200984651028,
                    0.038461538461538464,
                    0.21470019342359767,
                    0.20930232558139536,
                    0.2893370122245085,
                    0.18181818181818182,
                    0.012658227848101266,
                    -1.0
                ],
                [
                    0.3888888888888889,
                    0.0,
                    0.04373486482716273,
                    0.5597380241609013,
                    0.36997635933806144,
                    0.10740597130670802,
                    0.037921541637990365,
                    0.15783376773819865,
                    0.2692307692307692,
                    0.14216634429400388,
                    0.12403100775193798,
                    0.30936546836892626,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.6111111111111112,
                    1.0,
                    0.9324766934530486,
                    0.36685482897795185,
                    0.49113475177304966,
                    0.7056998836758434,
                    0.9497591190640055,
                    0.7341442224152911,
                    0.6923076923076923,
                    0.6431334622823984,
                    0.6666666666666666,
                    0.40129751940656516,
                    0.8181818181818182,
                    0.9873417721518988,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "blue whale": 0.1793066433959678,
                "mime": 0.10639981792779361,
                "dog": 0.7142935386762386
            },
            "question": "which of these creatures is most likely to bark?",
            "rate_limited": false,
            "answers": [
                "mime",
                "blue whale",
                "dog"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "blue whale": 0.1377539060681903,
                "mime": 0.16332483679320706,
                "dog": 0.7906221929744528
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.8680110366735256,
                    0.9280964051067787,
                    1.2038925582196955
                ],
                "result_count_important_words": [
                    179000.0,
                    551000.0,
                    13800000.0
                ],
                "wikipedia_search": [
                    0.07692307692307693,
                    0.5384615384615384,
                    1.3846153846153846
                ],
                "word_count_appended_bing": [
                    27.0,
                    16.0,
                    86.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "cosine_similarity_raw": [
                    0.017171353101730347,
                    0.03156939893960953,
                    0.6730952262878418
                ],
                "result_count_noun_chunks": [
                    746000.0,
                    1090000.0,
                    5070000.0
                ],
                "question_answer_similarity": [
                    0.6018534117611125,
                    4.58920219540596,
                    3.0077838450670242
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    9.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    78.0
                ],
                "result_count_bing": [
                    4820000.0,
                    2770000.0,
                    18200000.0
                ],
                "word_count_appended": [
                    222.0,
                    147.0,
                    665.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    1.1666666666666667,
                    1.8333333333333335
                ],
                "result_count": [
                    235000.0,
                    626000.0,
                    831000.0
                ]
            },
            "integer_answers": {
                "blue whale": 1,
                "mime": 0,
                "dog": 13
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What is the correct spelling of the Pennsylvania town famous for its groundhog?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "punxsutawney"
            ],
            "question": "what is the correct spelling of the pennsylvania town famous for its groundhog?",
            "lines": [
                [
                    0.49230769230769234,
                    0.42894078987481504,
                    0.5000000325663608,
                    -0.0,
                    0.0,
                    0.17015209125475286,
                    0.0,
                    0.0,
                    0.0,
                    0.031683168316831684,
                    0.18045112781954886,
                    0.07469762368616616,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.015384615384615385,
                    0.17927565392354122,
                    0.0,
                    -0.0,
                    0.0,
                    0.6615969581749049,
                    0.0,
                    0.0,
                    0.0,
                    0.031683168316831684,
                    0.21052631578947367,
                    0.06649798782113667,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.49230769230769234,
                    0.3917835562016436,
                    0.4999999674336392,
                    1.0,
                    1.0,
                    0.1682509505703422,
                    1.0,
                    1.0,
                    1.0,
                    0.9366336633663367,
                    0.6090225563909775,
                    0.8588043884926972,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "punksatawany",
                "puncksehtany",
                "punxsutawney"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "punksatawany": 0.33366647021590257,
                "puncksehtany": 0.14270831565118966,
                "punxsutawney": 0.7285768453459914
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.4481857421169969,
                    0.39898792692682,
                    5.1528263309561835
                ],
                "result_count_important_words": [
                    0,
                    0,
                    177000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    3.0
                ],
                "word_count_appended_bing": [
                    24.0,
                    28.0,
                    81.0
                ],
                "answer_relation_to_question_bing": [
                    2.144703949374075,
                    0.8963782696177062,
                    1.9589177810082181
                ],
                "cosine_similarity_raw": [
                    0.9151260256767273,
                    0.0,
                    0.9151259064674377
                ],
                "result_count_noun_chunks": [
                    0,
                    0,
                    25600.0
                ],
                "question_answer_similarity": [
                    0.0,
                    0.0,
                    -0.9890388045459986
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    252.0
                ],
                "result_count_bing": [
                    17900.0,
                    69600.0,
                    17700.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    77.0
                ],
                "word_count_appended": [
                    16.0,
                    16.0,
                    473.0
                ],
                "answer_relation_to_question": [
                    2.4615384615384617,
                    0.07692307692307693,
                    2.4615384615384617
                ],
                "result_count": [
                    0,
                    0,
                    56200.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "The author of which sci-fi classic is credited with inventing the modern waterbed?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "starship troopers"
            ],
            "question": "the author of which sci-fi classic is credited with inventing the modern waterbed?",
            "lines": [
                [
                    0.1890427431523322,
                    0.239430829594764,
                    0.4796809809083603,
                    0.07067971835527956,
                    0.9807417974322397,
                    0.002844345637035342,
                    0.3877551020408163,
                    0.18851435705368288,
                    0.4151332702594752,
                    0.5125,
                    0.375,
                    0.5122741530465168,
                    0.25,
                    0.6,
                    -1.0
                ],
                [
                    0.3747929391765008,
                    0.48454927143451737,
                    0.3689195130742927,
                    0.4569812248038246,
                    0.00891583452211127,
                    0.545983587798738,
                    0.3877551020408163,
                    0.3913857677902622,
                    0.1554812908716068,
                    0.175,
                    0.4166666666666667,
                    0.24650757495630873,
                    0.25,
                    0.2,
                    -1.0
                ],
                [
                    0.4361643176711669,
                    0.2760198989707186,
                    0.15139950601734697,
                    0.47233905684089583,
                    0.010342368045649072,
                    0.45117206656422665,
                    0.22448979591836735,
                    0.42009987515605496,
                    0.429385438868918,
                    0.3125,
                    0.20833333333333334,
                    0.24121827199717452,
                    0.5,
                    0.2,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "starship troopers",
                "2001: a space odyssey",
                "the time machine"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "starship troopers": 0.656278249766202,
                "the time machine": 0.22367495901153464,
                "2001: a space odyssey": 0.1425690095997216
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    4.0981932243721335,
                    1.9720605996504696,
                    1.929746175977396
                ],
                "result_count_important_words": [
                    38.0,
                    38.0,
                    22.0
                ],
                "wikipedia_search": [
                    2.4907996215568513,
                    0.9328877452296408,
                    2.576312633213508
                ],
                "word_count_appended_bing": [
                    9.0,
                    10.0,
                    5.0
                ],
                "answer_relation_to_question_bing": [
                    1.436584977568584,
                    2.907295628607104,
                    1.6561193938243117
                ],
                "cosine_similarity_raw": [
                    0.2014000564813614,
                    0.15489546954631805,
                    0.06356697529554367
                ],
                "result_count_noun_chunks": [
                    302000.0,
                    627000.0,
                    673000.0
                ],
                "question_answer_similarity": [
                    2.140003489330411,
                    13.836238151416183,
                    14.301234545186162
                ],
                "word_count_noun_chunks": [
                    1.0,
                    1.0,
                    2.0
                ],
                "word_count_raw": [
                    3.0,
                    1.0,
                    1.0
                ],
                "result_count_bing": [
                    1740.0,
                    334000.0,
                    276000.0
                ],
                "word_count_appended": [
                    41.0,
                    14.0,
                    25.0
                ],
                "answer_relation_to_question": [
                    1.1342564589139932,
                    2.248757635059005,
                    2.6169859060270015
                ],
                "result_count": [
                    2750.0,
                    25.0,
                    29.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "In 1973, Arabella and Anita became the first members of what species to go into space?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "spider"
            ],
            "question": "in 1973, arabella and anita became the first members of what species to go into space?",
            "lines": [
                [
                    0.5134966688158178,
                    0.1445378151260504,
                    0.8629942598893077,
                    0.23661198064976566,
                    0.9201596806387226,
                    0.48434283237620546,
                    0.4787664307381193,
                    0.7177681473456121,
                    0.73,
                    0.8362068965517241,
                    0.6627906976744186,
                    0.6084402151091959,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.2793036750483559,
                    0.5815126050420169,
                    0.09981199890366943,
                    0.7008173485808812,
                    0.033932135728542916,
                    0.03185610575360277,
                    0.021739130434782608,
                    0.0866738894907909,
                    0.09142857142857143,
                    0.01293103448275862,
                    0.023255813953488372,
                    0.1883994111022505,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.20719965613582633,
                    0.2739495798319328,
                    0.037193741207022867,
                    0.06257067076935317,
                    0.04590818363273453,
                    0.4838010618701918,
                    0.4994944388270981,
                    0.19555796316359697,
                    0.17857142857142858,
                    0.15086206896551724,
                    0.313953488372093,
                    0.20316037378855362,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "spider",
                "fruit fly",
                "ladybug"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "ladybug": 0.1049890814725439,
                "fruit fly": 0.22890571217015276,
                "spider": 0.7723488789400673
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.650641290655175,
                    1.130396466613503,
                    1.2189622427313216
                ],
                "result_count_important_words": [
                    9470.0,
                    430.0,
                    9880.0
                ],
                "wikipedia_search": [
                    2.92,
                    0.3657142857142857,
                    0.7142857142857143
                ],
                "word_count_appended_bing": [
                    57.0,
                    2.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    0.2890756302521008,
                    1.1630252100840337,
                    0.5478991596638656
                ],
                "cosine_similarity_raw": [
                    0.4063692092895508,
                    0.04699975997209549,
                    0.01751389540731907
                ],
                "result_count_noun_chunks": [
                    26500.0,
                    3200.0,
                    7220.0
                ],
                "question_answer_similarity": [
                    2.5772198028862476,
                    7.633427284657955,
                    0.6815308816730976
                ],
                "word_count_noun_chunks": [
                    158.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    53.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    8940000.0,
                    588000.0,
                    8930000.0
                ],
                "word_count_appended": [
                    194.0,
                    3.0,
                    35.0
                ],
                "answer_relation_to_question": [
                    1.5404900064474534,
                    0.8379110251450677,
                    0.621598968407479
                ],
                "result_count": [
                    461.0,
                    17.0,
                    23.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these songs was NOT recorded by the Beach Boys?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "surf city"
            ],
            "question": "which of these songs was not recorded by the beach boys?",
            "lines": [
                [
                    0.3773251778249205,
                    0.34455737082066873,
                    0.393589311646069,
                    0.25013400436415434,
                    0.48833502882332996,
                    0.3330404217926186,
                    0.2365544381285527,
                    0.286243631310792,
                    0.37202380952380953,
                    0.34726688102893893,
                    0.3983739837398374,
                    0.3238186656143054,
                    0.3055555555555556,
                    0.0,
                    -1.0
                ],
                [
                    0.3943779442503276,
                    0.36160714285714285,
                    0.3800454957843492,
                    0.3637892708124604,
                    0.49521871820956254,
                    0.335676625659051,
                    0.39462177525142106,
                    0.44997684113015285,
                    0.5,
                    0.36173633440514474,
                    0.3252032520325203,
                    0.35846732382323643,
                    0.4722222222222222,
                    0.5,
                    -1.0
                ],
                [
                    0.22829687792475195,
                    0.2938354863221885,
                    0.2263651925695819,
                    0.38607672482338534,
                    0.0164462529671075,
                    0.3312829525483304,
                    0.36882378662002624,
                    0.2637795275590551,
                    0.12797619047619047,
                    0.29099678456591643,
                    0.2764227642276423,
                    0.3177140105624582,
                    0.2222222222222222,
                    0.5,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "surf's up",
                "surf city",
                "surfer girl"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "surf city": 0.21570199080020808,
                "surf's up": 0.11043782264411883,
                "surfer girl": 0.0620409571578917
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.4094506750855569,
                    1.1322614094141086,
                    1.4582879155003345
                ],
                "result_count_important_words": [
                    241000.0,
                    96400.0,
                    120000.0
                ],
                "wikipedia_search": [
                    1.0238095238095237,
                    0.0,
                    2.9761904761904763
                ],
                "word_count_appended_bing": [
                    25.0,
                    43.0,
                    55.0
                ],
                "answer_relation_to_question_bing": [
                    1.2435410334346504,
                    1.1071428571428572,
                    1.6493161094224924
                ],
                "cosine_similarity_raw": [
                    0.058895185589790344,
                    0.06639128923416138,
                    0.1514488160610199
                ],
                "result_count_noun_chunks": [
                    923000.0,
                    216000.0,
                    1020000.0
                ],
                "question_answer_similarity": [
                    11.58759555965662,
                    6.3168052807450294,
                    5.283219248056412
                ],
                "word_count_noun_chunks": [
                    7.0,
                    1.0,
                    10.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    1900000.0,
                    1870000.0,
                    1920000.0
                ],
                "word_count_appended": [
                    95.0,
                    86.0,
                    130.0
                ],
                "answer_relation_to_question": [
                    0.9813985774006364,
                    0.8449764459973794,
                    2.1736249766019844
                ],
                "result_count": [
                    172000.0,
                    70500.0,
                    7130000.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "The mouse that came with the original 1998 iMac was criticized for being what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "round"
            ],
            "lines": [
                [
                    0.375,
                    0.36231884057971014,
                    0.15638847396493522,
                    0.24347519038833706,
                    0.30027932960893855,
                    0.168,
                    0.028759630653414102,
                    0.2982456140350877,
                    0.6666666666666666,
                    0.30267558528428096,
                    0.18490566037735848,
                    0.3357687486003062,
                    0.017241379310344827,
                    0.0,
                    1.0
                ],
                [
                    0.25,
                    0.2971014492753623,
                    0.25496208731250436,
                    0.40539016868714883,
                    0.3861731843575419,
                    0.5186666666666667,
                    0.9410104099276598,
                    0.43859649122807015,
                    0.0,
                    0.3193979933110368,
                    0.24150943396226415,
                    0.3462470733461473,
                    0.05172413793103448,
                    0.020833333333333332,
                    1.0
                ],
                [
                    0.375,
                    0.3405797101449275,
                    0.5886494387225604,
                    0.3511346409245141,
                    0.31354748603351956,
                    0.31333333333333335,
                    0.03022995941892607,
                    0.2631578947368421,
                    0.3333333333333333,
                    0.3779264214046823,
                    0.5735849056603773,
                    0.3179841780535465,
                    0.9310344827586207,
                    0.9791666666666666,
                    1.0
                ]
            ],
            "fraction_answers": {
                "wireless": 0.24569465139066995,
                "large": 0.3194008878099122,
                "round": 0.4349044607994178
            },
            "question": "the mouse that came with the original 1998 imac was criticized for being what?",
            "rate_limited": false,
            "answers": [
                "wireless",
                "large",
                "round"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "wireless": 0.12691597782648173,
                "large": 0.1371372219105358,
                "round": 0.7962475017889267
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.014612491601837,
                    2.0774824400768837,
                    1.907905068321279
                ],
                "result_count_important_words": [
                    48900.0,
                    1600000.0,
                    51400.0
                ],
                "wikipedia_search": [
                    2.6666666666666665,
                    0.0,
                    1.3333333333333333
                ],
                "word_count_appended_bing": [
                    49.0,
                    64.0,
                    152.0
                ],
                "answer_relation_to_question_bing": [
                    0.7246376811594203,
                    0.5942028985507246,
                    0.681159420289855
                ],
                "cosine_similarity_raw": [
                    0.02047910913825035,
                    0.03338734805583954,
                    0.07708378881216049
                ],
                "result_count_noun_chunks": [
                    170000.0,
                    250000.0,
                    150000.0
                ],
                "question_answer_similarity": [
                    2.6738502476364374,
                    4.452004334423691,
                    3.856168856844306
                ],
                "word_count_noun_chunks": [
                    1.0,
                    3.0,
                    54.0
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    47.0
                ],
                "result_count_bing": [
                    1260000.0,
                    3890000.0,
                    2350000.0
                ],
                "word_count_appended": [
                    181.0,
                    191.0,
                    226.0
                ],
                "answer_relation_to_question": [
                    1.5,
                    1.0,
                    1.5
                ],
                "result_count": [
                    43000.0,
                    55300.0,
                    44900.0
                ]
            },
            "integer_answers": {
                "wireless": 3,
                "large": 6,
                "round": 5
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these substances is both artificially made and found in nature?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "nylon"
            ],
            "lines": [
                [
                    0.4347826086956522,
                    0.16666666666666666,
                    0.40998680945592125,
                    0.08729518727437834,
                    0.26376146788990823,
                    0.3333333333333333,
                    0.34657039711191334,
                    0.42771804062126645,
                    0.2833333333333333,
                    0.29746192893401013,
                    0.26851851851851855,
                    0.32527956678686865,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.3478260869565218,
                    0.08333333333333333,
                    0.3732966375498108,
                    0.43601795054528153,
                    0.3532110091743119,
                    0.3333333333333333,
                    0.31227436823104693,
                    0.26523297491039427,
                    0.09999999999999999,
                    0.3116751269035533,
                    0.35185185185185186,
                    0.34434278259483403,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.2173913043478261,
                    0.75,
                    0.21671655299426795,
                    0.4766868621803401,
                    0.3830275229357798,
                    0.3333333333333333,
                    0.34115523465703973,
                    0.3070489844683393,
                    0.6166666666666667,
                    0.39086294416243655,
                    0.37962962962962965,
                    0.3303776506182973,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "latex": 0.3952413904994964,
                "teflon": 0.3037256548851475,
                "nylon": 0.3010329546153561
            },
            "question": "which of these substances is both artificially made and found in nature?",
            "rate_limited": false,
            "answers": [
                "teflon",
                "nylon",
                "latex"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "latex": 0.2333228432113928,
                "teflon": 0.13035051088164756,
                "nylon": 0.29888308289832444
            },
            "integer_answers": {
                "latex": 6,
                "teflon": 5,
                "nylon": 1
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3011182671474746,
                    1.3773711303793361,
                    1.3215106024731893
                ],
                "result_count_important_words": [
                    1920000.0,
                    1730000.0,
                    1890000.0
                ],
                "wikipedia_search": [
                    0.85,
                    0.3,
                    1.85
                ],
                "word_count_appended_bing": [
                    29.0,
                    38.0,
                    41.0
                ],
                "answer_relation_to_question_bing": [
                    0.3333333333333333,
                    0.16666666666666666,
                    1.5
                ],
                "cosine_similarity_raw": [
                    0.048627905547618866,
                    0.04427614063024521,
                    0.025704417377710342
                ],
                "result_count_noun_chunks": [
                    3580000.0,
                    2220000.0,
                    2570000.0
                ],
                "question_answer_similarity": [
                    0.3373199393681716,
                    1.684829980134964,
                    1.8419799357652664
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    2850000.0,
                    2850000.0,
                    2850000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    2300000.0,
                    3080000.0,
                    3340000.0
                ],
                "answer_relation_to_question": [
                    0.43478260869565216,
                    0.34782608695652173,
                    0.21739130434782608
                ],
                "word_count_appended": [
                    293.0,
                    307.0,
                    385.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Every U.S. state that starts with which of these letters has a Democratic governor?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "v"
            ],
            "lines": [
                [
                    0.16948529411764707,
                    0.2348694316436252,
                    0.1166343046951322,
                    0.29650389086959006,
                    0.3261538461538461,
                    0.33346550356859633,
                    0.3241296518607443,
                    0.34154589371980676,
                    0.18421052631578946,
                    0.2512703439133957,
                    0.22309552599758162,
                    0.3297837029038114,
                    0.20944962766412736,
                    0.2496119217634275,
                    -1.0
                ],
                [
                    0.1860294117647059,
                    0.30284178187403993,
                    0.34168646231759614,
                    0.48435498304797675,
                    0.3630769230769231,
                    0.3326724821570182,
                    0.35414165666266506,
                    0.3502415458937198,
                    0.5,
                    0.45216879004344945,
                    0.577589681580008,
                    0.32377968743986396,
                    0.4698279551485064,
                    0.4889785780813412,
                    -1.0
                ],
                [
                    0.6444852941176471,
                    0.46228878648233485,
                    0.5416792329872716,
                    0.2191411260824332,
                    0.31076923076923074,
                    0.3338620142743854,
                    0.3217286914765906,
                    0.30821256038647343,
                    0.3157894736842105,
                    0.29656086604315485,
                    0.19931479242241032,
                    0.34643660965632456,
                    0.32072241718736627,
                    0.2614095001552313,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "c": 0.394813567077701,
                "w": 0.2564435332276515,
                "v": 0.34874289969464745
            },
            "question": "every u.s. state that starts with which of these letters has a democratic governor?",
            "rate_limited": false,
            "answers": [
                "w",
                "c",
                "v"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "c": 0.2777643218584197,
                "w": 0.16896246226856335,
                "v": 0.5073009419815131
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9787022174228686,
                    1.9426781246391838,
                    2.0786196579379475
                ],
                "result_count_important_words": [
                    2700000.0,
                    2950000.0,
                    2680000.0
                ],
                "wikipedia_search": [
                    0.3684210526315789,
                    1.0,
                    0.631578947368421
                ],
                "word_count_appended_bing": [
                    1107.0,
                    2866.0,
                    989.0
                ],
                "answer_relation_to_question_bing": [
                    0.7046082949308756,
                    0.9085253456221198,
                    1.3868663594470045
                ],
                "cosine_similarity_raw": [
                    0.01763390377163887,
                    0.051659468561410904,
                    0.08189631253480911
                ],
                "result_count_noun_chunks": [
                    7070000.0,
                    7250000.0,
                    6380000.0
                ],
                "question_answer_similarity": [
                    1.6639529606327415,
                    2.718156263232231,
                    1.2298001367598772
                ],
                "word_count_noun_chunks": [
                    2447.0,
                    5489.0,
                    3747.0
                ],
                "word_count_raw": [
                    804.0,
                    1575.0,
                    842.0
                ],
                "result_count_bing": [
                    8410000.0,
                    8390000.0,
                    8420000.0
                ],
                "result_count": [
                    10600000.0,
                    11800000.0,
                    10100000.0
                ],
                "answer_relation_to_question": [
                    0.6779411764705883,
                    0.7441176470588236,
                    2.5779411764705884
                ],
                "word_count_appended": [
                    3412.0,
                    6140.0,
                    4027.0
                ]
            },
            "integer_answers": {
                "c": 9,
                "w": 0,
                "v": 5
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "In the King James Bible, which of these measures \u201csix cubits and a span\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "goliath"
            ],
            "question": "in the king james bible, which of these measures \u201csix cubits and a span\u201d?",
            "lines": [
                [
                    0.19268150071418538,
                    0.10656436487638533,
                    0.19498279797079138,
                    0.43394681206031266,
                    0.004026073619631902,
                    0.1686838124054463,
                    0.20229144289294665,
                    0.051010268300761843,
                    0.07132514202668781,
                    0.035422343324250684,
                    0.038461538461538464,
                    0.11195875015331842,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.5328204328956878,
                    0.3753729752770673,
                    0.11513398513250676,
                    0.5457123440285013,
                    0.01821319018404908,
                    0.2844175491679274,
                    0.3501611170784103,
                    0.3552500828088771,
                    0.5776868052016685,
                    0.06267029972752043,
                    0.09615384615384616,
                    0.3518519799196775,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.27449806639012675,
                    0.5180626598465473,
                    0.6898832168967018,
                    0.020340843911186055,
                    0.977760736196319,
                    0.5468986384266263,
                    0.44754744002864305,
                    0.593739648890361,
                    0.3509880527716438,
                    0.9019073569482289,
                    0.8653846153846154,
                    0.5361892699270041,
                    0,
                    1.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "stone tablet",
                "noah's ark",
                "goliath"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "stone tablet": 0.07769120652448376,
                "noah's ark": 0.25654396823801623,
                "goliath": 0.9021022746415348
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.6717525009199106,
                    2.111111879518065,
                    3.2171356195620247
                ],
                "result_count_important_words": [
                    5650.0,
                    9780.0,
                    12500.0
                ],
                "wikipedia_search": [
                    0.356625710133439,
                    2.888434026008342,
                    1.7549402638582186
                ],
                "word_count_appended_bing": [
                    2.0,
                    5.0,
                    45.0
                ],
                "answer_relation_to_question_bing": [
                    0.319693094629156,
                    1.1261189258312019,
                    1.554187979539642
                ],
                "cosine_similarity_raw": [
                    0.03921795263886452,
                    0.023157525807619095,
                    0.13875997066497803
                ],
                "result_count_noun_chunks": [
                    61600.0,
                    429000.0,
                    717000.0
                ],
                "question_answer_similarity": [
                    7.004248064011335,
                    8.80823299754411,
                    0.32831746339797974
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    54.0
                ],
                "result_count_bing": [
                    22300.0,
                    37600.0,
                    72300.0
                ],
                "word_count_appended": [
                    13.0,
                    23.0,
                    331.0
                ],
                "answer_relation_to_question": [
                    0.9634075035709269,
                    2.664102164478439,
                    1.3724903319506336
                ],
                "result_count": [
                    21.0,
                    95.0,
                    5100.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these are you most likely to find in a toolbox?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "hammer"
            ],
            "lines": [
                [
                    0.21428571428571427,
                    0.2549019607843137,
                    0.7188154521485658,
                    0.3402481905319875,
                    0.9472527472527472,
                    0.47614737032678556,
                    0.7423245614035088,
                    0.9699411217849395,
                    0.05555555555555555,
                    0.7647058823529411,
                    0.6263736263736264,
                    0.45727549695954783,
                    1.0,
                    0,
                    -1.0
                ],
                [
                    0.16666666666666666,
                    0.3137254901960784,
                    0.21204193873537788,
                    0.42028756764684777,
                    0.02879120879120879,
                    0.47614737032678556,
                    0.1118421052631579,
                    0.017250284061563887,
                    0.8888888888888888,
                    0.11990950226244344,
                    0.03296703296703297,
                    0.2807133528330583,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.6190476190476191,
                    0.43137254901960786,
                    0.06914260911605627,
                    0.23946424182116474,
                    0.023956043956043956,
                    0.04770525934642889,
                    0.14583333333333334,
                    0.01280859415349654,
                    0.05555555555555555,
                    0.11538461538461539,
                    0.34065934065934067,
                    0.2620111502073939,
                    0.0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "hammerhead shark": 0.18176468550774275,
                "hammer": 0.5821405907507872,
                "mc hammer": 0.23609472374147003
            },
            "question": "which of these are you most likely to find in a toolbox?",
            "rate_limited": false,
            "answers": [
                "hammer",
                "mc hammer",
                "hammerhead shark"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "hammerhead shark": 0.22172976760634577,
                "hammer": 0.579304156662912,
                "mc hammer": 0.11719082810183122
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3718264908786435,
                    0.842140058499175,
                    0.7860334506221817
                ],
                "result_count_important_words": [
                    677000.0,
                    102000.0,
                    133000.0
                ],
                "wikipedia_search": [
                    0.1111111111111111,
                    1.7777777777777777,
                    0.1111111111111111
                ],
                "word_count_appended_bing": [
                    57.0,
                    3.0,
                    31.0
                ],
                "answer_relation_to_question_bing": [
                    0.5098039215686274,
                    0.6274509803921569,
                    0.8627450980392157
                ],
                "cosine_similarity_raw": [
                    0.1629670113325119,
                    0.048073314130306244,
                    0.0156757403165102
                ],
                "result_count_noun_chunks": [
                    9390000.0,
                    167000.0,
                    124000.0
                ],
                "question_answer_similarity": [
                    2.6737168580293655,
                    3.3026772400480695,
                    1.8817427926696837
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    52600000.0,
                    52600000.0,
                    5270000.0
                ],
                "word_count_appended": [
                    338.0,
                    53.0,
                    51.0
                ],
                "answer_relation_to_question": [
                    0.42857142857142855,
                    0.3333333333333333,
                    1.2380952380952381
                ],
                "result_count": [
                    4310000.0,
                    131000.0,
                    109000.0
                ]
            },
            "integer_answers": {
                "hammerhead shark": 2,
                "hammer": 9,
                "mc hammer": 2
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which Las Vegas hotel features a replica of the Rialto Bridge?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "the venetian"
            ],
            "lines": [
                [
                    0.2649798485288287,
                    0.29793847254335987,
                    0.03035578726570824,
                    0.10272598544463916,
                    0.27877237851662406,
                    0.4138392857142857,
                    0.029008633034654882,
                    0.2222108267952071,
                    0.19819549042736595,
                    0.5,
                    0.4084507042253521,
                    0.3108444170056817,
                    0.02702702702702703,
                    0.0,
                    -1.0
                ],
                [
                    0.23656204824594013,
                    0.2620073446071542,
                    0.8463203824244319,
                    0.5243955590488556,
                    0.6061381074168798,
                    0.153125,
                    0.961499120330592,
                    0.7726099516264123,
                    0.03243769470404984,
                    0.40804597701149425,
                    0.4225352112676056,
                    0.34898441208354747,
                    0.972972972972973,
                    0.9444444444444444,
                    -1.0
                ],
                [
                    0.4984581032252311,
                    0.44005418284948594,
                    0.12332383030985986,
                    0.37287845550650517,
                    0.11508951406649616,
                    0.4330357142857143,
                    0.00949224663475308,
                    0.0051792215783805955,
                    0.7693668148685843,
                    0.09195402298850575,
                    0.16901408450704225,
                    0.3401711709107709,
                    0.0,
                    0.05555555555555555,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "caesars palace": 0.24454092266334895,
                "luxor": 0.22031063260919534,
                "the venetian": 0.5351484447274557
            },
            "question": "which las vegas hotel features a replica of the rialto bridge?",
            "rate_limited": false,
            "answers": [
                "luxor",
                "the venetian",
                "caesars palace"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "caesars palace": 0.42233635842718914,
                "luxor": 0.2716867801592893,
                "the venetian": 0.7745553692078216
            },
            "integer_answers": {
                "caesars palace": 4,
                "luxor": 1,
                "the venetian": 9
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.1759109190397714,
                    2.442890884584832,
                    2.381198196375396
                ],
                "result_count_important_words": [
                    7090.0,
                    235000.0,
                    2320.0
                ],
                "wikipedia_search": [
                    1.1891729425641957,
                    0.19462616822429907,
                    4.616200889211505
                ],
                "word_count_appended_bing": [
                    29.0,
                    30.0,
                    12.0
                ],
                "answer_relation_to_question_bing": [
                    0.8938154176300797,
                    0.7860220338214626,
                    1.3201625485484578
                ],
                "cosine_similarity_raw": [
                    0.02265060320496559,
                    0.6314995884895325,
                    0.09202064573764801
                ],
                "result_count_noun_chunks": [
                    130000.0,
                    452000.0,
                    3030.0
                ],
                "question_answer_similarity": [
                    1.2180064041167498,
                    6.217678481712937,
                    4.421163202263415
                ],
                "word_count_noun_chunks": [
                    2.0,
                    72.0,
                    0.0
                ],
                "result_count_bing": [
                    92700.0,
                    34300.0,
                    97000.0
                ],
                "word_count_raw": [
                    0.0,
                    17.0,
                    1.0
                ],
                "result_count": [
                    3270.0,
                    7110.0,
                    1350.0
                ],
                "answer_relation_to_question": [
                    1.0599193941153149,
                    0.9462481929837605,
                    1.9938324129009244
                ],
                "word_count_appended": [
                    174.0,
                    142.0,
                    32.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these U.S. rivers is the longest?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ohio"
            ],
            "question": "which of these u.s. rivers is the longest?",
            "lines": [
                [
                    0.2666666666666667,
                    0.140625,
                    0.21070699365797016,
                    0.31994159658430127,
                    0.37277701778385774,
                    0.21283783783783783,
                    0.3605683836589698,
                    0.34851936218678814,
                    0.03125,
                    0.3092857142857143,
                    0.27927927927927926,
                    0.32861536697128924,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.5083333333333334,
                    0.6875,
                    0.31405099775270745,
                    0.2879692568305001,
                    0.21272229822161423,
                    0.21283783783783783,
                    0.216696269982238,
                    0.30751708428246016,
                    0.1875,
                    0.38357142857142856,
                    0.35135135135135137,
                    0.3307958956341523,
                    0.0,
                    0.7692307692307693,
                    -1.0
                ],
                [
                    0.22500000000000003,
                    0.171875,
                    0.4752420085893224,
                    0.3920891465851986,
                    0.41450068399452805,
                    0.5743243243243243,
                    0.4227353463587922,
                    0.3439635535307517,
                    0.78125,
                    0.30714285714285716,
                    0.36936936936936937,
                    0.34058873739455847,
                    1.0,
                    0.23076923076923078,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "tennessee",
                "arkansas",
                "ohio"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "ohio": 0.6804049894328371,
                "tennessee": 0.12532341335537175,
                "arkansas": 0.34506905265649757
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.9858461009138677,
                    0.9923876869024568,
                    1.0217662121836755
                ],
                "result_count_important_words": [
                    406000.0,
                    244000.0,
                    476000.0
                ],
                "wikipedia_search": [
                    0.0625,
                    0.375,
                    1.5625
                ],
                "word_count_appended_bing": [
                    31.0,
                    39.0,
                    41.0
                ],
                "answer_relation_to_question_bing": [
                    0.28125,
                    1.375,
                    0.34375
                ],
                "cosine_similarity_raw": [
                    0.045893553644418716,
                    0.06840264797210693,
                    0.1035112515091896
                ],
                "result_count_noun_chunks": [
                    1530000.0,
                    1350000.0,
                    1510000.0
                ],
                "question_answer_similarity": [
                    1.4516853652894497,
                    1.3066158331930637,
                    1.7790436819195747
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_count_raw": [
                    0.0,
                    10.0,
                    3.0
                ],
                "result_count_bing": [
                    12600000.0,
                    12600000.0,
                    34000000.0
                ],
                "word_count_appended": [
                    433.0,
                    537.0,
                    430.0
                ],
                "answer_relation_to_question": [
                    0.5333333333333333,
                    1.0166666666666666,
                    0.45
                ],
                "result_count": [
                    545000.0,
                    311000.0,
                    606000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these is a common piece of playground equipment?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "monkey bars"
            ],
            "question": "which of these is a common piece of playground equipment?",
            "lines": [
                [
                    0.5604166666666667,
                    0.9444444444444445,
                    0.8496701089524507,
                    0.3869770916555688,
                    0.9998843109061761,
                    0.36726384364820847,
                    0.9999273980107055,
                    0.9998276159282882,
                    0.7568027210884354,
                    0.9272727272727272,
                    0.6923076923076923,
                    0.8782603746153956,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.1527777777777778,
                    0.05555555555555555,
                    0.07326876212841349,
                    0.3883870709267776,
                    9.089857371874329e-05,
                    0.31921824104234525,
                    5.2801446759641216e-05,
                    0.00014480262023789002,
                    0.21258503401360543,
                    0.03636363636363636,
                    0.15384615384615385,
                    0.06483672745964238,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.28680555555555554,
                    0.0,
                    0.07706112891913582,
                    0.22463583741765356,
                    2.4790520105111805e-05,
                    0.3135179153094462,
                    1.9800542534865456e-05,
                    2.7581451473883813e-05,
                    0.030612244897959183,
                    0.03636363636363636,
                    0.15384615384615385,
                    0.05690289792496202,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "monkey bars",
                "mosquito box",
                "mongoose pit"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "monkey bars": 0.9876379308623414,
                "mosquito box": 0.0098752922135764,
                "mongoose pit": 0.09697848126691634
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.5130414984615825,
                    0.2593469098385695,
                    0.22761159169984807
                ],
                "result_count_important_words": [
                    303000.0,
                    16.0,
                    6.0
                ],
                "wikipedia_search": [
                    3.0272108843537415,
                    0.8503401360544217,
                    0.12244897959183673
                ],
                "word_count_appended_bing": [
                    9.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    2.8333333333333335,
                    0.16666666666666666,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.14071935415267944,
                    0.012134512886404991,
                    0.012762591242790222
                ],
                "result_count_noun_chunks": [
                    145000.0,
                    21.0,
                    4.0
                ],
                "question_answer_similarity": [
                    4.983603470027447,
                    5.001761592924595,
                    2.8929256097762845
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    451000.0,
                    392000.0,
                    385000.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    121000.0,
                    11.0,
                    3.0
                ],
                "answer_relation_to_question": [
                    2.2416666666666667,
                    0.6111111111111112,
                    1.1472222222222221
                ],
                "word_count_appended": [
                    102.0,
                    4.0,
                    4.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which four-letter U.S. state is the smallest in total area?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ohio"
            ],
            "question": "which four-letter u.s. state is the smallest in total area?",
            "lines": [
                [
                    0.22534562211981568,
                    0.3412520543851785,
                    0.36916958345739087,
                    0.4011093346598475,
                    0.4610492845786963,
                    0.4742925468314069,
                    0.3333333333333333,
                    0.8779956427015251,
                    0.22268551815168408,
                    0.36503067484662577,
                    0.3076923076923077,
                    0.34644041968871087,
                    0.6153846153846154,
                    0,
                    -1.0
                ],
                [
                    0.6304147465437788,
                    0.3262778690837857,
                    0.2916225634988421,
                    0.30907688299730385,
                    0.27782193958664547,
                    0.2630530091669988,
                    0.3359580052493438,
                    0.08823529411764706,
                    0.24058078401561697,
                    0.3312883435582822,
                    0.38461538461538464,
                    0.3353616029619476,
                    0.38461538461538464,
                    0,
                    -1.0
                ],
                [
                    0.14423963133640552,
                    0.3324700765310358,
                    0.33920785304376705,
                    0.2898137823428486,
                    0.2611287758346582,
                    0.26265444400159427,
                    0.33070866141732286,
                    0.03376906318082789,
                    0.5367336978326989,
                    0.30368098159509205,
                    0.3076923076923077,
                    0.3181979773493414,
                    0.0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "ohio",
                "iowa",
                "utah"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "iowa": 0.23674489497152498,
                "ohio": 0.5796502881249076,
                "utah": 0.07844005643620391
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7322020984435544,
                    1.676808014809738,
                    1.5909898867467072
                ],
                "result_count_important_words": [
                    1270000.0,
                    1280000.0,
                    1260000.0
                ],
                "wikipedia_search": [
                    1.1134275907584203,
                    1.2029039200780849,
                    2.6836684891634945
                ],
                "word_count_appended_bing": [
                    32.0,
                    40.0,
                    32.0
                ],
                "answer_relation_to_question_bing": [
                    1.365008217540714,
                    1.3051114763351428,
                    1.329880306124143
                ],
                "cosine_similarity_raw": [
                    0.0474005788564682,
                    0.03744370862841606,
                    0.04355355724692345
                ],
                "result_count_noun_chunks": [
                    403000000.0,
                    40500000.0,
                    15500000.0
                ],
                "question_answer_similarity": [
                    2.590493056923151,
                    1.9961178917437792,
                    1.8717105938121676
                ],
                "word_count_noun_chunks": [
                    8.0,
                    5.0,
                    0.0
                ],
                "result_count_bing": [
                    11900000.0,
                    6600000.0,
                    6590000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    116000.0,
                    69900.0,
                    65700.0
                ],
                "answer_relation_to_question": [
                    0.676036866359447,
                    1.8912442396313365,
                    0.4327188940092166
                ],
                "word_count_appended": [
                    357.0,
                    324.0,
                    297.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Mardi Gras is celebrated right before what other observance?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "lent"
            ],
            "lines": [
                [
                    0.18793715419146048,
                    0.3141289437585734,
                    0.8675481987330943,
                    0.6132297093779424,
                    0.4172813487881981,
                    0.4517241379310345,
                    0.1971557853910795,
                    0.6467661691542289,
                    0.4016225749559083,
                    0.4667458432304038,
                    0.3902439024390244,
                    0.3974166208856349,
                    0.9850746268656716,
                    1.0,
                    1.0
                ],
                [
                    0.4645752862163761,
                    0.4513031550068587,
                    0.08071054284794756,
                    0.1351850487461589,
                    0.48472075869336145,
                    0.27241379310344827,
                    0.28183581124757595,
                    0.11201567917985829,
                    0.28557319223985894,
                    0.29572446555819476,
                    0.35772357723577236,
                    0.303896861981727,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.34748755959216343,
                    0.23456790123456792,
                    0.05174125841895818,
                    0.2515852418758987,
                    0.09799789251844046,
                    0.27586206896551724,
                    0.5210084033613446,
                    0.24121815166591287,
                    0.3128042328042328,
                    0.2375296912114014,
                    0.25203252032520324,
                    0.2986865171326381,
                    0.014925373134328358,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "kwanzaa": 0.25183415514693847,
                "ramadan": 0.22410334373147192,
                "lent": 0.5240625011215897
            },
            "question": "mardi gras is celebrated right before what other observance?",
            "rate_limited": false,
            "answers": [
                "lent",
                "kwanzaa",
                "ramadan"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "kwanzaa": 0.3026178103322456,
                "ramadan": 0.03263025571071399,
                "lent": 0.563976529319204
            },
            "integer_answers": {
                "kwanzaa": 3,
                "ramadan": 1,
                "lent": 10
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9870831044281745,
                    1.519484309908635,
                    1.4934325856631905
                ],
                "result_count_important_words": [
                    30500.0,
                    43600.0,
                    80600.0
                ],
                "wikipedia_search": [
                    1.2048677248677249,
                    0.8567195767195768,
                    0.9384126984126984
                ],
                "answer_relation_to_question": [
                    0.5638114625743814,
                    1.3937258586491283,
                    1.0424626787764903
                ],
                "word_count_appended_bing": [
                    48.0,
                    44.0,
                    31.0
                ],
                "answer_relation_to_question_bing": [
                    0.9423868312757202,
                    1.353909465020576,
                    0.7037037037037037
                ],
                "cosine_similarity_raw": [
                    0.40218690037727356,
                    0.037416622042655945,
                    0.02398674376308918
                ],
                "result_count_noun_chunks": [
                    42900.0,
                    7430.0,
                    16000.0
                ],
                "question_answer_similarity": [
                    1.887045793235302,
                    0.4159948118031025,
                    0.774184396257624
                ],
                "word_count_noun_chunks": [
                    66.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    262000.0,
                    158000.0,
                    160000.0
                ],
                "word_count_raw": [
                    26.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    39600.0,
                    46000.0,
                    9300.0
                ],
                "word_count_appended": [
                    393.0,
                    249.0,
                    200.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these is classified as a neurological condition or disorder?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "multiple sclerosis"
            ],
            "lines": [
                [
                    0.179516317016317,
                    0.22142857142857142,
                    0.025085304869273985,
                    0.09134108998673256,
                    0.0037449662326266473,
                    0.16727716727716727,
                    0.6586447118429386,
                    0.004173267900614398,
                    0.28756674294431733,
                    0.5117056856187291,
                    0.3950617283950617,
                    0.2984097426851436,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.23237179487179485,
                    0.5404761904761904,
                    0.017508679259076853,
                    0.3633260241673973,
                    0.032592174744198434,
                    0.25274725274725274,
                    0.12824572514249524,
                    0.02206422195602612,
                    0.20404271548436306,
                    0.24581939799331104,
                    0.37037037037037035,
                    0.3113347178961925,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.5881118881118881,
                    0.23809523809523808,
                    0.9574060158716492,
                    0.5453328858458701,
                    0.963662859023175,
                    0.57997557997558,
                    0.21310956301456618,
                    0.9737625101433595,
                    0.5083905415713196,
                    0.24247491638795987,
                    0.2345679012345679,
                    0.390255539418664,
                    1.0,
                    1.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "halitosis": 0.20313966401410669,
                "cystic fibrosis": 0.19434994750776208,
                "multiple sclerosis": 0.6025103884781312
            },
            "question": "which of these is classified as a neurological condition or disorder?",
            "rate_limited": false,
            "answers": [
                "halitosis",
                "cystic fibrosis",
                "multiple sclerosis"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "halitosis": 0.15893564954791747,
                "cystic fibrosis": 0.09132043844649664,
                "multiple sclerosis": 0.2484951865528776
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1936389707405743,
                    1.24533887158477,
                    1.561022157674656
                ],
                "result_count_important_words": [
                    2080000.0,
                    405000.0,
                    673000.0
                ],
                "wikipedia_search": [
                    0.8627002288329519,
                    0.6121281464530892,
                    1.5251716247139586
                ],
                "answer_relation_to_question": [
                    0.5385489510489511,
                    0.6971153846153846,
                    1.7643356643356642
                ],
                "answer_relation_to_question_bing": [
                    0.44285714285714284,
                    1.0809523809523809,
                    0.47619047619047616
                ],
                "word_count_appended": [
                    306.0,
                    147.0,
                    145.0
                ],
                "cosine_similarity_raw": [
                    0.011235890910029411,
                    0.00784226506948471,
                    0.4288291335105896
                ],
                "result_count_noun_chunks": [
                    108000.0,
                    571000.0,
                    25200000.0
                ],
                "question_answer_similarity": [
                    1.1146197230555117,
                    4.433605428785086,
                    6.654604081064463
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    4.0
                ],
                "result_count_bing": [
                    1370000.0,
                    2070000.0,
                    4750000.0
                ],
                "result_count": [
                    95600.0,
                    832000.0,
                    24600000.0
                ],
                "word_count_appended_bing": [
                    32.0,
                    30.0,
                    19.0
                ]
            },
            "integer_answers": {
                "halitosis": 3,
                "cystic fibrosis": 1,
                "multiple sclerosis": 10
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these Uranus moons is NOT named after a Shakespearean character?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "trinculo"
            ],
            "lines": [
                [
                    0.46838280766852197,
                    0.3398809523809524,
                    0.32080231739233567,
                    0.0,
                    0.12394303515798843,
                    0.06865848832607835,
                    0.11527672023404001,
                    0.11034198221325753,
                    0.4713541666666667,
                    0.282122905027933,
                    0.34782608695652173,
                    0.32328521754461165,
                    0.18181818181818182,
                    0.1764705882352941,
                    -1.0
                ],
                [
                    0.21732374768089052,
                    0.20625,
                    0.22612638718788894,
                    0.5,
                    0.40431686693368935,
                    0.445191927186387,
                    0.3972067486875326,
                    0.40075181076372973,
                    0.20208333333333334,
                    0.33379888268156427,
                    0.30434782608695654,
                    0.33153043017037004,
                    0.35454545454545455,
                    0.32352941176470584,
                    -1.0
                ],
                [
                    0.3142934446505875,
                    0.4538690476190476,
                    0.4530712954197755,
                    0.5,
                    0.4717400979083222,
                    0.48614958448753465,
                    0.48751653107842746,
                    0.48890620702301274,
                    0.3265625,
                    0.3840782122905028,
                    0.34782608695652173,
                    0.3451843522850183,
                    0.4636363636363636,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "oberon": 0.5242623643396596,
                "trinculo": 0.13959518237784085,
                "umbriel": 0.3361424532824996
            },
            "question": "which of these uranus moons is not named after a shakespearean character?",
            "rate_limited": false,
            "answers": [
                "oberon",
                "umbriel",
                "trinculo"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "oberon": 0.23142756255358532,
                "trinculo": 0.4564511175848187,
                "umbriel": 0.22395605793825202
            },
            "integer_answers": {
                "oberon": 8,
                "trinculo": 0,
                "umbriel": 5
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7671478245538834,
                    1.6846956982962995,
                    1.548156477149817
                ],
                "result_count_important_words": [
                    19200.0,
                    5130.0,
                    623.0
                ],
                "wikipedia_search": [
                    0.22916666666666666,
                    2.3833333333333333,
                    1.3875
                ],
                "word_count_appended_bing": [
                    35.0,
                    45.0,
                    35.0
                ],
                "answer_relation_to_question_bing": [
                    1.280952380952381,
                    2.35,
                    0.369047619047619
                ],
                "cosine_similarity_raw": [
                    0.11752565205097198,
                    0.1796182543039322,
                    0.030777890235185623
                ],
                "result_count_noun_chunks": [
                    17000.0,
                    4330.0,
                    484.0
                ],
                "question_answer_similarity": [
                    -0.7806879468262196,
                    0.0,
                    0.0
                ],
                "word_count_noun_chunks": [
                    35.0,
                    16.0,
                    4.0
                ],
                "result_count_bing": [
                    218000.0,
                    27700.0,
                    7000.0
                ],
                "word_count_raw": [
                    11.0,
                    6.0,
                    0.0
                ],
                "result_count": [
                    16900.0,
                    4300.0,
                    1270.0
                ],
                "answer_relation_to_question": [
                    0.31617192331478045,
                    2.826762523191095,
                    1.8570655534941247
                ],
                "word_count_appended": [
                    312.0,
                    238.0,
                    166.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "A spork is a utensil that combines what two things?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "spoon / fork"
            ],
            "question": "a spork is a utensil that combines what two things?",
            "lines": [
                [
                    0.0625,
                    0.3333333333333333,
                    0.042928236980752356,
                    0.321284667628654,
                    1.1833829368014343e-05,
                    0.20272108843537415,
                    2.339164871357628e-06,
                    1.2867221305177402e-05,
                    0.0,
                    0.23076923076923078,
                    0.3333333333333333,
                    0.06937155237549011,
                    0,
                    0,
                    1.0
                ],
                [
                    0.4375,
                    0.0,
                    0.03258330595501906,
                    0.27031725785773136,
                    2.9584573420035857e-05,
                    0.20136054421768707,
                    4.678329742715256e-06,
                    2.0219919193850203e-05,
                    0.0,
                    0.23076923076923078,
                    0.3333333333333333,
                    0.07790605234602611,
                    0,
                    0,
                    1.0
                ],
                [
                    0.5,
                    0.6666666666666666,
                    0.9244884570642286,
                    0.4083980745136147,
                    0.999958581597212,
                    0.5959183673469388,
                    0.9999929825053859,
                    0.999966912859501,
                    1.0,
                    0.5384615384615384,
                    0.3333333333333333,
                    0.8527223952784837,
                    0,
                    0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "spam / pork",
                "sponge / cork",
                "spoon / fork"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "spoon / fork": 0.758211856627242,
                "spam / pork": 0.07698874083154662,
                "sponge / cork": 0.12507854393811954
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.27748620950196046,
                    0.31162420938410446,
                    3.410889581113935
                ],
                "result_count_important_words": [
                    4.0,
                    8.0,
                    1710000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.0,
                    0.0,
                    2.0
                ],
                "cosine_similarity_raw": [
                    0.021660350263118744,
                    0.01644059643149376,
                    0.46647021174430847
                ],
                "result_count_noun_chunks": [
                    7.0,
                    11.0,
                    544000.0
                ],
                "question_answer_similarity": [
                    4.500819198787212,
                    3.7868259102106094,
                    5.721175268292427
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    29800.0,
                    29600.0,
                    87600.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    4.0,
                    10.0,
                    338000.0
                ],
                "answer_relation_to_question": [
                    0.25,
                    1.75,
                    2.0
                ],
                "word_count_appended": [
                    3.0,
                    3.0,
                    7.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these is NOT a suit in a traditional Tarot deck?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "gloves"
            ],
            "lines": [
                [
                    0.46964285714285714,
                    0,
                    0.48762140217600425,
                    0.32013387901234147,
                    0.4029706790123457,
                    0.30505331478905884,
                    0.49998055631169896,
                    0.39884743706399756,
                    0.24043062200956938,
                    0.4130218687872763,
                    0.41346153846153844,
                    0.38283634578197157,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.10535714285714282,
                    0,
                    0.15083160261162326,
                    0.3262927253056208,
                    0.3130787037037037,
                    0.4049605934167826,
                    0.29961096750999366,
                    0.313466787989081,
                    0.3181818181818182,
                    0.279324055666004,
                    0.2692307692307692,
                    0.30598477022859005,
                    0.19072164948453607,
                    0.2088607594936709,
                    -1.0
                ],
                [
                    0.425,
                    0,
                    0.36154699521237255,
                    0.3535733956820377,
                    0.28395061728395066,
                    0.2899860917941586,
                    0.20040847617830737,
                    0.28768577494692144,
                    0.44138755980861244,
                    0.30765407554671964,
                    0.3173076923076923,
                    0.3111788839894384,
                    0.30927835051546393,
                    0.2911392405063291,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "gloves": 0.1793845383771293,
                "cups": 0.3569081301889225,
                "swords": 0.4637073314339482
            },
            "question": "which of these is not a suit in a traditional tarot deck?",
            "rate_limited": false,
            "answers": [
                "gloves",
                "swords",
                "cups"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "gloves": 0.5191723276961996,
                "cups": 0.09527561993876078,
                "swords": 0.045090162435045
            },
            "integer_answers": {
                "gloves": 2,
                "cups": 4,
                "swords": 7
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.9373092337442275,
                    1.5521218381712794,
                    1.5105689280844932
                ],
                "result_count_important_words": [
                    98.0,
                    1010000.0,
                    1510000.0
                ],
                "wikipedia_search": [
                    1.0382775119617225,
                    0.7272727272727273,
                    0.23444976076555024
                ],
                "answer_relation_to_question": [
                    0.24285714285714285,
                    3.1571428571428575,
                    0.6
                ],
                "word_count_appended_bing": [
                    27.0,
                    72.0,
                    57.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.004843301139771938,
                    0.13661706447601318,
                    0.054171692579984665
                ],
                "result_count_noun_chunks": [
                    667000.0,
                    1230000.0,
                    1400000.0
                ],
                "question_answer_similarity": [
                    2.3634153008461,
                    2.2824889346957207,
                    1.924024797976017
                ],
                "word_count_noun_chunks": [
                    0.0,
                    60.0,
                    37.0
                ],
                "result_count_bing": [
                    841000.0,
                    410000.0,
                    906000.0
                ],
                "word_count_raw": [
                    0.0,
                    46.0,
                    33.0
                ],
                "word_count_appended": [
                    175.0,
                    444.0,
                    387.0
                ],
                "result_count": [
                    503000.0,
                    969000.0,
                    1120000.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "According to the title of her hit 2015 song, how long is Rihanna from wildin'?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "fourfiveseconds"
            ],
            "question": "according to the title of her hit 2015 song, how long is rihanna from wildin'?",
            "lines": [
                [
                    0.5473508774506924,
                    0.8116156719097896,
                    0.8861834313665928,
                    0.0,
                    0.5882352941176471,
                    0.7891442367879817,
                    0.9636363636363636,
                    0.9988969236426033,
                    0.7261086313018468,
                    0.75,
                    0.9130434782608695,
                    0.4261967784983683,
                    1.0,
                    1.0,
                    5.0
                ],
                [
                    0.2438344364421837,
                    0.11623752064928536,
                    0.0786180070146529,
                    0.5837562588541922,
                    0.29411764705882354,
                    0.1895734597156398,
                    0.03636363636363636,
                    0.0004902561588429955,
                    0.046223351136518674,
                    0.1346153846153846,
                    0.043478260869565216,
                    0.2992461896669213,
                    0.0,
                    0.0,
                    5.0
                ],
                [
                    0.208814686107124,
                    0.07214680744092508,
                    0.03519856161875428,
                    0.4162437411458078,
                    0.11764705882352941,
                    0.02128230349637843,
                    0.0,
                    0.0006128201985537443,
                    0.22766801756163457,
                    0.11538461538461539,
                    0.043478260869565216,
                    0.2745570318347104,
                    0.0,
                    0.0,
                    5.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "fourfiveseconds",
                "a few hours or so",
                "depends on the traffic"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "depends on the traffic": 0.0594118645269945,
                "a few hours or so": 0.09562836696574517,
                "fourfiveseconds": 1.0447680181440637
            },
            "categorical_data": {
                "question_type": 5
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.4095742279869463,
                    2.3939695173353703,
                    2.1964562546776834
                ],
                "result_count_important_words": [
                    53.0,
                    2.0,
                    0
                ],
                "wikipedia_search": [
                    5.808869050414774,
                    0.3697868090921494,
                    1.8213441404930766
                ],
                "word_count_appended_bing": [
                    42.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    5.681309703368527,
                    0.8136626445449975,
                    0.5050276520864756
                ],
                "cosine_similarity_raw": [
                    0.5677926540374756,
                    0.05037188157439232,
                    0.022552311420440674
                ],
                "result_count_noun_chunks": [
                    16300.0,
                    8.0,
                    10.0
                ],
                "question_answer_similarity": [
                    0.0,
                    27.90810813708231,
                    19.89970156736672
                ],
                "word_count_noun_chunks": [
                    7.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    27.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    3530000.0,
                    848000.0,
                    95200.0
                ],
                "word_count_appended": [
                    78.0,
                    14.0,
                    12.0
                ],
                "answer_relation_to_question": [
                    4.378807019605539,
                    1.9506754915374693,
                    1.6705174888569918
                ],
                "result_count": [
                    10.0,
                    5.0,
                    2.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "How did Mason jars get their name?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "named after inventor"
            ],
            "lines": [
                [
                    0.0,
                    0.0,
                    0.15369554125848628,
                    0.25523113929291164,
                    1.1825046888881578e-05,
                    0.3235955056179775,
                    3.795049850826078e-06,
                    4.292908695640097e-06,
                    0.0,
                    0.56,
                    0.3333333333333333,
                    0.08947846179237498,
                    0,
                    0,
                    5.0
                ],
                [
                    0.3608644144921624,
                    0.3767123287671233,
                    0.4403464473519183,
                    0.39869682594891903,
                    0.9999876608206377,
                    0.31235955056179776,
                    0.9999956356926716,
                    0.9999952020432226,
                    0.0,
                    0.24,
                    0.3333333333333333,
                    0.8456345931671894,
                    0,
                    0,
                    5.0
                ],
                [
                    0.6391355855078377,
                    0.6232876712328768,
                    0.4059580113895955,
                    0.34607203475816933,
                    5.141324734296338e-07,
                    0.36404494382022473,
                    5.692574776239117e-07,
                    5.050480818400113e-07,
                    1.0,
                    0.2,
                    0.3333333333333333,
                    0.06488694504043571,
                    0,
                    0,
                    5.0
                ]
            ],
            "fraction_answers": {
                "named after inventor": 0.14294615785837658,
                "invented in mason, al": 0.525660499348248,
                "masons used them": 0.3313933427933755
            },
            "question": "how did mason jars get their name?",
            "rate_limited": false,
            "answers": [
                "named after inventor",
                "invented in mason, al",
                "masons used them"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "named after inventor": 0.2952405103978026,
                "invented in mason, al": 0.12681941571163238,
                "masons used them": 0.18317508568592022
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 5
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.17895692358474996,
                    1.6912691863343787,
                    0.12977389008087142
                ],
                "result_count_important_words": [
                    20.0,
                    5270000.0,
                    3.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.3767123287671233,
                    0.6232876712328768
                ],
                "cosine_similarity_raw": [
                    0.0738634318113327,
                    0.2116229236125946,
                    0.1950964331626892
                ],
                "result_count_noun_chunks": [
                    17.0,
                    3960000.0,
                    2.0
                ],
                "question_answer_similarity": [
                    6.142987018451095,
                    9.59596635773778,
                    8.329375572502613
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    1440000.0,
                    1390000.0,
                    1620000.0
                ],
                "word_count_appended": [
                    14.0,
                    6.0,
                    5.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    0.7217288289843247,
                    1.2782711710156751
                ],
                "result_count": [
                    46.0,
                    3890000.0,
                    2.0
                ]
            },
            "integer_answers": {
                "named after inventor": 2,
                "invented in mason, al": 6,
                "masons used them": 4
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these is a common machine used in construction?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "forklift"
            ],
            "question": "which of these is a common machine used in construction?",
            "lines": [
                [
                    0.10144927536231885,
                    0.0,
                    0.11801763675387568,
                    0.0,
                    1.1627546432668754e-05,
                    0.3329137822529893,
                    7.762355043727172e-06,
                    1.0827632372582652e-05,
                    0.0,
                    0.05593220338983051,
                    0.22131147540983606,
                    0.08296928995240269,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.014492753623188406,
                    0.0,
                    0.09231559133845074,
                    0.0,
                    1.9379244054447924e-05,
                    0.33354310887350536,
                    1.826436480876982e-05,
                    2.547678205313565e-05,
                    0.0,
                    0.07288135593220339,
                    0.1885245901639344,
                    0.11493216384885513,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.8840579710144928,
                    1.0,
                    0.7896667719076735,
                    1.0,
                    0.9999689932095129,
                    0.33354310887350536,
                    0.9999739732801475,
                    0.9999636955855743,
                    1.0,
                    0.8711864406779661,
                    0.5901639344262295,
                    0.8020985461987422,
                    1.0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "spoonlift",
                "sporklift",
                "forklift"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sporklift": -0.0010626188711883868,
                "forklift": 0.8234764254165199,
                "spoonlift": 0.005630086568557352
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.24890786985720806,
                    0.3447964915465654,
                    2.4062956385962266
                ],
                "result_count_important_words": [
                    17.0,
                    40.0,
                    2190000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_count_appended_bing": [
                    27.0,
                    23.0,
                    72.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    2.0
                ],
                "cosine_similarity_raw": [
                    0.02307054027915001,
                    0.01804620586335659,
                    0.1543670892715454
                ],
                "result_count_noun_chunks": [
                    17.0,
                    40.0,
                    1570000.0
                ],
                "question_answer_similarity": [
                    0.0,
                    0.0,
                    1.2012842111289501
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    52900000.0,
                    53000000.0,
                    53000000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    15.0,
                    25.0,
                    1290000.0
                ],
                "answer_relation_to_question": [
                    0.30434782608695654,
                    0.043478260869565216,
                    2.6521739130434785
                ],
                "word_count_appended": [
                    33.0,
                    43.0,
                    514.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which animals race at the Kentucky Derby?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "horses"
            ],
            "question": "which animals race at the kentucky derby?",
            "lines": [
                [
                    0.3081168831168831,
                    0.7000000000000001,
                    0.7230202211611606,
                    0.4306795254596391,
                    0.9865787056546256,
                    0.6488888888888888,
                    0.9830292316054909,
                    0.915948275862069,
                    0.9655172413793104,
                    0.6773049645390071,
                    0.620253164556962,
                    0.5447785315887834,
                    0,
                    1.0,
                    -1.0
                ],
                [
                    0.641017316017316,
                    0.20000000000000004,
                    0.21378339703593086,
                    0.47626771653366856,
                    3.7474695020989654e-05,
                    0.17037037037037037,
                    5.305554915997049e-05,
                    0.04510467980295566,
                    0.0,
                    0.05851063829787234,
                    0.0379746835443038,
                    0.18656060118131879,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.050865800865800864,
                    0.10000000000000002,
                    0.06319638180290856,
                    0.09305275800669233,
                    0.013383819650353448,
                    0.18074074074074073,
                    0.01691771284534908,
                    0.03894704433497537,
                    0.03448275862068966,
                    0.2641843971631206,
                    0.34177215189873417,
                    0.26866086722989774,
                    0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "horses",
                "wiener dogs",
                "locusts"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "horses": 0.8866742944652064,
                "locusts": 0.015792980504585854,
                "wiener dogs": -0.03750433261901364
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.1791141263551337,
                    0.7462424047252751,
                    1.074643468919591
                ],
                "result_count_important_words": [
                    982000.0,
                    53.0,
                    16900.0
                ],
                "wikipedia_search": [
                    1.9310344827586206,
                    0.0,
                    0.06896551724137931
                ],
                "word_count_appended_bing": [
                    49.0,
                    3.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    1.4,
                    0.4,
                    0.2
                ],
                "cosine_similarity_raw": [
                    0.17737248539924622,
                    0.05244568735361099,
                    0.01550343818962574
                ],
                "result_count_noun_chunks": [
                    11900000.0,
                    586000.0,
                    506000.0
                ],
                "question_answer_similarity": [
                    2.5577216744422913,
                    2.828461046796292,
                    0.5526221748441458
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    17.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    4380000.0,
                    1150000.0,
                    1220000.0
                ],
                "word_count_appended": [
                    382.0,
                    33.0,
                    149.0
                ],
                "answer_relation_to_question": [
                    1.2324675324675325,
                    2.564069264069264,
                    0.20346320346320346
                ],
                "result_count": [
                    1290000.0,
                    49.0,
                    17500.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Catherine O'Hara and Eugene Levy do NOT kiss in which Christopher Guest film?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "best in show"
            ],
            "lines": [
                [
                    0.398059126608381,
                    0.36015159021584975,
                    0.4589707075583702,
                    0.28663031782274806,
                    0.027974053052241377,
                    0.08983536148890481,
                    0.008101270905696012,
                    0.08478639930252835,
                    0.4393939393939394,
                    0.3559870550161812,
                    0.35454545454545455,
                    0.3340292429779208,
                    0.28169014084507044,
                    0.35714285714285715,
                    -1.0
                ],
                [
                    0.30131202652706157,
                    0.31047240193507025,
                    0.12911083377363192,
                    0.34781547124473333,
                    0.47586779412332525,
                    0.4363636363636364,
                    0.4959142289236453,
                    0.4660527462946818,
                    0.0896464646464647,
                    0.25889967637540456,
                    0.32727272727272727,
                    0.32885008134631233,
                    0.323943661971831,
                    0.19047619047619047,
                    -1.0
                ],
                [
                    0.3006288468645575,
                    0.3293760078490801,
                    0.41191845866799787,
                    0.3655542109325186,
                    0.4961581528244334,
                    0.47380100214745885,
                    0.49598450017065876,
                    0.4491608544027899,
                    0.47095959595959597,
                    0.3851132686084142,
                    0.3181818181818182,
                    0.33712067567576687,
                    0.3943661971830986,
                    0.4523809523809524,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "a mighty wind": 0.3597145798178978,
                "best in show": 0.45181464044626524,
                "waiting for guffman": 0.188470779735837
            },
            "question": "catherine o'hara and eugene levy do not kiss in which christopher guest film?",
            "rate_limited": false,
            "answers": [
                "best in show",
                "a mighty wind",
                "waiting for guffman"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "a mighty wind": 0.1691569715695715,
                "best in show": 0.34629323055895384,
                "waiting for guffman": 0.3312447007039207
            },
            "integer_answers": {
                "a mighty wind": 6,
                "best in show": 6,
                "waiting for guffman": 2
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.655532112353267,
                    2.7383986984590023,
                    2.60606918918773
                ],
                "result_count_important_words": [
                    490000.0,
                    4070.0,
                    4000.0
                ],
                "wikipedia_search": [
                    0.36363636363636365,
                    2.462121212121212,
                    0.17424242424242425
                ],
                "word_count_appended_bing": [
                    16.0,
                    19.0,
                    20.0
                ],
                "answer_relation_to_question_bing": [
                    1.9578777369781042,
                    2.653386372909017,
                    2.3887358901128795
                ],
                "cosine_similarity_raw": [
                    0.05103388801217079,
                    0.4613268971443176,
                    0.10955937206745148
                ],
                "result_count_noun_chunks": [
                    762000.0,
                    62300.0,
                    93300.0
                ],
                "question_answer_similarity": [
                    10.79942603642121,
                    7.702619910240173,
                    6.8047969145700336
                ],
                "word_count_noun_chunks": [
                    31.0,
                    25.0,
                    15.0
                ],
                "result_count_bing": [
                    573000.0,
                    88900.0,
                    36600.0
                ],
                "word_count_raw": [
                    6.0,
                    13.0,
                    2.0
                ],
                "result_count": [
                    489000.0,
                    25000.0,
                    3980.0
                ],
                "answer_relation_to_question": [
                    1.631053974265904,
                    3.1790075755670153,
                    3.1899384501670807
                ],
                "word_count_appended": [
                    89.0,
                    149.0,
                    71.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Queen Victoria is credited with starting what fashion trend?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "white wedding dress"
            ],
            "lines": [
                [
                    0.18149717514124294,
                    0.3055555555555555,
                    0.03736106737732742,
                    0.22234368675636973,
                    0.4153846153846154,
                    0.34602829162132753,
                    0.9903169014084507,
                    0.12735012415750266,
                    0.5936507936507937,
                    0.2524271844660194,
                    0.29545454545454547,
                    0.37742603764297167,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.28115969415937436,
                    0.36111111111111116,
                    0.11452274249913501,
                    0.40554760076526175,
                    0.2,
                    0.352557127312296,
                    0.0044014084507042256,
                    0.24122029088329194,
                    0.11904761904761905,
                    0.3106796116504854,
                    0.3409090909090909,
                    0.2093147586555503,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.5373431306993827,
                    0.33333333333333337,
                    0.8481161901235376,
                    0.3721087124783685,
                    0.38461538461538464,
                    0.3014145810663765,
                    0.00528169014084507,
                    0.6314295849592054,
                    0.2873015873015873,
                    0.4368932038834951,
                    0.36363636363636365,
                    0.413259203701478,
                    0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "white wedding dress": 0.45497945891841207,
                "mini dress": 0.318830459893594,
                "little black dress": 0.22619008118799386
            },
            "question": "queen victoria is credited with starting what fashion trend?",
            "rate_limited": false,
            "answers": [
                "mini dress",
                "little black dress",
                "white wedding dress"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "white wedding dress": 0.6291834999797511,
                "mini dress": 0.07658598927411717,
                "little black dress": 0.037049364938785204
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.26455622585783,
                    1.2558885519333018,
                    2.479555222208868
                ],
                "result_count_important_words": [
                    18000.0,
                    80.0,
                    96.0
                ],
                "wikipedia_search": [
                    2.3746031746031746,
                    0.4761904761904762,
                    1.1492063492063491
                ],
                "word_count_appended_bing": [
                    13.0,
                    15.0,
                    16.0
                ],
                "answer_relation_to_question_bing": [
                    1.833333333333333,
                    2.1666666666666665,
                    1.9999999999999998
                ],
                "cosine_similarity_raw": [
                    0.015078761614859104,
                    0.046220872551202774,
                    0.34229594469070435
                ],
                "result_count_noun_chunks": [
                    718000.0,
                    1360000.0,
                    3560000.0
                ],
                "question_answer_similarity": [
                    4.946093179285526,
                    9.021511927247047,
                    8.277655154466629
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    2.0
                ],
                "result_count_bing": [
                    318000.0,
                    324000.0,
                    277000.0
                ],
                "word_count_appended": [
                    26.0,
                    32.0,
                    45.0
                ],
                "answer_relation_to_question": [
                    1.0889830508474576,
                    1.6869581649562462,
                    3.224058784196296
                ],
                "result_count": [
                    54.0,
                    26.0,
                    50.0
                ]
            },
            "integer_answers": {
                "white wedding dress": 7,
                "mini dress": 3,
                "little black dress": 3
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "The metric unit for what scientific value is also the name of a rental car chain?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "frequency",
                "bottom"
            ],
            "question": "the metric unit for what scientific value is also the name of a rental car chain?",
            "lines": [
                [
                    0.683681909488361,
                    0.4623898127121334,
                    0.18899912892859708,
                    0.4401076549303114,
                    0.0004448282178088285,
                    0.37607655502392345,
                    0.023079355042681,
                    0.03605624083498663,
                    0.16071428571428573,
                    0.04788732394366197,
                    0.1076923076923077,
                    0.1871663292978733,
                    0.0,
                    0,
                    1.0
                ],
                [
                    0.19276852180077986,
                    0.21292090933509145,
                    0.4581400240820213,
                    0.280690332190198,
                    0.5032664346582236,
                    0.3014354066985646,
                    0.4750237116661397,
                    0.7245751746743725,
                    0.7023809523809524,
                    0.4591549295774648,
                    0.4153846153846154,
                    0.39070818423270387,
                    0.0,
                    0,
                    1.0
                ],
                [
                    0.12354956871085902,
                    0.32468927795277513,
                    0.35286084698938164,
                    0.2792020128794906,
                    0.49628873712396754,
                    0.32248803827751193,
                    0.5018969332911792,
                    0.2393685844906409,
                    0.13690476190476192,
                    0.49295774647887325,
                    0.47692307692307695,
                    0.42212548646942283,
                    1.0,
                    0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "magnetic flux",
                "resistance",
                "frequency"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "magnetic flux": 0.09936519125461687,
                "frequency": 0.8578001328218928,
                "resistance": 0.49710854413349087
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.310164305085113,
                    2.734957289628927,
                    2.95487840528596
                ],
                "result_count_important_words": [
                    2920.0,
                    60100.0,
                    63500.0
                ],
                "wikipedia_search": [
                    0.6428571428571428,
                    2.8095238095238093,
                    0.5476190476190476
                ],
                "word_count_appended_bing": [
                    7.0,
                    27.0,
                    31.0
                ],
                "answer_relation_to_question_bing": [
                    1.8495592508485337,
                    0.8516836373403658,
                    1.2987571118111005
                ],
                "cosine_similarity_raw": [
                    0.03215208277106285,
                    0.07793769240379333,
                    0.060027848929166794
                ],
                "result_count_noun_chunks": [
                    8360.0,
                    168000.0,
                    55500.0
                ],
                "question_answer_similarity": [
                    5.875513277947903,
                    3.7472644597291946,
                    3.7273951396346092
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    393000.0,
                    315000.0,
                    337000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    51.0,
                    57700.0,
                    56900.0
                ],
                "answer_relation_to_question": [
                    3.4184095474418053,
                    0.9638426090038993,
                    0.6177478435542951
                ],
                "word_count_appended": [
                    17.0,
                    163.0,
                    175.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these is the name of a machine used to punish prisoners in the 1800s?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "treadmill"
            ],
            "question": "which of these is the name of a machine used to punish prisoners in the 1800s?",
            "lines": [
                [
                    0.5429553264604811,
                    0.5681818181818181,
                    0.743190559080613,
                    0.14184763585797272,
                    0.9995769771678993,
                    0.42096597145993414,
                    0.9979187486298191,
                    0.2980347261972906,
                    0.8296296296296296,
                    0.9619450317124736,
                    0.9512195121951219,
                    0.6544911767863982,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.37285223367697595,
                    0.22727272727272727,
                    0.08321998091689486,
                    0.25525397708776704,
                    0.0004174567422046911,
                    0.42371020856201974,
                    0.002049932950109757,
                    0.690707880175539,
                    0.1111111111111111,
                    0.02536997885835095,
                    0.024390243902439025,
                    0.23055751876265063,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.08419243986254296,
                    0.20454545454545456,
                    0.17358946000249215,
                    0.6028983870542602,
                    5.566089896062548e-06,
                    0.1553238199780461,
                    3.131842007112129e-05,
                    0.011257393627170388,
                    0.05925925925925926,
                    0.012684989429175475,
                    0.024390243902439025,
                    0.11495130445095104,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "treadmill",
                "elliptical trainer",
                "bowflex home gym"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "bowflex home gym": 0.197914836896855,
                "elliptical trainer": 0.18733881260735896,
                "treadmill": 0.7808898297787332
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.6179647071455934,
                    0.9222300750506027,
                    0.4598052178038043
                ],
                "result_count_important_words": [
                    701000.0,
                    1440.0,
                    22.0
                ],
                "wikipedia_search": [
                    2.488888888888889,
                    0.3333333333333333,
                    0.17777777777777778
                ],
                "word_count_appended_bing": [
                    78.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.1363636363636362,
                    0.45454545454545453,
                    0.4090909090909091
                ],
                "cosine_similarity_raw": [
                    0.15480893850326538,
                    0.017334984615445137,
                    0.03615923225879669
                ],
                "result_count_noun_chunks": [
                    781000.0,
                    1810000.0,
                    29500.0
                ],
                "question_answer_similarity": [
                    1.943046879954636,
                    3.4965013042092323,
                    8.258578458568081
                ],
                "word_count_noun_chunks": [
                    28.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    76700.0,
                    77200.0,
                    28300.0
                ],
                "word_count_raw": [
                    41.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    4310000.0,
                    1800.0,
                    24.0
                ],
                "answer_relation_to_question": [
                    1.0859106529209621,
                    0.7457044673539519,
                    0.16838487972508592
                ],
                "word_count_appended": [
                    455.0,
                    12.0,
                    6.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "\u201cSee ya!\u201d is the iconic home run call of what baseball announcer?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "michael kay",
                "middle"
            ],
            "question": "\u201csee ya!\u201d is the iconic home run call of what baseball announcer?",
            "lines": [
                [
                    0.5852747252747253,
                    0.2654234654234654,
                    0.2605364392920359,
                    -0.06766962021515872,
                    0.4030612244897959,
                    0.20901371652514697,
                    0.6123831459151876,
                    0.21052906870308863,
                    0.6561431636810762,
                    0.424,
                    0.17777777777777778,
                    0.39949080253300984,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.12412087912087913,
                    0.28132455779514604,
                    0.7079771643530207,
                    0.9336962737878678,
                    0.37244897959183676,
                    0.6525146962769431,
                    0.12613467009890258,
                    0.7871956481941574,
                    0.11704999730200444,
                    0.352,
                    0.6,
                    0.3527024690895673,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.29060439560439566,
                    0.4532519767813885,
                    0.03148639635494331,
                    0.13397334642729092,
                    0.22448979591836735,
                    0.13847158719790986,
                    0.2614821839859098,
                    0.002275283102753877,
                    0.2268068390169193,
                    0.224,
                    0.2222222222222222,
                    0.24780672837742285,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "vin scully",
                "michael kay",
                "harry caray"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "michael kay": 0.9076607296279372,
                "vin scully": 0.2729820264398375,
                "harry caray": 0.028704944336997548
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9974540126650493,
                    1.7635123454478365,
                    1.2390336418871142
                ],
                "result_count_important_words": [
                    45200.0,
                    9310.0,
                    19300.0
                ],
                "wikipedia_search": [
                    2.6245726547243047,
                    0.46819998920801775,
                    0.9072273560676772
                ],
                "word_count_appended_bing": [
                    8.0,
                    27.0,
                    10.0
                ],
                "answer_relation_to_question_bing": [
                    1.327117327117327,
                    1.4066227889757301,
                    2.2662598839069426
                ],
                "cosine_similarity_raw": [
                    0.15193651616573334,
                    0.4128696322441101,
                    0.018361859023571014
                ],
                "result_count_noun_chunks": [
                    8050.0,
                    30100.0,
                    87.0
                ],
                "question_answer_similarity": [
                    -0.3215141533873975,
                    4.436208833940327,
                    0.6365386256948113
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count_bing": [
                    32000.0,
                    99900.0,
                    21200.0
                ],
                "word_count_raw": [
                    0.0,
                    10.0,
                    0.0
                ],
                "word_count_appended": [
                    53.0,
                    44.0,
                    28.0
                ],
                "answer_relation_to_question": [
                    2.9263736263736266,
                    0.6206043956043956,
                    1.4530219780219782
                ],
                "result_count": [
                    79.0,
                    73.0,
                    44.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "According to the Book of Leviticus, which of these animals is kosher to eat?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "owl"
            ],
            "question": "according to the book of leviticus, which of these animals is kosher to eat?",
            "lines": [
                [
                    0.3429815886504042,
                    0.24032459425717853,
                    0.35145116634086826,
                    0.29778188075064066,
                    0.11840082009226038,
                    0.30327868852459017,
                    0.5215806186051982,
                    0.04873772791023843,
                    0.017323775388291517,
                    0.3069620253164557,
                    0.3723404255319149,
                    0.3399813385936464,
                    0.8888888888888888,
                    0.0,
                    -1.0
                ],
                [
                    0.15940224159402241,
                    0.4132334581772784,
                    0.28305735906300944,
                    0.27086770750309164,
                    0.08200922603792926,
                    0.29508196721311475,
                    0.04563095378101847,
                    0.9116409537166901,
                    0.49722648916197304,
                    0.31962025316455694,
                    0.32978723404255317,
                    0.307879266445335,
                    0.0,
                    0.2,
                    -1.0
                ],
                [
                    0.49761616975557343,
                    0.34644194756554303,
                    0.3654914745961223,
                    0.43135041174626776,
                    0.7995899538698104,
                    0.4016393442622951,
                    0.4327884276137834,
                    0.03962131837307153,
                    0.48544973544973546,
                    0.37341772151898733,
                    0.2978723404255319,
                    0.3521393949610186,
                    0.1111111111111111,
                    0.8,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "owl",
                "cricket",
                "rabbit"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "owl": 0.6399628294932653,
                "cricket": 0.11040884752851851,
                "rabbit": 0.5562974811838243
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.0398880315618784,
                    1.8472755986720097,
                    2.1128363697661117
                ],
                "result_count_important_words": [
                    88700.0,
                    7760.0,
                    73600.0
                ],
                "wikipedia_search": [
                    0.06929510155316607,
                    1.9889059566478922,
                    1.9417989417989419
                ],
                "answer_relation_to_question": [
                    1.3719263546016167,
                    0.6376089663760897,
                    1.9904646790222937
                ],
                "word_count_appended_bing": [
                    35.0,
                    31.0,
                    28.0
                ],
                "answer_relation_to_question_bing": [
                    0.48064918851435706,
                    0.8264669163545568,
                    0.6928838951310861
                ],
                "cosine_similarity_raw": [
                    0.048014383763074875,
                    0.03867059201002121,
                    0.049932535737752914
                ],
                "result_count_noun_chunks": [
                    139000.0,
                    2600000.0,
                    113000.0
                ],
                "question_answer_similarity": [
                    2.0564023729530163,
                    1.8705402594059706,
                    2.978791080415249
                ],
                "word_count_noun_chunks": [
                    8.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    111000.0,
                    108000.0,
                    147000.0
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    4.0
                ],
                "result_count": [
                    9240.0,
                    6400.0,
                    62400.0
                ],
                "word_count_appended": [
                    194.0,
                    202.0,
                    236.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "In Ancient Greece, Olympic athletes typically competed wearing what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "togas"
            ],
            "question": "in ancient greece, olympic athletes typically competed wearing what?",
            "lines": [
                [
                    0.27735042735042736,
                    0.4375,
                    0.4123938606479519,
                    0.059985020736591364,
                    0.0034230965170216378,
                    0.5910370210002165,
                    0.052130121137879404,
                    0.07310145737300368,
                    0.8487394957983193,
                    0.7581967213114754,
                    0.84375,
                    0.34694885243655843,
                    0.0,
                    0,
                    1.0
                ],
                [
                    0.7226495726495726,
                    0.5625,
                    0.3447003419557749,
                    0.19418502866900367,
                    0.004504544763913591,
                    0.11669192465901711,
                    0.06859942833809718,
                    0.014294361409880336,
                    0.15126050420168066,
                    0.11475409836065574,
                    0.09375,
                    0.3220376343245959,
                    1.0,
                    0,
                    1.0
                ],
                [
                    0.0,
                    0.0,
                    0.24290579739627316,
                    0.7458299505944049,
                    0.9920723587190647,
                    0.2922710543407664,
                    0.8792704505240234,
                    0.912604181217116,
                    0.0,
                    0.12704918032786885,
                    0.0625,
                    0.33101351323884565,
                    0.0,
                    0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "togas",
                "laurel wreaths",
                "nothing at all"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "laurel wreaths": 0.3583218563396626,
                "nothing at all": 0.09854618289064154,
                "togas": 0.5913183298891393
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.428641967055909,
                    2.2542634402721715,
                    2.3170945926719195
                ],
                "result_count_important_words": [
                    3830.0,
                    5040.0,
                    64600.0
                ],
                "wikipedia_search": [
                    1.6974789915966386,
                    0.3025210084033613,
                    0.0
                ],
                "word_count_appended_bing": [
                    27.0,
                    3.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.75,
                    2.25,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.06037804111838341,
                    0.050467122346162796,
                    0.03556351736187935
                ],
                "result_count_noun_chunks": [
                    157000.0,
                    30700.0,
                    1960000.0
                ],
                "question_answer_similarity": [
                    0.765728953294456,
                    2.4788371650502086,
                    9.52077002543956
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    273000.0,
                    53900.0,
                    135000.0
                ],
                "word_count_appended": [
                    185.0,
                    28.0,
                    31.0
                ],
                "answer_relation_to_question": [
                    1.3867521367521367,
                    3.6132478632478633,
                    0.0
                ],
                "result_count": [
                    3830.0,
                    5040.0,
                    1110000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "After Prince Edward Island, what is Canada\u2019s most densely populated province?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "new brunswick"
            ],
            "question": "after prince edward island, what is canada\u2019s most densely populated province?",
            "lines": [
                [
                    0.46137271711739797,
                    0.25143909630737815,
                    0.2366115331472725,
                    0.43364499349804364,
                    0.2708638360175695,
                    0.029639872542535924,
                    0.27015558698727016,
                    0.22402159244264508,
                    0.41031927645199956,
                    0.145679012345679,
                    0.12280701754385964,
                    0.3432589180370336,
                    0.16326530612244897,
                    0.2857142857142857,
                    1.0
                ],
                [
                    0.11992829865170292,
                    0.10469821288074632,
                    0.45691913141102014,
                    0.24541941044160917,
                    0.39824304538799415,
                    0.19479348283532735,
                    0.4016973125884017,
                    0.5195681511470985,
                    0.3139833278849297,
                    0.5777777777777777,
                    0.6140350877192983,
                    0.3249602861275333,
                    0.3877551020408163,
                    0.5714285714285714,
                    1.0
                ],
                [
                    0.41869898423089913,
                    0.6438626908118755,
                    0.30646933544170735,
                    0.3209355960603472,
                    0.3308931185944363,
                    0.7755666446221368,
                    0.32814710042432815,
                    0.2564102564102564,
                    0.2756973956630707,
                    0.2765432098765432,
                    0.2631578947368421,
                    0.331780795835433,
                    0.4489795918367347,
                    0.14285714285714285,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "new brunswick",
                "ontario",
                "nova scotia"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "ontario": 0.3732505020364135,
                "nova scotia": 0.11260450428948024,
                "new brunswick": 0.4553953197832356
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.4028124262592354,
                    2.2747220028927333,
                    2.3224655708480313
                ],
                "result_count_important_words": [
                    95500.0,
                    142000.0,
                    116000.0
                ],
                "wikipedia_search": [
                    2.4619156587119972,
                    1.8838999673095782,
                    1.6541843739784243
                ],
                "word_count_appended_bing": [
                    7.0,
                    35.0,
                    15.0
                ],
                "answer_relation_to_question_bing": [
                    1.760073674151647,
                    0.7328874901652243,
                    4.507038835683129
                ],
                "cosine_similarity_raw": [
                    0.043120235204696655,
                    0.08326923102140427,
                    0.05585116520524025
                ],
                "result_count_noun_chunks": [
                    66400.0,
                    154000.0,
                    76000.0
                ],
                "question_answer_similarity": [
                    4.986012026201934,
                    2.8218108136206865,
                    3.6900892794474203
                ],
                "word_count_noun_chunks": [
                    8.0,
                    19.0,
                    22.0
                ],
                "word_count_raw": [
                    2.0,
                    4.0,
                    1.0
                ],
                "result_count_bing": [
                    98600.0,
                    648000.0,
                    2580000.0
                ],
                "word_count_appended": [
                    59.0,
                    234.0,
                    112.0
                ],
                "answer_relation_to_question": [
                    2.30686358558699,
                    0.5996414932585146,
                    2.0934949211544955
                ],
                "result_count": [
                    92500.0,
                    136000.0,
                    113000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Auguste Rodin's statue The Thinker is meant to represent what real-life person?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "auguste rodin"
            ],
            "question": "auguste rodin's statue the thinker is meant to represent what real-life person?",
            "lines": [
                [
                    0.14466180371352785,
                    0.1369047619047619,
                    0.12077965712186965,
                    -3.086907650471192,
                    0.0009545852784642946,
                    0.1794425087108014,
                    0.3017241379310345,
                    0.0005505837120540505,
                    0.12333333333333332,
                    0.1038961038961039,
                    0.06896551724137931,
                    0.23307043593272767,
                    0.0,
                    0.03571428571428571,
                    1.0
                ],
                [
                    0.23856954696978955,
                    0.3333333333333333,
                    0.03562208543637686,
                    1.5602262951061472,
                    0.4942921030952101,
                    0.5165505226480837,
                    0.3922413793103448,
                    0.5001913045101205,
                    0.3107570207570208,
                    0.522077922077922,
                    0.4367816091954023,
                    0.36246142268954584,
                    0.013333333333333334,
                    0.0,
                    1.0
                ],
                [
                    0.6167686493166826,
                    0.5297619047619048,
                    0.8435982574417535,
                    2.526681355365045,
                    0.5047533116263256,
                    0.304006968641115,
                    0.30603448275862066,
                    0.4992581117778255,
                    0.5659096459096459,
                    0.37402597402597404,
                    0.4942528735632184,
                    0.4044681413777265,
                    0.9866666666666667,
                    0.9642857142857143,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "dante alighieri",
                "michelangelo",
                "auguste rodin"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "michelangelo": 0.32966784024454987,
                "auguste rodin": 0.9846124399173577,
                "dante alighieri": -0.04908004269823712
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.097633923394549,
                    3.2621528042059125,
                    3.6402132723995386
                ],
                "result_count_important_words": [
                    70.0,
                    91.0,
                    71.0
                ],
                "wikipedia_search": [
                    0.8633333333333333,
                    2.1752991452991455,
                    3.9613675213675212
                ],
                "word_count_appended_bing": [
                    6.0,
                    38.0,
                    43.0
                ],
                "answer_relation_to_question_bing": [
                    0.8214285714285714,
                    2.0,
                    3.178571428571429
                ],
                "cosine_similarity_raw": [
                    0.06081168353557587,
                    0.017935462296009064,
                    0.4247456192970276
                ],
                "result_count_noun_chunks": [
                    59.0,
                    53600.0,
                    53500.0
                ],
                "question_answer_similarity": [
                    -0.8808315396308899,
                    0.44520169869065285,
                    0.7209741529077291
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    74.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    27.0
                ],
                "result_count_bing": [
                    20600.0,
                    59300.0,
                    34900.0
                ],
                "word_count_appended": [
                    40.0,
                    201.0,
                    144.0
                ],
                "answer_relation_to_question": [
                    1.1572944297082228,
                    1.9085563757583164,
                    4.934149194533461
                ],
                "result_count": [
                    73.0,
                    37800.0,
                    38600.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What was the first theatrical feature film to be completely computer-animated?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "toy story"
            ],
            "question": "what was the first theatrical feature film to be completely computer-animated?",
            "lines": [
                [
                    0.20711562540356313,
                    0.174,
                    0.0990090281091075,
                    0.5100711548917874,
                    0.05457746478873239,
                    0.2772002772002772,
                    0.00012751278179985647,
                    0.18932038834951456,
                    0.009407195286527765,
                    0.12589928057553956,
                    0.2631578947368421,
                    0.30472328176338487,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.4841258611686628,
                    0.28802197802197804,
                    0.36178155839360215,
                    0.27205263135546953,
                    0.7423708920187794,
                    0.4857934857934858,
                    0.00011801714911263312,
                    0.5844660194174758,
                    0.3812912431868358,
                    0.30935251798561153,
                    0.10526315789473684,
                    0.32763719021177434,
                    0,
                    0.14285714285714285,
                    1.0
                ],
                [
                    0.3087585134277741,
                    0.5379780219780219,
                    0.5392094134972903,
                    0.21787621375274302,
                    0.20305164319248825,
                    0.23700623700623702,
                    0.9997544700690875,
                    0.2262135922330097,
                    0.6093015615266364,
                    0.564748201438849,
                    0.631578947368421,
                    0.36763952802484073,
                    0,
                    0.8571428571428571,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "gone with the wind",
                "toy story 2",
                "toy story"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "gone with the wind": 0.0258505317047327,
                "toy story 2": 0.1670030430261531,
                "toy story": 0.6847258929636958
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.8283396905803093,
                    1.9658231412706462,
                    2.2058371681490443
                ],
                "result_count_important_words": [
                    94.0,
                    87.0,
                    737000.0
                ],
                "wikipedia_search": [
                    0.05644317171916659,
                    2.287747459121015,
                    3.6558093691598184
                ],
                "word_count_appended_bing": [
                    5.0,
                    2.0,
                    12.0
                ],
                "answer_relation_to_question_bing": [
                    0.87,
                    1.44010989010989,
                    2.6898901098901096
                ],
                "cosine_similarity_raw": [
                    0.033866893500089645,
                    0.12375050783157349,
                    0.1844412386417389
                ],
                "result_count_noun_chunks": [
                    1950000.0,
                    6020000.0,
                    2330000.0
                ],
                "question_answer_similarity": [
                    19.574651796370745,
                    10.44037773553282,
                    8.361286416649818
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    400000.0,
                    701000.0,
                    342000.0
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    6.0
                ],
                "word_count_appended": [
                    35.0,
                    86.0,
                    157.0
                ],
                "answer_relation_to_question": [
                    1.2426937524213788,
                    2.904755167011977,
                    1.8525510805666445
                ],
                "result_count": [
                    186000.0,
                    2530000.0,
                    692000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What was the first animated film nominated for Worst Picture at the Golden Raspberry Awards?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "the emoji movie"
            ],
            "question": "what was the first animated film nominated for worst picture at the golden raspberry awards?",
            "lines": [
                [
                    0.7067044228694713,
                    0.40523809523809523,
                    0.8522581967460862,
                    0.3516168124411094,
                    0.8527033855482566,
                    0.3664566165777993,
                    0.8586180889946047,
                    0.20577444568511724,
                    0.3486676300329437,
                    0.5474452554744526,
                    0.6428571428571429,
                    0.4448597991855439,
                    0,
                    1.0,
                    1.0
                ],
                [
                    0.12202075004759187,
                    0.40507936507936504,
                    0.0879786010489082,
                    0.3793435939047159,
                    0.005811015664477008,
                    0.36500242365487157,
                    0.0056274293670592335,
                    0.7545063008454299,
                    0.4220992209922099,
                    0.2846715328467153,
                    0.21428571428571427,
                    0.2754941361288286,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.17127482708293673,
                    0.18968253968253967,
                    0.059763202205005674,
                    0.2690395936541746,
                    0.1414855987872663,
                    0.2685409597673291,
                    0.13575448163833614,
                    0.03971925346945286,
                    0.2292331489748464,
                    0.1678832116788321,
                    0.14285714285714285,
                    0.2796460646856274,
                    0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "the emoji movie",
                "eight crazy nights",
                "mars needs moms"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "eight crazy nights": 0.06430043007352901,
                "the emoji movie": 0.8895777512449903,
                "mars needs moms": 0.04715892383353673
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.5588783934843513,
                    2.203953089030629,
                    2.2371685174850193
                ],
                "result_count_important_words": [
                    14800.0,
                    97.0,
                    2340.0
                ],
                "wikipedia_search": [
                    2.0920057801976624,
                    2.5325953259532596,
                    1.3753988938490784
                ],
                "word_count_appended_bing": [
                    9.0,
                    3.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    2.026190476190476,
                    2.0253968253968253,
                    0.9484126984126984
                ],
                "cosine_similarity_raw": [
                    0.23503194749355316,
                    0.02426234446465969,
                    0.016481228172779083
                ],
                "result_count_noun_chunks": [
                    12900.0,
                    47300.0,
                    2490.0
                ],
                "question_answer_similarity": [
                    12.054349844052922,
                    13.004896894097328,
                    9.22338542714715
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    75600.0,
                    75300.0,
                    55400.0
                ],
                "word_count_raw": [
                    5.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    13500.0,
                    92.0,
                    2240.0
                ],
                "answer_relation_to_question": [
                    4.240226537216828,
                    0.7321245002855512,
                    1.0276489624976204
                ],
                "word_count_appended": [
                    75.0,
                    39.0,
                    23.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What book that heavily influenced \u201cThe Matrix\u201d makes a cameo in the movie?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "neuromancer"
            ],
            "lines": [
                [
                    0.5754636950851462,
                    0.6901960784313727,
                    0.5171811896341882,
                    -0.1829135133320409,
                    0.6088549979310752,
                    0.2687425624752082,
                    0.48159927305770106,
                    0.5227830127791403,
                    0.8431818181818183,
                    0.9556962025316456,
                    0.9090909090909091,
                    0.3485568346912764,
                    0,
                    1.0,
                    1.0
                ],
                [
                    0.1507528643806246,
                    0.07973856209150328,
                    0.3403488317968572,
                    0.20680081164226882,
                    0.37772654726015253,
                    0.05097183657278857,
                    0.48159927305770106,
                    0.4375887440299471,
                    0.030303030303030304,
                    0.0,
                    0.0,
                    0.3636825069497388,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.2737834405342292,
                    0.23006535947712417,
                    0.14246997856895455,
                    0.976112701689772,
                    0.013418454808772242,
                    0.6802856009520032,
                    0.03680145388459791,
                    0.03962824319091261,
                    0.1265151515151515,
                    0.04430379746835443,
                    0.09090909090909091,
                    0.2877606583589848,
                    0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "simulacra & simulation": 0.19380869292958558,
                "neuromancer": 0.5798794661967261,
                "gravity's rainbow": 0.22631184087368825
            },
            "question": "what book that heavily influenced \u201cthe matrix\u201d makes a cameo in the movie?",
            "rate_limited": false,
            "answers": [
                "neuromancer",
                "simulacra & simulation",
                "gravity's rainbow"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "simulacra & simulation": 0.25589760633838304,
                "neuromancer": 0.6522136761017001,
                "gravity's rainbow": 0.16845805914882833
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.439897842838935,
                    2.5457775486481715,
                    2.0143246085128936
                ],
                "result_count_important_words": [
                    1060.0,
                    1060.0,
                    81.0
                ],
                "wikipedia_search": [
                    5.0590909090909095,
                    0.18181818181818182,
                    0.759090909090909
                ],
                "word_count_appended_bing": [
                    60.0,
                    0.0,
                    6.0
                ],
                "answer_relation_to_question_bing": [
                    2.070588235294118,
                    0.23921568627450981,
                    0.6901960784313725
                ],
                "cosine_similarity_raw": [
                    0.11165349930524826,
                    0.07347741723060608,
                    0.030757637694478035
                ],
                "result_count_noun_chunks": [
                    40500.0,
                    33900.0,
                    3070.0
                ],
                "question_answer_similarity": [
                    -2.136249199975282,
                    2.4152292544022202,
                    11.400032398290932
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    4.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    27100.0,
                    5140.0,
                    68600.0
                ],
                "word_count_appended": [
                    302.0,
                    0.0,
                    14.0
                ],
                "answer_relation_to_question": [
                    2.877318475425731,
                    0.753764321903123,
                    1.368917202671146
                ],
                "result_count": [
                    10300.0,
                    6390.0,
                    227.0
                ]
            },
            "integer_answers": {
                "simulacra & simulation": 1,
                "neuromancer": 10,
                "gravity's rainbow": 2
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "In which town were a President, Governor, Senator, NFL owner and late night host all born?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "hope, ar"
            ],
            "lines": [
                [
                    0.23983871740885915,
                    0.2887989203778677,
                    0.12925113865931756,
                    0.22482535151494046,
                    0.4020930873037731,
                    0.3395784543325527,
                    0.24769433465085638,
                    0.24038772213247173,
                    0.38285929032272115,
                    0.4,
                    0.3333333333333333,
                    0.40906282063733546,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.341157020069985,
                    0.26855600539811064,
                    0.5120595743595259,
                    0.425805971447376,
                    0.022307904158633984,
                    0.32201405152224827,
                    0.024374176548089592,
                    0.02617124394184168,
                    0.17637664562531583,
                    0.2,
                    0.3333333333333333,
                    0.26035476113529715,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.4190042625211559,
                    0.4426450742240216,
                    0.3586892869811566,
                    0.34936867703768354,
                    0.5755990085375929,
                    0.33840749414519905,
                    0.7279314888010541,
                    0.7334410339256866,
                    0.44076406405196294,
                    0.4,
                    0.3333333333333333,
                    0.33058241822736734,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "muncie, in": 0.45414717848218444,
                "hope, ar": 0.24270922396164643,
                "brookline, ma": 0.3031435975561691
            },
            "question": "in which town were a president, governor, senator, nfl owner and late night host all born?",
            "rate_limited": false,
            "answers": [
                "brookline, ma",
                "hope, ar",
                "muncie, in"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "muncie, in": 0.07468058506442075,
                "hope, ar": 0.30089804766837025,
                "brookline, ma": 0.12630205429851368
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    4.090628206373355,
                    2.6035476113529716,
                    3.3058241822736734
                ],
                "result_count_important_words": [
                    752.0,
                    74.0,
                    2210.0
                ],
                "wikipedia_search": [
                    3.062874322581769,
                    1.4110131650025266,
                    3.5261125124157036
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.4439946018893386,
                    1.3427800269905532,
                    2.213225371120108
                ],
                "cosine_similarity_raw": [
                    0.01591062732040882,
                    0.06303378939628601,
                    0.044154129922389984
                ],
                "result_count_noun_chunks": [
                    744.0,
                    81.0,
                    2270.0
                ],
                "question_answer_similarity": [
                    8.583991593215615,
                    16.25757439993322,
                    13.33914421312511
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    290000.0,
                    275000.0,
                    289000.0
                ],
                "result_count": [
                    1460.0,
                    81.0,
                    2090.0
                ],
                "answer_relation_to_question": [
                    1.678871021862014,
                    2.388099140489895,
                    2.933029837648091
                ],
                "word_count_appended": [
                    6.0,
                    3.0,
                    6.0
                ]
            },
            "integer_answers": {
                "muncie, in": 6,
                "hope, ar": 2,
                "brookline, ma": 4
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Though it now conveys something different, which of these words originally meant \u201cparrot\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "popinjay"
            ],
            "lines": [
                [
                    0.12571428571428572,
                    0.0,
                    0.3420904241359691,
                    0.009540421605749829,
                    0.21902017291066284,
                    0.3464788732394366,
                    0.48578811369509045,
                    0.3480355819125278,
                    0.0,
                    0.352112676056338,
                    0.32926829268292684,
                    0.37956639526736424,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.1957142857142857,
                    0.0,
                    0.11780364221230614,
                    -0.04067507928221927,
                    0.5,
                    0.3464788732394366,
                    0.32299741602067183,
                    0.5707931801334322,
                    0.041666666666666664,
                    0.3436619718309859,
                    0.32926829268292684,
                    0.3290363433986637,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.6785714285714286,
                    1.0,
                    0.5401059336517248,
                    1.0311346576764695,
                    0.28097982708933716,
                    0.30704225352112674,
                    0.19121447028423771,
                    0.08117123795404003,
                    0.9583333333333334,
                    0.30422535211267604,
                    0.34146341463414637,
                    0.291397261333972,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "thespian": 0.254728799384763,
                "popinjay": 0.5004699308468743,
                "warble": 0.24480126976836267
            },
            "question": "though it now conveys something different, which of these words originally meant \u201cparrot\u201d?",
            "rate_limited": false,
            "answers": [
                "warble",
                "thespian",
                "popinjay"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "thespian": 0.0675362787562466,
                "popinjay": 0.2821995322550263,
                "warble": 0.17225926874830394
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.2773983716041855,
                    1.9742180603919823,
                    1.748383568003832
                ],
                "result_count_important_words": [
                    18800.0,
                    12500.0,
                    7400.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.125,
                    2.875
                ],
                "answer_relation_to_question": [
                    0.6285714285714286,
                    0.9785714285714285,
                    3.392857142857143
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_count_appended": [
                    125.0,
                    122.0,
                    108.0
                ],
                "cosine_similarity_raw": [
                    0.04515066742897034,
                    0.015548266470432281,
                    0.0712856650352478
                ],
                "result_count_noun_chunks": [
                    939000.0,
                    1540000.0,
                    219000.0
                ],
                "question_answer_similarity": [
                    -0.01999802637146786,
                    0.08526052010711282,
                    -2.161399037577212
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    24600.0,
                    24600.0,
                    21800.0
                ],
                "result_count": [
                    15200.0,
                    34700.0,
                    19500.0
                ],
                "word_count_appended_bing": [
                    27.0,
                    27.0,
                    28.0
                ]
            },
            "integer_answers": {
                "thespian": 3,
                "popinjay": 5,
                "warble": 4
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these artists does NOT have a namesake museum?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "\u00c9douard manet"
            ],
            "question": "which of these artists does not have a namesake museum?",
            "lines": [
                [
                    0.3441884790040308,
                    0.3055555555555556,
                    0.36480940447249355,
                    0.9614161315794076,
                    0.3567073170731707,
                    0.25512905360688287,
                    0.34878204543381075,
                    0.35108024691358025,
                    0.3980855855855856,
                    0.5,
                    0.5,
                    0.3439276632861057,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.3553880300015307,
                    0.3888888888888889,
                    0.3751144100632036,
                    0.5909437532678581,
                    0.45286116322701686,
                    0.4880873593646592,
                    0.4887783961317398,
                    0.48842592592592593,
                    0.3091216216216216,
                    0.2777777777777778,
                    0.36486486486486486,
                    0.3405998211456239,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.3004234909944385,
                    0.3055555555555556,
                    0.26007618546430283,
                    -0.5523598848472658,
                    0.19043151969981237,
                    0.25678358702845794,
                    0.1624395584344494,
                    0.16049382716049382,
                    0.2927927927927928,
                    0.2222222222222222,
                    0.13513513513513514,
                    0.3154725155682704,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "\u00c9douard manet",
                "james mcneill whistler",
                "john singer sargent"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "\u00c9douard manet": 0.4764541606576105,
                "john singer sargent": 0.07271779142883318,
                "james mcneill whistler": 0.21808986374749326
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.9364340202833659,
                    0.9564010731262567,
                    1.1071649065903775
                ],
                "result_count_important_words": [
                    66300.0,
                    4920.0,
                    148000.0
                ],
                "wikipedia_search": [
                    0.40765765765765766,
                    0.7635135135135135,
                    0.8288288288288288
                ],
                "word_count_appended_bing": [
                    0.0,
                    10.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    0.7777777777777778,
                    0.4444444444444444,
                    0.7777777777777777
                ],
                "cosine_similarity_raw": [
                    0.02975149266421795,
                    0.027483662590384483,
                    0.05280020833015442
                ],
                "result_count_noun_chunks": [
                    57900.0,
                    4500.0,
                    132000.0
                ],
                "question_answer_similarity": [
                    -1.6083155190572143,
                    -0.31699422653764486,
                    3.668113484978676
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    5920000.0,
                    288000.0,
                    5880000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    61100.0,
                    20100.0,
                    132000.0
                ],
                "answer_relation_to_question": [
                    0.6232460839838767,
                    0.5784478799938773,
                    0.798306036022246
                ],
                "word_count_appended": [
                    0.0,
                    36.0,
                    45.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these mammals averages the largest litter?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "jackrabbit"
            ],
            "lines": [
                [
                    0.27777777777777773,
                    0.3333333333333333,
                    0.2075082169275096,
                    0.687111442664507,
                    0.6780952380952381,
                    0.5259360218071033,
                    0.6843033509700176,
                    0.2470004137360364,
                    0.7863247863247863,
                    0.15481171548117154,
                    0.18085106382978725,
                    0.3315972849042629,
                    1.0,
                    0,
                    -1.0
                ],
                [
                    0.11904761904761903,
                    0.0,
                    0.4525528060973408,
                    0.319605813404062,
                    0.11428571428571428,
                    0.037120179587909884,
                    0.11675485008818343,
                    0.2772031443938767,
                    0.1111111111111111,
                    0.10669456066945607,
                    0.2127659574468085,
                    0.23099898690731213,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.6031746031746031,
                    0.6666666666666666,
                    0.33993897697514963,
                    -0.006717256068568913,
                    0.20761904761904762,
                    0.4369437986049868,
                    0.19894179894179895,
                    0.4757964418700869,
                    0.10256410256410257,
                    0.7384937238493724,
                    0.6063829787234043,
                    0.437403728188425,
                    0.0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "naked mole rat": 0.4688192804501178,
                "burmese cat": 0.16139544177226106,
                "jackrabbit": 0.3697852777776211
            },
            "question": "which of these mammals averages the largest litter?",
            "rate_limited": false,
            "answers": [
                "naked mole rat",
                "burmese cat",
                "jackrabbit"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "naked mole rat": 0.035285328494094034,
                "burmese cat": 0.03439178423615458,
                "jackrabbit": 0.7818273252691595
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3263891396170515,
                    0.9239959476292485,
                    1.7496149127537
                ],
                "result_count_important_words": [
                    194000.0,
                    33100.0,
                    56400.0
                ],
                "wikipedia_search": [
                    2.358974358974359,
                    0.3333333333333333,
                    0.3076923076923077
                ],
                "word_count_appended_bing": [
                    17.0,
                    20.0,
                    57.0
                ],
                "answer_relation_to_question_bing": [
                    1.0,
                    0.0,
                    2.0
                ],
                "cosine_similarity_raw": [
                    0.031003842130303383,
                    0.06761600077152252,
                    0.050790347158908844
                ],
                "result_count_noun_chunks": [
                    59700.0,
                    67000.0,
                    115000.0
                ],
                "question_answer_similarity": [
                    4.587753164814785,
                    2.133966182038421,
                    -0.04485023953020573
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    656000.0,
                    46300.0,
                    545000.0
                ],
                "word_count_appended": [
                    74.0,
                    51.0,
                    353.0
                ],
                "answer_relation_to_question": [
                    0.8333333333333333,
                    0.3571428571428571,
                    1.8095238095238095
                ],
                "result_count": [
                    178000.0,
                    30000.0,
                    54500.0
                ]
            },
            "integer_answers": {
                "naked mole rat": 6,
                "burmese cat": 1,
                "jackrabbit": 6
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these is NOT believed to be a function of the uvula?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "lubricant"
            ],
            "question": "which of these is not believed to be a function of the uvula?",
            "lines": [
                [
                    0.34659090909090906,
                    0.33432245301681507,
                    0.41556733213628494,
                    0.2871754329723377,
                    0.35465116279069764,
                    0.32306569343065694,
                    0.288853424993862,
                    0.32894736842105265,
                    0.05555555555555558,
                    0.302491103202847,
                    0.3505154639175258,
                    0.3377975853148929,
                    0.4891304347826087,
                    0.5,
                    -1.0
                ],
                [
                    0.3352272727272727,
                    0.34272997032640956,
                    0.1414986029797612,
                    0.28949046013050145,
                    0.2787467700258398,
                    0.1875912408759124,
                    0.22379081757917996,
                    0.2861842105263158,
                    0.4583333333333333,
                    0.30604982206405695,
                    0.2989690721649485,
                    0.3001737312782716,
                    0.021739130434782594,
                    0.0,
                    -1.0
                ],
                [
                    0.3181818181818182,
                    0.3229475766567755,
                    0.44293406488395387,
                    0.4233341068971608,
                    0.36660206718346255,
                    0.48934306569343067,
                    0.487355757426958,
                    0.3848684210526316,
                    0.4861111111111111,
                    0.3914590747330961,
                    0.3505154639175258,
                    0.3620286834068356,
                    0.4891304347826087,
                    0.5,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "taste",
                "speech",
                "lubricant"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "taste": 0.41941085696880637,
                "lubricant": 0.45443043301383557,
                "speech": 0.09505503399340141
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.9732144881106428,
                    1.1989576123303705,
                    0.8278278995589865
                ],
                "result_count_important_words": [
                    172000.0,
                    225000.0,
                    10300.0
                ],
                "wikipedia_search": [
                    1.7777777777777777,
                    0.16666666666666666,
                    0.05555555555555555
                ],
                "word_count_appended_bing": [
                    29.0,
                    39.0,
                    29.0
                ],
                "answer_relation_to_question_bing": [
                    0.3313550939663699,
                    0.314540059347181,
                    0.35410484668644904
                ],
                "cosine_similarity_raw": [
                    0.04249982163310051,
                    0.18045438826084137,
                    0.02872456982731819
                ],
                "result_count_noun_chunks": [
                    104000.0,
                    130000.0,
                    70000.0
                ],
                "question_answer_similarity": [
                    4.184671618044376,
                    4.139152303338051,
                    1.5074462099000812
                ],
                "word_count_noun_chunks": [
                    1.0,
                    44.0,
                    1.0
                ],
                "result_count_bing": [
                    6060000.0,
                    10700000.0,
                    365000.0
                ],
                "word_count_raw": [
                    0.0,
                    14.0,
                    0.0
                ],
                "result_count": [
                    90000.0,
                    137000.0,
                    82600.0
                ],
                "answer_relation_to_question": [
                    0.3068181818181818,
                    0.32954545454545453,
                    0.36363636363636365
                ],
                "word_count_appended": [
                    333.0,
                    327.0,
                    183.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What element gets its name from a sinister spirit of German folklore?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "nickel"
            ],
            "question": "what element gets its name from a sinister spirit of german folklore?",
            "lines": [
                [
                    0.26070487483530963,
                    0.5548478414720454,
                    0.42847325617056164,
                    0.820776338889167,
                    0.9999427383218226,
                    0.4266666666666667,
                    0.8631512868801005,
                    0.9866102889358703,
                    0.3224489795918367,
                    0.42045454545454547,
                    0.4473684210526316,
                    0.43550273752058444,
                    1.0,
                    0,
                    1.0
                ],
                [
                    0.6325757575757576,
                    0.21196036801132342,
                    0.3681020595438797,
                    -0.04668568357475203,
                    2.7776187175606184e-05,
                    0.29555555555555557,
                    0.06018518518518518,
                    0.005825698848954663,
                    0.5079081632653062,
                    0.2409090909090909,
                    0.2719298245614035,
                    0.27375599694743424,
                    0.0,
                    0,
                    1.0
                ],
                [
                    0.1067193675889328,
                    0.23319179051663128,
                    0.20342468428555868,
                    0.22590934468558502,
                    2.9485491001797332e-05,
                    0.2777777777777778,
                    0.07666352793471437,
                    0.007564012215175006,
                    0.16964285714285715,
                    0.3386363636363636,
                    0.2807017543859649,
                    0.2907412655319814,
                    0.0,
                    0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "nickel",
                "bismuth",
                "boron"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "bismuth": 0.07907394639814033,
                "nickel": 0.7634627965037483,
                "boron": 0.05041854780103563
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.6130164251235066,
                    1.6425359816846055,
                    1.7444475931918881
                ],
                "result_count_important_words": [
                    1100000.0,
                    76700.0,
                    97700.0
                ],
                "wikipedia_search": [
                    1.2897959183673469,
                    2.0316326530612248,
                    0.6785714285714286
                ],
                "word_count_appended_bing": [
                    51.0,
                    31.0,
                    32.0
                ],
                "answer_relation_to_question_bing": [
                    1.664543524416136,
                    0.6358811040339702,
                    0.6995753715498938
                ],
                "cosine_similarity_raw": [
                    0.042584702372550964,
                    0.03658458590507507,
                    0.020217783749103546
                ],
                "result_count_noun_chunks": [
                    2100000.0,
                    12400.0,
                    16100.0
                ],
                "question_answer_similarity": [
                    1.3047282281913795,
                    -0.07421282306313515,
                    0.3591115935705602
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    192000.0,
                    133000.0,
                    125000.0
                ],
                "word_count_appended": [
                    185.0,
                    106.0,
                    149.0
                ],
                "answer_relation_to_question": [
                    1.0428194993412385,
                    2.5303030303030303,
                    0.4268774703557312
                ],
                "result_count": [
                    2340000.0,
                    65.0,
                    69.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these is NOT a geometric shape?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "tarragon"
            ],
            "lines": [
                [
                    0.4285714285714286,
                    0.23529411764705882,
                    0.2987363433453165,
                    0.24774906815628073,
                    0.326530612244898,
                    0.284688995215311,
                    0.4805429483946882,
                    0.48153499189513005,
                    0.35,
                    0.33739837398373984,
                    0.27808988764044945,
                    0.28835668158731786,
                    0.2857142857142857,
                    0,
                    -1.0
                ],
                [
                    0.5,
                    0.5,
                    0.36178761729647885,
                    0.5086250853281601,
                    0.47959183673469385,
                    0.3656299840510367,
                    0.4943267776096823,
                    0.49418563676087107,
                    0.4,
                    0.3902439024390244,
                    0.42696629213483145,
                    0.400116073260825,
                    0.5,
                    0,
                    -1.0
                ],
                [
                    0.0714285714285714,
                    0.2647058823529412,
                    0.33947603935820464,
                    0.24362584651555919,
                    0.19387755102040816,
                    0.3496810207336523,
                    0.025130273995629504,
                    0.02427937134399888,
                    0.25,
                    0.27235772357723576,
                    0.2949438202247191,
                    0.31152724515185715,
                    0.2142857142857143,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "hexagon": 0.5607201446171551,
                "tarragon": 0.10438873759759944,
                "octagon": 0.33489111778524544
            },
            "question": "which of these is not a geometric shape?",
            "rate_limited": false,
            "answers": [
                "octagon",
                "tarragon",
                "hexagon"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "hexagon": 0.2887702952589943,
                "tarragon": 0.43914396811896894,
                "octagon": 0.19464335480584577
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.8465732736507285,
                    0.39953570695670015,
                    0.7538910193925713
                ],
                "result_count_important_words": [
                    463000.0,
                    135000.0,
                    11300000.0
                ],
                "wikipedia_search": [
                    0.3,
                    0.2,
                    0.5
                ],
                "answer_relation_to_question": [
                    0.2857142857142857,
                    0.0,
                    1.7142857142857144
                ],
                "answer_relation_to_question_bing": [
                    0.5294117647058824,
                    0.0,
                    0.47058823529411764
                ],
                "word_count_appended": [
                    360.0,
                    243.0,
                    504.0
                ],
                "cosine_similarity_raw": [
                    0.051331717520952225,
                    0.035250671207904816,
                    0.04094117507338524
                ],
                "result_count_noun_chunks": [
                    524000.0,
                    165000.0,
                    13500000.0
                ],
                "question_answer_similarity": [
                    0.9916807818226516,
                    -0.03390802681678906,
                    1.0078905124682933
                ],
                "word_count_noun_chunks": [
                    3.0,
                    0.0,
                    4.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    5400000.0,
                    3370000.0,
                    3770000.0
                ],
                "result_count": [
                    1360000.0,
                    160000.0,
                    2400000.0
                ],
                "word_count_appended_bing": [
                    79.0,
                    26.0,
                    73.0
                ]
            },
            "integer_answers": {
                "hexagon": 8,
                "tarragon": 0,
                "octagon": 5
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Who is the only person to win both the Super Bowl and an Olympic gold medal?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "bob hayes"
            ],
            "question": "who is the only person to win both the super bowl and an olympic gold medal?",
            "lines": [
                [
                    0.42160717217845634,
                    0.40118367346938777,
                    0.7667769611830259,
                    0.492300018874787,
                    0.45402014198448076,
                    0.8489208633093526,
                    0.39375,
                    0.508419689119171,
                    0.592528735632184,
                    0.4626334519572954,
                    0.5862068965517241,
                    0.36751536200114177,
                    0.8055555555555556,
                    0.92,
                    0.0
                ],
                [
                    0.3219137465724928,
                    0.30718367346938774,
                    0.07095274366524126,
                    -0.28014865154386,
                    0.02922238732045567,
                    0.021377183967112023,
                    0.15875,
                    0.03011658031088083,
                    0.3646141215106732,
                    0.21352313167259787,
                    0.20689655172413793,
                    0.29349479641570775,
                    0.0,
                    0.0,
                    0.0
                ],
                [
                    0.2564790812490509,
                    0.2916326530612245,
                    0.1622702951517329,
                    0.7878486326690729,
                    0.5167574706950636,
                    0.12970195272353546,
                    0.4475,
                    0.46146373056994816,
                    0.04285714285714286,
                    0.3238434163701068,
                    0.20689655172413793,
                    0.33898984158315043,
                    0.19444444444444445,
                    0.08,
                    0.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "bob hayes",
                "willie gault",
                "michael carter"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "bob hayes": 0.7979488672384695,
                "willie gault": 0.05252119473885747,
                "michael carter": 0.07152597354095637
            },
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.5726075340079926,
                    2.0544635749099545,
                    2.3729288910820534
                ],
                "result_count_important_words": [
                    31500.0,
                    12700.0,
                    35800.0
                ],
                "wikipedia_search": [
                    4.147701149425288,
                    2.5522988505747124,
                    0.3
                ],
                "word_count_appended_bing": [
                    17.0,
                    6.0,
                    6.0
                ],
                "answer_relation_to_question_bing": [
                    2.8082857142857143,
                    2.1502857142857144,
                    2.0414285714285714
                ],
                "cosine_similarity_raw": [
                    0.5169486403465271,
                    0.04783519357442856,
                    0.10940001159906387
                ],
                "result_count_noun_chunks": [
                    31400.0,
                    1860.0,
                    28500.0
                ],
                "question_answer_similarity": [
                    2.845189206302166,
                    -1.619085697690025,
                    4.553277147933841
                ],
                "word_count_noun_chunks": [
                    29.0,
                    0.0,
                    7.0
                ],
                "word_count_raw": [
                    23.0,
                    0.0,
                    2.0
                ],
                "result_count_bing": [
                    4130000.0,
                    104000.0,
                    631000.0
                ],
                "result_count": [
                    27500.0,
                    1770.0,
                    31300.0
                ],
                "answer_relation_to_question": [
                    2.9512502052491945,
                    2.2533962260074496,
                    1.7953535687433564
                ],
                "word_count_appended": [
                    130.0,
                    60.0,
                    91.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these is NOT a marsupial?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "quintana roo"
            ],
            "lines": [
                [
                    0.5,
                    0.5,
                    0.4727917637133819,
                    0.18376836484494402,
                    0.4926317577610736,
                    0.33599290780141844,
                    0.45655399835119537,
                    0.4577927278551978,
                    0.5,
                    0.4683333333333333,
                    0.4619883040935673,
                    0.4084577114427861,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.2013888888888889,
                    0.33333333333333337,
                    0.36049582888042675,
                    0.17720885310834011,
                    0.4660253960164777,
                    0.38475177304964536,
                    0.4572959604286892,
                    0.45466922953708155,
                    0.5,
                    0.28791666666666665,
                    0.3128654970760234,
                    0.30199004975124377,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.2986111111111111,
                    0.16666666666666669,
                    0.16671240740619137,
                    0.6390227820467158,
                    0.041342846222448715,
                    0.2792553191489362,
                    0.08615004122011544,
                    0.08753804260772063,
                    0.0,
                    0.24375000000000002,
                    0.22514619883040937,
                    0.28955223880597014,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "cuscus": 0.25172264618045476,
                "quintana roo": 0.10881273297187175,
                "wombat": 0.6394646208476736
            },
            "question": "which of these is not a marsupial?",
            "rate_limited": false,
            "answers": [
                "quintana roo",
                "cuscus",
                "wombat"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "cuscus": 0.15946509319464014,
                "quintana roo": 0.8501752383736512,
                "wombat": 0.10004507654825694
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.18308457711442785,
                    0.39601990049751246,
                    0.4208955223880597
                ],
                "result_count_important_words": [
                    52700.0,
                    51800.0,
                    502000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    1.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    0.5972222222222222,
                    0.4027777777777778
                ],
                "result_count": [
                    34700.0,
                    160000.0,
                    2160000.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.3333333333333333,
                    0.6666666666666666
                ],
                "cosine_similarity_raw": [
                    0.011475966311991215,
                    0.05884046107530594,
                    0.14057497680187225
                ],
                "result_count_noun_chunks": [
                    52700.0,
                    56600.0,
                    515000.0
                ],
                "question_answer_similarity": [
                    -0.7520406674593687,
                    -0.7676400542259216,
                    0.33061456913128495
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    14.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    11.0
                ],
                "result_count_bing": [
                    1850000.0,
                    1300000.0,
                    2490000.0
                ],
                "word_count_appended": [
                    76.0,
                    509.0,
                    615.0
                ],
                "word_count_appended_bing": [
                    13.0,
                    64.0,
                    94.0
                ]
            },
            "integer_answers": {
                "cuscus": 1,
                "quintana roo": 0,
                "wombat": 13
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "The bar exam is generally taken after you graduate from what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "law school"
            ],
            "question": "the bar exam is generally taken after you graduate from what?",
            "lines": [
                [
                    0.28197701149425286,
                    0.5787234042553192,
                    0.9414300449598423,
                    0.4032309518196919,
                    0.9998394865530166,
                    0.6588275981324572,
                    0.9663269442353349,
                    0.9921116103514721,
                    0.6855877616747181,
                    0.675,
                    0.8095238095238095,
                    0.5769090383592408,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.3252873563218391,
                    0.10283687943262412,
                    0.031084433832026085,
                    0.3039277182477894,
                    0.0001102764139580533,
                    0.17101850250734912,
                    0.032572818345011294,
                    0.0062172327582025585,
                    0.2839774557165861,
                    0.215,
                    0.09523809523809523,
                    0.25937621541100453,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.39273563218390806,
                    0.31843971631205675,
                    0.027485521208131684,
                    0.2928413299325187,
                    5.0237033025335396e-05,
                    0.17015389936019368,
                    0.0011002374196537148,
                    0.0016711568903253686,
                    0.03043478260869565,
                    0.11,
                    0.09523809523809523,
                    0.1637147462297546,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "law school",
                "puppy training",
                "clown college"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "puppy training": -0.0004454850603850956,
                "clown college": 0.004718587385999682,
                "law school": 1.0074916874955255
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.8845451917962044,
                    1.2968810770550228,
                    0.8185737311487731
                ],
                "result_count_important_words": [
                    801000.0,
                    27000.0,
                    912.0
                ],
                "wikipedia_search": [
                    3.4279388083735913,
                    1.4198872785829308,
                    0.15217391304347827
                ],
                "word_count_appended_bing": [
                    17.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.1574468085106382,
                    0.20567375886524822,
                    0.6368794326241134
                ],
                "cosine_similarity_raw": [
                    0.4650290012359619,
                    0.015354474075138569,
                    0.013576754368841648
                ],
                "result_count_noun_chunks": [
                    4500000.0,
                    28200.0,
                    7580.0
                ],
                "question_answer_similarity": [
                    8.161360159516335,
                    6.15147116035223,
                    5.927083604037762
                ],
                "word_count_noun_chunks": [
                    28.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    3810000.0,
                    989000.0,
                    984000.0
                ],
                "word_count_raw": [
                    17.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    816000.0,
                    90.0,
                    41.0
                ],
                "answer_relation_to_question": [
                    1.4098850574712642,
                    1.6264367816091954,
                    1.9636781609195404
                ],
                "word_count_appended": [
                    135.0,
                    43.0,
                    22.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which brand mascot was NOT a real person?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "sara lee"
            ],
            "lines": [
                [
                    0.4166970507211149,
                    0.2641129032258065,
                    0.4417259369515351,
                    0.18523063987353944,
                    0.48225715889949466,
                    0.2698653929656969,
                    0.4190721649484536,
                    0.482256760398886,
                    0.3032407407407408,
                    0.2849002849002849,
                    0.2857142857142857,
                    0.32695603612018154,
                    0.25,
                    0.16666666666666669,
                    -1.0
                ],
                [
                    0.3042465092097445,
                    0.2943548387096774,
                    0.4234232729841575,
                    0.36753576994789994,
                    0.4893879842784952,
                    0.37277464177160224,
                    0.43247422680412373,
                    0.4893989758332585,
                    0.4207175925925926,
                    0.3532763532763533,
                    0.39285714285714285,
                    0.3731249463777423,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.27905644006914065,
                    0.4415322580645161,
                    0.1348507900643074,
                    0.4472335901785606,
                    0.02835485682201011,
                    0.35735996526270086,
                    0.1484536082474227,
                    0.028344263767855538,
                    0.2760416666666667,
                    0.36182336182336183,
                    0.3214285714285714,
                    0.29991901750207617,
                    0.25,
                    0.33333333333333337,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "betty crocker": 0.47032403953849666,
                "sara lee": 0.1837753921938872,
                "little debbie": 0.3459005682676162
            },
            "question": "which brand mascot was not a real person?",
            "rate_limited": false,
            "answers": [
                "little debbie",
                "sara lee",
                "betty crocker"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "betty crocker": 0.21835044253510733,
                "sara lee": 0.3682877588442318,
                "little debbie": 0.2629862673755379
            },
            "integer_answers": {
                "betty crocker": 7,
                "sara lee": 0,
                "little debbie": 7
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3843517110385475,
                    1.0150004289780616,
                    1.6006478599833907
                ],
                "result_count_important_words": [
                    157000.0,
                    131000.0,
                    682000.0
                ],
                "wikipedia_search": [
                    1.574074074074074,
                    0.6342592592592593,
                    1.7916666666666667
                ],
                "word_count_appended_bing": [
                    24.0,
                    12.0,
                    20.0
                ],
                "answer_relation_to_question_bing": [
                    1.4153225806451613,
                    1.2338709677419355,
                    0.35080645161290325
                ],
                "cosine_similarity_raw": [
                    0.025272458791732788,
                    0.03321000933647156,
                    0.15835893154144287
                ],
                "result_count_noun_chunks": [
                    158000.0,
                    94400.0,
                    4200000.0
                ],
                "question_answer_similarity": [
                    4.103627513162792,
                    1.7269274834543467,
                    0.6879122257232666
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    2.0
                ],
                "result_count_bing": [
                    1060000.0,
                    586000.0,
                    657000.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    1.0
                ],
                "word_count_appended": [
                    151.0,
                    103.0,
                    97.0
                ],
                "answer_relation_to_question": [
                    0.4998176956733107,
                    1.174520944741533,
                    1.3256613595851563
                ],
                "result_count": [
                    158000.0,
                    94500.0,
                    4200000.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Rejected in the late 1700s, what was the name of the proposed 14th U.S. colony?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "vandalia"
            ],
            "lines": [
                [
                    0.26091277143908725,
                    0.2306149732620321,
                    0.2086859697451369,
                    -0.07189610347694714,
                    0.697444585627559,
                    0.24933545986177566,
                    0.6209016393442623,
                    0.8774551665243382,
                    0.008620689655172414,
                    0.5460526315789473,
                    0.5079365079365079,
                    0.42653604657199623,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.6661207213838793,
                    0.6978609625668449,
                    0.568477990349846,
                    -1.2384314651496962,
                    0.289425384723987,
                    0.3822434875066454,
                    0.2540983606557377,
                    0.08731853116994022,
                    0.24233716475095785,
                    0.40789473684210525,
                    0.4603174603174603,
                    0.2943344596166789,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.07296650717703351,
                    0.07152406417112299,
                    0.2228360399050171,
                    2.310327568626643,
                    0.013130029648454045,
                    0.3684210526315789,
                    0.125,
                    0.03522630230572161,
                    0.7490421455938697,
                    0.046052631578947366,
                    0.031746031746031744,
                    0.27912949381132485,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "roanoke": 0.3259000241478477,
                "new albion": 0.30895727622826746,
                "vandalia": 0.3651426996238848
            },
            "question": "rejected in the late 1700s, what was the name of the proposed 14th u.s. colony?",
            "rate_limited": false,
            "answers": [
                "roanoke",
                "vandalia",
                "new albion"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "roanoke": 0.32536480592355255,
                "new albion": 0.09076852570796085,
                "vandalia": 0.6667451647494494
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.5592162794319773,
                    1.7660067577000735,
                    1.674776962867949
                ],
                "result_count_important_words": [
                    6060.0,
                    2480.0,
                    1220.0
                ],
                "wikipedia_search": [
                    0.017241379310344827,
                    0.4846743295019157,
                    1.4980842911877394
                ],
                "word_count_appended_bing": [
                    32.0,
                    29.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.9224598930481284,
                    2.7914438502673797,
                    0.28609625668449196
                ],
                "cosine_similarity_raw": [
                    0.048826251178979874,
                    0.13300678133964539,
                    0.052136942744255066
                ],
                "result_count_noun_chunks": [
                    41100.0,
                    4090.0,
                    1650.0
                ],
                "question_answer_similarity": [
                    -0.12943960819393396,
                    -2.229635207913816,
                    4.159445099532604
                ],
                "word_count_noun_chunks": [
                    0.0,
                    25.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    13.0,
                    0.0
                ],
                "result_count_bing": [
                    46900.0,
                    71900.0,
                    69300.0
                ],
                "result_count": [
                    4940.0,
                    2050.0,
                    93.0
                ],
                "answer_relation_to_question": [
                    1.0436510857563488,
                    2.664482885535517,
                    0.291866028708134
                ],
                "word_count_appended": [
                    166.0,
                    124.0,
                    14.0
                ]
            },
            "integer_answers": {
                "roanoke": 6,
                "new albion": 2,
                "vandalia": 6
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What topic would a herpetologist study?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "venereal disease"
            ],
            "lines": [
                [
                    0.36467236467236464,
                    1.0,
                    0.3619837626283285,
                    0.326491276716233,
                    0.9838201759148659,
                    0.6821282401091405,
                    0.9039657853810265,
                    0.7178871548619448,
                    0.7333333333333334,
                    0.5,
                    0.42857142857142855,
                    0.4597317825803489,
                    0,
                    0,
                    1.0
                ],
                [
                    0.12962962962962962,
                    0.0,
                    0.28175971418203505,
                    0.3448202958839333,
                    0.005755239439678575,
                    0.17326057298772168,
                    0.053654743390357695,
                    0.10108043217286915,
                    0.0,
                    0.16216216216216217,
                    0.2857142857142857,
                    0.19753480801633794,
                    0,
                    0,
                    1.0
                ],
                [
                    0.5056980056980057,
                    0.0,
                    0.3562565231896365,
                    0.32868842739983367,
                    0.010424584645455533,
                    0.1446111869031378,
                    0.042379471228615864,
                    0.18103241296518607,
                    0.26666666666666666,
                    0.33783783783783783,
                    0.2857142857142857,
                    0.34273340940331304,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "venereal disease": 0.6218821087307512,
                "mushroom farming": 0.14461432363158422,
                "crocodile teeth": 0.23350356763766456
            },
            "question": "what topic would a herpetologist study?",
            "rate_limited": false,
            "answers": [
                "venereal disease",
                "mushroom farming",
                "crocodile teeth"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "venereal disease": 0.3727603250328644,
                "mushroom farming": 0.04810635733270982,
                "crocodile teeth": 0.10420843573584748
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3791953477410468,
                    0.5926044240490138,
                    1.0282002282099392
                ],
                "result_count_important_words": [
                    46500.0,
                    2760.0,
                    2180.0
                ],
                "wikipedia_search": [
                    1.4666666666666668,
                    0.0,
                    0.5333333333333333
                ],
                "word_count_appended_bing": [
                    3.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.0,
                    0.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.032720405608415604,
                    0.025468800216913223,
                    0.03220270946621895
                ],
                "result_count_noun_chunks": [
                    299000.0,
                    42100.0,
                    75400.0
                ],
                "question_answer_similarity": [
                    2.05005219951272,
                    2.1651408672332764,
                    2.0638481993228197
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    500000.0,
                    127000.0,
                    106000.0
                ],
                "word_count_appended": [
                    37.0,
                    12.0,
                    25.0
                ],
                "answer_relation_to_question": [
                    0.7293447293447293,
                    0.25925925925925924,
                    1.0113960113960114
                ],
                "result_count": [
                    9060.0,
                    53.0,
                    96.0
                ]
            },
            "integer_answers": {
                "venereal disease": 10,
                "mushroom farming": 1,
                "crocodile teeth": 1
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "The first edition of the CNN show Crossfire featured which host?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "tucker carlson"
            ],
            "question": "the first edition of the cnn show crossfire featured which host?",
            "lines": [
                [
                    0.390970695970696,
                    0.33659451659451656,
                    0.10968361762711926,
                    0.7800279807624353,
                    0.14542107685227795,
                    0.2882818116462976,
                    0.3539426523297491,
                    0.3578669634093487,
                    0.005405405405405406,
                    0.21115537848605578,
                    0.3055555555555556,
                    0.28466440993561337,
                    0.0,
                    0.25,
                    -1.0
                ],
                [
                    0.32263736263736265,
                    0.31134199134199136,
                    0.17146976094685804,
                    0.14475656651242685,
                    0.26092959042797975,
                    0.45794392523364486,
                    0.6328405017921147,
                    0.642122051433863,
                    0.3810810810810811,
                    0.29880478087649404,
                    0.3611111111111111,
                    0.3424956250568886,
                    0.38461538461538464,
                    0.0,
                    -1.0
                ],
                [
                    0.28639194139194135,
                    0.3520634920634921,
                    0.7188466214260227,
                    0.0752154527251378,
                    0.5936493327197423,
                    0.2537742631200575,
                    0.0132168458781362,
                    1.0985156788198363e-05,
                    0.6135135135135135,
                    0.4900398406374502,
                    0.3333333333333333,
                    0.3728399650074981,
                    0.6153846153846154,
                    0.75,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "john sununu",
                "pat buchanan",
                "tucker carlson"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "john sununu": -0.04381115706743991,
                "tucker carlson": 0.8606143412458493,
                "pat buchanan": 0.3222859567646924
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.4233220496780667,
                    1.7124781252844428,
                    1.8641998250374905
                ],
                "result_count_important_words": [
                    3160000.0,
                    5650000.0,
                    118000.0
                ],
                "wikipedia_search": [
                    0.02702702702702703,
                    1.9054054054054055,
                    3.0675675675675675
                ],
                "answer_relation_to_question": [
                    1.95485347985348,
                    1.6131868131868132,
                    1.4319597069597068
                ],
                "word_count_appended_bing": [
                    11.0,
                    13.0,
                    12.0
                ],
                "answer_relation_to_question_bing": [
                    1.6829725829725828,
                    1.5567099567099567,
                    1.7603174603174603
                ],
                "cosine_similarity_raw": [
                    0.036289069801568985,
                    0.05673115327954292,
                    0.23783200979232788
                ],
                "result_count_noun_chunks": [
                    3160000.0,
                    5670000.0,
                    97.0
                ],
                "question_answer_similarity": [
                    2.198370538651943,
                    0.40797071252018213,
                    0.2119814152829349
                ],
                "word_count_noun_chunks": [
                    0.0,
                    5.0,
                    8.0
                ],
                "result_count_bing": [
                    40100.0,
                    63700.0,
                    35300.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    3.0
                ],
                "result_count": [
                    3160000.0,
                    5670000.0,
                    12900000.0
                ],
                "word_count_appended": [
                    53.0,
                    75.0,
                    123.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "In which state must sellers of a \u201chaunted\u201d house legally disclose that it's haunted?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "new york",
                "top"
            ],
            "question": "in which state must sellers of a \u201chaunted\u201d house legally disclose that it's haunted?",
            "lines": [
                [
                    0.24793835095721886,
                    0.48589286961379985,
                    0.3504720278043698,
                    0.6118145281450647,
                    0.02274836666727329,
                    0.2475434618291761,
                    0.42441054091539526,
                    0.4171447856901789,
                    0.2563374909718966,
                    0.13963963963963963,
                    0.15151515151515152,
                    0.3556979903760346,
                    0.5757575757575758,
                    0.8,
                    -1.0
                ],
                [
                    0.29296426843596657,
                    0.3132376562609121,
                    0.21448251442785116,
                    0.1581180038976066,
                    0.9590711386922419,
                    0.15910808767951626,
                    0.2288488210818308,
                    0.5582180222747216,
                    0.5141094066420199,
                    0.42117117117117114,
                    0.4393939393939394,
                    0.3000604109271274,
                    0.3333333333333333,
                    0.0,
                    -1.0
                ],
                [
                    0.45909738060681454,
                    0.2008694741252881,
                    0.43504545776777903,
                    0.23006746795732874,
                    0.018180494640484813,
                    0.5933484504913077,
                    0.34674063800277394,
                    0.02463719203509956,
                    0.22955310238608345,
                    0.4391891891891892,
                    0.4090909090909091,
                    0.34424159869683807,
                    0.09090909090909091,
                    0.2,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "new york",
                "massachusetts",
                "california"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "new york": 0.8150644696485851,
                "california": 0.4065427715847518,
                "massachusetts": 0.1481910178843091
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.845583923008277,
                    2.400483287417019,
                    2.7539327895747046
                ],
                "result_count_important_words": [
                    153000.0,
                    82500.0,
                    125000.0
                ],
                "wikipedia_search": [
                    1.0253499638875865,
                    2.0564376265680795,
                    0.9182124095443338
                ],
                "word_count_appended_bing": [
                    10.0,
                    29.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    1.4576786088413995,
                    0.9397129687827362,
                    0.6026084223758643
                ],
                "cosine_similarity_raw": [
                    0.06943251192569733,
                    0.04249143600463867,
                    0.08618747442960739
                ],
                "result_count_noun_chunks": [
                    6180000.0,
                    8270000.0,
                    365000.0
                ],
                "question_answer_similarity": [
                    8.998722221702337,
                    2.3256394378840923,
                    3.3838902823626995
                ],
                "word_count_noun_chunks": [
                    19.0,
                    11.0,
                    3.0
                ],
                "word_count_raw": [
                    4.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    131000.0,
                    84200.0,
                    314000.0
                ],
                "word_count_appended": [
                    62.0,
                    187.0,
                    195.0
                ],
                "answer_relation_to_question": [
                    0.7438150528716566,
                    0.8788928053078997,
                    1.3772921418204436
                ],
                "result_count": [
                    125000.0,
                    5270000.0,
                    99900.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which nation\u2019s capital is right on the shore of the Caspian Sea?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "azerbaijan",
                "bottom"
            ],
            "question": "which nation\u2019s capital is right on the shore of the caspian sea?",
            "lines": [
                [
                    0.38611543422149736,
                    0.3403270887217805,
                    0.20549340690832282,
                    3.66033226931347,
                    0.17253521126760563,
                    0.23499491353001017,
                    0.2345597897503285,
                    0.17611389401529132,
                    0.17196059318636062,
                    0.3124484748557296,
                    0.4174757281553398,
                    0.3385928568217523,
                    0.10526315789473684,
                    0.03225806451612903,
                    -1.0
                ],
                [
                    0.27180775098884175,
                    0.33705759850879713,
                    0.27949123020732736,
                    -3.946073390707327,
                    0.426056338028169,
                    0.47202441505595116,
                    0.4664914586070959,
                    0.585288689691537,
                    0.44238146220755625,
                    0.3446001648804617,
                    0.2524271844660194,
                    0.32910669124978015,
                    0.27751196172248804,
                    0.5161290322580645,
                    -1.0
                ],
                [
                    0.34207681478966095,
                    0.32261531276942235,
                    0.5150153628843498,
                    1.285741121393857,
                    0.4014084507042254,
                    0.29298067141403866,
                    0.29894875164257556,
                    0.23859741629317163,
                    0.38565794460608316,
                    0.34295136026380874,
                    0.3300970873786408,
                    0.3323004519284676,
                    0.6172248803827751,
                    0.45161290322580644,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "turkmenistan",
                "iran",
                "azerbaijan"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "iran": 0.14976268364858955,
                "turkmenistan": 0.13500796050565256,
                "azerbaijan": 0.24450364982733871
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.0315571409305138,
                    1.9746401474986808,
                    1.9938027115708055
                ],
                "result_count_important_words": [
                    71400.0,
                    142000.0,
                    91000.0
                ],
                "wikipedia_search": [
                    0.859802965931803,
                    2.2119073110377814,
                    1.928289723030416
                ],
                "word_count_appended_bing": [
                    86.0,
                    52.0,
                    68.0
                ],
                "answer_relation_to_question_bing": [
                    1.7016354436089025,
                    1.6852879925439856,
                    1.6130765638471118
                ],
                "cosine_similarity_raw": [
                    0.07876575738191605,
                    0.10712917149066925,
                    0.19740572571754456
                ],
                "result_count_noun_chunks": [
                    66800.0,
                    222000.0,
                    90500.0
                ],
                "question_answer_similarity": [
                    -1.541832820046693,
                    1.662194854579866,
                    -0.5415895916521549
                ],
                "word_count_noun_chunks": [
                    22.0,
                    58.0,
                    129.0
                ],
                "result_count_bing": [
                    231000.0,
                    464000.0,
                    288000.0
                ],
                "word_count_raw": [
                    1.0,
                    16.0,
                    14.0
                ],
                "result_count": [
                    49000.0,
                    121000.0,
                    114000.0
                ],
                "answer_relation_to_question": [
                    1.9305771711074868,
                    1.3590387549442087,
                    1.7103840739483047
                ],
                "word_count_appended": [
                    379.0,
                    418.0,
                    416.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Aside from blood cells, what would you also find inside your blood vessels?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "plasma"
            ],
            "lines": [
                [
                    0.3603028187740992,
                    0.5106736242884251,
                    0.6404770639520677,
                    0.579677638116903,
                    0.5465937874950006,
                    0.3912231559290383,
                    0.5531738348776103,
                    0.5219998393846648,
                    0.08711650922177237,
                    0.371301775147929,
                    0.3142857142857143,
                    0.3899030315667874,
                    0.7083333333333334,
                    1.0,
                    1.0
                ],
                [
                    0.5125182821433321,
                    0.2883459835547122,
                    0.2887399913711652,
                    0.5845872246652679,
                    0.42394347420343953,
                    0.35014005602240894,
                    0.4342414603789241,
                    0.47559985366158347,
                    0.8813045434098065,
                    0.4467455621301775,
                    0.42857142857142855,
                    0.34906546295967444,
                    0.2916666666666667,
                    0.0,
                    1.0
                ],
                [
                    0.12717889908256882,
                    0.20098039215686275,
                    0.07078294467676716,
                    -0.1642648627821709,
                    0.029462738301559793,
                    0.2586367880485528,
                    0.012584704743465635,
                    0.0024003069537517066,
                    0.031578947368421054,
                    0.1819526627218935,
                    0.2571428571428571,
                    0.2610315054735382,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "marrow": 0.41110499926704197,
                "plasma": 0.49821872331238176,
                "plastids": 0.09067627742057628
            },
            "question": "aside from blood cells, what would you also find inside your blood vessels?",
            "rate_limited": false,
            "answers": [
                "plasma",
                "marrow",
                "plastids"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "marrow": 0.4046084166072911,
                "plasma": 0.7308693215874036,
                "plastids": 0.08012620476918583
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.339418189400724,
                    2.0943927777580464,
                    1.5661890328412291
                ],
                "result_count_important_words": [
                    120000.0,
                    94200.0,
                    2730.0
                ],
                "wikipedia_search": [
                    0.4355825461088619,
                    4.406522717049032,
                    0.15789473684210525
                ],
                "word_count_appended_bing": [
                    33.0,
                    45.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    2.0426944971537004,
                    1.1533839342188488,
                    0.803921568627451
                ],
                "cosine_similarity_raw": [
                    0.25211116671562195,
                    0.11365680396556854,
                    0.0278623104095459
                ],
                "result_count_noun_chunks": [
                    5850000.0,
                    5330000.0,
                    26900.0
                ],
                "question_answer_similarity": [
                    3.247341550886631,
                    3.2748449482023716,
                    -0.9202081970870495
                ],
                "word_count_noun_chunks": [
                    17.0,
                    7.0,
                    0.0
                ],
                "word_count_raw": [
                    4.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    4190000.0,
                    3750000.0,
                    2770000.0
                ],
                "word_count_appended": [
                    251.0,
                    302.0,
                    123.0
                ],
                "answer_relation_to_question": [
                    1.4412112750963968,
                    2.0500731285733282,
                    0.5087155963302753
                ],
                "result_count": [
                    410000.0,
                    318000.0,
                    22100.0
                ]
            },
            "integer_answers": {
                "marrow": 5,
                "plasma": 9,
                "plastids": 0
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What dish is made with ham, poached eggs and Hollandaise sauce?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "eggs benedict"
            ],
            "lines": [
                [
                    0.04946236559139784,
                    0.044642857142857144,
                    0.00633794488508366,
                    0.3073641525418077,
                    0.005493259417916089,
                    0.25613305613305615,
                    0.005378306685162201,
                    0.0021115036342226013,
                    0.0,
                    0.16666666666666666,
                    0.09375,
                    0.28155435120400146,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.8976357871519162,
                    0.8273809523809524,
                    0.986840754993025,
                    0.5406156836881585,
                    0.9902986014867078,
                    0.41995841995842,
                    0.9564138132437761,
                    0.976570430827953,
                    0.8927489177489177,
                    0.7092198581560284,
                    0.8125,
                    0.45628028373204704,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.05290184725668595,
                    0.12797619047619047,
                    0.0068213001218913015,
                    0.15202016377003377,
                    0.004208139095376087,
                    0.3239085239085239,
                    0.03820788007106179,
                    0.021318065537824338,
                    0.10725108225108224,
                    0.12411347517730496,
                    0.09375,
                    0.26216536506395155,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "eggs benedict": 0.8190331073834215,
                "benedict cumberbatch": 0.09390300233785187,
                "pope benedict": 0.08706389027872652
            },
            "question": "what dish is made with ham, poached eggs and hollandaise sauce?",
            "rate_limited": false,
            "answers": [
                "pope benedict",
                "eggs benedict",
                "benedict cumberbatch"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "eggs benedict": 0.7906221929744528,
                "benedict cumberbatch": 0.1294401344098707,
                "pope benedict": 0.10686918892271112
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6893261072240087,
                    2.737681702392282,
                    1.572992190383709
                ],
                "result_count_important_words": [
                    2210.0,
                    393000.0,
                    15700.0
                ],
                "wikipedia_search": [
                    0.0,
                    5.356493506493507,
                    0.6435064935064935
                ],
                "word_count_appended_bing": [
                    3.0,
                    26.0,
                    3.0
                ],
                "answer_relation_to_question_bing": [
                    0.26785714285714285,
                    4.964285714285714,
                    0.7678571428571428
                ],
                "cosine_similarity_raw": [
                    0.005281316116452217,
                    0.8223198652267456,
                    0.0056840889155864716
                ],
                "result_count_noun_chunks": [
                    1040.0,
                    481000.0,
                    10500.0
                ],
                "question_answer_similarity": [
                    4.335982605989557,
                    7.626459304417949,
                    2.144546722236555
                ],
                "word_count_noun_chunks": [
                    0.0,
                    83.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    32.0,
                    0.0
                ],
                "result_count_bing": [
                    616000.0,
                    1010000.0,
                    779000.0
                ],
                "word_count_appended": [
                    47.0,
                    200.0,
                    35.0
                ],
                "answer_relation_to_question": [
                    0.2967741935483871,
                    5.385814722911498,
                    0.31741108354011577
                ],
                "result_count": [
                    2180.0,
                    393000.0,
                    1670.0
                ]
            },
            "integer_answers": {
                "eggs benedict": 14,
                "benedict cumberbatch": 0,
                "pope benedict": 0
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "The core of a No. 2 pencil consists mainly of clay plus what chemical element?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "carbon",
                "top"
            ],
            "question": "the core of a no. 2 pencil consists mainly of clay plus what chemical element?",
            "lines": [
                [
                    0.3154232782064682,
                    0.2791977782222078,
                    0.7422119339931739,
                    0.37037458009608437,
                    0.40437930425569485,
                    0.2715909090909091,
                    0.4193849021435228,
                    0.35753575357535755,
                    0.37502947596634334,
                    0.42434782608695654,
                    0.415,
                    0.3679349238637986,
                    0.14093959731543623,
                    0.5,
                    1.0
                ],
                [
                    0.3296851780138194,
                    0.46191964798350343,
                    0.19373703097014275,
                    0.44852914984738207,
                    0.4361645770792866,
                    0.3784090909090909,
                    0.5125815470643057,
                    0.48404840484048406,
                    0.24070243404426078,
                    0.3739130434782609,
                    0.345,
                    0.3506011186614095,
                    0.8590604026845637,
                    0.5,
                    1.0
                ],
                [
                    0.3548915437797123,
                    0.2588825737942887,
                    0.06405103503668344,
                    0.18109627005653356,
                    0.15945611866501855,
                    0.35,
                    0.06803355079217148,
                    0.15841584158415842,
                    0.3842680899893958,
                    0.20173913043478262,
                    0.24,
                    0.2814639574747919,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "carbon",
                "lead",
                "tungsten"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "tungsten": 0.1305206248091411,
                "carbon": 0.46957641432293357,
                "lead": 0.249985885936798
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.943479390910389,
                    2.804808949291276,
                    2.2517116597983353
                ],
                "result_count_important_words": [
                    3150.0,
                    3850.0,
                    511.0
                ],
                "wikipedia_search": [
                    3.0002358077307467,
                    1.9256194723540863,
                    3.0741447199151666
                ],
                "word_count_appended_bing": [
                    83.0,
                    69.0,
                    48.0
                ],
                "answer_relation_to_question_bing": [
                    1.395988891111039,
                    2.3095982399175172,
                    1.2944128689714434
                ],
                "cosine_similarity_raw": [
                    0.34894177317619324,
                    0.09108307212591171,
                    0.030112802982330322
                ],
                "result_count_noun_chunks": [
                    13000.0,
                    17600.0,
                    5760.0
                ],
                "question_answer_similarity": [
                    4.167059488594532,
                    5.046371296048164,
                    2.0375019535422325
                ],
                "word_count_noun_chunks": [
                    21.0,
                    128.0,
                    0.0
                ],
                "word_count_raw": [
                    20.0,
                    20.0,
                    0.0
                ],
                "result_count_bing": [
                    239000.0,
                    333000.0,
                    308000.0
                ],
                "result_count": [
                    22900.0,
                    24700.0,
                    9030.0
                ],
                "answer_relation_to_question": [
                    2.2079629474452775,
                    2.307796246096736,
                    2.4842408064579864
                ],
                "word_count_appended": [
                    244.0,
                    215.0,
                    116.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these trees produces fruit called apples?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "jujube"
            ],
            "question": "which of these trees produces fruit called apples?",
            "lines": [
                [
                    0.4642857142857143,
                    0.5235294117647059,
                    0.4364692066531791,
                    0.5188824881727048,
                    0.5159450350012963,
                    0.36784511784511786,
                    0.43868131868131865,
                    0.4496179136307091,
                    0.5921911421911423,
                    0.4913733609385783,
                    0.4711111111111111,
                    0.3326003362035227,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.46031746031746035,
                    0.39803921568627454,
                    0.4502634893669596,
                    0.05833749424906839,
                    0.08996629504796474,
                    0.3501683501683502,
                    0.0734065934065934,
                    0.07232983827972277,
                    0.29924242424242425,
                    0.34644582470669427,
                    0.3422222222222222,
                    0.3862973706191178,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.07539682539682539,
                    0.0784313725490196,
                    0.11326730397986132,
                    0.42278001757822686,
                    0.39408866995073893,
                    0.281986531986532,
                    0.4879120879120879,
                    0.47805224808956814,
                    0.10856643356643358,
                    0.1621808143547274,
                    0.18666666666666668,
                    0.28110229317735946,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "cashew",
                "jujube",
                "hickory"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "jujube": 0.6859596720245226,
                "cashew": 0.6224228230869974,
                "hickory": 0.011769723262626092
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6630016810176134,
                    1.9314868530955889,
                    1.4055114658867973
                ],
                "result_count_important_words": [
                    998000.0,
                    167000.0,
                    1110000.0
                ],
                "wikipedia_search": [
                    2.960955710955711,
                    1.496212121212121,
                    0.5428321678321678
                ],
                "word_count_appended_bing": [
                    106.0,
                    77.0,
                    42.0
                ],
                "answer_relation_to_question_bing": [
                    2.6176470588235294,
                    1.9901960784313726,
                    0.39215686274509803
                ],
                "cosine_similarity_raw": [
                    0.06580217182636261,
                    0.06788180023431778,
                    0.017076198011636734
                ],
                "result_count_noun_chunks": [
                    506000.0,
                    81400.0,
                    538000.0
                ],
                "question_answer_similarity": [
                    1.474129954352975,
                    0.16573511285241693,
                    1.2011056495830417
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    4370000.0,
                    4160000.0,
                    3350000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    199000.0,
                    34700.0,
                    152000.0
                ],
                "answer_relation_to_question": [
                    2.321428571428571,
                    2.3015873015873014,
                    0.3769841269841269
                ],
                "word_count_appended": [
                    712.0,
                    502.0,
                    235.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "The two sides of a coin are technically called what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "obverse / reverse",
                "bottom"
            ],
            "question": "the two sides of a coin are technically called what?",
            "lines": [
                [
                    0.056221198156682035,
                    0.0,
                    0.04420288149772472,
                    0.3604723151221832,
                    0.0002190100744634253,
                    0.3305709023941068,
                    0.00021581957483543757,
                    0.00027770063871146905,
                    0.02666666666666667,
                    0.43478260869565216,
                    0.3333333333333333,
                    0.0960716440423934,
                    0,
                    0,
                    1.0
                ],
                [
                    0.1798941798941799,
                    0.0,
                    0.07556488009148864,
                    0.22791724892906665,
                    0.7993867717915024,
                    0.3333333333333333,
                    0.803927916262005,
                    0.6627788577247061,
                    0.05555555555555555,
                    0.17391304347826086,
                    0.3333333333333333,
                    0.3525720079568099,
                    0,
                    0,
                    1.0
                ],
                [
                    0.7638846219491381,
                    1.0,
                    0.8802322384107867,
                    0.41161043594875013,
                    0.20039421813403416,
                    0.3360957642725598,
                    0.1958562641631596,
                    0.3369434416365824,
                    0.9177777777777778,
                    0.391304347826087,
                    0.3333333333333333,
                    0.5513563480007967,
                    0,
                    0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "advert / convert",
                "prefix / suffix",
                "obverse / reverse"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "obverse / reverse": 0.6748253979306009,
                "advert / convert": 0.061558180804913556,
                "prefix / suffix": 0.056654744947219116
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.3842865761695736,
                    1.4102880318272395,
                    2.2054253920031868
                ],
                "result_count_important_words": [
                    8.0,
                    29800.0,
                    7260.0
                ],
                "wikipedia_search": [
                    0.08,
                    0.16666666666666666,
                    2.7533333333333334
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    2.0
                ],
                "cosine_similarity_raw": [
                    0.023101603612303734,
                    0.03949221968650818,
                    0.4600328207015991
                ],
                "result_count_noun_chunks": [
                    15.0,
                    35800.0,
                    18200.0
                ],
                "question_answer_similarity": [
                    4.175553178414702,
                    2.6400934364646673,
                    4.767914738506079
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    3590000.0,
                    3620000.0,
                    3650000.0
                ],
                "word_count_appended": [
                    10.0,
                    4.0,
                    9.0
                ],
                "answer_relation_to_question": [
                    0.16866359447004609,
                    0.5396825396825397,
                    2.291653865847414
                ],
                "result_count": [
                    8.0,
                    29200.0,
                    7320.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which animal is specifically mentioned in the Judeo-Christian Ten Commandments?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "pig"
            ],
            "lines": [
                [
                    0.2619047619047619,
                    0.36,
                    0.3530220464847878,
                    0.41750905911607317,
                    0.31910569105691056,
                    0.32359679266895763,
                    0.3409090909090909,
                    0.3857404021937843,
                    0.10144927536231885,
                    0.3446327683615819,
                    0.2028985507246377,
                    0.3262277248897058,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.08571428571428572,
                    0.48,
                    0.35243456079926094,
                    0.23465756019265496,
                    0.29471544715447157,
                    0.3201603665521191,
                    0.2892561983471074,
                    0.2870201096892139,
                    0.05434782608695652,
                    0.352165725047081,
                    0.6014492753623188,
                    0.34970747304717786,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.6523809523809524,
                    0.16,
                    0.2945433927159513,
                    0.3478333806912719,
                    0.3861788617886179,
                    0.35624284077892326,
                    0.36983471074380164,
                    0.3272394881170018,
                    0.8442028985507246,
                    0.3032015065913371,
                    0.1956521739130435,
                    0.32406480206311633,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "pig": 0.2669282974051865,
                "goat": 0.32581250059533867,
                "ox": 0.40725920199947485
            },
            "question": "which animal is specifically mentioned in the judeo-christian ten commandments?",
            "rate_limited": false,
            "answers": [
                "pig",
                "ox",
                "goat"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "pig": 0.5600560921501991,
                "goat": 0.15071485119430356,
                "ox": 0.5104700271270518
            },
            "integer_answers": {
                "pig": 3,
                "goat": 5,
                "ox": 6
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9573663493382347,
                    2.098244838283067,
                    1.944388812378698
                ],
                "result_count_important_words": [
                    16500.0,
                    14000.0,
                    17900.0
                ],
                "wikipedia_search": [
                    0.30434782608695654,
                    0.16304347826086957,
                    2.532608695652174
                ],
                "word_count_appended_bing": [
                    28.0,
                    83.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    0.36,
                    0.48,
                    0.16
                ],
                "cosine_similarity_raw": [
                    0.028892822563648224,
                    0.028844740241765976,
                    0.02410668134689331
                ],
                "result_count_noun_chunks": [
                    21100.0,
                    15700.0,
                    17900.0
                ],
                "question_answer_similarity": [
                    2.6946067265234888,
                    1.5144817251712084,
                    2.244919354096055
                ],
                "word_count_noun_chunks": [
                    0.0,
                    17.0,
                    0.0
                ],
                "result_count_bing": [
                    56500.0,
                    55900.0,
                    62200.0
                ],
                "word_count_raw": [
                    0.0,
                    10.0,
                    0.0
                ],
                "result_count": [
                    15700.0,
                    14500.0,
                    19000.0
                ],
                "answer_relation_to_question": [
                    0.5238095238095238,
                    0.17142857142857143,
                    1.3047619047619048
                ],
                "word_count_appended": [
                    183.0,
                    187.0,
                    161.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Mount Rushmore was named after a person with what profession?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "lawyer"
            ],
            "lines": [
                [
                    0.24763071895424835,
                    0.09375,
                    0.19266510696689385,
                    0.15028102021251624,
                    0.17818453492715727,
                    0.28401360544217685,
                    0.024766715779134217,
                    0.051794638800545204,
                    0.265625,
                    0.26957637997432604,
                    0.3333333333333333,
                    0.30312260311393113,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.3715686274509804,
                    0.3269230769230769,
                    0.66460097872643,
                    0.4813700883683983,
                    0.3772880089652596,
                    0.20578231292517007,
                    0.5472532098505578,
                    0.558836892321672,
                    0.6437190594059405,
                    0.337612323491656,
                    0.34408602150537637,
                    0.3756742722977804,
                    0.0,
                    1.0,
                    1.0
                ],
                [
                    0.3808006535947712,
                    0.5793269230769231,
                    0.14273391430667615,
                    0.36834889141908544,
                    0.4445274561075831,
                    0.5102040816326531,
                    0.427980074370308,
                    0.3893684688777828,
                    0.0906559405940594,
                    0.392811296534018,
                    0.3225806451612903,
                    0.32120312458828854,
                    1.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "architect": 0.3836101050188171,
                "prospector": 0.17105311839316162,
                "lawyer": 0.44533677658802134
            },
            "question": "mount rushmore was named after a person with what profession?",
            "rate_limited": false,
            "answers": [
                "prospector",
                "lawyer",
                "architect"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "architect": 0.23131395501586682,
                "prospector": 0.17624398372632563,
                "lawyer": 0.5991751911620458
            },
            "integer_answers": {
                "architect": 6,
                "prospector": 0,
                "lawyer": 8
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.5156130155696557,
                    1.878371361488902,
                    1.6060156229414426
                ],
                "result_count_important_words": [
                    706.0,
                    15600.0,
                    12200.0
                ],
                "wikipedia_search": [
                    1.0625,
                    2.574876237623762,
                    0.3626237623762376
                ],
                "answer_relation_to_question": [
                    0.9905228758169934,
                    1.4862745098039216,
                    1.5232026143790849
                ],
                "word_count_appended_bing": [
                    31.0,
                    32.0,
                    30.0
                ],
                "answer_relation_to_question_bing": [
                    0.1875,
                    0.6538461538461539,
                    1.1586538461538463
                ],
                "cosine_similarity_raw": [
                    0.04947086423635483,
                    0.1706504374742508,
                    0.03664996847510338
                ],
                "result_count_noun_chunks": [
                    1140000.0,
                    12300000.0,
                    8570000.0
                ],
                "question_answer_similarity": [
                    0.9013332910835743,
                    2.887090368196368,
                    2.209228537976742
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    3.0
                ],
                "result_count_bing": [
                    835000.0,
                    605000.0,
                    1500000.0
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_appended": [
                    210.0,
                    263.0,
                    306.0
                ],
                "result_count": [
                    47700.0,
                    101000.0,
                    119000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "The best-selling book \u201cThe Chocolate War\u201d is about what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "high school conformity"
            ],
            "lines": [
                [
                    0.2050387596899225,
                    0.15732368896925858,
                    0.7818881618432012,
                    0.24969023030179077,
                    0.07894736842105263,
                    0.738837134596852,
                    1.1587351247378362e-05,
                    1.1995058036089131e-05,
                    0.061224489795918366,
                    0.3333333333333333,
                    0.16666666666666666,
                    0.26084726459507346,
                    0,
                    0,
                    1.0
                ],
                [
                    0.1482388140894873,
                    0.18762671927228888,
                    0.05388397013968947,
                    0.1743281322598069,
                    0.9210526315789473,
                    0.1429489238676518,
                    0.14638687075854664,
                    0.12434876830745732,
                    0.2571428571428571,
                    0.4666666666666667,
                    0.6666666666666666,
                    0.5732855470875434,
                    0,
                    0,
                    1.0
                ],
                [
                    0.6467224262205902,
                    0.6550495917584526,
                    0.16422786801710929,
                    0.5759816374384024,
                    0.0,
                    0.1182139415354963,
                    0.853601541890206,
                    0.8756392366345066,
                    0.6816326530612244,
                    0.2,
                    0.16666666666666666,
                    0.1658671883173831,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "high school conformity": 0.25281839005186274,
                "the rise of hershey's": 0.42530022929500316,
                "sugar addiction": 0.32188138065313415
            },
            "question": "the best-selling book \u201cthe chocolate war\u201d is about what?",
            "rate_limited": false,
            "answers": [
                "high school conformity",
                "sugar addiction",
                "the rise of hershey's"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "high school conformity": 0.38608857016539566,
                "the rise of hershey's": 0.22551914790227076,
                "sugar addiction": 0.36232833223769456
            },
            "integer_answers": {
                "high school conformity": 2,
                "the rise of hershey's": 6,
                "sugar addiction": 4
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3042363229753673,
                    2.8664277354377172,
                    0.8293359415869155
                ],
                "result_count_important_words": [
                    3.0,
                    37900.0,
                    221000.0
                ],
                "wikipedia_search": [
                    0.30612244897959184,
                    1.2857142857142856,
                    3.4081632653061225
                ],
                "word_count_appended_bing": [
                    2.0,
                    8.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.47197106690777574,
                    0.5628801578168666,
                    1.9651487752753576
                ],
                "cosine_similarity_raw": [
                    0.31859180331230164,
                    0.021955814212560654,
                    0.0669170543551445
                ],
                "result_count_noun_chunks": [
                    3.0,
                    31100.0,
                    219000.0
                ],
                "question_answer_similarity": [
                    9.118160897865891,
                    6.366095930337906,
                    21.03363530896604
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    2300000.0,
                    445000.0,
                    368000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    3.0,
                    35.0,
                    0
                ],
                "answer_relation_to_question": [
                    1.0251937984496124,
                    0.7411940704474365,
                    3.233612131102951
                ],
                "word_count_appended": [
                    5.0,
                    7.0,
                    3.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which type of musical composition is also an Olympic figure skating jump?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "mazurka"
            ],
            "question": "which type of musical composition is also an olympic figure skating jump?",
            "lines": [
                [
                    0.42162698412698413,
                    0.09508881922675028,
                    0.34969562804243803,
                    0.41460892597379384,
                    0.19360902255639098,
                    0.3788300835654596,
                    0.20074788427474907,
                    0.3319633259563705,
                    0.13392857142857142,
                    0.3238095238095238,
                    0.32926829268292684,
                    0.3350587176069388,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.1607142857142857,
                    0.13688610240334378,
                    0.31729336750095366,
                    0.28537234328567773,
                    0.5921052631578947,
                    0.30362116991643456,
                    0.6042117693367447,
                    0.27916534935188114,
                    0.10267857142857142,
                    0.31746031746031744,
                    0.32926829268292684,
                    0.2793975091753431,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.4176587301587302,
                    0.768025078369906,
                    0.33301100445660825,
                    0.3000187307405285,
                    0.21428571428571427,
                    0.31754874651810583,
                    0.1950403463885062,
                    0.38887132469174834,
                    0.7633928571428572,
                    0.35873015873015873,
                    0.34146341463414637,
                    0.3855437732177181,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "sarabande",
                "bagatelle",
                "mazurka"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sarabande": 0.174160933356993,
                "mazurka": 0.45673404908891696,
                "bagatelle": 0.07139039043754775
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.3454110232485714,
                    1.9557825642274018,
                    2.6988064125240268
                ],
                "result_count_important_words": [
                    10200.0,
                    30700.0,
                    9910.0
                ],
                "wikipedia_search": [
                    0.26785714285714285,
                    0.20535714285714285,
                    1.5267857142857144
                ],
                "word_count_appended_bing": [
                    27.0,
                    27.0,
                    28.0
                ],
                "answer_relation_to_question_bing": [
                    0.2852664576802508,
                    0.4106583072100313,
                    2.3040752351097176
                ],
                "cosine_similarity_raw": [
                    0.03888268768787384,
                    0.03527987748384476,
                    0.037027522921562195
                ],
                "result_count_noun_chunks": [
                    10500.0,
                    8830.0,
                    12300.0
                ],
                "question_answer_similarity": [
                    -0.7862304467707872,
                    -0.5411567646078765,
                    -0.5689309755107388
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    136000.0,
                    109000.0,
                    114000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    102.0,
                    100.0,
                    113.0
                ],
                "answer_relation_to_question": [
                    1.6865079365079365,
                    0.6428571428571428,
                    1.6706349206349207
                ],
                "result_count": [
                    10300.0,
                    31500.0,
                    11400.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these celebrities has NOT been a ProActiv spokesperson?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "selena gomez"
            ],
            "lines": [
                [
                    0.3142857142857143,
                    0.2272727272727273,
                    0.10102856448601472,
                    0.334347103790002,
                    0.2655172413793103,
                    0.4873367351715744,
                    0.25,
                    0.2655007949125596,
                    0.4431818181818182,
                    0.30603448275862066,
                    0.4154929577464789,
                    0.2931116930038157,
                    0.06,
                    0.0,
                    -1.0
                ],
                [
                    0.32857142857142857,
                    0.36363636363636365,
                    0.4705751268372039,
                    0.27968802117338293,
                    0.2755968169761273,
                    0.3122747062403446,
                    0.2906862745098039,
                    0.27556968733439324,
                    0.23958333333333331,
                    0.33836206896551724,
                    0.352112676056338,
                    0.34394921442642157,
                    0.46,
                    0.5,
                    -1.0
                ],
                [
                    0.35714285714285715,
                    0.40909090909090906,
                    0.42839630867678136,
                    0.3859648750366151,
                    0.4588859416445623,
                    0.20038855858808108,
                    0.45931372549019606,
                    0.45892951775304713,
                    0.3172348484848485,
                    0.3556034482758621,
                    0.23239436619718312,
                    0.36293909256976287,
                    0.48,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "selena gomez": 0.22767365014989918,
                "katy perry": 0.46241288100162337,
                "lindsay lohan": 0.30991346884847737
            },
            "question": "which of these celebrities has not been a proactiv spokesperson?",
            "rate_limited": false,
            "answers": [
                "katy perry",
                "lindsay lohan",
                "selena gomez"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "selena gomez": 0.45241984334141067,
                "katy perry": 0.01765123390995639,
                "lindsay lohan": 0.3651498854614794
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2413298419771062,
                    0.9363047134414708,
                    0.822365444581423
                ],
                "result_count_important_words": [
                    102000.0,
                    85400.0,
                    16600.0
                ],
                "wikipedia_search": [
                    0.34090909090909094,
                    1.5625,
                    1.0965909090909092
                ],
                "answer_relation_to_question": [
                    0.37142857142857144,
                    0.34285714285714286,
                    0.2857142857142857
                ],
                "answer_relation_to_question_bing": [
                    0.5454545454545454,
                    0.2727272727272727,
                    0.18181818181818182
                ],
                "word_count_appended": [
                    90.0,
                    75.0,
                    67.0
                ],
                "cosine_similarity_raw": [
                    0.4791058599948883,
                    0.035334933549165726,
                    0.08598547428846359
                ],
                "result_count_noun_chunks": [
                    88500.0,
                    84700.0,
                    15500.0
                ],
                "question_answer_similarity": [
                    1.3517169521655887,
                    1.7977315420284867,
                    0.9305192669853568
                ],
                "word_count_noun_chunks": [
                    22.0,
                    2.0,
                    1.0
                ],
                "word_count_raw": [
                    8.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    54100.0,
                    802000.0,
                    1280000.0
                ],
                "result_count": [
                    88400.0,
                    84600.0,
                    15500.0
                ],
                "word_count_appended_bing": [
                    12.0,
                    21.0,
                    38.0
                ]
            },
            "integer_answers": {
                "selena gomez": 2,
                "katy perry": 10,
                "lindsay lohan": 2
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What was the least popular papal name of the 19th century?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "pius"
            ],
            "question": "what was the least popular papal name of the 19th century?",
            "lines": [
                [
                    0.75,
                    0.5,
                    0.35463770169353925,
                    -3.196354336334176,
                    0.3421955403087479,
                    0.3157894736842105,
                    0.34508627156789196,
                    0.3923941227312014,
                    0.0,
                    0.32588916459884204,
                    0.33793103448275863,
                    0.33850140494503156,
                    0.3333333333333333,
                    0.35714285714285715,
                    1.0
                ],
                [
                    0.0,
                    0.0,
                    0.523084706836448,
                    4.420121932610479,
                    0.31989708404802747,
                    0.3684210526315789,
                    0.31957989497374345,
                    0.21694036300777875,
                    1.0,
                    0.380479735318445,
                    0.41379310344827586,
                    0.3208139231825116,
                    0.5897435897435898,
                    0.5714285714285714,
                    1.0
                ],
                [
                    0.25,
                    0.5,
                    0.12227759147001273,
                    -0.22376759627630285,
                    0.3379073756432247,
                    0.3157894736842105,
                    0.3353338334583646,
                    0.39066551426101986,
                    0.0,
                    0.293631100082713,
                    0.2482758620689655,
                    0.3406846718724568,
                    0.07692307692307693,
                    0.07142857142857142,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "leo",
                "pius",
                "gregory"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "gregory": 0.13516558703642065,
                "pius": 0.5194243991424035,
                "leo": 0.19318446103922637
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3540056197801262,
                    1.2832556927300465,
                    1.3627386874898273
                ],
                "result_count_important_words": [
                    460000.0,
                    426000.0,
                    447000.0
                ],
                "wikipedia_search": [
                    0.0,
                    1.0,
                    0.0
                ],
                "answer_relation_to_question": [
                    0.75,
                    0.0,
                    0.25
                ],
                "word_count_appended_bing": [
                    49.0,
                    60.0,
                    36.0
                ],
                "answer_relation_to_question_bing": [
                    1.0,
                    0.0,
                    1.0
                ],
                "cosine_similarity_raw": [
                    0.27684077620506287,
                    0.40833553671836853,
                    0.09545353800058365
                ],
                "result_count_noun_chunks": [
                    454000.0,
                    251000.0,
                    452000.0
                ],
                "question_answer_similarity": [
                    0.6346324924379587,
                    -0.8776101469993591,
                    0.04442879994167015
                ],
                "word_count_noun_chunks": [
                    13.0,
                    23.0,
                    3.0
                ],
                "result_count_bing": [
                    2820000.0,
                    3290000.0,
                    2820000.0
                ],
                "word_count_raw": [
                    5.0,
                    8.0,
                    1.0
                ],
                "result_count": [
                    399000.0,
                    373000.0,
                    394000.0
                ],
                "word_count_appended": [
                    394.0,
                    460.0,
                    355.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these words is a term used in dentistry?": {
        "raw_data": {
            "data": {
                "word_count_appended_relation_to_question": [
                    1.362735925237748,
                    1.016806218688995,
                    0.620457856073257
                ],
                "result_count_important_words": [
                    19800.0,
                    7820.0,
                    77.0
                ],
                "wikipedia_search": [
                    0.2857142857142857,
                    0.7142857142857143,
                    0.0
                ],
                "answer_relation_to_question": [
                    2.126666666666667,
                    0.22,
                    0.6533333333333333
                ],
                "answer_relation_to_question_bing": [
                    0.5714285714285714,
                    0.42857142857142855,
                    0.0
                ],
                "word_count_appended": [
                    264.0,
                    131.0,
                    75.0
                ],
                "cosine_similarity_raw": [
                    0.12502805888652802,
                    0.016872473061084747,
                    0.008095212280750275
                ],
                "result_count_noun_chunks": [
                    6570.0,
                    28500.0,
                    74.0
                ],
                "question_answer_similarity": [
                    0.0,
                    -1.6459861546754837,
                    0.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    43300000.0,
                    22600000.0,
                    22600000.0
                ],
                "result_count": [
                    24900.0,
                    6260.0,
                    76.0
                ],
                "word_count_appended_bing": [
                    35.0,
                    27.0,
                    27.0
                ]
            },
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "z-best_answer_by_ml": [
                "mesiodens",
                "top"
            ],
            "lines": [
                [
                    0.7088888888888889,
                    0.5714285714285714,
                    0.8335440417307931,
                    -0.0,
                    0.7971571263926239,
                    0.48926553672316386,
                    0.7148788677474095,
                    0.1869451399954473,
                    0.2857142857142857,
                    0.5617021276595745,
                    0.39325842696629215,
                    0.45424530841258265,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.07333333333333332,
                    0.42857142857142855,
                    0.11248634518188079,
                    1.0,
                    0.2004097835830452,
                    0.25536723163841807,
                    0.28234104776690616,
                    0.8109492374231733,
                    0.7142857142857143,
                    0.27872340425531916,
                    0.30337078651685395,
                    0.338935406229665,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.21777777777777774,
                    0.0,
                    0.05396961308732606,
                    -0.0,
                    0.0024330900243309003,
                    0.25536723163841807,
                    0.0027800844856843703,
                    0.0021056225813794673,
                    0.0,
                    0.1595744680851064,
                    0.30337078651685395,
                    0.20681928535775232,
                    0,
                    0,
                    -1.0
                ]
            ],
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "question": "which of these words is a term used in dentistry?",
            "rate_limited": false,
            "answers": [
                "mesiodens",
                "tridentata",
                "dentifera"
            ],
            "ml_answers": {
                "tridentata": 0.1781665758545654,
                "mesiodens": 0.8444748666546794,
                "dentifera": -0.008001890593573403
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What costume was worn by the original lead singer of the Village People?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "construction worker"
            ],
            "question": "what costume was worn by the original lead singer of the village people?",
            "lines": [
                [
                    0.3268398268398269,
                    0.15833333333333333,
                    0.3658233831094721,
                    0.24399968074716918,
                    0.9224364592462752,
                    0.4752186588921283,
                    0.8714596949891068,
                    0.8090614886731392,
                    0.5696969696969697,
                    0.5088967971530249,
                    0.4603174603174603,
                    0.3405059303129656,
                    0.6296296296296297,
                    0.3333333333333333,
                    1.0
                ],
                [
                    0.41991341991341985,
                    0.575,
                    0.2876651765066413,
                    0.2225895205153428,
                    0.03987730061349693,
                    0.27405247813411077,
                    0.09049773755656108,
                    0.11779935275080906,
                    0.3678030303030303,
                    0.36476868327402134,
                    0.42857142857142855,
                    0.30476842485115835,
                    0.14814814814814814,
                    0.16666666666666666,
                    1.0
                ],
                [
                    0.2532467532467532,
                    0.26666666666666666,
                    0.3465114403838866,
                    0.533410798737488,
                    0.03768624014022787,
                    0.25072886297376096,
                    0.03804256745433216,
                    0.07313915857605179,
                    0.0625,
                    0.12633451957295375,
                    0.1111111111111111,
                    0.3547256448358761,
                    0.2222222222222222,
                    0.5,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "cowboy",
                "policeman",
                "construction worker"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "policeman": 0.12615469367418375,
                "construction worker": 0.49034423339604927,
                "cowboy": 0.39614090897506116
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.3835415121907593,
                    2.1333789739581084,
                    2.483079513851133
                ],
                "result_count_important_words": [
                    1040000.0,
                    108000.0,
                    45400.0
                ],
                "wikipedia_search": [
                    2.278787878787879,
                    1.4712121212121212,
                    0.25
                ],
                "word_count_appended_bing": [
                    58.0,
                    54.0,
                    14.0
                ],
                "answer_relation_to_question_bing": [
                    0.6333333333333333,
                    2.3,
                    1.0666666666666667
                ],
                "cosine_similarity_raw": [
                    0.09995158761739731,
                    0.07859691977500916,
                    0.09467510879039764
                ],
                "result_count_noun_chunks": [
                    1250000.0,
                    182000.0,
                    113000.0
                ],
                "question_answer_similarity": [
                    2.8865943551063538,
                    2.6333053037524223,
                    6.3104205541312695
                ],
                "word_count_noun_chunks": [
                    34.0,
                    8.0,
                    12.0
                ],
                "word_count_raw": [
                    2.0,
                    1.0,
                    3.0
                ],
                "result_count_bing": [
                    815000.0,
                    470000.0,
                    430000.0
                ],
                "word_count_appended": [
                    286.0,
                    205.0,
                    71.0
                ],
                "answer_relation_to_question": [
                    1.9610389610389611,
                    2.519480519480519,
                    1.5194805194805194
                ],
                "result_count": [
                    4210000.0,
                    182000.0,
                    172000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "The director of HBO\u2019s \u201cThe Jinx\u201d co-founded what company?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "moviefone",
                "bottom"
            ],
            "question": "the director of hbo\u2019s \u201cthe jinx\u201d co-founded what company?",
            "lines": [
                [
                    0.37581466646887207,
                    0.24379831187363293,
                    0.13163924988893078,
                    1.147913842801287,
                    0.3080357142857143,
                    0.013634278884240663,
                    0.06947282386595832,
                    0.4607329842931937,
                    0.4044583388960586,
                    0.05976095617529881,
                    0.06153846153846154,
                    0.26714751887546706,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.363348662414083,
                    0.514624413921533,
                    0.1465178717038541,
                    -0.06867616486525255,
                    0.29464285714285715,
                    0.035718753040263514,
                    0.04577033101757254,
                    0.2617801047120419,
                    0.5406709388396896,
                    0.30677290836653387,
                    0.4153846153846154,
                    0.2515316095165574,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.260836671117045,
                    0.24157727420483405,
                    0.7218428784072152,
                    -0.0792376779360345,
                    0.39732142857142855,
                    0.9506469680754959,
                    0.8847568451164691,
                    0.2774869109947644,
                    0.0548707222642517,
                    0.6334661354581673,
                    0.5230769230769231,
                    0.4813208716079756,
                    0,
                    1.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "the smoking gun",
                "ancestry.com",
                "moviefone"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "moviefone": 1.047322429834977,
                "ancestry.com": 0.12191312422293488,
                "the smoking gun": 0.1444288271380639
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6028851132528024,
                    1.5091896570993444,
                    2.8879252296478537
                ],
                "result_count_important_words": [
                    680.0,
                    448.0,
                    8660.0
                ],
                "wikipedia_search": [
                    1.6178333555842344,
                    2.1626837553587586,
                    0.2194828890570068
                ],
                "word_count_appended_bing": [
                    4.0,
                    27.0,
                    34.0
                ],
                "answer_relation_to_question_bing": [
                    0.9751932474945317,
                    2.058497655686132,
                    0.9663090968193362
                ],
                "cosine_similarity_raw": [
                    0.03402142971754074,
                    0.03786672651767731,
                    0.18655626475811005
                ],
                "result_count_noun_chunks": [
                    88.0,
                    50.0,
                    53.0
                ],
                "question_answer_similarity": [
                    10.575235941825667,
                    -0.6326839349349029,
                    -0.7299826070666313
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    9.0
                ],
                "result_count_bing": [
                    98100.0,
                    257000.0,
                    6840000.0
                ],
                "word_count_appended": [
                    15.0,
                    77.0,
                    159.0
                ],
                "answer_relation_to_question": [
                    1.8790733323443605,
                    1.8167433120704148,
                    1.304183355585225
                ],
                "result_count": [
                    69.0,
                    66.0,
                    89.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What is the name of the cultural explosion that took place in 1920s New York?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "harlem renaissance"
            ],
            "question": "what is the name of the cultural explosion that took place in 1920s new york?",
            "lines": [
                [
                    0.2116171654766125,
                    0.10785191819674579,
                    0.03437608343562512,
                    0.18103724531228138,
                    0.000273510734053081,
                    0.351925630810093,
                    0.08963474827245804,
                    2.4232046073196935e-05,
                    0.27261904761904765,
                    0.02912621359223301,
                    0.06666666666666667,
                    0.18428623229746624,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.12878995684295222,
                    0.07482799332329113,
                    0.008865451786662152,
                    0.29164505202790836,
                    0.00016907936286917732,
                    0.34130146082337315,
                    0.09200394866732478,
                    0.07915801717244332,
                    0.06071428571428572,
                    0.02912621359223301,
                    0.06666666666666667,
                    0.187359607374238,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.6595928776804352,
                    0.8173200884799632,
                    0.9567584647777128,
                    0.5273177026598103,
                    0.9995574099030777,
                    0.30677290836653387,
                    0.8183613030602171,
                    0.9208177507814835,
                    0.6666666666666666,
                    0.941747572815534,
                    0.8666666666666667,
                    0.6283541603282957,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "ridgewood rebirth",
                "brownsville bustle",
                "harlem renaissance"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "ridgewood rebirth": 0.09039059739534507,
                "brownsville bustle": 0.06230619963979098,
                "harlem renaissance": 0.940686290096314
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2900036260822636,
                    1.311517251619666,
                    4.39847912229807
                ],
                "result_count_important_words": [
                    9080.0,
                    9320.0,
                    82900.0
                ],
                "wikipedia_search": [
                    1.9083333333333332,
                    0.425,
                    4.666666666666666
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    26.0
                ],
                "answer_relation_to_question_bing": [
                    0.7549634273772206,
                    0.5237959532630379,
                    5.721240619359742
                ],
                "cosine_similarity_raw": [
                    0.029028447344899178,
                    0.007486318238079548,
                    0.8079225420951843
                ],
                "result_count_noun_chunks": [
                    6.0,
                    19600.0,
                    228000.0
                ],
                "question_answer_similarity": [
                    1.5618479251861572,
                    2.5160856740549207,
                    4.549285194836557
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    13.0
                ],
                "result_count_bing": [
                    5300000.0,
                    5140000.0,
                    4620000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    33.0
                ],
                "word_count_appended": [
                    6.0,
                    6.0,
                    194.0
                ],
                "answer_relation_to_question": [
                    1.4813201583362874,
                    0.9015296979006656,
                    4.617150143763046
                ],
                "result_count": [
                    55.0,
                    34.0,
                    201000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Pixies, Bon Iver, Iron & Wine and Bauhaus were all once signed to which record label?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "4ad"
            ],
            "lines": [
                [
                    0.3439990455738487,
                    0.177826699798015,
                    0.255165740002254,
                    -0.0,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.0023180343069077423,
                    0.15730337078651685,
                    0.3812222222222222,
                    0.024390243902439025,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.47980493915533284,
                    0.4787952104857811,
                    0.6605919952654253,
                    0.5718448795319769,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.8966156699119147,
                    0.29555446995603324,
                    0.6154444444444445,
                    0.9512195121951219,
                    0.3333333333333333,
                    0.3333333333333333,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.1761960152708184,
                    0.34337808971620387,
                    0.08424226473232063,
                    0.42815512046802306,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.10106629578117757,
                    0.5471421592574499,
                    0.0033333333333333335,
                    0.024390243902439025,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "4ad": 0.5916574610199545,
                "geffen": 0.21723120398536425,
                "subpop": 0.19111133499468116
            },
            "question": "pixies, bon iver, iron & wine and bauhaus were all once signed to which record label?",
            "rate_limited": false,
            "answers": [
                "subpop",
                "4ad",
                "geffen"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "4ad": 0.8232899199413797,
                "geffen": 0.04198881172203027,
                "subpop": 0.06852950850800085
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.6666666666666665,
                    2.6666666666666665,
                    2.6666666666666665
                ],
                "result_count_important_words": [
                    25.0,
                    9670.0,
                    1090.0
                ],
                "wikipedia_search": [
                    2.287333333333333,
                    3.6926666666666668,
                    0.02
                ],
                "word_count_appended_bing": [
                    8.0,
                    8.0,
                    8.0
                ],
                "answer_relation_to_question_bing": [
                    1.244786898586105,
                    3.351566473400468,
                    2.403646628013427
                ],
                "cosine_similarity_raw": [
                    0.06281088292598724,
                    0.16260947287082672,
                    0.02073683962225914
                ],
                "result_count_noun_chunks": [
                    32200.0,
                    60500.0,
                    112000.0
                ],
                "question_answer_similarity": [
                    0.0,
                    -1.0108989626169205,
                    -0.7568863211199641
                ],
                "word_count_noun_chunks": [
                    0.0,
                    74.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    11.0,
                    0.0
                ],
                "result_count_bing": [
                    1290000.0,
                    1290000.0,
                    1290000.0
                ],
                "word_count_appended": [
                    1.0,
                    39.0,
                    1.0
                ],
                "answer_relation_to_question": [
                    2.7519923645907896,
                    3.8384395132426627,
                    1.4095681221665473
                ],
                "result_count": [
                    228000.0,
                    228000.0,
                    228000.0
                ]
            },
            "integer_answers": {
                "4ad": 8,
                "geffen": 1,
                "subpop": 4
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these would an oologist study?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ostrich egg"
            ],
            "lines": [
                [
                    0.0,
                    0,
                    0.036031999610958945,
                    0.3367300813824786,
                    0.056074766355140186,
                    0.08895265423242468,
                    0.49590340664079346,
                    0.05420560747663551,
                    0.0,
                    0.15463917525773196,
                    0.52,
                    0.21302180685358255,
                    0,
                    0,
                    -1.0
                ],
                [
                    1.0,
                    0,
                    0.34105873400227965,
                    0.44847547374738744,
                    0.049399198931909215,
                    0.5117168818747011,
                    0.016817593790426907,
                    0.048598130841121495,
                    0.08333333333333333,
                    0.21649484536082475,
                    0.4,
                    0.2995638629283489,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.0,
                    0,
                    0.6229092663867615,
                    0.21479444487013397,
                    0.8945260347129506,
                    0.39933046389287424,
                    0.4872789995687796,
                    0.897196261682243,
                    0.9166666666666666,
                    0.6288659793814433,
                    0.08,
                    0.48741433021806857,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "ice cave": 0.17777813616452234,
                "human liver": 0.3104961868009393,
                "ostrich egg": 0.5117256770345383
            },
            "question": "which of these would an oologist study?",
            "rate_limited": false,
            "answers": [
                "ice cave",
                "human liver",
                "ostrich egg"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "ice cave": 0.12420513924514831,
                "human liver": 0.32505035480367345,
                "ostrich egg": 0.5813210385815812
            },
            "integer_answers": {
                "ice cave": 2,
                "human liver": 3,
                "ostrich egg": 6
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.4260436137071651,
                    0.5991277258566978,
                    0.9748286604361371
                ],
                "result_count_important_words": [
                    2300.0,
                    78.0,
                    2260.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.08333333333333333,
                    0.9166666666666666
                ],
                "word_count_appended_bing": [
                    13.0,
                    10.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.008457301184535027,
                    0.08005207777023315,
                    0.1462070196866989
                ],
                "result_count_noun_chunks": [
                    87.0,
                    78.0,
                    1440.0
                ],
                "question_answer_similarity": [
                    3.237850099802017,
                    4.3123452216386795,
                    2.0653700195252895
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    186000.0,
                    1070000.0,
                    835000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    84.0,
                    74.0,
                    1340.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_appended": [
                    15.0,
                    21.0,
                    61.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these foods is NOT mentioned in Train's hit song \u201cDrops of Jupiter\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "chicken pot pie",
                "bottom"
            ],
            "question": "which of these foods is not mentioned in train's hit song \u201cdrops of jupiter\u201d?",
            "lines": [
                [
                    0.43922811059907835,
                    0.3952020202020202,
                    0.30183726761166846,
                    0.25777431915875193,
                    0.208955223880597,
                    0.3864942528735632,
                    0.49796325546595727,
                    0.2564102564102564,
                    0.1781045751633987,
                    0.3076923076923077,
                    0.4090909090909091,
                    0.23148520298719938,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.12557603686635943,
                    0.2095959595959596,
                    0.34135695093939633,
                    0.4291993213289181,
                    0.3768656716417911,
                    0.40373563218390807,
                    0.3948374760994264,
                    0.4358974358974359,
                    0.3545751633986928,
                    0.34615384615384615,
                    0.18181818181818185,
                    0.34553277366807267,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.4351958525345622,
                    0.3952020202020202,
                    0.35680578144893516,
                    0.31302635951232993,
                    0.41417910447761197,
                    0.20977011494252873,
                    0.10719926843461636,
                    0.3076923076923077,
                    0.46732026143790856,
                    0.34615384615384615,
                    0.4090909090909091,
                    0.42298202334472795,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "deep-fried chicken",
                "soy latte",
                "chicken pot pie"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "soy latte": 0.12229125636208572,
                "deep-fried chicken": 0.19374774858598626,
                "chicken pot pie": 0.8682337037627902
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.759207158179209,
                    2.1625411686469826,
                    1.0782516731738085
                ],
                "result_count_important_words": [
                    98.0,
                    5060.0,
                    18900.0
                ],
                "wikipedia_search": [
                    1.9313725490196079,
                    0.8725490196078431,
                    0.19607843137254902
                ],
                "word_count_appended_bing": [
                    2.0,
                    7.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.6287878787878788,
                    1.7424242424242424,
                    0.6287878787878788
                ],
                "cosine_similarity_raw": [
                    0.05252905189990997,
                    0.042053159326314926,
                    0.03795797750353813
                ],
                "result_count_noun_chunks": [
                    38.0,
                    10.0,
                    30.0
                ],
                "question_answer_similarity": [
                    13.96716683357954,
                    4.082494009286165,
                    10.781235173344612
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    158000.0,
                    134000.0,
                    404000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    20.0,
                    16.0,
                    16.0
                ],
                "answer_relation_to_question": [
                    0.4861751152073733,
                    2.9953917050691246,
                    0.5184331797235023
                ],
                "result_count": [
                    78.0,
                    33.0,
                    23.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Who holds the record as the youngest solo artist with a Billboard #1 hit?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "stevie wonder"
            ],
            "lines": [
                [
                    0.2653737594151765,
                    0.2957977332170881,
                    0.20224554473515405,
                    0.23162578867103845,
                    0.3333333333333333,
                    0.3414425949637217,
                    0.12635869565217392,
                    0.3331193838254172,
                    0.33001806965710034,
                    0.35,
                    0.0,
                    0.3334803857344841,
                    0.45,
                    0.25,
                    0.0
                ],
                [
                    0.3873242700910759,
                    0.3220895088637024,
                    0.23148459699203763,
                    0.33327754450893626,
                    0.3333333333333333,
                    0.3183952198036705,
                    0.552536231884058,
                    0.3331193838254172,
                    0.5176102262687609,
                    0.25,
                    0.0,
                    0.333937578814628,
                    0.3,
                    0.0,
                    0.0
                ],
                [
                    0.3473019704937476,
                    0.3821127579192095,
                    0.5662698582728083,
                    0.4350966668200253,
                    0.3333333333333333,
                    0.34016218523260777,
                    0.3211050724637681,
                    0.3337612323491656,
                    0.1523717040741389,
                    0.4,
                    1.0,
                    0.3325820354508879,
                    0.25,
                    0.75,
                    0.0
                ]
            ],
            "fraction_answers": {
                "justin bieber": 0.27448537780033483,
                "stevie wonder": 0.42457834402926375,
                "michael jackson": 0.30093627817040147
            },
            "question": "who holds the record as the youngest solo artist with a billboard #1 hit?",
            "rate_limited": false,
            "answers": [
                "justin bieber",
                "michael jackson",
                "stevie wonder"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "justin bieber": 0.1923237100459994,
                "stevie wonder": 0.7278206031490109,
                "michael jackson": 0.1691569715695715
            },
            "integer_answers": {
                "justin bieber": 3,
                "stevie wonder": 7,
                "michael jackson": 4
            },
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.6678430858758726,
                    2.671500630517024,
                    2.660656283607103
                ],
                "result_count_important_words": [
                    27900.0,
                    122000.0,
                    70900.0
                ],
                "wikipedia_search": [
                    2.6401445572568023,
                    4.140881810150086,
                    1.2189736325931109
                ],
                "word_count_appended_bing": [
                    0.0,
                    0.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.4789886660854403,
                    1.610447544318512,
                    1.9105637895960477
                ],
                "cosine_similarity_raw": [
                    0.08675868809223175,
                    0.09930156916379929,
                    0.2429167479276657
                ],
                "result_count_noun_chunks": [
                    519000.0,
                    519000.0,
                    520000.0
                ],
                "question_answer_similarity": [
                    3.0850419979542494,
                    4.438949685543776,
                    5.795086540281773
                ],
                "word_count_noun_chunks": [
                    9.0,
                    6.0,
                    5.0
                ],
                "result_count_bing": [
                    8000000.0,
                    7460000.0,
                    7970000.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    3.0
                ],
                "result_count": [
                    506000.0,
                    506000.0,
                    506000.0
                ],
                "answer_relation_to_question": [
                    1.8576163159062355,
                    2.7112698906375314,
                    2.4311137934562335
                ],
                "word_count_appended": [
                    7.0,
                    5.0,
                    8.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these is a poetry term for a stanza with four lines?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "quatrain"
            ],
            "question": "which of these is a poetry term for a stanza with four lines?",
            "lines": [
                [
                    0.14792899408284024,
                    0.2069961489088575,
                    0.03168202909868393,
                    0.299007312681462,
                    0.22420951772596615,
                    0.3707627118644068,
                    0.16220130340333092,
                    0.2378449408672799,
                    0.29611713106295146,
                    0.256021409455843,
                    0.23776223776223776,
                    0.3216242792932623,
                    0.016129032258064516,
                    0.0,
                    -1.0
                ],
                [
                    0.16647295012679628,
                    0.08023106546854943,
                    0.0772770409053548,
                    0.5847252814512999,
                    0.4726924305333759,
                    0.3707627118644068,
                    0.717595944967415,
                    0.4007884362680683,
                    0.46466718266253865,
                    0.37555753791257807,
                    0.3146853146853147,
                    0.30357228446700757,
                    0.04838709677419355,
                    0.0,
                    -1.0
                ],
                [
                    0.6855980557903636,
                    0.7127727856225932,
                    0.8910409299959613,
                    0.11626740586723808,
                    0.30309805174065796,
                    0.2584745762711864,
                    0.12020275162925416,
                    0.36136662286465177,
                    0.2392156862745098,
                    0.3684210526315789,
                    0.44755244755244755,
                    0.3748034362397301,
                    0.9354838709677419,
                    1.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "limerick",
                "haiku",
                "quatrain"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "limerick": 0.06834296686238558,
                "quatrain": 0.9240972677253659,
                "haiku": 0.04569795413072971
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2864971171730493,
                    1.2142891378680303,
                    1.4992137449589205
                ],
                "result_count_important_words": [
                    224000.0,
                    991000.0,
                    166000.0
                ],
                "wikipedia_search": [
                    1.1844685242518058,
                    1.8586687306501546,
                    0.9568627450980391
                ],
                "word_count_appended_bing": [
                    34.0,
                    45.0,
                    64.0
                ],
                "answer_relation_to_question_bing": [
                    0.82798459563543,
                    0.3209242618741977,
                    2.8510911424903727
                ],
                "cosine_similarity_raw": [
                    0.015214604325592518,
                    0.03711061552166939,
                    0.4279029965400696
                ],
                "result_count_noun_chunks": [
                    72400.0,
                    122000.0,
                    110000.0
                ],
                "question_answer_similarity": [
                    1.2525554043240845,
                    2.449441134929657,
                    0.4870495181530714
                ],
                "word_count_noun_chunks": [
                    1.0,
                    3.0,
                    58.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    20.0
                ],
                "result_count_bing": [
                    1750000.0,
                    1750000.0,
                    1220000.0
                ],
                "word_count_appended": [
                    287.0,
                    421.0,
                    413.0
                ],
                "answer_relation_to_question": [
                    0.591715976331361,
                    0.6658918005071851,
                    2.7423922231614544
                ],
                "result_count": [
                    70200.0,
                    148000.0,
                    94900.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these startups is based in Germany?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "soundcloud"
            ],
            "question": "which of these startups is based in germany?",
            "lines": [
                [
                    0.39999999999999997,
                    0.0,
                    0.2760065819526924,
                    0.6365932703184416,
                    0.2801608579088472,
                    0.2866857551896922,
                    0.1958266452648475,
                    0.8086680761099366,
                    0.0,
                    0.2642169728783902,
                    0.3163265306122449,
                    0.28844902578660536,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.5666666666666667,
                    0.875,
                    0.5317116605201683,
                    0.11564919109210263,
                    0.5730563002680965,
                    0.2945597709377237,
                    0.29373996789727125,
                    0.0755813953488372,
                    0.42857142857142855,
                    0.3910761154855643,
                    0.35714285714285715,
                    0.4024824778821785,
                    0,
                    1.0,
                    -1.0
                ],
                [
                    0.03333333333333333,
                    0.125,
                    0.1922817575271393,
                    0.24775753858945573,
                    0.1467828418230563,
                    0.41875447387258413,
                    0.5104333868378812,
                    0.11575052854122622,
                    0.5714285714285714,
                    0.3447069116360455,
                    0.32653061224489793,
                    0.3090684963312162,
                    0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "zola",
                "soundcloud",
                "roku"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "zola": 0.045149096837227835,
                "roku": 0.133754006498315,
                "soundcloud": 0.6441961983902615
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.865347077359816,
                    1.2074474336465353,
                    0.9272054889936485
                ],
                "result_count_important_words": [
                    122000.0,
                    183000.0,
                    318000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.42857142857142855,
                    0.5714285714285714
                ],
                "answer_relation_to_question": [
                    1.2,
                    1.7,
                    0.1
                ],
                "word_count_appended_bing": [
                    31.0,
                    35.0,
                    32.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    1.75,
                    0.25
                ],
                "cosine_similarity_raw": [
                    0.04429653659462929,
                    0.08533486723899841,
                    0.030859466642141342
                ],
                "result_count_noun_chunks": [
                    1530000.0,
                    143000.0,
                    219000.0
                ],
                "question_answer_similarity": [
                    -1.1563779823482037,
                    -0.21007790137082338,
                    -0.45005402341485023
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    8010000.0,
                    8230000.0,
                    11700000.0
                ],
                "word_count_raw": [
                    0.0,
                    2.0,
                    0.0
                ],
                "result_count": [
                    418000.0,
                    855000.0,
                    219000.0
                ],
                "word_count_appended": [
                    302.0,
                    447.0,
                    394.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What product\u2019s packaging specifically states, \u201cDo not insert swab into ear canal\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "rice-a-roni"
            ],
            "lines": [
                [
                    0.37194441726196303,
                    0.3566616390145802,
                    0.49279680806077214,
                    0.2584289049670668,
                    0.49150743099787686,
                    0.35830527497194165,
                    0.48785292186474066,
                    0.49975658080951035,
                    0.3685064935064935,
                    0.4407114624505929,
                    0.33870967741935487,
                    0.4354069439465768,
                    0.5,
                    0.5,
                    1.0
                ],
                [
                    0.20570531795975822,
                    0.3238185017596782,
                    0.012396865853598393,
                    0.32467495659869555,
                    0.031847133757961776,
                    0.32491582491582494,
                    0.0370978332239002,
                    0.0007076139258420655,
                    0.3327922077922078,
                    0.07509881422924902,
                    0.17204301075268819,
                    0.1499716984125229,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.42235026477827875,
                    0.3195198592257416,
                    0.49480632608562947,
                    0.4168961384342377,
                    0.47664543524416136,
                    0.3167789001122334,
                    0.47504924491135914,
                    0.4995358052646476,
                    0.2987012987012987,
                    0.4841897233201581,
                    0.489247311827957,
                    0.4146213576409003,
                    0.5,
                    0.5,
                    1.0
                ]
            ],
            "fraction_answers": {
                "rice-a-roni": 0.15705877781836147,
                "oreo cookies": 0.12737976206477097,
                "q-tips": 0.7155614601168675
            },
            "question": "what product\u2019s packaging specifically states, \u201cdo not insert swab into ear canal\u201d?",
            "rate_limited": false,
            "answers": [
                "rice-a-roni",
                "q-tips",
                "oreo cookies"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "rice-a-roni": 0.4980065775170711,
                "oreo cookies": 0.42594445424846233,
                "q-tips": 0.09505945033210296
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.0334888968547709,
                    5.600452825399634,
                    1.3660582777455954
                ],
                "result_count_important_words": [
                    37.0,
                    1410.0,
                    76.0
                ],
                "wikipedia_search": [
                    1.051948051948052,
                    1.3376623376623376,
                    1.6103896103896103
                ],
                "word_count_appended_bing": [
                    30.0,
                    61.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.433383609854198,
                    1.7618149824032179,
                    1.804801407742584
                ],
                "cosine_similarity_raw": [
                    0.00928478129208088,
                    0.6285114288330078,
                    0.006694549694657326
                ],
                "result_count_noun_chunks": [
                    43.0,
                    88200.0,
                    82.0
                ],
                "question_answer_similarity": [
                    10.437109580263495,
                    7.5749405776150525,
                    3.5905128036392853
                ],
                "word_count_noun_chunks": [
                    0.0,
                    30.0,
                    0.0
                ],
                "result_count_bing": [
                    50500.0,
                    62400.0,
                    65300.0
                ],
                "word_count_raw": [
                    0.0,
                    38.0,
                    0.0
                ],
                "word_count_appended": [
                    30.0,
                    215.0,
                    8.0
                ],
                "answer_relation_to_question": [
                    2.048889323808592,
                    4.7087149126438685,
                    1.2423957635475396
                ],
                "result_count": [
                    16.0,
                    882.0,
                    44.0
                ]
            },
            "integer_answers": {
                "rice-a-roni": 1,
                "oreo cookies": 3,
                "q-tips": 10
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Where do marsupials keep their undeveloped young?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "in their pouches"
            ],
            "lines": [
                [
                    0.984375,
                    0.9696969696969697,
                    0.9653586890029502,
                    0.4179999846109188,
                    0.00032759069642422157,
                    0.7814761215629522,
                    0.0004294784738424394,
                    0.197179294244757,
                    0,
                    0.08163265306122448,
                    0.275,
                    0.26855823992560385,
                    0,
                    0,
                    3.0
                ],
                [
                    0.015625,
                    0.0,
                    0.018044688514824006,
                    0.48067960832710066,
                    0.00010079713736129893,
                    0.022534628902212115,
                    0.00016250536848092304,
                    0.00021222820319885782,
                    0,
                    0.030612244897959183,
                    0.05,
                    0.10265070217179645,
                    0,
                    0,
                    3.0
                ],
                [
                    0.0,
                    0.030303030303030304,
                    0.016596622482225848,
                    0.10132040706198056,
                    0.9995716121662145,
                    0.19598924953483565,
                    0.9994080161576766,
                    0.8026084775520441,
                    0,
                    0.8877551020408163,
                    0.675,
                    0.6287910579025997,
                    0,
                    0,
                    3.0
                ]
            ],
            "fraction_answers": {
                "in their pouches": 0.44927582011596756,
                "underwater": 0.48521305229103856,
                "in a paper bag": 0.06551112759299396
            },
            "question": "where do marsupials keep their undeveloped young?",
            "rate_limited": false,
            "answers": [
                "in their pouches",
                "in a paper bag",
                "underwater"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "in their pouches": 0.3811610440003659,
                "underwater": 0.3781749382740332,
                "in a paper bag": 0.14130421857663164
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 3
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.8056747197768115,
                    0.30795210651538935,
                    1.8863731737077991
                ],
                "result_count_important_words": [
                    37.0,
                    14.0,
                    86100.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    11.0,
                    2.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    2.909090909090909,
                    0.0,
                    0.09090909090909091
                ],
                "cosine_similarity_raw": [
                    0.5574354529380798,
                    0.010419701226055622,
                    0.009583531878888607
                ],
                "result_count_noun_chunks": [
                    51100.0,
                    55.0,
                    208000.0
                ],
                "question_answer_similarity": [
                    6.696236303076148,
                    7.700345363467932,
                    1.6231229975819588
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    378000.0,
                    10900.0,
                    94800.0
                ],
                "word_count_appended": [
                    16.0,
                    6.0,
                    174.0
                ],
                "answer_relation_to_question": [
                    1.96875,
                    0.03125,
                    0.0
                ],
                "result_count": [
                    39.0,
                    12.0,
                    119000.0
                ]
            },
            "integer_answers": {
                "in their pouches": 4,
                "underwater": 6,
                "in a paper bag": 1
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which company was named after its founder\u2019s unusual hairstyle?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "kinko's"
            ],
            "question": "which company was named after its founder\u2019s unusual hairstyle?",
            "lines": [
                [
                    0.7050572166851237,
                    0.5,
                    0.48021444001332664,
                    0.32478489657592846,
                    0.032125435540069684,
                    0.2593026662760035,
                    0.01630792261108213,
                    0.0323508727063487,
                    0.18666666666666668,
                    0.15432098765432098,
                    0.21621621621621623,
                    0.37394257730555786,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.04688076781100037,
                    0.022727272727272728,
                    0.1633780691214447,
                    0.35938693557637663,
                    0.9407665505226481,
                    0.518605332552007,
                    0.8959150808591713,
                    0.9398376062911579,
                    0.48,
                    0.17901234567901234,
                    0.08108108108108109,
                    0.3547138138711511,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.24806201550387597,
                    0.4772727272727273,
                    0.35640749086522866,
                    0.3158281678476949,
                    0.02710801393728223,
                    0.22209200117198946,
                    0.08777699652974665,
                    0.027811521002493445,
                    0.3333333333333333,
                    0.6666666666666666,
                    0.7027027027027027,
                    0.271343608823291,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "kinko's",
                "red robin",
                "miracle-gro"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "red robin": 0.17906754841792968,
                "miracle-gro": 0.412315741338,
                "kinko's": 0.7827481250146562
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.8697128865277892,
                    1.7735690693557553,
                    1.356718044116455
                ],
                "result_count_important_words": [
                    7190.0,
                    395000.0,
                    38700.0
                ],
                "wikipedia_search": [
                    0.56,
                    1.44,
                    1.0
                ],
                "word_count_appended_bing": [
                    16.0,
                    6.0,
                    52.0
                ],
                "answer_relation_to_question_bing": [
                    1.0,
                    0.045454545454545456,
                    0.9545454545454546
                ],
                "cosine_similarity_raw": [
                    0.05375957489013672,
                    0.018290027976036072,
                    0.03989949822425842
                ],
                "result_count_noun_chunks": [
                    5060.0,
                    147000.0,
                    4350.0
                ],
                "question_answer_similarity": [
                    3.1395772583782673,
                    3.4740625619888306,
                    3.052995825186372
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    885000.0,
                    1770000.0,
                    758000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    4610.0,
                    135000.0,
                    3890.0
                ],
                "answer_relation_to_question": [
                    2.115171650055371,
                    0.1406423034330011,
                    0.7441860465116279
                ],
                "word_count_appended": [
                    25.0,
                    29.0,
                    108.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Before a performance, an orchestra usually tunes to what instrument?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "oboe"
            ],
            "lines": [
                [
                    0.38404422192465026,
                    0.3092074592074592,
                    0.35565325206439746,
                    0.2088085995985237,
                    0.40606060606060607,
                    0.36014344262295084,
                    0.40674466620784583,
                    0.3897849462365591,
                    0.23219795418965544,
                    0.4823386114494519,
                    0.5963302752293578,
                    0.3503595825545981,
                    0.9333333333333333,
                    0.92,
                    1.0
                ],
                [
                    0.28023580694736366,
                    0.41689976689976693,
                    0.3150141673765744,
                    0.6277283574065281,
                    0.27575757575757576,
                    0.32838114754098363,
                    0.19545767377838955,
                    0.27923387096774194,
                    0.41594850516012344,
                    0.10475030450669914,
                    0.11009174311926606,
                    0.3320536385273092,
                    0.0,
                    0.04,
                    1.0
                ],
                [
                    0.33571997112798607,
                    0.2738927738927739,
                    0.32933258055902814,
                    0.16346304299494818,
                    0.3181818181818182,
                    0.3114754098360656,
                    0.3977976600137646,
                    0.33098118279569894,
                    0.3518535406502211,
                    0.41291108404384896,
                    0.29357798165137616,
                    0.3175867789180928,
                    0.06666666666666667,
                    0.04,
                    1.0
                ]
            ],
            "fraction_answers": {
                "french horn": 0.26582518271345157,
                "bassoon": 0.2816743208094492,
                "oboe": 0.45250049647709917
            },
            "question": "before a performance, an orchestra usually tunes to what instrument?",
            "rate_limited": false,
            "answers": [
                "oboe",
                "french horn",
                "bassoon"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "french horn": 0.17907688006701095,
                "bassoon": 0.24094556524703542,
                "oboe": 0.6744027467711207
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7517979127729904,
                    1.660268192636546,
                    1.587933894590464
                ],
                "result_count_important_words": [
                    591000.0,
                    284000.0,
                    578000.0
                ],
                "wikipedia_search": [
                    0.9287918167586218,
                    1.6637940206404938,
                    1.4074141626008845
                ],
                "word_count_appended_bing": [
                    65.0,
                    12.0,
                    32.0
                ],
                "answer_relation_to_question_bing": [
                    1.2368298368298367,
                    1.6675990675990677,
                    1.0955710955710956
                ],
                "cosine_similarity_raw": [
                    0.11880962550640106,
                    0.10523372143507004,
                    0.11001693457365036
                ],
                "result_count_noun_chunks": [
                    1160000.0,
                    831000.0,
                    985000.0
                ],
                "question_answer_similarity": [
                    1.648685345891863,
                    4.956340620294213,
                    1.2906514583155513
                ],
                "word_count_noun_chunks": [
                    14.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    23.0,
                    1.0,
                    1.0
                ],
                "result_count_bing": [
                    703000.0,
                    641000.0,
                    608000.0
                ],
                "word_count_appended": [
                    396.0,
                    86.0,
                    339.0
                ],
                "answer_relation_to_question": [
                    1.536176887698601,
                    1.1209432277894547,
                    1.3428798845119443
                ],
                "result_count": [
                    1340000.0,
                    910000.0,
                    1050000.0
                ]
            },
            "integer_answers": {
                "french horn": 3,
                "bassoon": 0,
                "oboe": 11
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Who is credited with coining the phrase: \u201cFor whom the bell tolls\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ernest hemingway"
            ],
            "question": "who is credited with coining the phrase: \u201cfor whom the bell tolls\u201d?",
            "lines": [
                [
                    0.2361111111111111,
                    0.297008547008547,
                    0.348547209051515,
                    0.5476221663723629,
                    0.2631578947368421,
                    0.2134453781512605,
                    0.32637571157495254,
                    0.13237063778580024,
                    0.3137254901960784,
                    0.33766233766233766,
                    0.525,
                    0.2866409099675185,
                    0.43636363636363634,
                    0.6428571428571429,
                    0.0
                ],
                [
                    0.375,
                    0.1111111111111111,
                    0.09373102742012508,
                    0.6136205203971605,
                    0.2736842105263158,
                    0.4789915966386555,
                    0.269449715370019,
                    0.4428399518652226,
                    0.6752450980392157,
                    0.18181818181818182,
                    0.175,
                    0.2676354703056908,
                    0.0,
                    0.0,
                    0.0
                ],
                [
                    0.38888888888888884,
                    0.5918803418803419,
                    0.5577217635283599,
                    -0.16124268676952347,
                    0.4631578947368421,
                    0.30756302521008405,
                    0.40417457305502846,
                    0.42478941034897716,
                    0.011029411764705883,
                    0.4805194805194805,
                    0.3,
                    0.44572361972679075,
                    0.5636363636363636,
                    0.35714285714285715,
                    0.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "john donne",
                "william shakespeare",
                "ernest hemingway"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "ernest hemingway": 0.634467197486036,
                "john donne": 0.09476596517458105,
                "william shakespeare": 0.15140567690642276
            },
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.4332045498375925,
                    1.3381773515284539,
                    2.228618098633954
                ],
                "result_count_important_words": [
                    3440.0,
                    2840.0,
                    4260.0
                ],
                "wikipedia_search": [
                    1.2549019607843137,
                    2.700980392156863,
                    0.04411764705882353
                ],
                "word_count_appended_bing": [
                    21.0,
                    7.0,
                    12.0
                ],
                "answer_relation_to_question_bing": [
                    0.891025641025641,
                    0.3333333333333333,
                    1.7756410256410255
                ],
                "cosine_similarity_raw": [
                    0.16659016907215118,
                    0.04479929059743881,
                    0.26656636595726013
                ],
                "result_count_noun_chunks": [
                    110000.0,
                    368000.0,
                    353000.0
                ],
                "question_answer_similarity": [
                    1.90947618894279,
                    2.1396025301655754,
                    -0.5622290147002786
                ],
                "word_count_noun_chunks": [
                    24.0,
                    0.0,
                    31.0
                ],
                "word_count_raw": [
                    9.0,
                    0.0,
                    5.0
                ],
                "result_count_bing": [
                    38100.0,
                    85500.0,
                    54900.0
                ],
                "word_count_appended": [
                    26.0,
                    14.0,
                    37.0
                ],
                "answer_relation_to_question": [
                    0.9444444444444444,
                    1.5,
                    1.5555555555555554
                ],
                "result_count": [
                    25.0,
                    26.0,
                    44.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these is NOT a real part of a sewing machine?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "lapse knob",
                "top"
            ],
            "question": "which of these is not a real part of a sewing machine?",
            "lines": [
                [
                    0.37814484366208506,
                    0.3333333333333333,
                    0.4270972853323824,
                    0.37562168750513386,
                    0.03899793388429751,
                    0.4753261486103233,
                    0.4999987834579478,
                    0.4999979838790973,
                    0.36363636363636365,
                    0.4854368932038835,
                    0.4772727272727273,
                    0.3884414063742976,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.2549077975729057,
                    0.30508072174738843,
                    0.23317218933635872,
                    0.32618326079768645,
                    0.475077479338843,
                    0.26233692569483835,
                    0.22141187004411184,
                    0.3659279599679034,
                    0.5,
                    0.24271844660194175,
                    0.3068181818181818,
                    0.3170889804582102,
                    0.0,
                    0.5,
                    -1.0
                ],
                [
                    0.3669473587650093,
                    0.3615859449192782,
                    0.33973052533125897,
                    0.2981950516971797,
                    0.4859245867768595,
                    0.26233692569483835,
                    0.2785893464979404,
                    0.1340740561529994,
                    0.13636363636363635,
                    0.27184466019417475,
                    0.21590909090909088,
                    0.2944696131674922,
                    0.5,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "lapse knob",
                "throat plate",
                "feed dog"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "feed dog": 0.04094622381631206,
                "lapse knob": 0.6249195905689128,
                "throat plate": 0.09921262920303725
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.6693515617542146,
                    1.0974661172507387,
                    1.2331823209950468
                ],
                "result_count_important_words": [
                    2.0,
                    458000.0,
                    364000.0
                ],
                "wikipedia_search": [
                    0.5454545454545454,
                    0.0,
                    1.4545454545454546
                ],
                "word_count_appended_bing": [
                    2.0,
                    17.0,
                    25.0
                ],
                "answer_relation_to_question_bing": [
                    1.0,
                    1.1695156695156694,
                    0.8304843304843305
                ],
                "cosine_similarity_raw": [
                    0.041580915451049805,
                    0.15218836069107056,
                    0.09141156822443008
                ],
                "result_count_noun_chunks": [
                    2.0,
                    133000.0,
                    363000.0
                ],
                "question_answer_similarity": [
                    4.230471029877663,
                    5.912016853690147,
                    6.863977894186974
                ],
                "word_count_noun_chunks": [
                    0.0,
                    2.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    4350000.0,
                    41900000.0,
                    41900000.0
                ],
                "word_count_appended": [
                    6.0,
                    106.0,
                    94.0
                ],
                "answer_relation_to_question": [
                    0.7311309380274897,
                    1.4705532145625657,
                    0.7983158474099444
                ],
                "result_count": [
                    3570000.0,
                    193000.0,
                    109000.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "The man famously known as the Science Guy holds a patent for which of these items?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ballet shoe"
            ],
            "lines": [
                [
                    0.30780494015788135,
                    0.18225108225108225,
                    0.12068199972408458,
                    0.42282793452805056,
                    0.15178571428571427,
                    0.35185185185185186,
                    0.09302325581395349,
                    0.007476635514018692,
                    0.23888888888888887,
                    0.20833333333333334,
                    0.36363636363636365,
                    0.2248363705141521,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.27324929971988793,
                    0.5251082251082251,
                    0.14554702170233577,
                    0.2974400935604148,
                    0.5357142857142857,
                    0.31296296296296294,
                    0.5813953488372093,
                    0.9688473520249221,
                    0.711111111111111,
                    0.625,
                    0.36363636363636365,
                    0.49670524654195886,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.41894576012223067,
                    0.29264069264069265,
                    0.7337709785735796,
                    0.27973197191153465,
                    0.3125,
                    0.3351851851851852,
                    0.32558139534883723,
                    0.02367601246105919,
                    0.05,
                    0.16666666666666666,
                    0.2727272727272727,
                    0.2784583829438891,
                    0,
                    1.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "pulse rate monitor": 0.20564602849995192,
                "mechanical pencil": 0.448978254686129,
                "ballet shoe": 0.34537571681391904
            },
            "question": "the man famously known as the science guy holds a patent for which of these items?",
            "rate_limited": false,
            "answers": [
                "pulse rate monitor",
                "mechanical pencil",
                "ballet shoe"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "pulse rate monitor": 0.12523368309219907,
                "mechanical pencil": 0.4289524847469227,
                "ballet shoe": 0.6532297972916821
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7986909641132167,
                    3.973641972335671,
                    2.2276670635511127
                ],
                "result_count_important_words": [
                    8.0,
                    50.0,
                    28.0
                ],
                "wikipedia_search": [
                    1.1944444444444444,
                    3.5555555555555554,
                    0.25
                ],
                "word_count_appended_bing": [
                    4.0,
                    4.0,
                    3.0
                ],
                "answer_relation_to_question_bing": [
                    0.5467532467532468,
                    1.5753246753246752,
                    0.8779220779220779
                ],
                "cosine_similarity_raw": [
                    0.017867956310510635,
                    0.021549426019191742,
                    0.10864078998565674
                ],
                "result_count_noun_chunks": [
                    24.0,
                    3110.0,
                    76.0
                ],
                "question_answer_similarity": [
                    9.788729492109269,
                    6.885923039168119,
                    6.475969016551971
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    190000.0,
                    169000.0,
                    181000.0
                ],
                "word_count_appended": [
                    10.0,
                    30.0,
                    8.0
                ],
                "answer_relation_to_question": [
                    1.846829640947288,
                    1.6394957983193277,
                    2.513674560733384
                ],
                "result_count": [
                    17.0,
                    60.0,
                    35.0
                ]
            },
            "integer_answers": {
                "pulse rate monitor": 3,
                "mechanical pencil": 7,
                "ballet shoe": 3
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which alum from \u201cThe Hills\u201d founded a wildly popular millennial skincare line?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "emily weiss"
            ],
            "lines": [
                [
                    0.5469387755102041,
                    0.44570745861068445,
                    0.6120818865067976,
                    -0.04123031383057541,
                    0.1348314606741573,
                    0.7626279300099043,
                    0.24475524475524477,
                    0.24516129032258063,
                    0.3932291666666667,
                    0.19047619047619047,
                    0.25,
                    0.30182110838955745,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.22315148281534838,
                    0.1764635603345281,
                    0.1907682835862498,
                    0.2483397479870632,
                    0.6067415730337079,
                    0.16705183228788378,
                    0.4755244755244755,
                    0.44516129032258067,
                    0.5166495901639344,
                    0.5476190476190477,
                    0.39285714285714285,
                    0.4048033064515427,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.22990974167444755,
                    0.3778289810547875,
                    0.1971498299069526,
                    0.7928905658435123,
                    0.25842696629213485,
                    0.07032023770221195,
                    0.27972027972027974,
                    0.3096774193548387,
                    0.0901212431693989,
                    0.2619047619047619,
                    0.35714285714285715,
                    0.29337558515889983,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "emily weiss": 0.4347428712922437,
                "whitney port": 0.2513191763517916,
                "lauren conrad": 0.3139379523559647
            },
            "question": "which alum from \u201cthe hills\u201d founded a wildly popular millennial skincare line?",
            "rate_limited": false,
            "answers": [
                "emily weiss",
                "lauren conrad",
                "whitney port"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "emily weiss": 0.693692889839821,
                "whitney port": 0.1382318303807124,
                "lauren conrad": 0.37612599762387366
            },
            "integer_answers": {
                "emily weiss": 6,
                "whitney port": 1,
                "lauren conrad": 7
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.4145688671164596,
                    3.2384264516123418,
                    2.3470046812711987
                ],
                "result_count_important_words": [
                    35.0,
                    68.0,
                    40.0
                ],
                "wikipedia_search": [
                    1.5729166666666667,
                    2.0665983606557377,
                    0.3604849726775956
                ],
                "word_count_appended_bing": [
                    7.0,
                    11.0,
                    10.0
                ],
                "answer_relation_to_question_bing": [
                    2.228537293053422,
                    0.8823178016726404,
                    1.8891449052739375
                ],
                "cosine_similarity_raw": [
                    0.25154754519462585,
                    0.07840012013912201,
                    0.08102274686098099
                ],
                "result_count_noun_chunks": [
                    38.0,
                    69.0,
                    48.0
                ],
                "question_answer_similarity": [
                    -0.11563500203192234,
                    0.6964964511571452,
                    2.2237498014001176
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    23100.0,
                    5060.0,
                    2130.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    12.0,
                    54.0,
                    23.0
                ],
                "answer_relation_to_question": [
                    3.8285714285714287,
                    1.5620603797074386,
                    1.6093681917211329
                ],
                "word_count_appended": [
                    8.0,
                    23.0,
                    11.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What car brand is sung about by Will Smith, Charli XCX and Janis Joplin?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "porsche"
            ],
            "lines": [
                [
                    0.47250276701715554,
                    0.28678878428162097,
                    0.4530487405919576,
                    0.3745326544279725,
                    0.39824224114254325,
                    0.3333333333333333,
                    0.32585895117540686,
                    0.3958218801539307,
                    0.004477611940298508,
                    0.2644628099173554,
                    0.23684210526315788,
                    0.2774532792728293,
                    0.0,
                    0,
                    1.0
                ],
                [
                    0.38554925290536807,
                    0.3729192249965889,
                    0.33006118148499336,
                    0.46095148978453654,
                    0.32957978577313923,
                    0.3333333333333333,
                    0.3515370705244123,
                    0.3326003298515668,
                    0.6001105583195135,
                    0.28512396694214875,
                    0.2631578947368421,
                    0.3845163220303863,
                    0.08333333333333333,
                    0,
                    1.0
                ],
                [
                    0.1419479800774765,
                    0.3402919907217901,
                    0.21689007792304904,
                    0.16451585578749095,
                    0.2721779730843175,
                    0.3333333333333333,
                    0.32260397830018084,
                    0.2715777899945025,
                    0.39541182974018796,
                    0.45041322314049587,
                    0.5,
                    0.3380303986967843,
                    0.9166666666666666,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "porsche": 0.3587585459589443,
                "rolls-royce": 0.29410501219365864,
                "mercedes-benz": 0.34713644184739717
            },
            "question": "what car brand is sung about by will smith, charli xcx and janis joplin?",
            "rate_limited": false,
            "answers": [
                "rolls-royce",
                "mercedes-benz",
                "porsche"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "porsche": 0.671306739815384,
                "rolls-royce": 0.07900863413967375,
                "mercedes-benz": 0.1093476985273776
            },
            "integer_answers": {
                "porsche": 3,
                "rolls-royce": 5,
                "mercedes-benz": 5
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.2196262341826345,
                    3.0761305762430906,
                    2.7042431895742745
                ],
                "result_count_important_words": [
                    9010.0,
                    9720.0,
                    8920.0
                ],
                "wikipedia_search": [
                    0.01791044776119403,
                    2.400442233278054,
                    1.5816473189607518
                ],
                "word_count_appended_bing": [
                    27.0,
                    30.0,
                    57.0
                ],
                "answer_relation_to_question_bing": [
                    0.8603663528448628,
                    1.1187576749897667,
                    1.0208759721653704
                ],
                "cosine_similarity_raw": [
                    0.04062693566083908,
                    0.029598083347082138,
                    0.019449517130851746
                ],
                "result_count_noun_chunks": [
                    14400.0,
                    12100.0,
                    9880.0
                ],
                "question_answer_similarity": [
                    4.242357361596078,
                    5.221229505375959,
                    1.8634825125336647
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    11.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    156000.0,
                    156000.0,
                    156000.0
                ],
                "result_count": [
                    14500.0,
                    12000.0,
                    9910.0
                ],
                "answer_relation_to_question": [
                    1.890011068068622,
                    1.542197011621472,
                    0.5677919203099059
                ],
                "word_count_appended": [
                    64.0,
                    69.0,
                    109.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which person is most likely to use a Reuleaux triangle at work?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "banksy"
            ],
            "lines": [
                [
                    0.4965033010987736,
                    0.3389250058045043,
                    0.16945508310603025,
                    0.46353960348186013,
                    0.06962025316455696,
                    0.35054347826086957,
                    0.06790123456790123,
                    0.08379888268156424,
                    0.02611754966887417,
                    0.09090909090909091,
                    0.09375,
                    0.21357557264180946,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.20866310192366436,
                    0.3335074684621933,
                    0.12688118408278357,
                    -0.09629011091222073,
                    0.34810126582278483,
                    0.34782608695652173,
                    0.36419753086419754,
                    0.36312849162011174,
                    0.15378565970453387,
                    0.07575757575757576,
                    0.0625,
                    0.33178567335061326,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.2948335969775621,
                    0.32756752573330233,
                    0.7036637328111862,
                    0.6327505074303607,
                    0.5822784810126582,
                    0.3016304347826087,
                    0.5679012345679012,
                    0.553072625698324,
                    0.820096790626592,
                    0.8333333333333334,
                    0.84375,
                    0.4546387540075773,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "banksy": 0.5762930847484505,
                "adam levine": 0.2183203273027299,
                "greta gerwig": 0.2053865879488195
            },
            "question": "which person is most likely to use a reuleaux triangle at work?",
            "rate_limited": false,
            "answers": [
                "greta gerwig",
                "adam levine",
                "banksy"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "banksy": 0.5827066102259297,
                "adam levine": 0.13171778124794792,
                "greta gerwig": 0.23471487233925584
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2814534358508567,
                    1.9907140401036796,
                    2.7278325240454637
                ],
                "result_count_important_words": [
                    11.0,
                    59.0,
                    92.0
                ],
                "wikipedia_search": [
                    0.07835264900662252,
                    0.46135697911360163,
                    2.460290371879776
                ],
                "word_count_appended_bing": [
                    3.0,
                    2.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    1.0167750174135128,
                    1.00052240538658,
                    0.9827025771999071
                ],
                "cosine_similarity_raw": [
                    0.03144431486725807,
                    0.023544244468212128,
                    0.13057279586791992
                ],
                "result_count_noun_chunks": [
                    15.0,
                    65.0,
                    99.0
                ],
                "question_answer_similarity": [
                    -0.7148150510620326,
                    0.1484870333224535,
                    -0.9757517650723457
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    129000.0,
                    128000.0,
                    111000.0
                ],
                "word_count_appended": [
                    6.0,
                    5.0,
                    55.0
                ],
                "answer_relation_to_question": [
                    1.9860132043950942,
                    0.8346524076946573,
                    1.1793343879102482
                ],
                "result_count": [
                    11.0,
                    55.0,
                    92.0
                ]
            },
            "integer_answers": {
                "banksy": 8,
                "adam levine": 1,
                "greta gerwig": 3
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "By definition, which of these would an herbivore eat?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "shawarma",
                "bottom"
            ],
            "question": "by definition, which of these would an herbivore eat?",
            "lines": [
                [
                    0.31405599147534635,
                    0.14356435643564358,
                    0.3147302639810999,
                    0.3368646178742039,
                    0.49411764705882355,
                    0.25301991511589944,
                    0.44275582573454914,
                    0.4772727272727273,
                    0.12048192771084337,
                    0.3756476683937824,
                    0.3263157894736842,
                    0.3512466515847977,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.4015305628208854,
                    0.17821782178217824,
                    0.4501590312034116,
                    0.5386940410537935,
                    0.15294117647058825,
                    0.4831864185439112,
                    0.23910840932117527,
                    0.1774891774891775,
                    0.5301204819277109,
                    0.2772020725388601,
                    0.29473684210526313,
                    0.27804293901291594,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.28441344570376825,
                    0.6782178217821783,
                    0.23511070481548849,
                    0.12444134107200258,
                    0.35294117647058826,
                    0.26379366634018936,
                    0.3181357649442756,
                    0.34523809523809523,
                    0.3493975903614458,
                    0.3471502590673575,
                    0.37894736842105264,
                    0.37071040940228644,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "tabbouleh",
                "kibbeh",
                "shawarma"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "shawarma": 0.49624489350552536,
                "tabbouleh": 0.2805150244407632,
                "kibbeh": 0.27002797979487403
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.053739954754393,
                    0.8341288170387477,
                    1.1121312282068594
                ],
                "result_count_important_words": [
                    43700.0,
                    23600.0,
                    31400.0
                ],
                "wikipedia_search": [
                    0.12048192771084337,
                    0.5301204819277109,
                    0.3493975903614458
                ],
                "word_count_appended_bing": [
                    31.0,
                    28.0,
                    36.0
                ],
                "answer_relation_to_question_bing": [
                    0.2871287128712871,
                    0.3564356435643564,
                    1.3564356435643563
                ],
                "cosine_similarity_raw": [
                    0.04153791442513466,
                    0.05941172316670418,
                    0.031029772013425827
                ],
                "result_count_noun_chunks": [
                    44100.0,
                    16400.0,
                    31900.0
                ],
                "question_answer_similarity": [
                    -0.5239059180021286,
                    -0.8377994634211063,
                    -0.19353636913001537
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    775000.0,
                    1480000.0,
                    808000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    75600.0,
                    23400.0,
                    54000.0
                ],
                "answer_relation_to_question": [
                    0.6281119829506927,
                    0.8030611256417708,
                    0.5688268914075365
                ],
                "word_count_appended": [
                    145.0,
                    107.0,
                    134.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "The actor who played Don Draper provides the voice for what car company\u2019s ads?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "jaguar"
            ],
            "lines": [
                [
                    0.37814709279938385,
                    0.18521078565562551,
                    0.5215220832730078,
                    0.5822662502763025,
                    0.18874773139745918,
                    0.419281232173417,
                    0.14620689655172414,
                    0.1596779604159678,
                    0.6269208965637537,
                    0.3378607809847199,
                    0.35789473684210527,
                    0.3402702042614314,
                    0.7777777777777778,
                    0.75,
                    1.0
                ],
                [
                    0.36049595687331537,
                    0.3863981658910484,
                    0.28301735247995113,
                    0.24786561416360686,
                    0.4192377495462795,
                    0.33371363377067886,
                    0.4772413793103448,
                    0.37739013753773903,
                    0.33484459488923773,
                    0.31918505942275044,
                    0.28421052631578947,
                    0.3332806442603006,
                    0.1111111111111111,
                    0.25,
                    1.0
                ],
                [
                    0.2613569503273007,
                    0.42839104845332604,
                    0.1954605642470411,
                    0.16986813556009067,
                    0.39201451905626133,
                    0.24700513405590416,
                    0.37655172413793103,
                    0.4629319020462932,
                    0.03823450854700854,
                    0.34295415959252973,
                    0.35789473684210527,
                    0.3264491514782681,
                    0.1111111111111111,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "jaguar": 0.26501597467536936,
                "bmw": 0.3227137089694395,
                "mercedes-benz": 0.4122703163551911
            },
            "question": "the actor who played don draper provides the voice for what car company\u2019s ads?",
            "rate_limited": false,
            "answers": [
                "mercedes-benz",
                "bmw",
                "jaguar"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "jaguar": 0.6107807549403994,
                "bmw": 0.21816757588868702,
                "mercedes-benz": 0.5465037702352603
            },
            "integer_answers": {
                "jaguar": 3,
                "bmw": 2,
                "mercedes-benz": 9
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.7221616340914507,
                    2.666245154082404,
                    2.6115932118261442
                ],
                "result_count_important_words": [
                    1060000.0,
                    3460000.0,
                    2730000.0
                ],
                "wikipedia_search": [
                    3.761525379382522,
                    2.0090675693354263,
                    0.22940705128205127
                ],
                "word_count_appended_bing": [
                    34.0,
                    27.0,
                    34.0
                ],
                "answer_relation_to_question_bing": [
                    0.9260539282781276,
                    1.9319908294552421,
                    2.14195524226663
                ],
                "cosine_similarity_raw": [
                    0.06537061929702759,
                    0.03547504544258118,
                    0.0245001669973135
                ],
                "result_count_noun_chunks": [
                    952000.0,
                    2250000.0,
                    2760000.0
                ],
                "question_answer_similarity": [
                    3.793542579282075,
                    1.6148776626214385,
                    1.106713646557182
                ],
                "word_count_noun_chunks": [
                    21.0,
                    3.0,
                    3.0
                ],
                "result_count_bing": [
                    147000.0,
                    117000.0,
                    86600.0
                ],
                "word_count_raw": [
                    6.0,
                    2.0,
                    0.0
                ],
                "result_count": [
                    1040000.0,
                    2310000.0,
                    2160000.0
                ],
                "answer_relation_to_question": [
                    1.8907354639969194,
                    1.8024797843665767,
                    1.3067847516365036
                ],
                "word_count_appended": [
                    199.0,
                    188.0,
                    202.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these do adverbs NOT typically modify?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "pronoun"
            ],
            "lines": [
                [
                    0.5,
                    0.5,
                    0.48416897113382734,
                    0.32544746535615543,
                    0.3462745098039216,
                    0.23618482672494534,
                    0.3669527896995708,
                    0.336950623982637,
                    0.5000000000000001,
                    0.4189663823381836,
                    0.4078341013824885,
                    0.3593101923047063,
                    0.4985687022900763,
                    0.5,
                    -1.0
                ],
                [
                    0.11712826726674747,
                    0.09090909090909088,
                    0.13496327981950984,
                    0.3791113996727453,
                    0.34980392156862744,
                    0.4163284420855448,
                    0.3812589413447782,
                    0.3304395008138904,
                    0.17996632996632997,
                    0.21700953336678375,
                    0.21198156682027652,
                    0.3141530817227256,
                    0.036736641221374045,
                    0.07482993197278914,
                    -1.0
                ],
                [
                    0.38287173273325253,
                    0.4090909090909091,
                    0.38086774904666276,
                    0.29544113497109925,
                    0.303921568627451,
                    0.34748673118950985,
                    0.25178826895565093,
                    0.3326098752034726,
                    0.32003367003367006,
                    0.36402408429503263,
                    0.38018433179723504,
                    0.3265367259725681,
                    0.46469465648854963,
                    0.4251700680272109,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "adjective": 0.2878969276525322,
                "adverb": 0.5379114387783981,
                "pronoun": 0.1741916335690697
            },
            "question": "which of these do adverbs not typically modify?",
            "rate_limited": false,
            "answers": [
                "pronoun",
                "adverb",
                "adjective"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "adjective": 0.7465153967762261,
                "adverb": 0.015878103149579853,
                "pronoun": 0.9016824609325857
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.8441388461717625,
                    1.1150815096636464,
                    1.0407796441645911
                ],
                "result_count_important_words": [
                    186000.0,
                    166000.0,
                    347000.0
                ],
                "wikipedia_search": [
                    0.0,
                    1.9202020202020202,
                    1.0797979797979798
                ],
                "word_count_appended_bing": [
                    40.0,
                    125.0,
                    52.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    2.4545454545454546,
                    0.5454545454545454
                ],
                "cosine_similarity_raw": [
                    0.031964004039764404,
                    0.7370358109474182,
                    0.24053671956062317
                ],
                "result_count_noun_chunks": [
                    601000.0,
                    625000.0,
                    617000.0
                ],
                "question_answer_similarity": [
                    1.4027007594704628,
                    0.9714584313333035,
                    1.643831044435501
                ],
                "word_count_noun_chunks": [
                    3.0,
                    971.0,
                    74.0
                ],
                "word_count_raw": [
                    0.0,
                    250.0,
                    44.0
                ],
                "result_count_bing": [
                    16900000.0,
                    5360000.0,
                    9770000.0
                ],
                "result_count": [
                    784000.0,
                    766000.0,
                    1000000.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    2.2972303963995153,
                    0.7027696036004847
                ],
                "word_count_appended": [
                    323.0,
                    1128.0,
                    542.0
                ]
            },
            "integer_answers": {
                "adjective": 3,
                "adverb": 10,
                "pronoun": 1
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "One of Tupac Shakur\u2019s biggest posthumous hits samples what singer?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "bruce hornsby"
            ],
            "lines": [
                [
                    0.46698005698005696,
                    0.5317460317460317,
                    0.26459023970296947,
                    0.45633501858049685,
                    0.025604551920341393,
                    0.6239799121155053,
                    0.05934959349593496,
                    0.1415929203539823,
                    0.36768018018018017,
                    0.18811881188118812,
                    0.11764705882352941,
                    0.22859529643311194,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.12703703703703703,
                    0.4047619047619048,
                    0.19540915839806128,
                    0.36276600365520895,
                    0.23826458036984352,
                    0.07470182046453233,
                    0.532520325203252,
                    0.5162241887905604,
                    0.4671546546546546,
                    0.12871287128712872,
                    0.11764705882352941,
                    0.31580469351932094,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.405982905982906,
                    0.06349206349206349,
                    0.5400006018989693,
                    0.18089897776429417,
                    0.7361308677098151,
                    0.3013182674199623,
                    0.408130081300813,
                    0.3421828908554572,
                    0.16516516516516516,
                    0.6831683168316832,
                    0.7647058823529411,
                    0.4556000100475671,
                    0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "john mellencamp": 0.2677695613050026,
                "bruce hornsby": 0.46513661775551046,
                "christopher cross": 0.26709382093948686
            },
            "question": "one of tupac shakur\u2019s biggest posthumous hits samples what singer?",
            "rate_limited": false,
            "answers": [
                "christopher cross",
                "john mellencamp",
                "bruce hornsby"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "john mellencamp": 0.1287238274202652,
                "bruce hornsby": 0.6197955968384589,
                "christopher cross": 0.18242109037754156
            },
            "integer_answers": {
                "john mellencamp": 3,
                "bruce hornsby": 6,
                "christopher cross": 4
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6001670750317836,
                    2.2106328546352465,
                    3.1892000703329697
                ],
                "result_count_important_words": [
                    73.0,
                    655.0,
                    502.0
                ],
                "wikipedia_search": [
                    1.1030405405405406,
                    1.4014639639639639,
                    0.4954954954954955
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    13.0
                ],
                "answer_relation_to_question_bing": [
                    1.5952380952380953,
                    1.2142857142857144,
                    0.19047619047619047
                ],
                "cosine_similarity_raw": [
                    0.04487049952149391,
                    0.03313843533396721,
                    0.09157592803239822
                ],
                "result_count_noun_chunks": [
                    9600.0,
                    35000.0,
                    23200.0
                ],
                "question_answer_similarity": [
                    3.2179248952306807,
                    2.5581068880856037,
                    1.275640265084803
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    99400.0,
                    11900.0,
                    48000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count": [
                    72.0,
                    670.0,
                    2070.0
                ],
                "answer_relation_to_question": [
                    2.334900284900285,
                    0.6351851851851852,
                    2.02991452991453
                ],
                "word_count_appended": [
                    19.0,
                    13.0,
                    69.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which position was first held in the last year of the previous century?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "mayor of london"
            ],
            "question": "which position was first held in the last year of the previous century?",
            "lines": [
                [
                    0.43216783216783217,
                    0.15084745762711865,
                    0.24164427040825748,
                    0.3507513617935255,
                    1.508131846918585e-05,
                    0.3304157549234136,
                    2.3118213540048692e-05,
                    3.100014208398455e-05,
                    0.04523809523809523,
                    0.11304347826086956,
                    0.25,
                    0.12398710079210411,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.22553133141368437,
                    0.5734463276836158,
                    0.24534897687784488,
                    0.3671697953899151,
                    0.9998914145070219,
                    0.33916849015317285,
                    0.9998627356071059,
                    0.9997545822085018,
                    0.2261904761904762,
                    0.6782608695652174,
                    0.5,
                    0.5562903049273173,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.34230083641848347,
                    0.27570621468926554,
                    0.5130067527138976,
                    0.28207884281655937,
                    9.350417450895227e-05,
                    0.3304157549234136,
                    0.00011414617935399041,
                    0.00021441764941422647,
                    0.7285714285714286,
                    0.20869565217391303,
                    0.25,
                    0.3197225942805784,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "mayor of singapore",
                "mayor of london",
                "mayor of baghdad"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "mayor of london": 0.46252638022218867,
                "mayor of baghdad": 0.29820055336945994,
                "mayor of singapore": -0.04540842615597983
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.6199355039605206,
                    2.7814515246365867,
                    1.598612971402892
                ],
                "result_count_important_words": [
                    16.0,
                    692000.0,
                    79.0
                ],
                "wikipedia_search": [
                    0.22619047619047616,
                    1.130952380952381,
                    3.6428571428571432
                ],
                "word_count_appended_bing": [
                    2.0,
                    4.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.7542372881355932,
                    2.867231638418079,
                    1.3785310734463276
                ],
                "cosine_similarity_raw": [
                    0.03227108716964722,
                    0.032765842974185944,
                    0.0685109794139862
                ],
                "result_count_noun_chunks": [
                    12.0,
                    387000.0,
                    83.0
                ],
                "question_answer_similarity": [
                    11.40628607198596,
                    11.940206594765186,
                    9.173084773123264
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    1510000.0,
                    1550000.0,
                    1510000.0
                ],
                "word_count_appended": [
                    13.0,
                    78.0,
                    24.0
                ],
                "answer_relation_to_question": [
                    2.160839160839161,
                    1.1276566570684219,
                    1.7115041820924173
                ],
                "result_count": [
                    10.0,
                    663000.0,
                    62.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What is the grammatically correct way to announce people have arrived?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "they're here"
            ],
            "lines": [
                [
                    0.38744343891402716,
                    0.44000000000000006,
                    0.21004873493846968,
                    0.4380747507787122,
                    0.04234156863528331,
                    0.6425470332850941,
                    0.44341779939407305,
                    0.8759124087591241,
                    0.0,
                    0.7115384615384616,
                    0.3333333333333333,
                    0.37786737443081436,
                    0,
                    0,
                    1.0
                ],
                [
                    0.4332579185520362,
                    0.20666666666666664,
                    0.33951411565257633,
                    0.2967852168339446,
                    0.9546825380974255,
                    0.19247467438494936,
                    0.5548526390323741,
                    0.08759124087591241,
                    0.5,
                    0.07692307692307693,
                    0.3333333333333333,
                    0.3849612004576095,
                    0,
                    0,
                    1.0
                ],
                [
                    0.17929864253393665,
                    0.35333333333333333,
                    0.450437149408954,
                    0.2651400323873432,
                    0.0029758932672911383,
                    0.1649782923299566,
                    0.001729561573552798,
                    0.0364963503649635,
                    0.5,
                    0.21153846153846154,
                    0.3333333333333333,
                    0.23717142511157616,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "their here": 0.22803603959855853,
                "there here": 0.36342021840082533,
                "they're here": 0.4085437420006161
            },
            "question": "what is the grammatically correct way to announce people have arrived?",
            "rate_limited": false,
            "answers": [
                "they're here",
                "there here",
                "their here"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "their here": 0.10746396581328271,
                "there here": 0.10483264576317793,
                "they're here": 0.4238356690071211
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.2672042465848863,
                    2.309767202745657,
                    1.4230285506694569
                ],
                "result_count_important_words": [
                    1910000.0,
                    2390000.0,
                    7450.0
                ],
                "wikipedia_search": [
                    0.0,
                    2.0,
                    2.0
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    2.2,
                    1.0333333333333332,
                    1.7666666666666666
                ],
                "cosine_similarity_raw": [
                    0.05089009553194046,
                    0.08225665241479874,
                    0.10913081467151642
                ],
                "result_count_noun_chunks": [
                    2280000.0,
                    228000.0,
                    95000.0
                ],
                "question_answer_similarity": [
                    17.52471286058426,
                    11.872575849294662,
                    10.60664401948452
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    444000.0,
                    133000.0,
                    114000.0
                ],
                "word_count_appended": [
                    37.0,
                    4.0,
                    11.0
                ],
                "answer_relation_to_question": [
                    1.5497737556561086,
                    1.7330316742081449,
                    0.7171945701357466
                ],
                "result_count": [
                    106000.0,
                    2390000.0,
                    7450.0
                ]
            },
            "integer_answers": {
                "their here": 1,
                "there here": 5,
                "they're here": 6
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these actors turned down the role of Frasier Crane?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "john lithgow"
            ],
            "question": "which of these actors turned down the role of frasier crane?",
            "lines": [
                [
                    0.334452296819788,
                    0.09836065573770492,
                    0.0885860640017796,
                    0.33859827922468017,
                    0.08128262490678598,
                    0.2846473029045643,
                    0.015330648304534233,
                    0.1868818323789693,
                    0.16491862567811935,
                    0.09782608695652174,
                    0.09523809523809523,
                    0.23549269701853856,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.15600706713780918,
                    0.44808743169398907,
                    0.7950442562498132,
                    3.73401296863992,
                    0.44146159582401195,
                    0.18423236514522823,
                    0.09471222554241912,
                    0.39406559083810516,
                    0.35925256178420734,
                    0.6956521739130435,
                    0.7619047619047619,
                    0.41204132516181635,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.5095406360424027,
                    0.453551912568306,
                    0.11636967974840721,
                    -3.0726112478646006,
                    0.4772557792692021,
                    0.5311203319502075,
                    0.8899571261530467,
                    0.41905257678292557,
                    0.47582881253767334,
                    0.20652173913043478,
                    0.14285714285714285,
                    0.352465977819645,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "bob balaban",
                "john lithgow",
                "dan aykroyd"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "john lithgow": 0.8458562193962784,
                "dan aykroyd": 0.1562561496921213,
                "bob balaban": 0.03024648698815962
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1774634850926928,
                    2.060206625809082,
                    1.762329889098225
                ],
                "result_count_important_words": [
                    1180.0,
                    7290.0,
                    68500.0
                ],
                "wikipedia_search": [
                    0.4947558770343581,
                    1.077757685352622,
                    1.42748643761302
                ],
                "word_count_appended_bing": [
                    2.0,
                    16.0,
                    3.0
                ],
                "answer_relation_to_question_bing": [
                    0.29508196721311475,
                    1.3442622950819672,
                    1.360655737704918
                ],
                "cosine_similarity_raw": [
                    0.029031062498688698,
                    0.2605486512184143,
                    0.03813619539141655
                ],
                "result_count_noun_chunks": [
                    3590.0,
                    7570.0,
                    8050.0
                ],
                "question_answer_similarity": [
                    0.08255415223538876,
                    0.9103952795267105,
                    -0.7491379380226135
                ],
                "word_count_noun_chunks": [
                    0.0,
                    3.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    9.0,
                    0.0
                ],
                "result_count_bing": [
                    34300.0,
                    22200.0,
                    64000.0
                ],
                "word_count_appended": [
                    9.0,
                    64.0,
                    19.0
                ],
                "answer_relation_to_question": [
                    1.003356890459364,
                    0.4680212014134275,
                    1.5286219081272083
                ],
                "result_count": [
                    1090.0,
                    5920.0,
                    6400.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What generation of the iPod was the first to offer video?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "u2 special edition"
            ],
            "lines": [
                [
                    0.23642049487063682,
                    0.21116724597849812,
                    0.13295137908355617,
                    0.36328898009034727,
                    0.4896551724137931,
                    0.47173647824318826,
                    0.3778060144006777,
                    0.42745861733203505,
                    0.005108556832694763,
                    0.2556818181818182,
                    0.3,
                    0.3303793209022658,
                    0,
                    0,
                    1.0
                ],
                [
                    0.501546765802357,
                    0.39122104827704257,
                    0.6692121477259302,
                    0.3134969339667955,
                    0.2620689655172414,
                    0.058560390402602684,
                    0.17746717492587885,
                    0.32424537487828625,
                    0.572972972972973,
                    0.5340909090909091,
                    0.25,
                    0.3108609855428548,
                    0,
                    0,
                    1.0
                ],
                [
                    0.26203273932700616,
                    0.39761170574445925,
                    0.19783647319051367,
                    0.3232140859428573,
                    0.2482758620689655,
                    0.46970313135420905,
                    0.44472681067344344,
                    0.24829600778967867,
                    0.4219184701943323,
                    0.21022727272727273,
                    0.45,
                    0.3587596935548793,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "third generation": 0.3001378398607926,
                "u2 special edition": 0.36381197242523927,
                "fifth generation": 0.33605018771396816
            },
            "question": "what generation of the ipod was the first to offer video?",
            "rate_limited": false,
            "answers": [
                "third generation",
                "u2 special edition",
                "fifth generation"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "third generation": 0.10255437968158654,
                "u2 special edition": 0.5073212434532997,
                "fifth generation": 0.32498995956951027
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3215172836090632,
                    1.2434439421714192,
                    1.4350387742195172
                ],
                "result_count_important_words": [
                    892000.0,
                    419000.0,
                    1050000.0
                ],
                "wikipedia_search": [
                    0.01532567049808429,
                    1.7189189189189191,
                    1.265755410582997
                ],
                "word_count_appended_bing": [
                    6.0,
                    5.0,
                    9.0
                ],
                "answer_relation_to_question_bing": [
                    0.8446689839139925,
                    1.5648841931081703,
                    1.590446822977837
                ],
                "cosine_similarity_raw": [
                    0.055243492126464844,
                    0.2780686914920807,
                    0.08220431953668594
                ],
                "result_count_noun_chunks": [
                    439000.0,
                    333000.0,
                    255000.0
                ],
                "question_answer_similarity": [
                    8.978684231638908,
                    7.748074210714549,
                    7.988233543932438
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    23200000.0,
                    2880000.0,
                    23100000.0
                ],
                "word_count_appended": [
                    45.0,
                    94.0,
                    37.0
                ],
                "answer_relation_to_question": [
                    0.9456819794825473,
                    2.006187063209428,
                    1.0481309573080246
                ],
                "result_count": [
                    213000.0,
                    114000.0,
                    108000.0
                ]
            },
            "integer_answers": {
                "third generation": 4,
                "u2 special edition": 4,
                "fifth generation": 4
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which organization began as the North West Police Agency?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "pinkerton"
            ],
            "lines": [
                [
                    0.29746219176221794,
                    0.49924242424242427,
                    0.3919071067240065,
                    4.226847840769482,
                    0.5827505827505828,
                    0.2744630071599045,
                    0.5978994748687172,
                    0.5567164179104478,
                    1.0,
                    0.31272210376687987,
                    0.3333333333333333,
                    0.34657301730441575,
                    0.047619047619047616,
                    0.6666666666666666,
                    -1.0
                ],
                [
                    0.11588200628601363,
                    0.1590909090909091,
                    0.17296991991317662,
                    0.2703570749405698,
                    0.3341103341103341,
                    0.37708830548926014,
                    0.3218304576144036,
                    0.3044776119402985,
                    0.0,
                    0.31982942430703626,
                    0.3333333333333333,
                    0.2895314611973883,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.5866558019517684,
                    0.3416666666666667,
                    0.43512297336281686,
                    -3.4972049157100518,
                    0.08313908313908314,
                    0.34844868735083534,
                    0.08027006751687922,
                    0.13880597014925372,
                    0.0,
                    0.3674484719260839,
                    0.3333333333333333,
                    0.3638955214981959,
                    0.9523809523809523,
                    0.3333333333333333,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "pinkerton": 0.06194971049279647,
                "fbi": 0.7238716582055804,
                "nra": 0.2141786313016231
            },
            "question": "which organization began as the north west police agency?",
            "rate_limited": false,
            "answers": [
                "fbi",
                "nra",
                "pinkerton"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "pinkerton": 0.5057061298780129,
                "fbi": 0.4767826224263234,
                "nra": 0.09366398117748181
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.0794381038264946,
                    1.73718876718433,
                    2.1833731289891753
                ],
                "result_count_important_words": [
                    797000.0,
                    429000.0,
                    107000.0
                ],
                "wikipedia_search": [
                    5.0,
                    0.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    65.0,
                    65.0,
                    65.0
                ],
                "answer_relation_to_question_bing": [
                    1.996969696969697,
                    0.6363636363636364,
                    1.3666666666666667
                ],
                "cosine_similarity_raw": [
                    0.05844486877322197,
                    0.025794899091124535,
                    0.06488962471485138
                ],
                "result_count_noun_chunks": [
                    746000.0,
                    408000.0,
                    186000.0
                ],
                "question_answer_similarity": [
                    1.0856999319512397,
                    0.06944339349865913,
                    -0.8982852664776146
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    20.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    1150000.0,
                    1580000.0,
                    1460000.0
                ],
                "result_count": [
                    750000.0,
                    430000.0,
                    107000.0
                ],
                "answer_relation_to_question": [
                    1.7847731505733078,
                    0.6952920377160817,
                    3.5199348117106104
                ],
                "word_count_appended": [
                    440.0,
                    450.0,
                    517.0
                ]
            },
            "integer_answers": {
                "pinkerton": 5,
                "fbi": 8,
                "nra": 1
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "The metal band that holds the eraser on the end of a pencil is called what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ferrule"
            ],
            "lines": [
                [
                    0.09639671840958607,
                    0.12777777777777777,
                    0.07271303740153966,
                    0.4560681975686811,
                    0.0038883306391716546,
                    0.38692390139335475,
                    0.0709186101856259,
                    0.0506631616680607,
                    0.04591836734693877,
                    0.17455138662316477,
                    0.25471698113207547,
                    0.24879361895225713,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.5333792892156862,
                    0.3333333333333333,
                    0.7818324590790396,
                    -0.836822073734617,
                    0.4805801913582944,
                    0.21864951768488747,
                    0.8215135649690624,
                    0.8603178396463138,
                    0.8649659863945578,
                    0.5269168026101142,
                    0.4716981132075472,
                    0.42890744482396587,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.3702239923747277,
                    0.5388888888888889,
                    0.14545450351942066,
                    1.380753876165936,
                    0.515531478002534,
                    0.39442658092175775,
                    0.10756782484531176,
                    0.08901899868562552,
                    0.0891156462585034,
                    0.29853181076672103,
                    0.27358490566037735,
                    0.32229893622377703,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "tragus": 0.14209500636415956,
                "aglet": 0.3232426744509701,
                "ferrule": 0.5346623191848705
            },
            "question": "the metal band that holds the eraser on the end of a pencil is called what?",
            "rate_limited": false,
            "answers": [
                "tragus",
                "ferrule",
                "aglet"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "tragus": 0.13412876539199733,
                "aglet": 0.10994925386666303,
                "ferrule": 0.776432132067992
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7415553326658,
                    3.002352113767761,
                    2.256092553566439
                ],
                "result_count_important_words": [
                    7450.0,
                    86300.0,
                    11300.0
                ],
                "wikipedia_search": [
                    0.3214285714285714,
                    6.054761904761905,
                    0.6238095238095238
                ],
                "word_count_appended_bing": [
                    27.0,
                    50.0,
                    29.0
                ],
                "answer_relation_to_question_bing": [
                    0.3833333333333333,
                    1.0,
                    1.6166666666666667
                ],
                "cosine_similarity_raw": [
                    0.02090507745742798,
                    0.2247776836156845,
                    0.04181832820177078
                ],
                "result_count_noun_chunks": [
                    8480.0,
                    144000.0,
                    14900.0
                ],
                "question_answer_similarity": [
                    -0.521819218993187,
                    0.9574661054648459,
                    -1.5798161615384743
                ],
                "word_count_noun_chunks": [
                    0.0,
                    70.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    43.0,
                    0.0
                ],
                "result_count_bing": [
                    361000.0,
                    204000.0,
                    368000.0
                ],
                "word_count_appended": [
                    107.0,
                    323.0,
                    183.0
                ],
                "answer_relation_to_question": [
                    0.5783803104575164,
                    3.2002757352941176,
                    2.221343954248366
                ],
                "result_count": [
                    89.0,
                    11000.0,
                    11800.0
                ]
            },
            "integer_answers": {
                "tragus": 0,
                "aglet": 3,
                "ferrule": 11
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these words is NOT a synonym for \u201cnonsense\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "polliwog"
            ],
            "question": "which of these words is not a synonym for \u201cnonsense\u201d?",
            "lines": [
                [
                    0.2473973607038123,
                    0.37777777777777777,
                    0.31241333510946734,
                    0.5,
                    0.344218593802066,
                    0.33433508640120213,
                    0.23266614113856804,
                    0.3634412797502926,
                    0.5,
                    0.3034246575342466,
                    0.326530612244898,
                    0.32976303776628674,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.47224462365591396,
                    0.4666666666666667,
                    0.4557840782648216,
                    0.5,
                    0.3117294235254915,
                    0.33433508640120213,
                    0.2686533913699146,
                    0.30296527506827936,
                    0.5,
                    0.41575342465753423,
                    0.4013605442176871,
                    0.37314214461066547,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.28035801564027374,
                    0.1555555555555556,
                    0.23180258662571113,
                    0.0,
                    0.3440519826724425,
                    0.3313298271975958,
                    0.4986804674915173,
                    0.333593445181428,
                    0.0,
                    0.2808219178082192,
                    0.27210884353741494,
                    0.29709481762304785,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "flapdoodle",
                "polliwog",
                "balderdash"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "balderdash": 0.00588910232706221,
                "flapdoodle": 0.42693470110376575,
                "polliwog": 0.9485967518794176
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.0214217734022797,
                    0.7611471323360074,
                    1.217431094261713
                ],
                "result_count_important_words": [
                    15600.0,
                    13500.0,
                    77.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_appended_bing": [
                    51.0,
                    29.0,
                    67.0
                ],
                "answer_relation_to_question_bing": [
                    0.7333333333333334,
                    0.2,
                    2.0666666666666664
                ],
                "cosine_similarity_raw": [
                    0.07134507596492767,
                    0.016816698014736176,
                    0.10200386494398117
                ],
                "result_count_noun_chunks": [
                    7000.0,
                    10100.0,
                    8530.0
                ],
                "question_answer_similarity": [
                    0.0,
                    0.0,
                    -0.18329207599163055
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    3.0
                ],
                "result_count_bing": [
                    8820000.0,
                    8820000.0,
                    8980000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_appended": [
                    287.0,
                    123.0,
                    320.0
                ],
                "answer_relation_to_question": [
                    1.515615835777126,
                    0.16653225806451613,
                    1.3178519061583578
                ],
                "result_count": [
                    9350.0,
                    11300.0,
                    9360.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which item is traditionally used to send a message out to sea?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "bottle"
            ],
            "question": "which item is traditionally used to send a message out to sea?",
            "lines": [
                [
                    0.44171146953405016,
                    0.43223574706385626,
                    0.8407173239886271,
                    0.3291404277603777,
                    0.9764136027934442,
                    0.4692669336583667,
                    0.9667791545888131,
                    0.9757553236656733,
                    0.2590909090909091,
                    0.8677130044843049,
                    0.9166666666666666,
                    0.5441976200972289,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.3546274961597542,
                    0.34114669650175605,
                    0.0693078195410597,
                    0.654217820624005,
                    0.023584143944395498,
                    0.46839630854953856,
                    0.033217540183307935,
                    0.02424422736715323,
                    0.02535885167464115,
                    0.10538116591928251,
                    0.05952380952380952,
                    0.37296673492718835,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.2036610343061956,
                    0.22661755643438775,
                    0.0899748564703132,
                    0.016641751615617217,
                    2.2532621602925635e-06,
                    0.06233675779209472,
                    3.305227878936113e-06,
                    4.489671734658006e-07,
                    0.7155502392344497,
                    0.026905829596412557,
                    0.023809523809523808,
                    0.08283564497558292,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "bottle",
                "butter dish",
                "butterscotch lozenge"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "butter dish": 0.020838276131112315,
                "butterscotch lozenge": 0.08001048839714282,
                "bottle": 1.0284103560968645
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.720988100486144,
                    1.8648336746359413,
                    0.4141782248779145
                ],
                "result_count_important_words": [
                    1170000.0,
                    40200.0,
                    4.0
                ],
                "wikipedia_search": [
                    1.0363636363636364,
                    0.1014354066985646,
                    2.862200956937799
                ],
                "word_count_appended_bing": [
                    77.0,
                    5.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.728942988255425,
                    1.3645867860070242,
                    0.906470225737551
                ],
                "cosine_similarity_raw": [
                    0.3377391993999481,
                    0.027842851355671883,
                    0.03614536672830582
                ],
                "result_count_noun_chunks": [
                    6520000.0,
                    162000.0,
                    3.0
                ],
                "question_answer_similarity": [
                    2.9882559925317764,
                    5.939623814076185,
                    0.15108995977789164
                ],
                "word_count_noun_chunks": [
                    142.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    25.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    53900000.0,
                    53800000.0,
                    7160000.0
                ],
                "result_count": [
                    1300000.0,
                    31400.0,
                    3.0
                ],
                "answer_relation_to_question": [
                    1.7668458781362006,
                    1.4185099846390168,
                    0.8146441372247823
                ],
                "word_count_appended": [
                    387.0,
                    47.0,
                    12.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "In 2008, the creators of what startup raised funds by selling novelty breakfast cereals?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "airbnb"
            ],
            "question": "in 2008, the creators of what startup raised funds by selling novelty breakfast cereals?",
            "lines": [
                [
                    0.38059783353901,
                    0.6666666666666666,
                    0.3099223490234653,
                    0.07122009089231564,
                    0.26627834408154777,
                    0.3896511172089377,
                    0.2737916541579105,
                    0.2882130789107533,
                    0.3501488095238095,
                    0.4166666666666667,
                    0.40594059405940597,
                    0.3950977536088884,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.3708791208791209,
                    0.3333333333333333,
                    0.5900828642672418,
                    -0.12668885745008668,
                    0.726024547534845,
                    0.4155233241865935,
                    0.7145001501050735,
                    0.7056251242297754,
                    0.47693452380952384,
                    0.5,
                    0.5643564356435643,
                    0.40672366082134304,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.24852304558186913,
                    0.0,
                    0.09999478670929286,
                    1.055468766557771,
                    0.007697108383607239,
                    0.19482555860446885,
                    0.011708195737015911,
                    0.006161796859471278,
                    0.17291666666666666,
                    0.08333333333333333,
                    0.0297029702970297,
                    0.19817858556976858,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "kickstarter",
                "airbnb",
                "cards against humanity"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "kickstarter": 0.660110702076757,
                "airbnb": 1.0024182292667956,
                "cards against humanity": 0.18128109274653612
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.5558797824799955,
                    3.660512947392087,
                    1.7836072701279173
                ],
                "result_count_important_words": [
                    912.0,
                    2380.0,
                    39.0
                ],
                "wikipedia_search": [
                    2.801190476190476,
                    3.8154761904761907,
                    1.3833333333333333
                ],
                "word_count_appended_bing": [
                    41.0,
                    57.0,
                    3.0
                ],
                "answer_relation_to_question_bing": [
                    2.0,
                    1.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.1109529361128807,
                    0.21125106513500214,
                    0.03579837083816528
                ],
                "result_count_noun_chunks": [
                    1450.0,
                    3550.0,
                    31.0
                ],
                "question_answer_similarity": [
                    0.7047234084457159,
                    -1.2535873278975487,
                    10.44387247134
                ],
                "word_count_noun_chunks": [
                    0.0,
                    41.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    17.0,
                    0.0
                ],
                "result_count_bing": [
                    99400.0,
                    106000.0,
                    49700.0
                ],
                "result_count": [
                    1280.0,
                    3490.0,
                    37.0
                ],
                "answer_relation_to_question": [
                    2.66418483477307,
                    2.5961538461538463,
                    1.7396613190730839
                ],
                "word_count_appended": [
                    100.0,
                    120.0,
                    20.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "In the nursery rhyme, Mary had a little what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "lamb"
            ],
            "question": "in the nursery rhyme, mary had a little what?",
            "lines": [
                [
                    0.25,
                    0.0,
                    0.022840176283750943,
                    0.005561237031837518,
                    0.003362885967594008,
                    0.0671271696776479,
                    0.0026413229795099217,
                    0.0037614633721348672,
                    0.05,
                    0.1173131504257332,
                    0.1222707423580786,
                    0.13701430379928511,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.6041666666666666,
                    1.0,
                    0.9644348052204147,
                    0.5085583068644152,
                    0.42494649954142466,
                    0.06411618845200141,
                    0.6039052539757096,
                    0.5227411332876251,
                    0.95,
                    0.6915799432355724,
                    0.43231441048034935,
                    0.46848999847697737,
                    0.946257197696737,
                    0.9017341040462428,
                    1.0
                ],
                [
                    0.14583333333333331,
                    0.0,
                    0.012725018495834364,
                    0.4858804561037473,
                    0.5716906144909814,
                    0.8687566418703507,
                    0.3934534230447805,
                    0.4734974033402401,
                    0.0,
                    0.19110690633869443,
                    0.44541484716157204,
                    0.3944956977237375,
                    0.053742802303262956,
                    0.09826589595375723,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "laryngitis",
                "lamb",
                "log"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "lamb": 0.8016615088759368,
                "log": 0.06051463147983009,
                "laryngitis": -0.006403349412823333
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.5480572151971405,
                    1.8739599939079095,
                    1.57798279089495
                ],
                "result_count_important_words": [
                    8660.0,
                    1980000.0,
                    1290000.0
                ],
                "wikipedia_search": [
                    0.2,
                    3.8,
                    0.0
                ],
                "word_count_appended_bing": [
                    28.0,
                    99.0,
                    102.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    1.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.009241589345037937,
                    0.3902294933795929,
                    0.005148795433342457
                ],
                "result_count_noun_chunks": [
                    9930.0,
                    1380000.0,
                    1250000.0
                ],
                "question_answer_similarity": [
                    0.02328207530081272,
                    2.1290753707289696,
                    2.0341347260400653
                ],
                "word_count_noun_chunks": [
                    0.0,
                    493.0,
                    28.0
                ],
                "word_count_raw": [
                    0.0,
                    156.0,
                    17.0
                ],
                "result_count_bing": [
                    758000.0,
                    724000.0,
                    9810000.0
                ],
                "word_count_appended": [
                    124.0,
                    731.0,
                    202.0
                ],
                "answer_relation_to_question": [
                    0.5,
                    1.2083333333333333,
                    0.29166666666666663
                ],
                "result_count": [
                    11000.0,
                    1390000.0,
                    1870000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which Beatle shares a last name with two U.S. presidents?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "george"
            ],
            "question": "which beatle shares a last name with two u.s. presidents?",
            "lines": [
                [
                    0.16666666666666666,
                    0.14058355437665784,
                    0.44184060657572327,
                    0.46694267605248413,
                    0.5168031629483197,
                    0.3337722185648453,
                    0.40617283950617283,
                    0.5258964143426295,
                    0.0,
                    0.27432885906040266,
                    0.1350210970464135,
                    0.2759756648180922,
                    0.4,
                    0.0,
                    -1.0
                ],
                [
                    0.5,
                    0.1149425287356322,
                    0.3440789194609361,
                    0.5264097626936268,
                    0.43208133295679185,
                    0.3337722185648453,
                    0.4345679012345679,
                    0.3399734395750332,
                    0.0,
                    0.3162751677852349,
                    0.16033755274261605,
                    0.3959418821430792,
                    0.6,
                    1.0,
                    -1.0
                ],
                [
                    0.3333333333333333,
                    0.74447391688771,
                    0.21408047396334062,
                    0.0066475612538889955,
                    0.05111550409488845,
                    0.3324555628703094,
                    0.15925925925925927,
                    0.1341301460823373,
                    1.0,
                    0.40939597315436244,
                    0.7046413502109705,
                    0.32808245303882866,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "paul",
                "george",
                "ringo"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "ringo": 0.3686108201017437,
                "paul": 0.23896485658545896,
                "george": 0.8427219586611471
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1039026592723689,
                    1.5837675285723167,
                    1.3123298121553146
                ],
                "result_count_important_words": [
                    3290000.0,
                    3520000.0,
                    1290000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_appended_bing": [
                    32.0,
                    38.0,
                    167.0
                ],
                "answer_relation_to_question_bing": [
                    0.4217506631299735,
                    0.3448275862068966,
                    2.23342175066313
                ],
                "cosine_similarity_raw": [
                    0.08781041204929352,
                    0.06838147342205048,
                    0.042545873671770096
                ],
                "result_count_noun_chunks": [
                    39600000.0,
                    25600000.0,
                    10100000.0
                ],
                "question_answer_similarity": [
                    1.8400910291820765,
                    2.0744342543184757,
                    0.026196187362074852
                ],
                "word_count_noun_chunks": [
                    4.0,
                    6.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    3.0,
                    0.0
                ],
                "result_count_bing": [
                    5070000.0,
                    5070000.0,
                    5050000.0
                ],
                "word_count_appended": [
                    327.0,
                    377.0,
                    488.0
                ],
                "answer_relation_to_question": [
                    0.5,
                    1.5,
                    1.0
                ],
                "result_count": [
                    18300000.0,
                    15300000.0,
                    1810000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these bank robbers was NEVER the FBI's \u201cPublic Enemy No. 1\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "clyde barrow",
                "middle"
            ],
            "question": "which of these bank robbers was never the fbi's \u201cpublic enemy no. 1\u201d?",
            "lines": [
                [
                    0.23348159110587435,
                    0.3392700455880625,
                    0.37829044543772844,
                    0.2556375911620318,
                    0.0790622802797617,
                    0.39653846153846156,
                    0.3377218088151116,
                    0.21314663555036953,
                    0.2779162331350079,
                    0.26666666666666666,
                    0.08333333333333331,
                    0.3133644081043192,
                    0.16666666666666669,
                    0.4285714285714286,
                    -1.0
                ],
                [
                    0.3827633359740095,
                    0.32463782530287977,
                    0.4048243309480496,
                    0.5126832610232283,
                    0.49948192280649817,
                    0.16,
                    0.3279908414424728,
                    0.49915726695189944,
                    0.29169063302949483,
                    0.41944444444444445,
                    0.4722222222222222,
                    0.36825290051935655,
                    0.5,
                    0.4285714285714286,
                    -1.0
                ],
                [
                    0.3837550729201161,
                    0.3360921291090577,
                    0.21688522361422197,
                    0.23167914781474008,
                    0.42145579691374013,
                    0.4434615384615385,
                    0.3342873497424156,
                    0.2876960974977311,
                    0.4303931338354973,
                    0.3138888888888889,
                    0.4444444444444444,
                    0.31838269137632424,
                    0.33333333333333337,
                    0.14285714285714285,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "baby face nelson",
                "clyde barrow",
                "pretty boy floyd"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "clyde barrow": 0.7933539170007389,
                "pretty boy floyd": 0.1013894763928607,
                "baby face nelson": 0.08296131104606798
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.2396271027481696,
                    1.5809651937677214,
                    2.179407703484109
                ],
                "result_count_important_words": [
                    56700.0,
                    60100.0,
                    57900.0
                ],
                "wikipedia_search": [
                    2.6650052023799056,
                    2.499712403646062,
                    0.835282393974032
                ],
                "word_count_appended_bing": [
                    30.0,
                    2.0,
                    4.0
                ],
                "answer_relation_to_question_bing": [
                    1.9287594529432497,
                    2.1043460963654432,
                    1.9668944506913069
                ],
                "cosine_similarity_raw": [
                    0.0791594460606575,
                    0.06190190464258194,
                    0.18413680791854858
                ],
                "result_count_noun_chunks": [
                    17700.0,
                    52.0,
                    13100.0
                ],
                "question_answer_similarity": [
                    10.978502730658874,
                    -0.5698225698433816,
                    12.054886929690838
                ],
                "word_count_noun_chunks": [
                    4.0,
                    0.0,
                    2.0
                ],
                "result_count_bing": [
                    26900.0,
                    88400.0,
                    14700.0
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    5.0
                ],
                "result_count": [
                    45500.0,
                    56.0,
                    8490.0
                ],
                "answer_relation_to_question": [
                    3.1982209067295075,
                    1.406839968311886,
                    1.3949391249586065
                ],
                "word_count_appended": [
                    84.0,
                    29.0,
                    67.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these outfits can be defined as a \u201cCanadian tuxedo\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "jean jacket / jeans"
            ],
            "lines": [
                [
                    0.0,
                    0.3333333333333333,
                    0.1577529001161312,
                    0.2512944300771902,
                    0.0,
                    0.25715746421267893,
                    0.9999867189263892,
                    0.3289902280130293,
                    0.0,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.25929686618587655,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.36904761904761907,
                    0.3333333333333333,
                    0.1467306923909904,
                    0.46840606948299485,
                    0.0,
                    0.27044989775051126,
                    0.0,
                    0.3322475570032573,
                    0.1111111111111111,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.29170897445911115,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.6309523809523809,
                    0.3333333333333333,
                    0.6955164074928785,
                    0.28029950043981494,
                    1.0,
                    0.4723926380368098,
                    1.3281073610741108e-05,
                    0.33876221498371334,
                    0.8888888888888888,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.44899415935501225,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "cotton t-shirt / shorts": 0.24914182677046628,
                "jean jacket / jeans": 0.4796516226019257,
                "wool sweater / slacks": 0.27120655062760796
            },
            "question": "which of these outfits can be defined as a \u201ccanadian tuxedo\u201d?",
            "rate_limited": false,
            "answers": [
                "wool sweater / slacks",
                "cotton t-shirt / shorts",
                "jean jacket / jeans"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "cotton t-shirt / shorts": 0.10103074982006632,
                "jean jacket / jeans": 0.47217137453320795,
                "wool sweater / slacks": 0.1017591768970551
            },
            "integer_answers": {
                "cotton t-shirt / shorts": 1,
                "jean jacket / jeans": 7,
                "wool sweater / slacks": 4
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.0371874647435062,
                    1.1668358978364446,
                    1.795976637420049
                ],
                "result_count_important_words": [
                    1280000.0,
                    0,
                    17.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.3333333333333333,
                    2.6666666666666665
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "cosine_similarity_raw": [
                    0.04592807963490486,
                    0.04271908104419708,
                    0.20249220728874207
                ],
                "result_count_noun_chunks": [
                    1010000.0,
                    1020000.0,
                    1040000.0
                ],
                "question_answer_similarity": [
                    8.091569856274873,
                    15.082468923646957,
                    9.025520334020257
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    50300.0,
                    52900.0,
                    92400.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    3.0,
                    3.0,
                    3.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    1.4761904761904763,
                    2.5238095238095237
                ],
                "result_count": [
                    0,
                    0,
                    13.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What makeup item often contains dried cochineal bugs as an ingredient?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "lipstick"
            ],
            "lines": [
                [
                    0.3391568092205035,
                    0.36101626016260163,
                    0.22502266078615404,
                    0.3281592940631894,
                    0.3304964539007092,
                    0.30303030303030304,
                    0.11180954928447238,
                    0.11073431589930872,
                    0.1989795918367347,
                    0.2612859097127223,
                    0.3157894736842105,
                    0.33898237930573166,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.32438580527752503,
                    0.23430894308943093,
                    0.10328677617987099,
                    0.2816521449245784,
                    0.20851063829787234,
                    0.25170068027210885,
                    0.07235522268289421,
                    0.07147515325420634,
                    0.07312925170068027,
                    0.2079343365253078,
                    0.23684210526315788,
                    0.2853409199353945,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.33645738550197146,
                    0.4046747967479675,
                    0.671690563033975,
                    0.39018856101223226,
                    0.46099290780141844,
                    0.4452690166975881,
                    0.8158352280326334,
                    0.8177905308464849,
                    0.7278911564625851,
                    0.53077975376197,
                    0.4473684210526316,
                    0.37567670075887394,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "lipstick": 0.6017582158364523,
                "eyeliner": 0.1679229983859305,
                "mascara": 0.23031878577761722
            },
            "question": "what makeup item often contains dried cochineal bugs as an ingredient?",
            "rate_limited": false,
            "answers": [
                "mascara",
                "eyeliner",
                "lipstick"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "lipstick": 0.7745553692078216,
                "eyeliner": 0.13918375967050825,
                "mascara": 0.3221183380254922
            },
            "integer_answers": {
                "lipstick": 13,
                "eyeliner": 0,
                "mascara": 1
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.3728766551401215,
                    1.9973864395477614,
                    2.6297369053121176
                ],
                "result_count_important_words": [
                    8360.0,
                    5410.0,
                    61000.0
                ],
                "wikipedia_search": [
                    1.3928571428571428,
                    0.5119047619047619,
                    5.095238095238096
                ],
                "answer_relation_to_question": [
                    1.356627236882014,
                    1.2975432211101001,
                    1.3458295420078858
                ],
                "word_count_appended_bing": [
                    36.0,
                    27.0,
                    51.0
                ],
                "answer_relation_to_question_bing": [
                    1.4440650406504063,
                    0.9372357723577236,
                    1.6186991869918699
                ],
                "cosine_similarity_raw": [
                    0.040849216282367706,
                    0.018750039860606194,
                    0.12193453311920166
                ],
                "result_count_noun_chunks": [
                    8490.0,
                    5480.0,
                    62700.0
                ],
                "question_answer_similarity": [
                    2.2088891826570034,
                    1.8958426211029291,
                    2.6264174357056618
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    63.0
                ],
                "result_count_bing": [
                    49000.0,
                    40700.0,
                    72000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    18.0
                ],
                "result_count": [
                    4660.0,
                    2940.0,
                    6500.0
                ],
                "word_count_appended": [
                    191.0,
                    152.0,
                    388.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What hometown city of Charlie Parker is one of the birthplaces of bebop jazz?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "kansas city"
            ],
            "question": "what hometown city of charlie parker is one of the birthplaces of bebop jazz?",
            "lines": [
                [
                    0.4339672048253693,
                    0.8954545454545455,
                    0.8451846749910547,
                    0.43385302626831684,
                    0.206401766004415,
                    0.2032967032967033,
                    0.07038012796386903,
                    0.00575496763471856,
                    0.1103154305200341,
                    0.2515923566878981,
                    0.13333333333333333,
                    0.3489433549716021,
                    0.0,
                    1.0,
                    1.0
                ],
                [
                    0.023837902264600714,
                    0.03532467532467533,
                    0.06539288952921621,
                    0.18164803002731356,
                    0.44885945548197204,
                    0.43223443223443225,
                    0.15305482373604315,
                    0.012515259383879936,
                    0.3438192668371697,
                    0.5700636942675159,
                    0.6444444444444445,
                    0.33512818515512777,
                    0.4375,
                    0.0,
                    1.0
                ],
                [
                    0.54219489291003,
                    0.06922077922077922,
                    0.08942243547972908,
                    0.3844989437043696,
                    0.34473877851361295,
                    0.36446886446886445,
                    0.7765650483000878,
                    0.9817297729814015,
                    0.5458653026427963,
                    0.17834394904458598,
                    0.2222222222222222,
                    0.31592845987327,
                    0.5625,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "kansas city",
                "chicago",
                "new orleans"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "new orleans": -0.014127702469694869,
                "kansas city": 0.9033343746189233,
                "chicago": 0.06411613802007338
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.4426034848012153,
                    2.345897296085895,
                    2.2114992191128904
                ],
                "result_count_important_words": [
                    5610.0,
                    12200.0,
                    61900.0
                ],
                "wikipedia_search": [
                    0.22063086104006818,
                    0.6876385336743392,
                    1.0917306052855924
                ],
                "word_count_appended_bing": [
                    6.0,
                    29.0,
                    10.0
                ],
                "answer_relation_to_question_bing": [
                    4.4772727272727275,
                    0.17662337662337663,
                    0.34610389610389614
                ],
                "cosine_similarity_raw": [
                    0.4267655909061432,
                    0.03301933407783508,
                    0.045152757316827774
                ],
                "result_count_noun_chunks": [
                    5610.0,
                    12200.0,
                    957000.0
                ],
                "question_answer_similarity": [
                    8.147160571068525,
                    3.4110991014167666,
                    7.220359071157873
                ],
                "word_count_noun_chunks": [
                    0.0,
                    7.0,
                    9.0
                ],
                "result_count_bing": [
                    11100.0,
                    23600.0,
                    19900.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    5610.0,
                    12200.0,
                    9370.0
                ],
                "answer_relation_to_question": [
                    1.301901614476108,
                    0.07151370679380215,
                    1.62658467873009
                ],
                "word_count_appended": [
                    79.0,
                    179.0,
                    56.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Who is the director of \u201cTyler Perry\u2019s Madea\u2019s Family Reunion\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "tyler perry"
            ],
            "lines": [
                [
                    0.8729718354029918,
                    0.7275362318840579,
                    0.9789312606346264,
                    0.4348694138685192,
                    0.9997500624843789,
                    0.15185264510601842,
                    0.2843866171003718,
                    0.5833333333333334,
                    0.6392543859649122,
                    0.8474025974025974,
                    0.8103448275862069,
                    0.7318589772296638,
                    0.9950980392156863,
                    1.0,
                    0.0
                ],
                [
                    0.09968588322246857,
                    0.18550724637681157,
                    0.011959257828212074,
                    0.3171647754190037,
                    0.00014996250937265683,
                    0.3769543799528807,
                    0.30111524163568776,
                    0.26282051282051283,
                    0.15021929824561403,
                    0.07467532467532467,
                    0.05172413793103448,
                    0.13724151622085706,
                    0.0,
                    0.0,
                    0.0
                ],
                [
                    0.027342281374539434,
                    0.08695652173913043,
                    0.009109481537161467,
                    0.24796581071247714,
                    9.997500624843789e-05,
                    0.47119297494110085,
                    0.4144981412639405,
                    0.15384615384615385,
                    0.21052631578947367,
                    0.07792207792207792,
                    0.13793103448275862,
                    0.13089950654947913,
                    0.004901960784313725,
                    0.0,
                    0.0
                ]
            ],
            "fraction_answers": {
                "george lucas": 0.14065839548841289,
                "abraham lincoln": 0.14094230256777537,
                "tyler perry": 0.7183993019438117
            },
            "question": "who is the director of \u201ctyler perry\u2019s madea\u2019s family reunion\u201d?",
            "rate_limited": false,
            "answers": [
                "tyler perry",
                "george lucas",
                "abraham lincoln"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "george lucas": 0.05963498183813205,
                "abraham lincoln": 0.09569297740300124,
                "tyler perry": 0.8648556707336332
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    4.3911538633779825,
                    0.8234490973251424,
                    0.7853970392968748
                ],
                "result_count_important_words": [
                    153000.0,
                    162000.0,
                    223000.0
                ],
                "wikipedia_search": [
                    3.8355263157894735,
                    0.9013157894736842,
                    1.263157894736842
                ],
                "word_count_appended_bing": [
                    47.0,
                    3.0,
                    8.0
                ],
                "answer_relation_to_question_bing": [
                    3.6376811594202896,
                    0.9275362318840579,
                    0.43478260869565216
                ],
                "cosine_similarity_raw": [
                    0.7948856949806213,
                    0.009710838086903095,
                    0.007396838627755642
                ],
                "result_count_noun_chunks": [
                    91.0,
                    41.0,
                    24.0
                ],
                "question_answer_similarity": [
                    5.762457792647183,
                    4.202752765268087,
                    3.2857967764139175
                ],
                "word_count_noun_chunks": [
                    203.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    53.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    709000.0,
                    1760000.0,
                    2200000.0
                ],
                "result_count": [
                    260000.0,
                    39.0,
                    26.0
                ],
                "answer_relation_to_question": [
                    5.237831012417952,
                    0.5981152993348116,
                    0.16405368824723662
                ],
                "word_count_appended": [
                    261.0,
                    23.0,
                    24.0
                ]
            },
            "integer_answers": {
                "george lucas": 0,
                "abraham lincoln": 2,
                "tyler perry": 12
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What is the scarlet pimpernel in the classic novel of the same name?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "flower"
            ],
            "question": "what is the scarlet pimpernel in the classic novel of the same name?",
            "lines": [
                [
                    0.0,
                    0,
                    0.15022956588904404,
                    0.3933117271160782,
                    0.016682490166824902,
                    0.462952326249282,
                    0.04737516005121639,
                    0.05972903413781382,
                    0.0,
                    0.372972972972973,
                    0.2967032967032967,
                    0.31830016719241655,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.0,
                    0,
                    0.48252607896858346,
                    0.24149503745581852,
                    0.24955920249559202,
                    0.11430212521539346,
                    0.9154929577464789,
                    0.89350750254941,
                    0.0,
                    0.27387387387387385,
                    0.3076923076923077,
                    0.2626375444086541,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    1.0,
                    0,
                    0.3672443551423725,
                    0.3651932354281033,
                    0.7337583073375831,
                    0.42274554853532453,
                    0.03713188220230474,
                    0.046763463312776185,
                    1.0,
                    0.35315315315315315,
                    0.3956043956043956,
                    0.41906228839892934,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "dress",
                "dagger",
                "flower"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "dagger": 0.19573346400715566,
                "flower": 0.8229886475598878,
                "dress": 0.09379171163613181
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2732006687696662,
                    1.0505501776346164,
                    1.6762491535957174
                ],
                "result_count_important_words": [
                    148000.0,
                    2860000.0,
                    116000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_appended_bing": [
                    27.0,
                    28.0,
                    36.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.016340222209692,
                    0.05248356610536575,
                    0.03994456306099892
                ],
                "result_count_noun_chunks": [
                    123000.0,
                    1840000.0,
                    96300.0
                ],
                "question_answer_similarity": [
                    3.3082328606396914,
                    2.031268745660782,
                    3.0717219412326813
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    8.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    2.0
                ],
                "result_count_bing": [
                    806000.0,
                    199000.0,
                    736000.0
                ],
                "word_count_appended": [
                    207.0,
                    152.0,
                    196.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count": [
                    123000.0,
                    1840000.0,
                    5410000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these Guinness World Record holders weighed the most?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "heaviest watermelon",
                "top"
            ],
            "question": "which of these guinness world record holders weighed the most?",
            "lines": [
                [
                    0.3913885555729881,
                    0.18608118810493338,
                    0.33490399140602983,
                    0.18547448032281832,
                    0.47674418604651164,
                    0.30790802019068986,
                    0.48739495798319327,
                    0.44,
                    0.8318181818181818,
                    0.3392857142857143,
                    0.36363636363636365,
                    0.438397357396309,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.3491255175919233,
                    0.5319769275516713,
                    0.374968776777605,
                    0.20166051403866417,
                    0.45348837209302323,
                    0.34492428491306787,
                    0.4117647058823529,
                    0.48,
                    0.16818181818181818,
                    0.39285714285714285,
                    0.36363636363636365,
                    0.3509211155474848,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.25948592683508864,
                    0.2819418843433954,
                    0.2901272318163652,
                    0.6128650056385175,
                    0.06976744186046512,
                    0.3471676948962423,
                    0.10084033613445378,
                    0.08,
                    0.0,
                    0.26785714285714285,
                    0.2727272727272727,
                    0.2106815270562062,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "heaviest watermelon",
                "heaviest squash",
                "heaviest bag of rice"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "heaviest squash": 0.38253698636424605,
                "heaviest watermelon": 0.40836076798051096,
                "heaviest bag of rice": 0.15227494323464952
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.191986786981545,
                    1.7546055777374239,
                    1.053407635281031
                ],
                "result_count_important_words": [
                    58.0,
                    49.0,
                    12.0
                ],
                "wikipedia_search": [
                    3.327272727272727,
                    0.6727272727272727,
                    0.0
                ],
                "word_count_appended_bing": [
                    4.0,
                    4.0,
                    3.0
                ],
                "answer_relation_to_question_bing": [
                    0.7443247524197335,
                    2.127907710206685,
                    1.1277675373735816
                ],
                "cosine_similarity_raw": [
                    0.10581852495670319,
                    0.11847766488790512,
                    0.09167055785655975
                ],
                "result_count_noun_chunks": [
                    44.0,
                    48.0,
                    8.0
                ],
                "question_answer_similarity": [
                    3.5090243108570576,
                    3.8152507292106748,
                    11.594900820404291
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    54900.0,
                    61500.0,
                    61900.0
                ],
                "result_count": [
                    41.0,
                    39.0,
                    6.0
                ],
                "answer_relation_to_question": [
                    1.5655542222919523,
                    1.3965020703676931,
                    1.0379437073403546
                ],
                "word_count_appended": [
                    19.0,
                    22.0,
                    15.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Before it was discontinued, how large was a McDonald's Supersized drink?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "42oz"
            ],
            "lines": [
                [
                    0.1528801528801529,
                    0.6666666666666667,
                    0.24071388651818026,
                    0.24697548392689953,
                    0.06521739130434782,
                    0.328125,
                    0.06382978723404255,
                    0.06036745406824147,
                    0.125,
                    0.22826086956521738,
                    0.3176470588235294,
                    0.21246133046780882,
                    0.0,
                    0,
                    5.0
                ],
                [
                    0.6120666120666121,
                    0.16666666666666669,
                    0.5190978507310615,
                    0.3211479586075576,
                    0.6086956521739131,
                    0.35,
                    0.6382978723404256,
                    0.7952755905511811,
                    0.125,
                    0.43478260869565216,
                    0.36470588235294116,
                    0.4621024755821681,
                    1.0,
                    0,
                    5.0
                ],
                [
                    0.23505323505323505,
                    0.16666666666666669,
                    0.24018826275075825,
                    0.4318765574655429,
                    0.32608695652173914,
                    0.321875,
                    0.2978723404255319,
                    0.14435695538057744,
                    0.75,
                    0.33695652173913043,
                    0.3176470588235294,
                    0.3254361939500231,
                    0.0,
                    0,
                    5.0
                ]
            ],
            "fraction_answers": {
                "42oz": 0.49214147459755225,
                "76oz": 0.20831885241962209,
                "60oz": 0.29953967298282574
            },
            "question": "before it was discontinued, how large was a mcdonald's supersized drink?",
            "rate_limited": false,
            "answers": [
                "76oz",
                "42oz",
                "60oz"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "42oz": 0.4944274791058199,
                "76oz": 0.17465879337275242,
                "60oz": 0.13031897174928292
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 5
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.062306652339044,
                    2.3105123779108405,
                    1.6271809697501154
                ],
                "result_count_important_words": [
                    3.0,
                    30.0,
                    14.0
                ],
                "wikipedia_search": [
                    0.25,
                    0.25,
                    1.5
                ],
                "word_count_appended_bing": [
                    27.0,
                    31.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    1.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "cosine_similarity_raw": [
                    0.024352703243494034,
                    0.05251643806695938,
                    0.02429952658712864
                ],
                "result_count_noun_chunks": [
                    23.0,
                    303.0,
                    55.0
                ],
                "question_answer_similarity": [
                    1.8175161466933787,
                    2.363358463626355,
                    3.178220785688609
                ],
                "word_count_noun_chunks": [
                    0.0,
                    4.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    105000.0,
                    112000.0,
                    103000.0
                ],
                "result_count": [
                    3.0,
                    28.0,
                    15.0
                ],
                "answer_relation_to_question": [
                    0.45864045864045866,
                    1.8361998361998362,
                    0.7051597051597052
                ],
                "word_count_appended": [
                    21.0,
                    40.0,
                    31.0
                ]
            },
            "integer_answers": {
                "42oz": 10,
                "76oz": 1,
                "60oz": 2
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Physicist Stephen Hawking\u2019s Breakthrough Initiatives project is focused on what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "alien life",
                "top"
            ],
            "question": "physicist stephen hawking\u2019s breakthrough initiatives project is focused on what?",
            "lines": [
                [
                    0.10639730639730641,
                    0.6666666666666666,
                    0.6142678549674624,
                    0.3503101768769511,
                    0.016881827209533268,
                    0.3718791064388962,
                    0.12322695035460993,
                    0.007972958077672043,
                    0.08541666666666665,
                    0.38,
                    0.4666666666666667,
                    0.3730300542778167,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.6020875420875421,
                    0.3333333333333333,
                    0.17683893625924524,
                    0.3638593716755536,
                    0.17130089374379345,
                    0.4178712220762155,
                    0.027925531914893616,
                    0.5033618694198478,
                    0.13541666666666666,
                    0.15333333333333332,
                    0.13333333333333333,
                    0.28940423458516185,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.2915151515151515,
                    0.0,
                    0.20889320877329232,
                    0.28583045144749536,
                    0.8118172790466733,
                    0.2102496714848883,
                    0.8488475177304965,
                    0.4886651725024801,
                    0.7791666666666667,
                    0.4666666666666667,
                    0.4,
                    0.33756571113702144,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "alien life",
                "time travel",
                "artificial intelligence"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "artificial intelligence": 0.1656470318909103,
                "time travel": 0.02467605202884144,
                "alien life": 0.9455411050695367
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.611210379944717,
                    2.025829642096133,
                    2.36295997795915
                ],
                "result_count_important_words": [
                    55600.0,
                    12600.0,
                    383000.0
                ],
                "wikipedia_search": [
                    0.5125,
                    0.8125,
                    4.675
                ],
                "word_count_appended_bing": [
                    7.0,
                    2.0,
                    6.0
                ],
                "answer_relation_to_question_bing": [
                    2.0,
                    1.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.07384993135929108,
                    0.0212603397667408,
                    0.025114042684435844
                ],
                "result_count_noun_chunks": [
                    217000.0,
                    13700000.0,
                    13300000.0
                ],
                "question_answer_similarity": [
                    5.672374637797475,
                    5.891769088804722,
                    4.6282908990979195
                ],
                "word_count_noun_chunks": [
                    10.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    28300.0,
                    31800.0,
                    16000.0
                ],
                "word_count_appended": [
                    57.0,
                    23.0,
                    70.0
                ],
                "answer_relation_to_question": [
                    0.531986531986532,
                    3.0104377104377105,
                    1.4575757575757575
                ],
                "result_count": [
                    13600.0,
                    138000.0,
                    654000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Dominique Ansel is credited with creating what food craze?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "cronut"
            ],
            "lines": [
                [
                    0.22114624505928854,
                    0.23025641025641025,
                    0.021502256258065993,
                    0.47362092526786315,
                    0.18125,
                    0.2005475701574264,
                    0.0006185975157123769,
                    0.07399523592316659,
                    0.0,
                    0.05088495575221239,
                    0.06422018348623854,
                    0.19062803679023496,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.23197314762532156,
                    0.16102564102564104,
                    0.05111069014371639,
                    0.5263790747321369,
                    0.4625,
                    0.33744010951403147,
                    0.0022022071559360617,
                    0.04921190005574984,
                    0.24393939393939396,
                    0.15486725663716813,
                    0.09174311926605505,
                    0.38480744614668616,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.5468806073153899,
                    0.6087179487179488,
                    0.9273870535982176,
                    0.0,
                    0.35625,
                    0.4620123203285421,
                    0.9971791953283515,
                    0.8767928640210836,
                    0.7560606060606061,
                    0.7942477876106194,
                    0.8440366972477065,
                    0.4245645170630789,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "ramen burger": 0.19265714187441693,
                "cronut": 0.6852949712351103,
                "rainbow bagel": 0.12204788689047279
            },
            "question": "dominique ansel is credited with creating what food craze?",
            "rate_limited": false,
            "answers": [
                "rainbow bagel",
                "ramen burger",
                "cronut"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "ramen burger": 0.23871471702857464,
                "cronut": 0.7906221929744528,
                "rainbow bagel": 0.15104784383726919
            },
            "integer_answers": {
                "ramen burger": 2,
                "cronut": 12,
                "rainbow bagel": 0
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1437682207414097,
                    2.308844676880117,
                    2.5473871023784733
                ],
                "result_count_important_words": [
                    25.0,
                    89.0,
                    40300.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.9757575757575758,
                    3.0242424242424244
                ],
                "word_count_appended_bing": [
                    7.0,
                    10.0,
                    92.0
                ],
                "answer_relation_to_question_bing": [
                    1.1512820512820512,
                    0.8051282051282052,
                    3.043589743589744
                ],
                "cosine_similarity_raw": [
                    0.016849393025040627,
                    0.040050871670246124,
                    0.7267102003097534
                ],
                "result_count_noun_chunks": [
                    14600.0,
                    9710.0,
                    173000.0
                ],
                "question_answer_similarity": [
                    2.7380473613739014,
                    3.043047212995589,
                    0.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    145.0
                ],
                "result_count_bing": [
                    5860.0,
                    9860.0,
                    13500.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    77.0
                ],
                "result_count": [
                    29.0,
                    74.0,
                    57.0
                ],
                "answer_relation_to_question": [
                    1.1057312252964426,
                    1.1598657381266078,
                    2.7344030365769494
                ],
                "word_count_appended": [
                    23.0,
                    70.0,
                    359.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "How does the second verse of \u201cThe Star-Spangled Banner\u201d begin?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "on the shore"
            ],
            "lines": [
                [
                    0.1439958592132505,
                    0.22273425499231952,
                    0.3637799558175296,
                    0.22182574101272395,
                    0.003394014193150262,
                    0.21284403669724772,
                    0.01005620770083738,
                    0.002071576541181513,
                    0.5833333333333334,
                    0.03529411764705882,
                    0.1111111111111111,
                    0.0453411203301235,
                    0.0,
                    0,
                    5.0
                ],
                [
                    0.3432712215320911,
                    0.5545314900153611,
                    0.2482138644181911,
                    0.38012396174727164,
                    0.6572045664918235,
                    0.5743119266055046,
                    0.9482659733109012,
                    0.8643474533895278,
                    0.06250000000000001,
                    0.5176470588235295,
                    0.4444444444444444,
                    0.48295626749763615,
                    0.7142857142857143,
                    0,
                    5.0
                ],
                [
                    0.5127329192546585,
                    0.22273425499231952,
                    0.3880061797642793,
                    0.3980502972400044,
                    0.3394014193150262,
                    0.21284403669724772,
                    0.04167781898826139,
                    0.13358097006929065,
                    0.3541666666666667,
                    0.4470588235294118,
                    0.4444444444444444,
                    0.47170261217224035,
                    0.2857142857142857,
                    0,
                    5.0
                ]
            ],
            "fraction_answers": {
                "on the shore": 0.5224695340432305,
                "travels far": 0.15044471758383596,
                "where the foe": 0.3270857483729336
            },
            "question": "how does the second verse of \u201cthe star-spangled banner\u201d begin?",
            "rate_limited": false,
            "answers": [
                "travels far",
                "on the shore",
                "where the foe"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "on the shore": 0.46363366563711295,
                "travels far": 0.26716352298764634,
                "where the foe": 0.4620127898196733
            },
            "integer_answers": {
                "on the shore": 9,
                "travels far": 1,
                "where the foe": 3
            },
            "categorical_data": {
                "question_type": 5
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.272046721980741,
                    2.897737604985817,
                    2.8302156730334422
                ],
                "result_count_important_words": [
                    2630.0,
                    248000.0,
                    10900.0
                ],
                "wikipedia_search": [
                    2.333333333333333,
                    0.25,
                    1.4166666666666665
                ],
                "word_count_appended_bing": [
                    2.0,
                    8.0,
                    8.0
                ],
                "answer_relation_to_question_bing": [
                    0.6682027649769584,
                    1.6635944700460827,
                    0.6682027649769584
                ],
                "cosine_similarity_raw": [
                    0.05813048407435417,
                    0.039663515985012054,
                    0.062001731246709824
                ],
                "result_count_noun_chunks": [
                    29.0,
                    12100.0,
                    1870.0
                ],
                "question_answer_similarity": [
                    7.946084663271904,
                    13.616531466512242,
                    14.258675966411829
                ],
                "word_count_noun_chunks": [
                    0.0,
                    5.0,
                    2.0
                ],
                "result_count_bing": [
                    116000.0,
                    313000.0,
                    116000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    11.0,
                    2130.0,
                    1100.0
                ],
                "answer_relation_to_question": [
                    0.7199792960662525,
                    1.7163561076604554,
                    2.5636645962732922
                ],
                "word_count_appended": [
                    3.0,
                    44.0,
                    38.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these film composers most recently won an Oscar?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "hans zimmer"
            ],
            "lines": [
                [
                    0.2654353148275531,
                    0.29545454545454547,
                    0.31486712641627623,
                    1.1155468231376293,
                    0.33712574850299404,
                    0.014243690973834959,
                    0.2991869918699187,
                    0.041682313180623355,
                    0.633674007152268,
                    0.3394919168591224,
                    0.3838383838383838,
                    0.34514277117682796,
                    0.2,
                    0.25,
                    -1.0
                ],
                [
                    0.25150990007470353,
                    0.6818181818181819,
                    0.4084554500868702,
                    0.07337292286749557,
                    0.09880239520958084,
                    0.01553388037363885,
                    0.5089430894308943,
                    0.06458880961321818,
                    0.23200451722190857,
                    0.36027713625866054,
                    0.30303030303030304,
                    0.3196959462518601,
                    0.7,
                    0.75,
                    -1.0
                ],
                [
                    0.48305478509774336,
                    0.022727272727272728,
                    0.2766774234968536,
                    -0.18891974600512493,
                    0.5640718562874252,
                    0.9702224286525262,
                    0.191869918699187,
                    0.8937288772061585,
                    0.13432147562582347,
                    0.3002309468822171,
                    0.31313131313131315,
                    0.33516128257131195,
                    0.1,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "ennio morricone": 0.34540640238499837,
                "danny elfman": 0.31401984531233623,
                "hans zimmer": 0.3405737523026654
            },
            "question": "which of these film composers most recently won an oscar?",
            "rate_limited": false,
            "answers": [
                "ennio morricone",
                "hans zimmer",
                "danny elfman"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "ennio morricone": 0.0998322557733811,
                "danny elfman": 0.10022805209863705,
                "hans zimmer": 0.622159261266539
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7257138558841398,
                    1.5984797312593004,
                    1.6758064128565597
                ],
                "result_count_important_words": [
                    184000.0,
                    313000.0,
                    118000.0
                ],
                "wikipedia_search": [
                    3.16837003576134,
                    1.1600225861095428,
                    0.6716073781291173
                ],
                "word_count_appended_bing": [
                    38.0,
                    30.0,
                    31.0
                ],
                "answer_relation_to_question_bing": [
                    1.1818181818181819,
                    2.7272727272727275,
                    0.09090909090909091
                ],
                "cosine_similarity_raw": [
                    0.08479215204715729,
                    0.10999502241611481,
                    0.07450785487890244
                ],
                "result_count_noun_chunks": [
                    111000.0,
                    172000.0,
                    2380000.0
                ],
                "question_answer_similarity": [
                    -1.6189286317676306,
                    -0.10648188239429146,
                    0.27416830882430077
                ],
                "word_count_noun_chunks": [
                    2.0,
                    7.0,
                    1.0
                ],
                "word_count_raw": [
                    1.0,
                    3.0,
                    0.0
                ],
                "result_count_bing": [
                    27600.0,
                    30100.0,
                    1880000.0
                ],
                "result_count": [
                    563000.0,
                    165000.0,
                    942000.0
                ],
                "answer_relation_to_question": [
                    1.3271765741377655,
                    1.2575495003735178,
                    2.4152739254887168
                ],
                "word_count_appended": [
                    147.0,
                    156.0,
                    130.0
                ]
            },
            "integer_answers": {
                "ennio morricone": 3,
                "danny elfman": 5,
                "hans zimmer": 6
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these figure skating jumps was invented the most recently?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "salchow"
            ],
            "lines": [
                [
                    0.06541405962458594,
                    0.05555555555555556,
                    0.3071242117278146,
                    0.9895528445480137,
                    0.26724357342835325,
                    0.3878453038674033,
                    0.0306439674315322,
                    0.06243386243386243,
                    0.013447286636385369,
                    0.32857142857142857,
                    0.24,
                    0.329252106535553,
                    0.22857142857142856,
                    0.2857142857142857,
                    -1.0
                ],
                [
                    0.8212771439087229,
                    0.8333333333333334,
                    0.5475391184346996,
                    0.8631500662168717,
                    0.1244591499109188,
                    0.20994475138121546,
                    0.05151739452257587,
                    0.7671957671957672,
                    0.9759144154912742,
                    0.2985714285714286,
                    0.35428571428571426,
                    0.33779559356545236,
                    0.24285714285714285,
                    0.17142857142857143,
                    -1.0
                ],
                [
                    0.1133087964666912,
                    0.11111111111111112,
                    0.14533666983748578,
                    -0.8527029107648855,
                    0.608297276660728,
                    0.4022099447513812,
                    0.9178386380458919,
                    0.17037037037037037,
                    0.010638297872340425,
                    0.37285714285714283,
                    0.4057142857142857,
                    0.3329522998989947,
                    0.5285714285714286,
                    0.5428571428571428,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "salchow": 0.47137639936454917,
                "lutz": 0.25652642247472873,
                "axel": 0.27209717816072204
            },
            "question": "which of these figure skating jumps was invented the most recently?",
            "rate_limited": false,
            "answers": [
                "lutz",
                "salchow",
                "axel"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "salchow": 0.25311278599413306,
                "lutz": 0.03478126853699378,
                "axel": 0.20263627888869937
            },
            "integer_answers": {
                "salchow": 6,
                "lutz": 0,
                "axel": 8
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6462605326777648,
                    1.6889779678272618,
                    1.6647614994949735
                ],
                "result_count_important_words": [
                    41400.0,
                    69600.0,
                    1240000.0
                ],
                "wikipedia_search": [
                    0.053789146545541476,
                    3.903657661965097,
                    0.0425531914893617
                ],
                "answer_relation_to_question": [
                    0.3270702981229297,
                    4.106385719543614,
                    0.566543982333456
                ],
                "word_count_appended_bing": [
                    42.0,
                    62.0,
                    71.0
                ],
                "answer_relation_to_question_bing": [
                    0.16666666666666669,
                    2.5,
                    0.33333333333333337
                ],
                "cosine_similarity_raw": [
                    0.11600205302238464,
                    0.20680773258209229,
                    0.05489424616098404
                ],
                "result_count_noun_chunks": [
                    118000.0,
                    1450000.0,
                    322000.0
                ],
                "question_answer_similarity": [
                    -0.4557744115591049,
                    -0.3975550327450037,
                    0.3927432168275118
                ],
                "word_count_noun_chunks": [
                    16.0,
                    17.0,
                    37.0
                ],
                "result_count_bing": [
                    35100.0,
                    19000.0,
                    36400.0
                ],
                "word_count_raw": [
                    10.0,
                    6.0,
                    19.0
                ],
                "result_count": [
                    105000.0,
                    48900.0,
                    239000.0
                ],
                "word_count_appended": [
                    230.0,
                    209.0,
                    261.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these Muppets is green?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "kermit the frog"
            ],
            "lines": [
                [
                    0.2648981161091888,
                    0.18055555555555555,
                    0.3154793282152522,
                    0.19679137082721668,
                    0.026736454944122225,
                    0.020228671943711522,
                    0.11887382690302398,
                    0.0688723835246455,
                    0.021739130434782608,
                    0.2989010989010989,
                    0.31868131868131866,
                    0.31658512922747467,
                    0.15,
                    0.14285714285714285,
                    -1.0
                ],
                [
                    0.5757400999615533,
                    0.5555555555555556,
                    0.5649750429997504,
                    0.5033141093610392,
                    0.5785825434997878,
                    0.8870461113205177,
                    0.5495307612095933,
                    0.7224848075624578,
                    0.8478260869565217,
                    0.378021978021978,
                    0.2857142857142857,
                    0.29808539838975884,
                    0.3,
                    0.5714285714285714,
                    -1.0
                ],
                [
                    0.15936178392925798,
                    0.2638888888888889,
                    0.11954562878499733,
                    0.29989451981174414,
                    0.39468100155609,
                    0.09272521673577083,
                    0.3315954118873827,
                    0.2086428089128967,
                    0.13043478260869565,
                    0.3230769230769231,
                    0.3956043956043956,
                    0.3853294723827665,
                    0.55,
                    0.2857142857142857,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "swedish chef": 0.17437139486603817,
                "miss piggy": 0.28146393713529255,
                "kermit the frog": 0.5441646679986694
            },
            "question": "which of these muppets is green?",
            "rate_limited": false,
            "answers": [
                "swedish chef",
                "kermit the frog",
                "miss piggy"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "swedish chef": 0.11069017427532946,
                "miss piggy": 0.31756707766663667,
                "kermit the frog": 0.6853292456574969
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.6331702584549493,
                    0.5961707967795177,
                    0.770658944765533
                ],
                "result_count_important_words": [
                    1140000.0,
                    5270000.0,
                    3180000.0
                ],
                "wikipedia_search": [
                    0.043478260869565216,
                    1.6956521739130435,
                    0.2608695652173913
                ],
                "answer_relation_to_question": [
                    0.5297962322183776,
                    1.1514801999231066,
                    0.31872356785851597
                ],
                "answer_relation_to_question_bing": [
                    0.3611111111111111,
                    1.1111111111111112,
                    0.5277777777777778
                ],
                "word_count_appended": [
                    136.0,
                    172.0,
                    147.0
                ],
                "cosine_similarity_raw": [
                    0.06835941225290298,
                    0.12242121249437332,
                    0.02590365894138813
                ],
                "result_count_noun_chunks": [
                    1020000.0,
                    10700000.0,
                    3090000.0
                ],
                "question_answer_similarity": [
                    1.9475614503026009,
                    4.981088106986135,
                    2.9679299630224705
                ],
                "word_count_noun_chunks": [
                    3.0,
                    6.0,
                    11.0
                ],
                "word_count_raw": [
                    1.0,
                    4.0,
                    2.0
                ],
                "result_count_bing": [
                    80500.0,
                    3530000.0,
                    369000.0
                ],
                "result_count": [
                    189000.0,
                    4090000.0,
                    2790000.0
                ],
                "word_count_appended_bing": [
                    29.0,
                    26.0,
                    36.0
                ]
            },
            "integer_answers": {
                "swedish chef": 0,
                "miss piggy": 3,
                "kermit the frog": 11
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "The fictional religion Bokononism was invented by the author of what novel?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "slaughterhouse-five"
            ],
            "question": "the fictional religion bokononism was invented by the author of what novel?",
            "lines": [
                [
                    0.22748768472906403,
                    0.5858585858585859,
                    0.9513608096974995,
                    0.37102420879349646,
                    0.9353078721745908,
                    0.34603564843269824,
                    0.555111821086262,
                    0.8427618218740934,
                    0.5468614718614718,
                    0.8983739837398373,
                    0.9456521739130435,
                    0.5927482096307878,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.17136288998357965,
                    0.2424242424242424,
                    0.023057922814565198,
                    0.25115850222371866,
                    0.034294621979735,
                    0.2698217578365089,
                    0.025559105431309903,
                    0.009283434870902234,
                    0.15357142857142858,
                    0.056910569105691054,
                    0.03260869565217391,
                    0.20050366043882797,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.6011494252873564,
                    0.17171717171717168,
                    0.025581267487935357,
                    0.3778172889827849,
                    0.030397505845674203,
                    0.38414259373079285,
                    0.4193290734824281,
                    0.14795474325500435,
                    0.29956709956709954,
                    0.044715447154471545,
                    0.021739130434782608,
                    0.2067481299303842,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "slaughterhouse-five",
                "infinite jest",
                "animal farm"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "infinite jest": 0.11066660199059868,
                "slaughterhouse-five": 0.9407214593565053,
                "animal farm": 0.17819806499770277
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.5564892577847265,
                    1.203021962632968,
                    1.2404887795823052
                ],
                "result_count_important_words": [
                    1390.0,
                    64.0,
                    1050.0
                ],
                "wikipedia_search": [
                    2.1874458874458873,
                    0.6142857142857143,
                    1.1982683982683981
                ],
                "word_count_appended_bing": [
                    87.0,
                    3.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.7575757575757578,
                    0.7272727272727273,
                    0.5151515151515151
                ],
                "cosine_similarity_raw": [
                    0.4531952738761902,
                    0.010983994230628014,
                    0.012186028063297272
                ],
                "result_count_noun_chunks": [
                    5810.0,
                    64.0,
                    1020.0
                ],
                "question_answer_similarity": [
                    5.173544262186624,
                    3.5021424405276775,
                    5.2682666555047035
                ],
                "word_count_noun_chunks": [
                    8.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    56300.0,
                    43900.0,
                    62500.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    1200.0,
                    44.0,
                    39.0
                ],
                "answer_relation_to_question": [
                    1.13743842364532,
                    0.8568144499178982,
                    3.005747126436782
                ],
                "word_count_appended": [
                    221.0,
                    14.0,
                    11.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What TV series derived from a nearly 20-year-old Michael Crichton screenplay?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "er"
            ],
            "lines": [
                [
                    0.2524350217877544,
                    0.3227938580460184,
                    0.1716308156937272,
                    0.7845441344844274,
                    0.06854043392504931,
                    0.20309477756286268,
                    0.06966134409545534,
                    0.08008008008008008,
                    0.022727272727272728,
                    0.016645326504481434,
                    0.006557377049180328,
                    0.32053674433006074,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.22441502912076156,
                    0.11452333247069935,
                    0.1277338981324469,
                    -0.0476483362648449,
                    0.17702169625246547,
                    0.3984526112185687,
                    0.06878399719248991,
                    0.17917917917917917,
                    0.30612627286125815,
                    0.07554417413572344,
                    0.029508196721311476,
                    0.26042323818957336,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.523149949091484,
                    0.5626828094832822,
                    0.7006352861738259,
                    0.2631042017804175,
                    0.7544378698224852,
                    0.3984526112185687,
                    0.8615546587120547,
                    0.7407407407407407,
                    0.6711464544114691,
                    0.9078104993597952,
                    0.9639344262295082,
                    0.41904001748036585,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "numb3rs": 0.13671880637211667,
                "the expanse": 0.16566051330616932,
                "er": 0.6976206803217141
            },
            "question": "what tv series derived from a nearly 20-year-old michael crichton screenplay?",
            "rate_limited": false,
            "answers": [
                "the expanse",
                "numb3rs",
                "er"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "numb3rs": 0.1560818417605178,
                "the expanse": 0.18959469099313875,
                "er": 0.7906221929744528
            },
            "integer_answers": {
                "numb3rs": 1,
                "the expanse": 1,
                "er": 12
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.884830698970547,
                    2.34380914370616,
                    3.7713601573232927
                ],
                "result_count_important_words": [
                    3970.0,
                    3920.0,
                    49100.0
                ],
                "wikipedia_search": [
                    0.09090909090909091,
                    1.2245050914450326,
                    2.6845858176458766
                ],
                "word_count_appended_bing": [
                    6.0,
                    27.0,
                    882.0
                ],
                "answer_relation_to_question_bing": [
                    1.9367631482761105,
                    0.6871399948241961,
                    3.3760968568996934
                ],
                "cosine_similarity_raw": [
                    0.04533408582210541,
                    0.033739276230335236,
                    0.18506385385990143
                ],
                "result_count_noun_chunks": [
                    16000.0,
                    35800.0,
                    148000.0
                ],
                "question_answer_similarity": [
                    5.145800360478461,
                    -0.3125239424407482,
                    1.72569220373407
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    1055.0
                ],
                "result_count_bing": [
                    10500.0,
                    20600.0,
                    20600.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    398.0
                ],
                "result_count": [
                    13900.0,
                    35900.0,
                    153000.0
                ],
                "answer_relation_to_question": [
                    1.262175108938772,
                    1.1220751456038078,
                    2.6157497454574203
                ],
                "word_count_appended": [
                    26.0,
                    118.0,
                    1418.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "A galleon is an old-fashioned type of what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ship"
            ],
            "question": "a galleon is an old-fashioned type of what?",
            "lines": [
                [
                    0.43934240362811794,
                    0.36821592481969845,
                    0.84974907499836,
                    0.3712400826918309,
                    0.5321707757065545,
                    0.34961154273029965,
                    0.37596302003081666,
                    0.37689218412109976,
                    0.8754371805219262,
                    0.5420375865479723,
                    0.319047619047619,
                    0.3747576222947109,
                    0.9829545454545454,
                    1.0,
                    1.0
                ],
                [
                    0.28174603174603174,
                    0.34656152108982297,
                    0.0838082970204986,
                    0.25055296224738,
                    0.07095610342754059,
                    0.3490566037735849,
                    0.23266563944530047,
                    0.26475131294408405,
                    0.07586763518966909,
                    0.18397626112759644,
                    0.2,
                    0.3028953870765316,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.2789115646258503,
                    0.2852225540904786,
                    0.06644262798114133,
                    0.3782069550607891,
                    0.396873120865905,
                    0.3013318534961154,
                    0.3913713405238829,
                    0.3583565029348162,
                    0.04869518428840463,
                    0.27398615232443124,
                    0.48095238095238096,
                    0.3223469906287576,
                    0.017045454545454544,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "ship",
                "measurement",
                "dress"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "ship": 0.74558389370299,
                "dress": 0.05342586257201182,
                "measurement": 0.029569025996440675
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.4990304891788435,
                    1.2115815483061263,
                    1.2893879625150304
                ],
                "result_count_important_words": [
                    122000.0,
                    75500.0,
                    127000.0
                ],
                "wikipedia_search": [
                    2.6263115415657787,
                    0.22760290556900725,
                    0.14608555286521388
                ],
                "word_count_appended_bing": [
                    67.0,
                    42.0,
                    101.0
                ],
                "answer_relation_to_question_bing": [
                    0.7364318496393969,
                    0.6931230421796459,
                    0.5704451081809572
                ],
                "cosine_similarity_raw": [
                    0.34923872351646423,
                    0.034444406628608704,
                    0.027307283133268356
                ],
                "result_count_noun_chunks": [
                    122000.0,
                    85700.0,
                    116000.0
                ],
                "question_answer_similarity": [
                    2.444509470835328,
                    1.6498193964362144,
                    2.490384327247739
                ],
                "word_count_noun_chunks": [
                    173.0,
                    0.0,
                    3.0
                ],
                "word_count_raw": [
                    60.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    6300000.0,
                    6290000.0,
                    5430000.0
                ],
                "result_count": [
                    177000.0,
                    23600.0,
                    132000.0
                ],
                "answer_relation_to_question": [
                    0.8786848072562359,
                    0.5634920634920635,
                    0.5578231292517006
                ],
                "word_count_appended": [
                    548.0,
                    186.0,
                    277.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "The damage in which movie could be assessed with the Torino Scale?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "deep impact"
            ],
            "question": "the damage in which movie could be assessed with the torino scale?",
            "lines": [
                [
                    0.06410256410256411,
                    0.3106060606060606,
                    0.09356693267743052,
                    0.05046446729326038,
                    0.9076691913771426,
                    0.4471830985915493,
                    0.9065544499266487,
                    0.9572218305939969,
                    0.6249163655827646,
                    0.6783216783216783,
                    0.7288135593220338,
                    0.30566380841347884,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.5160256410256411,
                    0.3560606060606061,
                    0.05500891346759525,
                    0.4232057184923963,
                    0.0009505994909693048,
                    0.289906103286385,
                    0.0009524947092134721,
                    0.005936259414536415,
                    0.13953488372093023,
                    0.16083916083916083,
                    0.0847457627118644,
                    0.2517493697161931,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.4198717948717949,
                    0.3333333333333333,
                    0.8514241538549743,
                    0.5263298142143433,
                    0.09138020913188802,
                    0.26291079812206575,
                    0.09249305536413781,
                    0.03684190999146663,
                    0.23554875069630518,
                    0.16083916083916083,
                    0.1864406779661017,
                    0.442586821870328,
                    1.0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "twister",
                "dante's peak",
                "deep impact"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "deep impact": 0.8271689327472612,
                "twister": 0.1794883014782716,
                "dante's peak": 0.08925866311201541
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.5283190420673942,
                    1.2587468485809656,
                    2.21293410935164
                ],
                "result_count_important_words": [
                    296000.0,
                    311.0,
                    30200.0
                ],
                "wikipedia_search": [
                    1.8747490967482938,
                    0.4186046511627907,
                    0.7066462520889155
                ],
                "word_count_appended_bing": [
                    43.0,
                    5.0,
                    11.0
                ],
                "answer_relation_to_question_bing": [
                    0.6212121212121212,
                    0.7121212121212122,
                    0.6666666666666666
                ],
                "cosine_similarity_raw": [
                    0.04634547233581543,
                    0.027246955782175064,
                    0.4217264950275421
                ],
                "result_count_noun_chunks": [
                    258000.0,
                    1600.0,
                    9930.0
                ],
                "question_answer_similarity": [
                    0.78478649770841,
                    6.581385902594775,
                    8.185096438974142
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    7.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    381000.0,
                    247000.0,
                    224000.0
                ],
                "word_count_appended": [
                    97.0,
                    23.0,
                    23.0
                ],
                "answer_relation_to_question": [
                    0.19230769230769232,
                    1.5480769230769231,
                    1.2596153846153846
                ],
                "result_count": [
                    296000.0,
                    310.0,
                    29800.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What is traditional to wear on St. Patrick\u2019s Day?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "the color green"
            ],
            "question": "what is traditional to wear on st. patrick\u2019s day?",
            "lines": [
                [
                    0.20990117856491758,
                    0.29089481946624807,
                    0.06899467363956008,
                    0.3231328848914997,
                    0.00033944971429649047,
                    0.32242990654205606,
                    5.184589917453922e-05,
                    0.0001790936258707425,
                    0.20133590237636478,
                    0.19166666666666668,
                    0.5909090909090909,
                    0.164586120770076,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.5585858829585304,
                    0.2878335949764521,
                    0.9134622081200283,
                    0.5915622986763912,
                    0.9994908254285553,
                    0.3348909657320872,
                    0.9998851983661136,
                    0.9997166578456373,
                    0.7986640976236352,
                    0.7083333333333334,
                    0.3181818181818182,
                    0.7443155707658646,
                    0,
                    1.0,
                    1.0
                ],
                [
                    0.23151293847655205,
                    0.4212715855572998,
                    0.017543118240411573,
                    0.08530481643210902,
                    0.00016972485714824523,
                    0.3426791277258567,
                    6.295573471194048e-05,
                    0.00010424852849192475,
                    0.0,
                    0.1,
                    0.09090909090909091,
                    0.09109830846405935,
                    0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "fancy gown",
                "the color green",
                "minions onesie"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "minions onesie": 0.06682221350830742,
                "the color green": 0.8699547118908977,
                "fancy gown": 0.1659074719021409
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.8229306038503801,
                    3.721577853829323,
                    0.45549154232029676
                ],
                "result_count_important_words": [
                    28.0,
                    540000.0,
                    34.0
                ],
                "wikipedia_search": [
                    1.006679511881824,
                    3.993320488118176,
                    0.0
                ],
                "word_count_appended_bing": [
                    13.0,
                    7.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.5817896389324961,
                    0.5756671899529042,
                    0.8425431711145996
                ],
                "cosine_similarity_raw": [
                    0.018519550561904907,
                    0.2451915293931961,
                    0.004708923865109682
                ],
                "result_count_noun_chunks": [
                    67.0,
                    374000.0,
                    39.0
                ],
                "question_answer_similarity": [
                    5.814212366938591,
                    10.644131233915687,
                    1.5349113069241866
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    2070000.0,
                    2150000.0,
                    2200000.0
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count": [
                    36.0,
                    106000.0,
                    18.0
                ],
                "answer_relation_to_question": [
                    1.049505892824588,
                    2.792929414792652,
                    1.1575646923827603
                ],
                "word_count_appended": [
                    23.0,
                    85.0,
                    12.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "In which state is happy hour currently banned?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "illinois"
            ],
            "lines": [
                [
                    0.4535248672139063,
                    0.27708333333333335,
                    0.6031405784280088,
                    0.25970209133061684,
                    0.40414507772020725,
                    0.3800183879865155,
                    0.41920374707259955,
                    0.3855507868383405,
                    0.39244089834515367,
                    0.5191873589164786,
                    0.5619047619047619,
                    0.3805536582073918,
                    1.0,
                    0.75,
                    -1.0
                ],
                [
                    0.30583051665861904,
                    0.3319444444444445,
                    0.07892177834667964,
                    0.23225752984005626,
                    0.4450777202072539,
                    0.43518234753294516,
                    0.5199063231850117,
                    0.38412017167381973,
                    0.2851418439716312,
                    0.3905191873589165,
                    0.3142857142857143,
                    0.2732887798068684,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.24064461612747465,
                    0.3909722222222222,
                    0.31793764322531154,
                    0.5080403788293268,
                    0.15077720207253886,
                    0.18479926448053938,
                    0.06088992974238876,
                    0.23032904148783978,
                    0.32241725768321516,
                    0.09029345372460497,
                    0.12380952380952381,
                    0.34615756198573977,
                    0.0,
                    0.25,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "arizona": 0.2854625969508543,
                "illinois": 0.48474682480695097,
                "rhode island": 0.22979057824219468
            },
            "question": "in which state is happy hour currently banned?",
            "rate_limited": false,
            "answers": [
                "illinois",
                "arizona",
                "rhode island"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "arizona": 0.1599056350514332,
                "illinois": 0.7745553692078216,
                "rhode island": 0.34373755675368045
            },
            "integer_answers": {
                "arizona": 3,
                "illinois": 9,
                "rhode island": 2
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.902768291036959,
                    1.366443899034342,
                    1.7307878099286989
                ],
                "result_count_important_words": [
                    1790000.0,
                    2220000.0,
                    260000.0
                ],
                "wikipedia_search": [
                    1.5697635933806147,
                    1.140567375886525,
                    1.2896690307328607
                ],
                "answer_relation_to_question": [
                    0.9070497344278126,
                    0.6116610333172381,
                    0.4812892322549493
                ],
                "word_count_appended_bing": [
                    59.0,
                    33.0,
                    13.0
                ],
                "answer_relation_to_question_bing": [
                    0.5541666666666667,
                    0.663888888888889,
                    0.7819444444444444
                ],
                "cosine_similarity_raw": [
                    0.24079617857933044,
                    0.031508512794971466,
                    0.1269325464963913
                ],
                "result_count_noun_chunks": [
                    5390000.0,
                    5370000.0,
                    3220000.0
                ],
                "question_answer_similarity": [
                    1.4304169341921806,
                    1.2792546339333057,
                    2.798243007622659
                ],
                "word_count_noun_chunks": [
                    31.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    1240000.0,
                    1420000.0,
                    603000.0
                ],
                "word_count_raw": [
                    6.0,
                    0.0,
                    2.0
                ],
                "result_count": [
                    780000.0,
                    859000.0,
                    291000.0
                ],
                "word_count_appended": [
                    460.0,
                    346.0,
                    80.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Who wrote a #1 hit song for the Monkees?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "james taylor"
            ],
            "lines": [
                [
                    0.24800136225357855,
                    0.2977935613958844,
                    0.5125856982477803,
                    0.4592827845818843,
                    0.3333333333333333,
                    0.33270142180094786,
                    0.24833702882483372,
                    0.3333333333333333,
                    0.5372058867787953,
                    0,
                    0,
                    0.3336143901068016,
                    0,
                    0,
                    0.0
                ],
                [
                    0.35061644633631217,
                    0.35315643909999556,
                    0.14416734866222453,
                    0.3607110767257043,
                    0.3333333333333333,
                    0.33364928909952607,
                    0.5964523281596452,
                    0.3333333333333333,
                    0.2949306756646522,
                    0,
                    0,
                    0.3326306913996627,
                    0,
                    0,
                    0.0
                ],
                [
                    0.40138219141010933,
                    0.3490499995041199,
                    0.3432469530899951,
                    0.1800061386924114,
                    0.3333333333333333,
                    0.33364928909952607,
                    0.15521064301552107,
                    0.3333333333333333,
                    0.16786343755655248,
                    0,
                    0,
                    0.3337549184935357,
                    0,
                    0,
                    0.0
                ]
            ],
            "fraction_answers": {
                "neil diamond": 0.34329809618143897,
                "james taylor": 0.3636188800657173,
                "jackson browne": 0.2930830237528438
            },
            "question": "who wrote a #1 hit song for the monkees?",
            "rate_limited": false,
            "answers": [
                "james taylor",
                "neil diamond",
                "jackson browne"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "neil diamond": 0.10190819660860118,
                "james taylor": 0.3330912675920115,
                "jackson browne": 0.11226047315010274
            },
            "integer_answers": {
                "neil diamond": 3,
                "james taylor": 5,
                "jackson browne": 2
            },
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3344575604272062,
                    1.3305227655986507,
                    1.3350196739741427
                ],
                "result_count_important_words": [
                    112000.0,
                    269000.0,
                    70000.0
                ],
                "wikipedia_search": [
                    2.1488235471151813,
                    1.1797227026586088,
                    0.6714537502262099
                ],
                "word_count_appended_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "answer_relation_to_question_bing": [
                    0.8933806841876533,
                    1.0594693172999867,
                    1.0471499985123598
                ],
                "cosine_similarity_raw": [
                    0.055027689784765244,
                    0.01547681912779808,
                    0.03684864193201065
                ],
                "result_count_noun_chunks": [
                    7280000000.0,
                    7280000000.0,
                    7280000000.0
                ],
                "question_answer_similarity": [
                    3.3408411890268326,
                    2.623826676979661,
                    1.309371791430749
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    35100000.0,
                    35200000.0,
                    35200000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    65100000.0,
                    65100000.0,
                    65100000.0
                ],
                "answer_relation_to_question": [
                    0.9920054490143142,
                    1.4024657853452487,
                    1.6055287656404373
                ],
                "word_count_appended": [
                    0.0,
                    0.0,
                    0.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Anne of Green Gables literally means Anne of what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "green pastures"
            ],
            "lines": [
                [
                    0.47450420168067226,
                    0.32885530366638677,
                    0.5141846846509843,
                    0.29398007696491074,
                    0.5529411764705883,
                    0.33042635658914726,
                    0.6147540983606558,
                    0.22439331862590609,
                    0.5,
                    0.46511627906976744,
                    0.36585365853658536,
                    0.5083634718075225,
                    0,
                    0,
                    1.0
                ],
                [
                    0.2872268907563026,
                    0.32213825916596694,
                    0.3051330210084451,
                    0.31345494110442695,
                    0.047058823529411764,
                    0.3313953488372093,
                    0.03278688524590164,
                    0.0003151591553734636,
                    0.07446808510638298,
                    0.06976744186046512,
                    0.3170731707317073,
                    0.1351712241593088,
                    0,
                    0,
                    1.0
                ],
                [
                    0.2382689075630252,
                    0.34900643716764623,
                    0.18068229434057054,
                    0.3925649819306623,
                    0.4,
                    0.3381782945736434,
                    0.3524590163934426,
                    0.7752915222187204,
                    0.425531914893617,
                    0.46511627906976744,
                    0.3170731707317073,
                    0.3564653040331687,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "green pastures": 0.43111438553526055,
                "green walls": 0.3825531769096642,
                "green jars": 0.18633243755507514
            },
            "question": "anne of green gables literally means anne of what?",
            "rate_limited": false,
            "answers": [
                "green pastures",
                "green jars",
                "green walls"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "green pastures": 0.5061159046529368,
                "green walls": 0.3619938809295726,
                "green jars": 0.309172930751728
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.050180830845135,
                    0.8110273449558527,
                    2.138791824199012
                ],
                "result_count_important_words": [
                    75.0,
                    4.0,
                    43.0
                ],
                "wikipedia_search": [
                    2.0,
                    0.2978723404255319,
                    1.702127659574468
                ],
                "word_count_appended_bing": [
                    15.0,
                    13.0,
                    13.0
                ],
                "answer_relation_to_question_bing": [
                    0.9865659109991602,
                    0.9664147774979008,
                    1.0470193115029387
                ],
                "cosine_similarity_raw": [
                    0.05005403608083725,
                    0.02970360592007637,
                    0.017588773742318153
                ],
                "result_count_noun_chunks": [
                    17800.0,
                    25.0,
                    61500.0
                ],
                "question_answer_similarity": [
                    4.595996670424938,
                    4.900460876524448,
                    6.137243613600731
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    341000.0,
                    342000.0,
                    349000.0
                ],
                "word_count_appended": [
                    20.0,
                    3.0,
                    20.0
                ],
                "answer_relation_to_question": [
                    1.8980168067226888,
                    1.1489075630252101,
                    0.9530756302521007
                ],
                "result_count": [
                    47.0,
                    4.0,
                    34.0
                ]
            },
            "integer_answers": {
                "green pastures": 8,
                "green walls": 4,
                "green jars": 0
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these modes of transportation has only one wheel?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "bus"
            ],
            "lines": [
                [
                    0.08870967741935484,
                    0.0,
                    0.036324522614702086,
                    0.6515017310824474,
                    0.30093457943925234,
                    0.1325898389095415,
                    0.020722084960548337,
                    0.3081081081081081,
                    0.09574468085106383,
                    0.11617312072892938,
                    0.018691588785046728,
                    0.26991804524045726,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.42699490662139217,
                    0.5,
                    0.5526133784043324,
                    0.33917109880097696,
                    0.4672897196261682,
                    0.7001239157372986,
                    0.9703514784410616,
                    0.4720720720720721,
                    0.5425531914893617,
                    0.39635535307517084,
                    0.411214953271028,
                    0.3701184386789127,
                    0.9259259259259259,
                    0.46153846153846156,
                    -1.0
                ],
                [
                    0.48429541595925296,
                    0.5,
                    0.4110620989809654,
                    0.009327170116575717,
                    0.23177570093457944,
                    0.16728624535315986,
                    0.008926436598390054,
                    0.21981981981981982,
                    0.3617021276595745,
                    0.4874715261958998,
                    0.5700934579439252,
                    0.35996351608063004,
                    0.07407407407407407,
                    0.5384615384615384,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "unicycle": 0.316018509155599,
                "bus": 0.5383087781201545,
                "monster truck": 0.14567271272424653
            },
            "question": "which of these modes of transportation has only one wheel?",
            "rate_limited": false,
            "answers": [
                "monster truck",
                "bus",
                "unicycle"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "unicycle": 0.7260559555895089,
                "bus": 0.8324687186410895,
                "monster truck": 0.044372874260536724
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.8097541357213718,
                    1.110355316036738,
                    1.0798905482418901
                ],
                "result_count_important_words": [
                    1040000.0,
                    48700000.0,
                    448000.0
                ],
                "wikipedia_search": [
                    0.19148936170212766,
                    1.0851063829787233,
                    0.723404255319149
                ],
                "word_count_appended_bing": [
                    2.0,
                    44.0,
                    61.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    1.0,
                    1.0
                ],
                "cosine_similarity_raw": [
                    0.006783303339034319,
                    0.10319596529006958,
                    0.07676243782043457
                ],
                "result_count_noun_chunks": [
                    1710000.0,
                    2620000.0,
                    1220000.0
                ],
                "question_answer_similarity": [
                    5.47492903470993,
                    2.8502421528100967,
                    0.07838136423379183
                ],
                "word_count_noun_chunks": [
                    0.0,
                    25.0,
                    2.0
                ],
                "word_count_raw": [
                    0.0,
                    6.0,
                    7.0
                ],
                "result_count_bing": [
                    428000.0,
                    2260000.0,
                    540000.0
                ],
                "word_count_appended": [
                    102.0,
                    348.0,
                    428.0
                ],
                "answer_relation_to_question": [
                    0.1774193548387097,
                    0.8539898132427843,
                    0.9685908319185059
                ],
                "result_count": [
                    1610000.0,
                    2500000.0,
                    1240000.0
                ]
            },
            "integer_answers": {
                "unicycle": 4,
                "bus": 9,
                "monster truck": 1
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these is NOT a machine used for printing?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "hydraulophone"
            ],
            "lines": [
                [
                    0.4982014388489209,
                    0.5,
                    0.45385086093595184,
                    0.5,
                    0.4894996110967073,
                    0.33333333333333337,
                    0.4937595795927305,
                    0.46811195228263364,
                    0.5,
                    0.3304794520547945,
                    0.364406779661017,
                    0.41345403780827406,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.35322991606714627,
                    0.3235632183908046,
                    0.29731279112378983,
                    0.5,
                    0.041958344136202586,
                    0.33333333333333337,
                    0.09710970002189623,
                    0.10082587749483829,
                    0.30978260869565216,
                    0.261986301369863,
                    0.2542372881355932,
                    0.27521275249180194,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.14856864508393286,
                    0.1764367816091954,
                    0.24883634794025833,
                    0.0,
                    0.46854204476709016,
                    0.33333333333333337,
                    0.40913072038537335,
                    0.43106217022252813,
                    0.19021739130434784,
                    0.4075342465753425,
                    0.3813559322033898,
                    0.31133320969992406,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "spirit duplicator": 0.4156081961458807,
                "hectograph": 0.47524131145651305,
                "hydraulophone": 0.10915049239760616
            },
            "question": "which of these is not a machine used for printing?",
            "rate_limited": false,
            "answers": [
                "hydraulophone",
                "hectograph",
                "spirit duplicator"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "spirit duplicator": 0.035669042525297276,
                "hectograph": 0.04424620419912512,
                "hydraulophone": 0.2017875957928535
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.3461838487669039,
                    0.8991489900327923,
                    0.7546671612003037
                ],
                "result_count_important_words": [
                    2850.0,
                    184000.0,
                    41500.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.7608695652173914,
                    1.2391304347826086
                ],
                "word_count_appended_bing": [
                    32.0,
                    58.0,
                    28.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.7057471264367816,
                    1.2942528735632184
                ],
                "cosine_similarity_raw": [
                    0.02854781597852707,
                    0.12538212537765503,
                    0.1553696095943451
                ],
                "result_count_noun_chunks": [
                    27800.0,
                    348000.0,
                    60100.0
                ],
                "question_answer_similarity": [
                    0.0,
                    0.0,
                    3.38682275544852
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    78900000.0,
                    78900000.0,
                    78900000.0
                ],
                "word_count_appended": [
                    198.0,
                    278.0,
                    108.0
                ],
                "answer_relation_to_question": [
                    0.007194244604316547,
                    0.5870803357314149,
                    1.4057254196642686
                ],
                "result_count": [
                    2430.0,
                    106000.0,
                    7280.0
                ]
            },
            "integer_answers": {
                "spirit duplicator": 5,
                "hectograph": 6,
                "hydraulophone": 1
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these countries is closest to the International Date Line?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "japan"
            ],
            "lines": [
                [
                    0.32908212010919013,
                    0.17856300141576217,
                    0.5359196305173226,
                    0.4010674837634104,
                    0.43844856661045534,
                    0.33116619260463226,
                    0.3206724782067248,
                    0.19943555973659455,
                    0.32252483171803087,
                    0.31831255992329816,
                    0.3333333333333333,
                    0.3468759052836018,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.31242569002123144,
                    0.5297703319175713,
                    0.29885548316194366,
                    0.2642045304237973,
                    0.19856661045531196,
                    0.33441690369768384,
                    0.29140722291407223,
                    0.4590780809031044,
                    0.37791367531208453,
                    0.3221476510067114,
                    0.3333333333333333,
                    0.32939995651990345,
                    0.5,
                    0,
                    -1.0
                ],
                [
                    0.35849218986957837,
                    0.2916666666666667,
                    0.1652248863207337,
                    0.3347279858127923,
                    0.36298482293423273,
                    0.33441690369768384,
                    0.387920298879203,
                    0.34148635936030103,
                    0.2995614929698846,
                    0.3595397890699904,
                    0.3333333333333333,
                    0.32372413819649476,
                    0.5,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "brazil": 0.3501168822820576,
                "japan": 0.31195397409402736,
                "spain": 0.33792914362391496
            },
            "question": "which of these countries is closest to the international date line?",
            "rate_limited": false,
            "answers": [
                "japan",
                "brazil",
                "spain"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "brazil": 0.19459574406323787,
                "japan": 0.3069154346179429,
                "spain": 0.1805632185619136
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7343795264180089,
                    1.6469997825995173,
                    1.6186206909824739
                ],
                "result_count_important_words": [
                    5150000.0,
                    4680000.0,
                    6230000.0
                ],
                "wikipedia_search": [
                    1.2900993268721235,
                    1.5116547012483381,
                    1.1982459718795384
                ],
                "word_count_appended_bing": [
                    29.0,
                    29.0,
                    29.0
                ],
                "answer_relation_to_question_bing": [
                    0.7142520056630486,
                    2.1190813276702847,
                    1.1666666666666665
                ],
                "cosine_similarity_raw": [
                    0.07343737035989761,
                    0.04095233604311943,
                    0.022640859708189964
                ],
                "result_count_noun_chunks": [
                    2120000.0,
                    4880000.0,
                    3630000.0
                ],
                "question_answer_similarity": [
                    1.9795766919851303,
                    1.3040526881814003,
                    1.6521402150392532
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    81500000.0,
                    82300000.0,
                    82300000.0
                ],
                "word_count_appended": [
                    332.0,
                    336.0,
                    375.0
                ],
                "answer_relation_to_question": [
                    1.3163284804367605,
                    1.2497027600849258,
                    1.4339687594783135
                ],
                "result_count": [
                    10400000.0,
                    4710000.0,
                    8610000.0
                ]
            },
            "integer_answers": {
                "brazil": 5,
                "japan": 5,
                "spain": 3
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Craig Newmark founded a website named what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "craigslist.org"
            ],
            "question": "craig newmark founded a website named what?",
            "lines": [
                [
                    0.29493868450390187,
                    0.048534798534798536,
                    0.22971943288145613,
                    0.0,
                    0.00014564520827264782,
                    0.0267639902676399,
                    0.00015584820384945063,
                    0.0002557544757033248,
                    0.0,
                    0.0798611111111111,
                    0.313953488372093,
                    0.08109542522154009,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.35956150130063175,
                    0.8846153846153846,
                    0.502044597902243,
                    1.0,
                    0.9976696766676376,
                    0.6621195803916876,
                    0.997428504636484,
                    0.9974424552429667,
                    1.0,
                    0.75,
                    0.37209302325581395,
                    0.7292436020603329,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.3454998141954664,
                    0.06684981684981685,
                    0.26823596921630094,
                    0.0,
                    0.0021846781240897175,
                    0.3111164293406725,
                    0.002415647159666485,
                    0.002301790281329923,
                    0.0,
                    0.1701388888888889,
                    0.313953488372093,
                    0.18966097271812699,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "craigscatalog.com",
                "craigslist.org",
                "gmail.craig"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "craigscatalog.com": 0.048396264489166174,
                "craigslist.org": 0.9112359716307091,
                "gmail.craig": 0.03877577090313743
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.4054771261077005,
                    3.6462180103016646,
                    0.9483048635906349
                ],
                "result_count_important_words": [
                    2.0,
                    12800.0,
                    31.0
                ],
                "wikipedia_search": [
                    0.0,
                    3.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    27.0,
                    32.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    0.19413919413919414,
                    3.5384615384615383,
                    0.2673992673992674
                ],
                "cosine_similarity_raw": [
                    0.09922666102647781,
                    0.21685674786567688,
                    0.11586377024650574
                ],
                "result_count_noun_chunks": [
                    2.0,
                    7800.0,
                    18.0
                ],
                "question_answer_similarity": [
                    0.0,
                    0.22004786285106093,
                    0.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    13.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    4.0,
                    0.0
                ],
                "result_count_bing": [
                    67100.0,
                    1660000.0,
                    780000.0
                ],
                "word_count_appended": [
                    23.0,
                    216.0,
                    49.0
                ],
                "answer_relation_to_question": [
                    1.4746934225195094,
                    1.7978075065031587,
                    1.727499070977332
                ],
                "result_count": [
                    2.0,
                    13700.0,
                    30.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these can be counted in order to determine temperature?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "cricket chirps"
            ],
            "lines": [
                [
                    0.08223684210526316,
                    0.14575645756457564,
                    0.018364230661508534,
                    0.3812330599813103,
                    0.0,
                    0.33122119815668205,
                    0.0,
                    0.0,
                    0.5844155844155844,
                    0.047058823529411764,
                    0.2222222222222222,
                    0.11955960579465527,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.12512693371359432,
                    0.15867158671586715,
                    0.07294710411280471,
                    0.5356193886262302,
                    0.0,
                    0.33410138248847926,
                    0.0,
                    0.0,
                    0.0,
                    0.047058823529411764,
                    0.2222222222222222,
                    0.1124792590375718,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.7926362241811425,
                    0.6955719557195572,
                    0.9086886652256868,
                    0.08314755139245948,
                    1.0,
                    0.3346774193548387,
                    1.0,
                    1.0,
                    0.4155844155844156,
                    0.9058823529411765,
                    0.5555555555555556,
                    0.7679611351677729,
                    0,
                    1.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "rings on spiderweb": 0.1486206172639395,
                "cricket chirps": 0.7276696365478927,
                "ladybug flight time": 0.12370974618816778
            },
            "question": "which of these can be counted in order to determine temperature?",
            "rate_limited": false,
            "answers": [
                "rings on spiderweb",
                "ladybug flight time",
                "cricket chirps"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "rings on spiderweb": 0.1928544589888621,
                "cricket chirps": 0.7610576120966963,
                "ladybug flight time": 0.18009609183043274
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.4782384231786211,
                    0.4499170361502872,
                    3.0718445406710915
                ],
                "result_count_important_words": [
                    0,
                    0,
                    76400.0
                ],
                "wikipedia_search": [
                    0.5844155844155844,
                    0.0,
                    0.4155844155844156
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    5.0
                ],
                "answer_relation_to_question_bing": [
                    0.2915129151291513,
                    0.3173431734317343,
                    1.3911439114391144
                ],
                "cosine_similarity_raw": [
                    0.009538698010146618,
                    0.037889983505010605,
                    0.4719885587692261
                ],
                "result_count_noun_chunks": [
                    0,
                    0,
                    138000.0
                ],
                "question_answer_similarity": [
                    6.427035982720554,
                    9.029765372164547,
                    1.4017470171675086
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    4.0
                ],
                "result_count_bing": [
                    5750000.0,
                    5800000.0,
                    5810000.0
                ],
                "word_count_appended": [
                    4.0,
                    4.0,
                    77.0
                ],
                "answer_relation_to_question": [
                    0.32894736842105265,
                    0.5005077348543773,
                    3.17054489672457
                ],
                "result_count": [
                    0,
                    0,
                    71900.0
                ]
            },
            "integer_answers": {
                "rings on spiderweb": 1,
                "cricket chirps": 11,
                "ladybug flight time": 1
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these bottled water brands is headquartered in France?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "evian"
            ],
            "question": "which of these bottled water brands is headquartered in france?",
            "lines": [
                [
                    0.4287037373473315,
                    0.1084474885844749,
                    0.6670029075153712,
                    -0.38915296334888616,
                    0.5663915978994749,
                    0.39444306623666586,
                    0.9001161440185831,
                    0.565987833906374,
                    0.3619047619047619,
                    0.5102040816326531,
                    0.4296875,
                    0.3548631769381644,
                    0.8571428571428571,
                    1.0,
                    -1.0
                ],
                [
                    0.07291397404745903,
                    0.3578767123287671,
                    0.10981613969543488,
                    2.6099350124097516,
                    0.2726931732933233,
                    0.21111386752666833,
                    0.06090301974448316,
                    0.2581327691087014,
                    0.0,
                    0.11352040816326531,
                    0.0390625,
                    0.2756309270244439,
                    0.047619047619047616,
                    0.0,
                    -1.0
                ],
                [
                    0.4983822886052094,
                    0.533675799086758,
                    0.2231809527891939,
                    -1.2207820490608652,
                    0.1609152288072018,
                    0.39444306623666586,
                    0.0389808362369338,
                    0.17587939698492464,
                    0.638095238095238,
                    0.3762755102040816,
                    0.53125,
                    0.36950589603739165,
                    0.09523809523809523,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "evian",
                "la croix",
                "dasani"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "la croix": 0.05363553658454364,
                "dasani": 0.526000926187827,
                "evian": 0.903379503015968
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.774315884690822,
                    1.3781546351222196,
                    1.8475294801869582
                ],
                "result_count_important_words": [
                    1240000.0,
                    83900.0,
                    53700.0
                ],
                "wikipedia_search": [
                    1.0857142857142856,
                    0.0,
                    1.9142857142857141
                ],
                "word_count_appended_bing": [
                    55.0,
                    5.0,
                    68.0
                ],
                "answer_relation_to_question_bing": [
                    0.4337899543378996,
                    1.4315068493150684,
                    2.134703196347032
                ],
                "cosine_similarity_raw": [
                    0.25088149309158325,
                    0.04130542278289795,
                    0.0839456170797348
                ],
                "result_count_noun_chunks": [
                    214000.0,
                    97600.0,
                    66500.0
                ],
                "question_answer_similarity": [
                    -0.18803596124053001,
                    1.261102150194347,
                    -0.5898732572532026
                ],
                "word_count_noun_chunks": [
                    54.0,
                    3.0,
                    6.0
                ],
                "result_count_bing": [
                    1590000.0,
                    851000.0,
                    1590000.0
                ],
                "word_count_raw": [
                    3.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    151000.0,
                    72700.0,
                    42900.0
                ],
                "answer_relation_to_question": [
                    1.714814949389326,
                    0.2916558961898361,
                    1.9935291544208376
                ],
                "word_count_appended": [
                    400.0,
                    89.0,
                    295.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "The \u201cS\u201d in \u201cUSB cable\u201d shares its name with a well-known what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "podcast"
            ],
            "lines": [
                [
                    0.1111111111111111,
                    0.0,
                    0.19900551311865466,
                    0.1889586613394305,
                    0.15083043646195443,
                    0.14730878186968838,
                    0.28619528619528617,
                    0.28978622327790976,
                    0.4215686274509804,
                    0.38875305623471884,
                    0.42105263157894735,
                    0.3713543902936298,
                    0,
                    0,
                    1.0
                ],
                [
                    0.4444444444444444,
                    1.0,
                    0.4349176203265923,
                    0.22808708332736752,
                    0.8265739667825415,
                    0.2507082152974504,
                    0.47474747474747475,
                    0.3681710213776722,
                    0.5686274509803921,
                    0.5843520782396088,
                    0.5263157894736842,
                    0.4383028117197107,
                    0,
                    0,
                    1.0
                ],
                [
                    0.4444444444444444,
                    0.0,
                    0.366076866554753,
                    0.582954255333202,
                    0.022595596755504054,
                    0.6019830028328612,
                    0.23905723905723905,
                    0.342042755344418,
                    0.00980392156862745,
                    0.02689486552567237,
                    0.05263157894736842,
                    0.19034279798665948,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "sports team": 0.2399022770292291,
                "podcast": 0.5121039963930782,
                "airline": 0.2479937265776926
            },
            "question": "the \u201cs\u201d in \u201cusb cable\u201d shares its name with a well-known what?",
            "rate_limited": false,
            "answers": [
                "airline",
                "podcast",
                "sports team"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sports team": 0.11730857945046533,
                "podcast": 0.5203059655593976,
                "airline": 0.30614754159833957
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.4854175611745193,
                    1.7532112468788428,
                    0.7613711919466379
                ],
                "result_count_important_words": [
                    170000.0,
                    282000.0,
                    142000.0
                ],
                "wikipedia_search": [
                    0.4215686274509804,
                    0.5686274509803921,
                    0.00980392156862745
                ],
                "word_count_appended_bing": [
                    40.0,
                    50.0,
                    5.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    1.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.037452664226293564,
                    0.08185111731290817,
                    0.06889534741640091
                ],
                "result_count_noun_chunks": [
                    122000.0,
                    155000.0,
                    144000.0
                ],
                "question_answer_similarity": [
                    2.8294338546693325,
                    3.4153359830379486,
                    8.729054778814316
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    1040000.0,
                    1770000.0,
                    4250000.0
                ],
                "word_count_appended": [
                    159.0,
                    239.0,
                    11.0
                ],
                "answer_relation_to_question": [
                    0.3333333333333333,
                    1.3333333333333333,
                    1.3333333333333333
                ],
                "result_count": [
                    7810.0,
                    42800.0,
                    1170.0
                ]
            },
            "integer_answers": {
                "sports team": 2,
                "podcast": 10,
                "airline": 0
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What SportsCenter anchor shares their last name with Linus & Lucy from \u201cPeanuts\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "scott van pelt"
            ],
            "lines": [
                [
                    0.338162623539982,
                    0.3512639623750735,
                    0.12380415743913603,
                    0.2873738733448794,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.449438202247191,
                    0.0033981393794217593,
                    0.34523809523809523,
                    0.3333333333333333,
                    1.0,
                    0.33207371758541177,
                    0.6666666666666666,
                    0.2857142857142857,
                    1.0
                ],
                [
                    0.27560022960966357,
                    0.3724279835390946,
                    0.6296858665488221,
                    0.5281511140389037,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.34831460674157305,
                    0.2612667817948861,
                    0.19047619047619047,
                    0.3333333333333333,
                    0.0,
                    0.3339631412072941,
                    0.0,
                    0.5714285714285714,
                    1.0
                ],
                [
                    0.38623714685035443,
                    0.27630805408583187,
                    0.24650997601204183,
                    0.18447501261621685,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.20224719101123595,
                    0.7353350788256922,
                    0.4642857142857143,
                    0.3333333333333333,
                    0.0,
                    0.3339631412072941,
                    0.3333333333333333,
                    0.14285714285714285,
                    1.0
                ]
            ],
            "fraction_answers": {
                "jemele hill": 0.37022383739501025,
                "kenny mayne": 0.30753941364891846,
                "scott van pelt": 0.3222367489560714
            },
            "question": "what sportscenter anchor shares their last name with linus & lucy from \u201cpeanuts\u201d?",
            "rate_limited": false,
            "answers": [
                "jemele hill",
                "scott van pelt",
                "kenny mayne"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "jemele hill": 0.2655068640541657,
                "kenny mayne": 0.1923237100459994,
                "scott van pelt": 0.38837152559445953
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9924423055124707,
                    2.0037788472437645,
                    2.0037788472437645
                ],
                "result_count_important_words": [
                    40.0,
                    31.0,
                    18.0
                ],
                "wikipedia_search": [
                    0.6904761904761905,
                    0.38095238095238093,
                    0.9285714285714286
                ],
                "word_count_appended_bing": [
                    3.0,
                    0.0,
                    0.0
                ],
                "answer_relation_to_question_bing": [
                    1.0537918871252205,
                    1.1172839506172838,
                    0.8289241622574955
                ],
                "cosine_similarity_raw": [
                    0.07089301943778992,
                    0.3605721592903137,
                    0.1411571055650711
                ],
                "result_count_noun_chunks": [
                    61.0,
                    4690.0,
                    13200.0
                ],
                "question_answer_similarity": [
                    2.472496133297682,
                    4.544085974339396,
                    1.5871789250522852
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    2.0,
                    4.0,
                    1.0
                ],
                "result_count_bing": [
                    302000.0,
                    302000.0,
                    302000.0
                ],
                "word_count_appended": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question": [
                    1.014487870619946,
                    0.8268006888289907,
                    1.1587114405510632
                ],
                "result_count": [
                    175000.0,
                    175000.0,
                    175000.0
                ]
            },
            "integer_answers": {
                "jemele hill": 6,
                "kenny mayne": 3,
                "scott van pelt": 5
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "The phrase \u201ccul-de-sac\u201d literally means what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "bottom of the bag"
            ],
            "question": "the phrase \u201ccul-de-sac\u201d literally means what?",
            "lines": [
                [
                    0.14926977687626777,
                    0.07654723127035831,
                    0.07361482210051332,
                    0.30523139053151865,
                    0.00015481314053936897,
                    0.06382978723404255,
                    7.310832216400633e-05,
                    1.531076266405168e-06,
                    0.018336314847942754,
                    0.06716417910447761,
                    0.18181818181818182,
                    0.06972059449956544,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.4232769889565022,
                    0.47255700325732897,
                    0.468074812728049,
                    0.36794485027846807,
                    0.0028485617859243893,
                    0.884438881935753,
                    0.0007798221030827343,
                    0.9734902828188083,
                    0.5697383539130562,
                    0.40298507462686567,
                    0.18181818181818182,
                    0.3754016066949479,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.4274532341672301,
                    0.4508957654723127,
                    0.4583103651714377,
                    0.3268237591900132,
                    0.9969966250735363,
                    0.051731330830204425,
                    0.9991470695747533,
                    0.026508186104925297,
                    0.4119253312390011,
                    0.5298507462686567,
                    0.6363636363636364,
                    0.5548777988054866,
                    0,
                    1.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "top of the peak",
                "end of the line",
                "bottom of the bag"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "end of the line": 0.4667924485400464,
                "top of the peak": 0.07080338584645303,
                "bottom of the bag": 0.7651605611827943
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.41832356699739265,
                    2.252409640169687,
                    3.32926679283292
                ],
                "result_count_important_words": [
                    9.0,
                    96.0,
                    123000.0
                ],
                "wikipedia_search": [
                    0.07334525939177101,
                    2.2789534156522246,
                    1.6477013249560044
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    7.0
                ],
                "answer_relation_to_question_bing": [
                    0.30618892508143325,
                    1.8902280130293159,
                    1.8035830618892508
                ],
                "cosine_similarity_raw": [
                    0.015415973030030727,
                    0.09802141040563583,
                    0.09597659856081009
                ],
                "result_count_noun_chunks": [
                    67.0,
                    42600000.0,
                    1160000.0
                ],
                "question_answer_similarity": [
                    11.997496904339641,
                    14.462526919320226,
                    12.84621163085103
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    153000.0,
                    2120000.0,
                    124000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    4.0
                ],
                "result_count": [
                    5.0,
                    92.0,
                    32200.0
                ],
                "answer_relation_to_question": [
                    0.7463488843813388,
                    2.116384944782511,
                    2.1372661708361504
                ],
                "word_count_appended": [
                    9.0,
                    54.0,
                    71.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which action star is the creator of an eponymous martial art?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "jackie chan"
            ],
            "question": "which action star is the creator of an eponymous martial art?",
            "lines": [
                [
                    0.33700407345487354,
                    0.2732378278150396,
                    0.8441778989914636,
                    0.35171211652872936,
                    0.592823712948518,
                    0.3424479166666667,
                    0.874536005939124,
                    0.8748698110400238,
                    0.38594057278128474,
                    0.39712918660287083,
                    0.3028169014084507,
                    0.3659284239636717,
                    0.5,
                    0.6666666666666666,
                    -1.0
                ],
                [
                    0.3020731629647321,
                    0.3861071187763938,
                    0.062399278403524425,
                    0.18447908303768173,
                    0.11960478419136765,
                    0.3229166666666667,
                    0.04365256124721604,
                    0.04359470316917125,
                    0.34114313762970877,
                    0.3014354066985646,
                    0.323943661971831,
                    0.32416651359611987,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.3609227635803943,
                    0.34065505340856667,
                    0.09342282260501199,
                    0.4638088004335889,
                    0.2875715028601144,
                    0.3346354166666667,
                    0.08181143281365999,
                    0.08153548579080494,
                    0.2729162895890066,
                    0.3014354066985646,
                    0.3732394366197183,
                    0.3099050624402085,
                    0.5,
                    0.3333333333333333,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "jackie chan",
                "steven seagal",
                "chuck norris"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "steven seagal": 0.19275533160573607,
                "jackie chan": 0.8617612288769718,
                "chuck norris": 0.10442916159821351
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.19557054378203,
                    1.9449990815767193,
                    1.8594303746412508
                ],
                "result_count_important_words": [
                    58900.0,
                    2940.0,
                    5510.0
                ],
                "wikipedia_search": [
                    1.9297028639064233,
                    1.7057156881485436,
                    1.3645814479450327
                ],
                "word_count_appended_bing": [
                    43.0,
                    46.0,
                    53.0
                ],
                "answer_relation_to_question_bing": [
                    1.0929513112601583,
                    1.5444284751055752,
                    1.3626202136342667
                ],
                "cosine_similarity_raw": [
                    0.20103856921195984,
                    0.014860210940241814,
                    0.02224837988615036
                ],
                "result_count_noun_chunks": [
                    58800.0,
                    2930.0,
                    5480.0
                ],
                "question_answer_similarity": [
                    1.5002634518314153,
                    0.7869141064584255,
                    1.9784231455996633
                ],
                "word_count_noun_chunks": [
                    3.0,
                    0.0,
                    3.0
                ],
                "result_count_bing": [
                    263000.0,
                    248000.0,
                    257000.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    1.0
                ],
                "result_count": [
                    114000.0,
                    23000.0,
                    55300.0
                ],
                "answer_relation_to_question": [
                    1.6850203672743678,
                    1.5103658148236605,
                    1.8046138179019717
                ],
                "word_count_appended": [
                    83.0,
                    63.0,
                    63.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Blue spruce, red cedar & white pine are all kinds of what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "evergreen trees"
            ],
            "lines": [
                [
                    0.25914379399402443,
                    0.1629115947931279,
                    0.03544457386853066,
                    0.24184512436559077,
                    0.3342541436464088,
                    0.15117014547754587,
                    0.0010601158816325785,
                    0.048764066557660864,
                    0.298035298035298,
                    0.0,
                    0,
                    0.3333333333333333,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.05233706385780119,
                    0.11730659004178166,
                    0.05462827215823001,
                    0.13596316450215923,
                    0.3314917127071823,
                    0.6957621758380772,
                    0.0009687265814918389,
                    0.0009618159084351255,
                    0.06953046953046953,
                    0.0,
                    0,
                    0.3333333333333333,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.6885191421481744,
                    0.7197818151650904,
                    0.9099271539732393,
                    0.62219171113225,
                    0.3342541436464088,
                    0.15306767868437698,
                    0.9979711575368756,
                    0.950274117533904,
                    0.6324342324342325,
                    1.0,
                    0,
                    0.3333333333333333,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "marvel supervillains": 0.13786794803530472,
                "evergreen trees": 0.7185964988913759,
                "colgate flavors": 0.1435355530733195
            },
            "question": "blue spruce, red cedar & white pine are all kinds of what?",
            "rate_limited": false,
            "answers": [
                "colgate flavors",
                "marvel supervillains",
                "evergreen trees"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "marvel supervillains": 0.17843876458768138,
                "evergreen trees": 0.9232052724579368,
                "colgate flavors": 0.2811653746051813
            },
            "integer_answers": {
                "marvel supervillains": 1,
                "evergreen trees": 10,
                "colgate flavors": 2
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.333333333333333,
                    2.333333333333333,
                    2.333333333333333
                ],
                "result_count_important_words": [
                    58.0,
                    53.0,
                    54600.0
                ],
                "wikipedia_search": [
                    2.086247086247086,
                    0.48671328671328673,
                    4.427039627039627
                ],
                "word_count_appended_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "answer_relation_to_question_bing": [
                    1.1403811635518952,
                    0.8211461302924716,
                    5.038472706155633
                ],
                "cosine_similarity_raw": [
                    0.010836290195584297,
                    0.01670122519135475,
                    0.27818742394447327
                ],
                "result_count_noun_chunks": [
                    5070.0,
                    100.0,
                    98800.0
                ],
                "question_answer_similarity": [
                    3.01923155086115,
                    1.6973849569913,
                    7.76753655821085
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    2.0
                ],
                "result_count_bing": [
                    239000.0,
                    1100000.0,
                    242000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    3.0
                ],
                "result_count": [
                    1210000.0,
                    1200000.0,
                    1210000.0
                ],
                "answer_relation_to_question": [
                    1.8140065579581708,
                    0.36635944700460826,
                    4.81963399503722
                ],
                "word_count_appended": [
                    0.0,
                    0.0,
                    9.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these actors was a high school cheerleader?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "michael douglas"
            ],
            "lines": [
                [
                    0.3644688644688645,
                    0.2,
                    0.2447598026784328,
                    0.2644841029754693,
                    0.13405047579644186,
                    0.3338632750397456,
                    0.30392156862745096,
                    0.14762623720852205,
                    0.36481481481481487,
                    0.3973509933774834,
                    0.23529411764705882,
                    0.3191827414903692,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.21520146520146521,
                    0.275,
                    0.5605042145066768,
                    0.4041569489050635,
                    0.06495655771617707,
                    0.3338632750397456,
                    0.22450980392156863,
                    0.07062573393725885,
                    0.10555555555555556,
                    0.33112582781456956,
                    0.0392156862745098,
                    0.3153674310773668,
                    1.0,
                    0,
                    -1.0
                ],
                [
                    0.42032967032967034,
                    0.525,
                    0.19473598281489043,
                    0.3313589481194672,
                    0.800992966487381,
                    0.3322734499205087,
                    0.4715686274509804,
                    0.7817480288542191,
                    0.5296296296296297,
                    0.271523178807947,
                    0.7254901960784313,
                    0.36544982743226406,
                    0.0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "john travolta": 0.4423154235327222,
                "george clooney": 0.25460130724035795,
                "michael douglas": 0.3030832692269198
            },
            "question": "which of these actors was a high school cheerleader?",
            "rate_limited": false,
            "answers": [
                "george clooney",
                "michael douglas",
                "john travolta"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "john travolta": 0.29179235509667106,
                "george clooney": 0.21354534493052876,
                "michael douglas": 0.3146608826667013
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2767309659614767,
                    1.2614697243094672,
                    1.4617993097290563
                ],
                "result_count_important_words": [
                    310000.0,
                    229000.0,
                    481000.0
                ],
                "wikipedia_search": [
                    1.0944444444444446,
                    0.31666666666666665,
                    1.588888888888889
                ],
                "word_count_appended_bing": [
                    12.0,
                    2.0,
                    37.0
                ],
                "answer_relation_to_question_bing": [
                    0.4,
                    0.55,
                    1.05
                ],
                "cosine_similarity_raw": [
                    0.05280975252389908,
                    0.12093525379896164,
                    0.04201653599739075
                ],
                "result_count_noun_chunks": [
                    176000.0,
                    84200.0,
                    932000.0
                ],
                "question_answer_similarity": [
                    1.4658440416678786,
                    2.239949580281973,
                    1.8364829276688397
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    21000000.0,
                    21000000.0,
                    20900000.0
                ],
                "result_count": [
                    162000.0,
                    78500.0,
                    968000.0
                ],
                "answer_relation_to_question": [
                    1.0934065934065935,
                    0.6456043956043956,
                    1.260989010989011
                ],
                "word_count_appended": [
                    60.0,
                    50.0,
                    41.0
                ]
            },
            "integer_answers": {
                "john travolta": 8,
                "george clooney": 2,
                "michael douglas": 3
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "The French word for cake sounds almost exactly like the Spanish word for what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "cat"
            ],
            "question": "the french word for cake sounds almost exactly like the spanish word for what?",
            "lines": [
                [
                    0.35,
                    0.05555555555555556,
                    0.29631074701956256,
                    0.28681912328510833,
                    0.3813953488372093,
                    0.4098504837291117,
                    0.9998792938510126,
                    0.12960609911054638,
                    0.0588235294117647,
                    0.3085553997194951,
                    0.23478260869565218,
                    0.3196488170039828,
                    0.05357142857142857,
                    0.0,
                    1.0
                ],
                [
                    0.11666666666666667,
                    0.16666666666666669,
                    0.3976323272358078,
                    0.35656368739508243,
                    0.31627906976744186,
                    0.17941952506596306,
                    6.39562431201999e-05,
                    0.3303684879288437,
                    0.2156862745098039,
                    0.3267882187938289,
                    0.3391304347826087,
                    0.3489077233536778,
                    0.39285714285714285,
                    0.0,
                    1.0
                ],
                [
                    0.5333333333333333,
                    0.7777777777777778,
                    0.3060569257446296,
                    0.35661718931980924,
                    0.3023255813953488,
                    0.41072999120492526,
                    5.674990586721963e-05,
                    0.5400254129606099,
                    0.7254901960784313,
                    0.364656381486676,
                    0.4260869565217391,
                    0.33144345964233934,
                    0.5535714285714286,
                    1.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "candle",
                "car",
                "cat"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "candle": 0.01492452951901294,
                "car": 0.42206682864501155,
                "cat": 0.5934704983976854
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.2375417190278797,
                    2.4423540634757446,
                    2.3201042174963753
                ],
                "result_count_important_words": [
                    1110000.0,
                    71.0,
                    63.0
                ],
                "wikipedia_search": [
                    0.17647058823529413,
                    0.6470588235294118,
                    2.1764705882352944
                ],
                "word_count_appended_bing": [
                    27.0,
                    39.0,
                    49.0
                ],
                "answer_relation_to_question_bing": [
                    0.2222222222222222,
                    0.6666666666666666,
                    3.1111111111111107
                ],
                "cosine_similarity_raw": [
                    0.031638253480196,
                    0.04245675355195999,
                    0.032678890973329544
                ],
                "result_count_noun_chunks": [
                    1020000.0,
                    2600000.0,
                    4250000.0
                ],
                "question_answer_similarity": [
                    3.22331203520298,
                    4.0071108639240265,
                    4.007712125778198
                ],
                "word_count_noun_chunks": [
                    3.0,
                    22.0,
                    31.0
                ],
                "result_count_bing": [
                    9320000.0,
                    4080000.0,
                    9340000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    4.0
                ],
                "result_count": [
                    1640000.0,
                    1360000.0,
                    1300000.0
                ],
                "answer_relation_to_question": [
                    1.75,
                    0.5833333333333334,
                    2.6666666666666665
                ],
                "word_count_appended": [
                    220.0,
                    233.0,
                    260.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "In computing, what unit is half a byte?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "nibble"
            ],
            "lines": [
                [
                    0.34293945074614224,
                    0.2616279069767442,
                    0.6170152798188766,
                    0.5522246340505271,
                    0.46401225114854516,
                    0.34991119005328597,
                    0.4774774774774775,
                    0.9098712446351931,
                    1.0,
                    0.6594533029612756,
                    0.4722222222222222,
                    0.5017473363094983,
                    0.8571428571428571,
                    0.9012345679012346,
                    1.0
                ],
                [
                    0.23159136402630828,
                    0.11046511627906977,
                    0.014212299352627699,
                    0.0,
                    0.0,
                    0.3019538188277087,
                    0.0,
                    0.0,
                    0.0,
                    0.018223234624145785,
                    0.17222222222222222,
                    0.029609307693500425,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.42546918522754956,
                    0.627906976744186,
                    0.3687724208284957,
                    0.44777536594947287,
                    0.5359877488514548,
                    0.3481349911190053,
                    0.5225225225225225,
                    0.09012875536480687,
                    0.0,
                    0.3223234624145786,
                    0.35555555555555557,
                    0.46864335599700124,
                    0.14285714285714285,
                    0.09876543209876543,
                    1.0
                ]
            ],
            "fraction_answers": {
                "demibyte": 0.0627340973589702,
                "octet": 0.3396316368236098,
                "nibble": 0.59763426581742
            },
            "question": "in computing, what unit is half a byte?",
            "rate_limited": false,
            "answers": [
                "nibble",
                "demibyte",
                "octet"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "demibyte": 0.15104784383726919,
                "octet": 0.44646454127428176,
                "nibble": 0.7906221929744528
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.006989345237993,
                    0.1184372307740017,
                    1.874573423988005
                ],
                "result_count_important_words": [
                    1060000.0,
                    0,
                    1160000.0
                ],
                "wikipedia_search": [
                    4.0,
                    0.0,
                    0.0
                ],
                "answer_relation_to_question": [
                    1.3717578029845687,
                    0.926365456105233,
                    1.701876740910198
                ],
                "result_count": [
                    90900.0,
                    0,
                    105000.0
                ],
                "answer_relation_to_question_bing": [
                    1.0465116279069768,
                    0.4418604651162791,
                    2.511627906976744
                ],
                "cosine_similarity_raw": [
                    0.327290803194046,
                    0.007538800127804279,
                    0.195612370967865
                ],
                "result_count_noun_chunks": [
                    1060000.0,
                    0,
                    105000.0
                ],
                "question_answer_similarity": [
                    0.9009848218411207,
                    0.0,
                    0.7305701039731503
                ],
                "word_count_noun_chunks": [
                    162.0,
                    0.0,
                    27.0
                ],
                "word_count_raw": [
                    73.0,
                    0.0,
                    8.0
                ],
                "result_count_bing": [
                    1970000.0,
                    1700000.0,
                    1960000.0
                ],
                "word_count_appended": [
                    579.0,
                    16.0,
                    283.0
                ],
                "word_count_appended_bing": [
                    85.0,
                    31.0,
                    64.0
                ]
            },
            "integer_answers": {
                "demibyte": 0,
                "octet": 4,
                "nibble": 10
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "From 2000 to 2002, what MLB team's stadium was named for a company that no longer exists?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "angels"
            ],
            "question": "from 2000 to 2002, what mlb team's stadium was named for a company that no longer exists?",
            "lines": [
                [
                    0.5262709044013645,
                    0.2534270101483216,
                    0.622698751753947,
                    0.6173152987816414,
                    0.35974499089253187,
                    0.4460260972716489,
                    0.3648509763617677,
                    0.11791590493601463,
                    0.09745484400656815,
                    0.3689604685212299,
                    0.31683168316831684,
                    0.33339697042288013,
                    0.23076923076923078,
                    1.0,
                    1.0
                ],
                [
                    0.21779543500568543,
                    0.2818173302107728,
                    0.2540553375108886,
                    0.4377507347813061,
                    0.42805100182149364,
                    0.34519572953736655,
                    0.41109969167523125,
                    0.7842778793418648,
                    0.13392857142857142,
                    0.2972181551976574,
                    0.297029702970297,
                    0.33200137347146463,
                    0.6153846153846154,
                    0.0,
                    1.0
                ],
                [
                    0.25593366059295003,
                    0.46475565964090554,
                    0.1232459107351645,
                    -0.055066033562947446,
                    0.2122040072859745,
                    0.20877817319098457,
                    0.22404933196300103,
                    0.09780621572212066,
                    0.7686165845648604,
                    0.33382137628111275,
                    0.38613861386138615,
                    0.3346016561056551,
                    0.15384615384615385,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "angels",
                "rays",
                "astros"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "astros": 0.267020638755752,
                "rays": 0.23661321312711223,
                "angels": 0.8525931245698897
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.0005727338059214,
                    2.988012361243182,
                    3.0114149049508963
                ],
                "result_count_important_words": [
                    3550000.0,
                    4000000.0,
                    2180000.0
                ],
                "wikipedia_search": [
                    0.7796387520525452,
                    1.0714285714285714,
                    6.148932676518883
                ],
                "word_count_appended_bing": [
                    32.0,
                    30.0,
                    39.0
                ],
                "answer_relation_to_question_bing": [
                    1.267135050741608,
                    1.4090866510538642,
                    2.3237782982045276
                ],
                "cosine_similarity_raw": [
                    0.17927560210227966,
                    0.07314278930425644,
                    0.03548262268304825
                ],
                "result_count_noun_chunks": [
                    1290000.0,
                    8580000.0,
                    1070000.0
                ],
                "question_answer_similarity": [
                    3.3344497391954064,
                    2.364525593817234,
                    -0.29744106717407703
                ],
                "word_count_noun_chunks": [
                    3.0,
                    8.0,
                    2.0
                ],
                "result_count_bing": [
                    37600.0,
                    29100.0,
                    17600.0
                ],
                "word_count_raw": [
                    7.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    3950000.0,
                    4700000.0,
                    2330000.0
                ],
                "answer_relation_to_question": [
                    3.6838963308095516,
                    1.524568045039798,
                    1.7915356241506504
                ],
                "word_count_appended": [
                    252.0,
                    203.0,
                    228.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "In the popular comic strip, what color is Hagar the Horrible\u2019s beard?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "red"
            ],
            "question": "in the popular comic strip, what color is hagar the horrible\u2019s beard?",
            "lines": [
                [
                    0.08380281690140845,
                    0.3329911019849418,
                    0.2934721284471434,
                    0.38096408261723425,
                    0.49990959081868175,
                    0.3333333333333333,
                    0.5519288963325276,
                    0.016463281003618716,
                    0.15693043667884207,
                    0.35904255319148937,
                    0.35428571428571426,
                    0.39267374266666,
                    0.2711864406779661,
                    0.5416666666666666,
                    1.0
                ],
                [
                    0.66908953722334,
                    0.35249828884325807,
                    0.17029625279153196,
                    0.24562824606653122,
                    0.00018081836263654445,
                    0.3333333333333333,
                    0.012237595873855698,
                    1.6035663315213034e-05,
                    0.444854985553072,
                    0.125,
                    0.28,
                    0.15806328270488848,
                    0.01694915254237288,
                    0.0,
                    1.0
                ],
                [
                    0.24710764587525152,
                    0.3145106091718001,
                    0.5362316187613246,
                    0.37340767131623454,
                    0.49990959081868175,
                    0.3333333333333333,
                    0.4358335077936167,
                    0.9835206833330661,
                    0.3982145777680859,
                    0.5159574468085106,
                    0.3657142857142857,
                    0.4492629746284515,
                    0.711864406779661,
                    0.4583333333333333,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "white",
                "blond",
                "red"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "white": 0.30654450937226263,
                "red": 0.665050164530748,
                "blond": 0.13938322408443143
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.74871619866662,
                    1.1064429789342194,
                    3.1448408223991606
                ],
                "result_count_important_words": [
                    290000.0,
                    6430.0,
                    229000.0
                ],
                "wikipedia_search": [
                    0.6277217467153683,
                    1.779419942212288,
                    1.5928583110723435
                ],
                "word_count_appended_bing": [
                    62.0,
                    49.0,
                    64.0
                ],
                "answer_relation_to_question_bing": [
                    0.6659822039698836,
                    0.7049965776865161,
                    0.6290212183436003
                ],
                "cosine_similarity_raw": [
                    0.05255678668618202,
                    0.03049769625067711,
                    0.09603164345026016
                ],
                "result_count_noun_chunks": [
                    46200.0,
                    45.0,
                    2760000.0
                ],
                "question_answer_similarity": [
                    4.233473824337125,
                    2.729550626128912,
                    4.149502996355295
                ],
                "word_count_noun_chunks": [
                    16.0,
                    1.0,
                    42.0
                ],
                "result_count_bing": [
                    136000.0,
                    136000.0,
                    136000.0
                ],
                "word_count_raw": [
                    13.0,
                    0.0,
                    11.0
                ],
                "word_count_appended": [
                    135.0,
                    47.0,
                    194.0
                ],
                "answer_relation_to_question": [
                    0.3352112676056338,
                    2.67635814889336,
                    0.9884305835010061
                ],
                "result_count": [
                    141000.0,
                    51.0,
                    141000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "The son of the first NBA slam dunk champion now plays for what team?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "chicago bulls"
            ],
            "question": "the son of the first nba slam dunk champion now plays for what team?",
            "lines": [
                [
                    0.2639802954211927,
                    0.24334180573541833,
                    0.36242018169563706,
                    0.2503760371675074,
                    0.15869642224583777,
                    0.29893778452200304,
                    0.1622746185852982,
                    0.16483516483516483,
                    0.18348241212547675,
                    0.26519337016574585,
                    0.42105263157894735,
                    0.3294190354079828,
                    0.46153846153846156,
                    0.5,
                    1.0
                ],
                [
                    0.3942385492946321,
                    0.2703643904687539,
                    0.36353892952467204,
                    0.4175843282483777,
                    0.3790294013460857,
                    0.30576631259484066,
                    0.3786407766990291,
                    0.36106750392464676,
                    0.371192433517062,
                    0.3867403314917127,
                    0.2631578947368421,
                    0.3219000885808221,
                    0.38461538461538464,
                    0.0,
                    1.0
                ],
                [
                    0.34178115528417513,
                    0.4862938037958277,
                    0.2740408887796909,
                    0.3320396345841149,
                    0.4622741764080765,
                    0.3952959028831563,
                    0.4590846047156727,
                    0.4740973312401884,
                    0.44532515435746123,
                    0.34806629834254144,
                    0.3157894736842105,
                    0.3486808760111951,
                    0.15384615384615385,
                    0.5,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "atlanta hawks",
                "los angeles lakers",
                "chicago bulls"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "atlanta hawks": 0.21551626632663823,
                "los angeles lakers": -0.0017751623756384807,
                "chicago bulls": 0.4494609507359033
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.3059332478558794,
                    2.2533006200657546,
                    2.4407661320783656
                ],
                "result_count_important_words": [
                    117000.0,
                    273000.0,
                    331000.0
                ],
                "wikipedia_search": [
                    1.1008944727528605,
                    2.227154601102372,
                    2.6719509261447674
                ],
                "word_count_appended_bing": [
                    8.0,
                    5.0,
                    6.0
                ],
                "answer_relation_to_question_bing": [
                    1.2167090286770916,
                    1.3518219523437693,
                    2.4314690189791386
                ],
                "cosine_similarity_raw": [
                    0.11341878026723862,
                    0.11376889050006866,
                    0.08576063066720963
                ],
                "result_count_noun_chunks": [
                    105000.0,
                    230000.0,
                    302000.0
                ],
                "question_answer_similarity": [
                    4.11739744618535,
                    6.867113427259028,
                    5.4603434056043625
                ],
                "word_count_noun_chunks": [
                    6.0,
                    5.0,
                    2.0
                ],
                "result_count_bing": [
                    394000.0,
                    403000.0,
                    521000.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    1.0
                ],
                "result_count": [
                    89600.0,
                    214000.0,
                    261000.0
                ],
                "answer_relation_to_question": [
                    1.5838817725271563,
                    2.3654312957677925,
                    2.0506869317050507
                ],
                "word_count_appended": [
                    48.0,
                    70.0,
                    63.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Who is the only actor to appear as both a student and a guest on \u201cInside the Actors Studio\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "bradley cooper"
            ],
            "lines": [
                [
                    0.19552384824536884,
                    0.34481534090909094,
                    0.3815108602083751,
                    -0.5029677575685773,
                    0.26548672566371684,
                    0.3340961098398169,
                    0.2777161862527716,
                    0.19710144927536233,
                    0.288332481689661,
                    0.23214285714285715,
                    0.21739130434782608,
                    0.2646036711669378,
                    0.0,
                    0.0,
                    0.0
                ],
                [
                    0.5069029044799633,
                    0.14879261363636365,
                    0.2590060142601899,
                    1.9558769515532988,
                    0.336283185840708,
                    0.34096109839816935,
                    0.32483370288248337,
                    0.3507246376811594,
                    0.12284321239993186,
                    0.16071428571428573,
                    0.13043478260869565,
                    0.2836629401570631,
                    0.0,
                    0.0,
                    0.0
                ],
                [
                    0.2975732472746678,
                    0.5063920454545454,
                    0.359483125531435,
                    -0.45290919398472157,
                    0.39823008849557523,
                    0.32494279176201374,
                    0.397450110864745,
                    0.45217391304347826,
                    0.5888243059104071,
                    0.6071428571428571,
                    0.6521739130434783,
                    0.4517333886759991,
                    1.0,
                    1.0,
                    0.0
                ]
            ],
            "fraction_answers": {
                "tobey maguire": 0.35150259497230796,
                "bradley cooper": 0.4702293280867486,
                "ryan gosling": 0.1782680769409434
            },
            "question": "who is the only actor to appear as both a student and a guest on \u201cinside the actors studio\u201d?",
            "rate_limited": false,
            "answers": [
                "ryan gosling",
                "tobey maguire",
                "bradley cooper"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "tobey maguire": 0.1877323290332801,
                "bradley cooper": 0.6580514912745106,
                "ryan gosling": 0.139938983953725
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.5876220270016268,
                    1.7019776409423788,
                    2.710400332055995
                ],
                "result_count_important_words": [
                    50100.0,
                    58600.0,
                    71700.0
                ],
                "wikipedia_search": [
                    1.7299948901379663,
                    0.7370592743995912,
                    3.5329458354624426
                ],
                "word_count_appended_bing": [
                    5.0,
                    3.0,
                    15.0
                ],
                "answer_relation_to_question_bing": [
                    1.3792613636363638,
                    0.5951704545454546,
                    2.0255681818181817
                ],
                "cosine_similarity_raw": [
                    0.028064705431461334,
                    0.019053002819418907,
                    0.02644430100917816
                ],
                "result_count_noun_chunks": [
                    68000.0,
                    121000.0,
                    156000.0
                ],
                "question_answer_similarity": [
                    1.295287961140275,
                    -5.0369508396834135,
                    1.1663726305123419
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    3.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    146000.0,
                    149000.0,
                    142000.0
                ],
                "word_count_appended": [
                    13.0,
                    9.0,
                    34.0
                ],
                "answer_relation_to_question": [
                    0.9776192412268442,
                    2.534514522399817,
                    1.4878662363733388
                ],
                "result_count": [
                    30.0,
                    38.0,
                    45.0
                ]
            },
            "integer_answers": {
                "tobey maguire": 2,
                "bradley cooper": 10,
                "ryan gosling": 2
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "According to the old saying, what kind of animal can NOT change its spots?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "tiger"
            ],
            "lines": [
                [
                    0.3519868173258004,
                    0.4278692810457516,
                    0.48297840972934664,
                    0.3649204933253706,
                    0.47639653815892996,
                    0.3651741293532338,
                    0.4842488935173132,
                    0.4802057054143217,
                    0.4863121874177415,
                    0.35756302521008404,
                    0.36250000000000004,
                    0.33741115652829057,
                    0.48175182481751827,
                    0.5,
                    1.0
                ],
                [
                    0.24612366603892027,
                    0.16626143790849673,
                    0.056054565337792794,
                    0.3445936692250927,
                    0.4760031471282455,
                    0.3756218905472637,
                    0.2721947409528769,
                    0.4758393169027751,
                    0.3476472195281712,
                    0.3021008403361345,
                    0.2833333333333334,
                    0.32904961566497587,
                    0.07664233576642338,
                    0.01315789473684209,
                    1.0
                ],
                [
                    0.40188951663527933,
                    0.4058692810457516,
                    0.46096702493286057,
                    0.2904858374495367,
                    0.047600314712824554,
                    0.2592039800995025,
                    0.24355636552980997,
                    0.043954977682903185,
                    0.1660405930540873,
                    0.3403361344537815,
                    0.3541666666666667,
                    0.3335392278067337,
                    0.4416058394160584,
                    0.4868421052631579,
                    1.0
                ]
            ],
            "fraction_answers": {
                "tiger": 0.3891345907501494,
                "leopard": 0.46219661808466517,
                "zebra": 0.1486687911651854
            },
            "question": "according to the old saying, what kind of animal can not change its spots?",
            "rate_limited": false,
            "answers": [
                "zebra",
                "leopard",
                "tiger"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "tiger": 0.34629323055895384,
                "leopard": 0.11531274684001594,
                "zebra": 0.330715345108869
            },
            "integer_answers": {
                "tiger": 6,
                "leopard": 8,
                "zebra": 0
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.2762438086039327,
                    2.393305380690338,
                    2.330450810705729
                ],
                "result_count_important_words": [
                    1210000.0,
                    17500000.0,
                    19700000.0
                ],
                "wikipedia_search": [
                    0.16425375098710188,
                    1.8282333656619458,
                    4.007512883350953
                ],
                "answer_relation_to_question": [
                    1.7761581920903955,
                    3.046516007532957,
                    1.1773258003766478
                ],
                "word_count_appended_bing": [
                    33.0,
                    52.0,
                    35.0
                ],
                "answer_relation_to_question_bing": [
                    0.7213071895424836,
                    3.3373856209150325,
                    0.9413071895424837
                ],
                "cosine_similarity_raw": [
                    0.025274302810430527,
                    0.6591870188713074,
                    0.057957641780376434
                ],
                "result_count_noun_chunks": [
                    816000.0,
                    996000.0,
                    18800000.0
                ],
                "question_answer_similarity": [
                    2.0059347860515118,
                    2.3077887427061796,
                    3.111291691660881
                ],
                "word_count_noun_chunks": [
                    5.0,
                    116.0,
                    16.0
                ],
                "result_count_bing": [
                    27100000.0,
                    25000000.0,
                    48400000.0
                ],
                "word_count_raw": [
                    0.0,
                    74.0,
                    2.0
                ],
                "result_count": [
                    1200000.0,
                    1220000.0,
                    23000000.0
                ],
                "word_count_appended": [
                    339.0,
                    471.0,
                    380.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these things can be mathematically described as an oblate spheroid?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "planet earth"
            ],
            "question": "which of these things can be mathematically described as an oblate spheroid?",
            "lines": [
                [
                    1.0,
                    0,
                    0.109026103720661,
                    0.2887105842382496,
                    0.6128830519074422,
                    0.2902869757174393,
                    0.4809093558729233,
                    0.1841620626151013,
                    0.0625,
                    0.5,
                    0.25,
                    0.3511535838856043,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.0,
                    0,
                    0.06108980851504473,
                    0.08636642478004221,
                    0.008755472170106316,
                    0.293598233995585,
                    0.0002914602156805596,
                    0.00798035604665439,
                    0.1875,
                    0.07317073170731707,
                    0.25,
                    0.15397234580560698,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.0,
                    0,
                    0.8298840877642942,
                    0.6249229909817082,
                    0.3783614759224515,
                    0.41611479028697573,
                    0.5187991839113961,
                    0.8078575813382444,
                    0.75,
                    0.4268292682926829,
                    0.5,
                    0.49487407030878866,
                    1.0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "nfl football",
                "unshelled peanut",
                "planet earth"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "unshelled peanut": 0.023262627240281865,
                "planet earth": 0.7812952373161252,
                "nfl football": 0.04480687591914176
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7557679194280216,
                    0.769861729028035,
                    2.4743703515439432
                ],
                "result_count_important_words": [
                    3300.0,
                    2.0,
                    3560.0
                ],
                "wikipedia_search": [
                    0.25,
                    0.75,
                    3.0
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    4.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.03209580481052399,
                    0.01798401027917862,
                    0.24430660903453827
                ],
                "result_count_noun_chunks": [
                    1500.0,
                    65.0,
                    6580.0
                ],
                "question_answer_similarity": [
                    3.511842432373669,
                    1.0505512850359082,
                    7.6014915853738785
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    2.0
                ],
                "result_count_bing": [
                    26300.0,
                    26600.0,
                    37700.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    1960.0,
                    28.0,
                    1210.0
                ],
                "answer_relation_to_question": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    41.0,
                    6.0,
                    35.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What tech mogul became a billionaire the youngest?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "evan spiegel"
            ],
            "lines": [
                [
                    0.7347562808029812,
                    0.6233974358974359,
                    0.44254662695241587,
                    -0.03201012515269249,
                    0.008794910179640719,
                    0.24464668094218414,
                    0.010310517930682057,
                    0.06176735798016231,
                    0.4163832199546485,
                    0.42735042735042733,
                    0.5454545454545454,
                    0.3266936451626309,
                    0.1111111111111111,
                    0.3,
                    1.0
                ],
                [
                    0.14565302319990384,
                    0.16506410256410256,
                    0.2451562391435868,
                    0.4607635381042576,
                    0.966816367265469,
                    0.5182012847965739,
                    0.9649201911804491,
                    0.8295761947700632,
                    0.34268707482993194,
                    0.3803418803418803,
                    0.3939393939393939,
                    0.3547618730571236,
                    0.8888888888888888,
                    0.4,
                    1.0
                ],
                [
                    0.11959069599711504,
                    0.21153846153846154,
                    0.3122971339039973,
                    0.5712465870484349,
                    0.02438872255489022,
                    0.23715203426124196,
                    0.02476929088886885,
                    0.10865644724977457,
                    0.24092970521541948,
                    0.19230769230769232,
                    0.06060606060606061,
                    0.3185444817802455,
                    0.0,
                    0.3,
                    1.0
                ]
            ],
            "fraction_answers": {
                "mark zuckerberg": 0.504055003720116,
                "evan spiegel": 0.3015144738975838,
                "larry page": 0.19443052238230019
            },
            "question": "what tech mogul became a billionaire the youngest?",
            "rate_limited": false,
            "answers": [
                "evan spiegel",
                "mark zuckerberg",
                "larry page"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "mark zuckerberg": 0.4234840831440445,
                "evan spiegel": 0.46310304549881814,
                "larry page": 0.17930681769438886
            },
            "integer_answers": {
                "mark zuckerberg": 7,
                "evan spiegel": 6,
                "larry page": 1
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3067745806505235,
                    1.4190474922284944,
                    1.274177927120982
                ],
                "result_count_important_words": [
                    3430.0,
                    321000.0,
                    8240.0
                ],
                "wikipedia_search": [
                    1.2491496598639455,
                    1.0280612244897958,
                    0.7227891156462585
                ],
                "word_count_appended_bing": [
                    18.0,
                    13.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.8701923076923077,
                    0.4951923076923077,
                    0.6346153846153846
                ],
                "cosine_similarity_raw": [
                    0.2256964147090912,
                    0.12502837181091309,
                    0.15926986932754517
                ],
                "result_count_noun_chunks": [
                    13700.0,
                    184000.0,
                    24100.0
                ],
                "question_answer_similarity": [
                    -0.14885316602885723,
                    2.1426380281336606,
                    2.6564052049070597
                ],
                "word_count_noun_chunks": [
                    1.0,
                    8.0,
                    0.0
                ],
                "result_count_bing": [
                    45700.0,
                    96800.0,
                    44300.0
                ],
                "word_count_raw": [
                    3.0,
                    4.0,
                    3.0
                ],
                "result_count": [
                    2820.0,
                    310000.0,
                    7820.0
                ],
                "answer_relation_to_question": [
                    2.9390251232119247,
                    0.5826120927996153,
                    0.47836278398846016
                ],
                "word_count_appended": [
                    100.0,
                    89.0,
                    45.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these was the nickname of a real French monarch?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "charles the bald",
                "top"
            ],
            "question": "which of these was the nickname of a real french monarch?",
            "lines": [
                [
                    0.5353203781512605,
                    0.0,
                    0.847469567947483,
                    0.3088569703345698,
                    0.16935222772893682,
                    0.342143906020558,
                    0.2542372881355932,
                    0.00010013526207623314,
                    0.375,
                    0.801980198019802,
                    0.6363636363636364,
                    0.29174609508729965,
                    1.0,
                    0,
                    -1.0
                ],
                [
                    0.203125,
                    1.0,
                    0.09438065243888785,
                    0.36482204277905617,
                    0.8305730580529475,
                    0.3303964757709251,
                    0.4110169491525424,
                    0.9997631721579466,
                    0.375,
                    0.0594059405940594,
                    0.18181818181818182,
                    0.4187008952476107,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.2615546218487395,
                    0.0,
                    0.058149779613629146,
                    0.326320986886374,
                    7.471421811570742e-05,
                    0.3274596182085169,
                    0.3347457627118644,
                    0.00013669257997708014,
                    0.25,
                    0.13861386138613863,
                    0.18181818181818182,
                    0.2895530096650896,
                    0.0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "charles the bald",
                "charles the odd",
                "charles the rude"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "charles the rude": 0.08270061770276108,
                "charles the bald": 0.567563022612382,
                "charles the odd": 0.20550609079463092
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1669843803491986,
                    1.674803580990443,
                    1.1582120386603585
                ],
                "result_count_important_words": [
                    60.0,
                    97.0,
                    79.0
                ],
                "wikipedia_search": [
                    1.5,
                    1.5,
                    1.0
                ],
                "word_count_appended_bing": [
                    7.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    1.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.3190549910068512,
                    0.0355323888361454,
                    0.021892204880714417
                ],
                "result_count_noun_chunks": [
                    63.0,
                    629000.0,
                    86.0
                ],
                "question_answer_similarity": [
                    9.891935932449996,
                    11.684360790066421,
                    10.451265814714134
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    2330000.0,
                    2250000.0,
                    2230000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    136000.0,
                    667000.0,
                    60.0
                ],
                "answer_relation_to_question": [
                    2.141281512605042,
                    0.8125,
                    1.046218487394958
                ],
                "word_count_appended": [
                    81.0,
                    6.0,
                    14.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which actor currently stars in a show that is both one word long and pluralized?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "paul giamatti"
            ],
            "lines": [
                [
                    0.45467320261437905,
                    0.21904761904761905,
                    0.3344232318917145,
                    4.467772566598791,
                    0.09566326530612244,
                    0.3754556500607533,
                    0.010942087002935683,
                    0.20272410516312955,
                    0.30431887366818877,
                    0.3728813559322034,
                    0.34285714285714286,
                    0.3081369486488709,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.2644771241830065,
                    0.20476190476190478,
                    0.32898909592631104,
                    -3.6608373768337805,
                    0.09693877551020408,
                    0.16281895504252733,
                    0.010942087002935683,
                    0.4846373139056066,
                    0.33656773211567736,
                    0.3389830508474576,
                    0.18571428571428572,
                    0.2634938804171268,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.28084967320261434,
                    0.5761904761904763,
                    0.33658767218197444,
                    0.19306481023498973,
                    0.8073979591836735,
                    0.4617253948967193,
                    0.9781158259941286,
                    0.31263858093126384,
                    0.359113394216134,
                    0.288135593220339,
                    0.4714285714285714,
                    0.4283691709340023,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "william h. macy": 0.6240746707326542,
                "taraji p. henson": -0.08187609761722807,
                "paul giamatti": 0.457801426884574
            },
            "question": "which actor currently stars in a show that is both one word long and pluralized?",
            "rate_limited": false,
            "answers": [
                "william h. macy",
                "taraji p. henson",
                "paul giamatti"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "william h. macy": 0.18252344438126572,
                "taraji p. henson": 0.1672060788906518,
                "paul giamatti": 0.3069989951990106
            },
            "integer_answers": {
                "william h. macy": 3,
                "taraji p. henson": 1,
                "paul giamatti": 8
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.8488216918932254,
                    1.5809632825027609,
                    2.5702150256040137
                ],
                "result_count_important_words": [
                    82.0,
                    82.0,
                    7330.0
                ],
                "wikipedia_search": [
                    1.217275494672755,
                    1.3462709284627095,
                    1.436453576864536
                ],
                "word_count_appended_bing": [
                    24.0,
                    13.0,
                    33.0
                ],
                "answer_relation_to_question_bing": [
                    0.6571428571428571,
                    0.6142857142857143,
                    1.7285714285714286
                ],
                "cosine_similarity_raw": [
                    0.0314224436879158,
                    0.030911851674318314,
                    0.031625814735889435
                ],
                "result_count_noun_chunks": [
                    64000.0,
                    153000.0,
                    98700.0
                ],
                "question_answer_similarity": [
                    2.2620009689126164,
                    -1.8534555127844214,
                    0.0977473184466362
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    92700.0,
                    40200.0,
                    114000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    75.0,
                    76.0,
                    633.0
                ],
                "answer_relation_to_question": [
                    2.2733660130718953,
                    1.3223856209150326,
                    1.4042483660130718
                ],
                "word_count_appended": [
                    22.0,
                    20.0,
                    17.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "New Mexico exempts which group of people from paying state income taxes?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "registered lobbyists"
            ],
            "question": "new mexico exempts which group of people from paying state income taxes?",
            "lines": [
                [
                    0.6514820367593981,
                    0.19928030303030306,
                    0.4585537966193516,
                    0.6084988759297891,
                    0.0009277099835866696,
                    0.2728442728442728,
                    0.003223406893131664,
                    0.004267425320056899,
                    0.5150498622001857,
                    0.04316546762589928,
                    0.05714285714285714,
                    0.10487410649449141,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.20676439557998777,
                    0.3728409090909091,
                    0.19206939601758388,
                    0.011887963488703676,
                    0.899165061014772,
                    0.34491634491634493,
                    0.6496404661542277,
                    0.6448553816974869,
                    0.20149161026463167,
                    0.841726618705036,
                    0.8857142857142857,
                    0.41084792965239303,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.14175356766061414,
                    0.4278787878787879,
                    0.34937680736306453,
                    0.3796131605815072,
                    0.09990722900164133,
                    0.38223938223938225,
                    0.34713612695264073,
                    0.3508771929824561,
                    0.2834585275351826,
                    0.11510791366906475,
                    0.05714285714285714,
                    0.48427796385311567,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "hybrid car owners",
                "centenarians",
                "registered lobbyists"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "hybrid car owners": 0.32940694968852385,
                "registered lobbyists": 0.38624147345183246,
                "centenarians": 0.3053503561175735
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.9438669584504227,
                    3.697631366871537,
                    4.358501674678041
                ],
                "result_count_important_words": [
                    13.0,
                    2620.0,
                    1400.0
                ],
                "wikipedia_search": [
                    3.6053490354013,
                    1.4104412718524217,
                    1.984209692746278
                ],
                "word_count_appended_bing": [
                    2.0,
                    31.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.7971212121212121,
                    1.4913636363636362,
                    1.7115151515151514
                ],
                "cosine_similarity_raw": [
                    0.026421042159199715,
                    0.01106669194996357,
                    0.02013046108186245
                ],
                "result_count_noun_chunks": [
                    18.0,
                    2720.0,
                    1480.0
                ],
                "question_answer_similarity": [
                    9.64158707903698,
                    0.18836326524615288,
                    6.014922112226486
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    424000.0,
                    536000.0,
                    594000.0
                ],
                "word_count_appended": [
                    6.0,
                    117.0,
                    16.0
                ],
                "answer_relation_to_question": [
                    5.211856294075185,
                    1.6541151646399022,
                    1.1340285412849131
                ],
                "result_count": [
                    13.0,
                    12600.0,
                    1400.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these utensils is tined?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "fork"
            ],
            "lines": [
                [
                    0.0,
                    0.0,
                    0.25349380597795457,
                    0.32500422247505123,
                    0.057580332111278845,
                    0.8702290076335878,
                    0.4068965517241379,
                    0.01954921225217981,
                    0.8333333333333334,
                    0.4087423312883436,
                    0.4695121951219512,
                    0.3348555407597997,
                    0.3582089552238806,
                    0.6179775280898876,
                    -1.0
                ],
                [
                    1.0,
                    1.0,
                    0.5245143213330276,
                    0.35042849358865497,
                    0.46150528358852705,
                    0.09648854961832061,
                    0.28620689655172415,
                    0.9598713470864638,
                    0.16666666666666669,
                    0.27223926380368096,
                    0.18902439024390244,
                    0.32662634196251905,
                    0.04477611940298507,
                    0.14606741573033707,
                    -1.0
                ],
                [
                    0.0,
                    0.0,
                    0.22199187268901785,
                    0.3245672839362938,
                    0.48091438430019406,
                    0.033282442748091605,
                    0.30689655172413793,
                    0.020579440661356384,
                    0.0,
                    0.31901840490797545,
                    0.34146341463414637,
                    0.3385181172776812,
                    0.5970149253731343,
                    0.23595505617977527,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "fork": 0.3539559297136705,
                "spoon": 0.2300144210308432,
                "knife": 0.41602964925548636
            },
            "question": "which of these utensils is tined?",
            "rate_limited": false,
            "answers": [
                "fork",
                "knife",
                "spoon"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "fork": 0.4266555500301792,
                "spoon": 0.11395905726861419,
                "knife": 0.24686074842117361
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.6697110815195993,
                    0.6532526839250381,
                    0.6770362345553624
                ],
                "result_count_important_words": [
                    354000.0,
                    249000.0,
                    267000.0
                ],
                "wikipedia_search": [
                    1.6666666666666665,
                    0.3333333333333333,
                    0.0
                ],
                "word_count_appended_bing": [
                    77.0,
                    31.0,
                    56.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    1.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.11733929812908173,
                    0.24279150366783142,
                    0.10275742411613464
                ],
                "result_count_noun_chunks": [
                    7780000.0,
                    382000000.0,
                    8190000.0
                ],
                "question_answer_similarity": [
                    1.3906403183937073,
                    1.4994266480207443,
                    1.38877072930336
                ],
                "word_count_noun_chunks": [
                    24.0,
                    3.0,
                    40.0
                ],
                "word_count_raw": [
                    55.0,
                    13.0,
                    21.0
                ],
                "result_count_bing": [
                    2850000.0,
                    316000.0,
                    109000.0
                ],
                "word_count_appended": [
                    533.0,
                    355.0,
                    416.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count": [
                    267000.0,
                    2140000.0,
                    2230000.0
                ]
            },
            "integer_answers": {
                "fork": 6,
                "spoon": 3,
                "knife": 5
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Where is the Hershey Company headquarters located?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "pennsylvania"
            ],
            "question": "where is the hershey company headquarters located?",
            "lines": [
                [
                    0.13333333333333336,
                    0.19999999999999998,
                    0.1156583017423948,
                    -0.16885281601350965,
                    0.051601423487544484,
                    0.04058490002984184,
                    0.09905254091300603,
                    0.13996193035494345,
                    0.030303030303030304,
                    0.32386363636363635,
                    0.2692307692307692,
                    0.23733338976435586,
                    0.0,
                    0.0,
                    3.0
                ],
                [
                    0.6888888888888889,
                    0.7666666666666666,
                    0.7160531876816902,
                    1.194958885556722,
                    0.8629893238434164,
                    0.8027454491196657,
                    0.602928509905254,
                    0.8274549322584257,
                    0.7818181818181819,
                    0.40767045454545453,
                    0.47115384615384615,
                    0.47498223795604333,
                    1.0,
                    1.0,
                    3.0
                ],
                [
                    0.1777777777777778,
                    0.03333333333333333,
                    0.1682885105759149,
                    -0.02610606954321237,
                    0.08540925266903915,
                    0.1566696508504924,
                    0.2980189491817399,
                    0.03258313738663084,
                    0.1878787878787879,
                    0.2684659090909091,
                    0.25961538461538464,
                    0.28768437227960086,
                    0.0,
                    0.0,
                    3.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "transylvania",
                "pennsylvania",
                "lithuania"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "pennsylvania": 0.8137961965446759,
                "lithuania": 0.07411326805684869,
                "transylvania": 0.06379133950745564
            },
            "categorical_data": {
                "question_type": 3
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.9493335590574234,
                    1.8999289518241733,
                    1.1507374891184035
                ],
                "result_count_important_words": [
                    115000.0,
                    700000.0,
                    346000.0
                ],
                "wikipedia_search": [
                    0.09090909090909091,
                    2.3454545454545457,
                    0.5636363636363637
                ],
                "word_count_appended_bing": [
                    28.0,
                    49.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    0.6,
                    2.3,
                    0.1
                ],
                "cosine_similarity_raw": [
                    0.02039094641804695,
                    0.12624257802963257,
                    0.029669828712940216
                ],
                "result_count_noun_chunks": [
                    125000.0,
                    739000.0,
                    29100.0
                ],
                "question_answer_similarity": [
                    -0.19127713702619076,
                    1.353654146194458,
                    -0.02957305870950222
                ],
                "word_count_noun_chunks": [
                    0.0,
                    46.0,
                    0.0
                ],
                "result_count_bing": [
                    2720000.0,
                    53800000.0,
                    10500000.0
                ],
                "word_count_raw": [
                    0.0,
                    24.0,
                    0.0
                ],
                "word_count_appended": [
                    228.0,
                    287.0,
                    189.0
                ],
                "answer_relation_to_question": [
                    0.4,
                    2.0666666666666664,
                    0.5333333333333333
                ],
                "result_count": [
                    23200.0,
                    388000.0,
                    38400.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these does a plant typically need to grow?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "water"
            ],
            "question": "which of these does a plant typically need to grow?",
            "lines": [
                [
                    0.14299242424242423,
                    0.0,
                    0.1959616641951237,
                    0.4146927176758664,
                    0.018615424208630008,
                    0.12723845428840716,
                    0.18087356450639483,
                    0.04859861146824376,
                    0.36105769230769236,
                    0.0986267166042447,
                    0.07142857142857142,
                    0.2955991855001849,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.5274621212121212,
                    1.0,
                    0.44951682354639605,
                    0.30671077305885336,
                    0.5027022389980269,
                    0.6833176248821866,
                    0.8191075564346836,
                    0.7932630496271535,
                    0.6100961538461539,
                    0.4756554307116105,
                    0.6071428571428571,
                    0.43538798254717703,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.3295454545454545,
                    0.0,
                    0.3545215122584803,
                    0.27859650926528023,
                    0.47868233679334304,
                    0.18944392082940623,
                    1.8879058921542894e-05,
                    0.15813833890460272,
                    0.02884615384615385,
                    0.4257178526841448,
                    0.32142857142857145,
                    0.2690128319526381,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "diesel fuel",
                "water",
                "confidence"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "water": 0.7920869848797062,
                "confidence": 0.225744125048077,
                "diesel fuel": 0.08258256023782763
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1823967420007395,
                    1.7415519301887081,
                    1.0760513278105523
                ],
                "result_count_important_words": [
                    594000.0,
                    2690000.0,
                    62.0
                ],
                "wikipedia_search": [
                    1.4442307692307692,
                    2.440384615384615,
                    0.11538461538461539
                ],
                "word_count_appended_bing": [
                    6.0,
                    51.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    4.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.01458599790930748,
                    0.03345884755253792,
                    0.026388069614768028
                ],
                "result_count_noun_chunks": [
                    3780000.0,
                    61700000.0,
                    12300000.0
                ],
                "question_answer_similarity": [
                    5.036822967231274,
                    3.7252833247184753,
                    3.3838098347187042
                ],
                "word_count_noun_chunks": [
                    0.0,
                    8.0,
                    0.0
                ],
                "result_count_bing": [
                    13500000.0,
                    72500000.0,
                    20100000.0
                ],
                "word_count_raw": [
                    0.0,
                    2.0,
                    0.0
                ],
                "word_count_appended": [
                    79.0,
                    381.0,
                    341.0
                ],
                "answer_relation_to_question": [
                    0.4289772727272727,
                    1.5823863636363638,
                    0.9886363636363636
                ],
                "result_count": [
                    217000.0,
                    5860000.0,
                    5580000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "In which version of \u201cDragnet\u201d is the line \u201cJust the facts, ma\u2019am\u201d first said?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "'50s tv show"
            ],
            "lines": [
                [
                    0.3982233872142129,
                    0.3159420289855073,
                    0.5147012162615172,
                    0.4047773689742971,
                    0.2777777777777778,
                    0.1827768014059754,
                    0.2222222222222222,
                    0.39789997236805746,
                    0.012077294685990338,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.37486165339823874,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.1681010630551915,
                    0.3405797101449275,
                    0.2856342090534163,
                    0.2857209373967589,
                    0.16666666666666666,
                    0.22495606326889278,
                    0.18518518518518517,
                    0.41171594363083724,
                    0.8140096618357487,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3077698054527323,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.4336755497305956,
                    0.34347826086956523,
                    0.1996645746850665,
                    0.309501693628944,
                    0.5555555555555556,
                    0.5922671353251318,
                    0.5925925925925926,
                    0.1903840840011053,
                    0.17391304347826086,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3173685411490289,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "'80s movie": 0.3645889748068761,
                "'50s movie": 0.32141715936308535,
                "'50s tv show": 0.31399386583003863
            },
            "question": "in which version of \u201cdragnet\u201d is the line \u201cjust the facts, ma\u2019am\u201d first said?",
            "rate_limited": false,
            "answers": [
                "'50s tv show",
                "'50s movie",
                "'80s movie"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "'80s movie": 0.1244002590289454,
                "'50s movie": 0.12298677217676027,
                "'50s tv show": 0.223660283735346
            },
            "integer_answers": {
                "'80s movie": 5,
                "'50s movie": 2,
                "'50s tv show": 5
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.8743082669911937,
                    1.5388490272636615,
                    1.5868427057451446
                ],
                "result_count_important_words": [
                    12.0,
                    10.0,
                    32.0
                ],
                "wikipedia_search": [
                    0.036231884057971016,
                    2.442028985507246,
                    0.5217391304347826
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.6318840579710145,
                    0.681159420289855,
                    0.6869565217391305
                ],
                "cosine_similarity_raw": [
                    0.12402669340372086,
                    0.06882879883050919,
                    0.048112839460372925
                ],
                "result_count_noun_chunks": [
                    144000.0,
                    149000.0,
                    68900.0
                ],
                "question_answer_similarity": [
                    16.162886361591518,
                    11.408925983123481,
                    12.358498983085155
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    104000.0,
                    128000.0,
                    337000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    3.0,
                    3.0,
                    3.0
                ],
                "answer_relation_to_question": [
                    1.9911169360710645,
                    0.8405053152759575,
                    2.168377748652978
                ],
                "result_count": [
                    10.0,
                    6.0,
                    20.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What popular corn chip loosely translates to \u201clittle golden things\u201d in Spanish?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "fritos"
            ],
            "question": "what popular corn chip loosely translates to \u201clittle golden things\u201d in spanish?",
            "lines": [
                [
                    0.22079843944438837,
                    0.0723404255319149,
                    0.3532293287020472,
                    1.0902443405513753,
                    0.0,
                    0.3333333333333333,
                    0.24509803921568626,
                    0.09721175584024115,
                    0.15845959595959597,
                    0.2702702702702703,
                    0.3103448275862069,
                    0.27235652059595716,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.6267146652829229,
                    0.876595744680851,
                    0.520834091227577,
                    0.27435952269213515,
                    0.5,
                    0.3333333333333333,
                    0.3137254901960784,
                    0.7535795026375283,
                    0.05967281510759772,
                    0.32432432432432434,
                    0.3275862068965517,
                    0.30795686852024884,
                    0.12030075187969924,
                    1.0,
                    1.0
                ],
                [
                    0.15248689527268885,
                    0.05106382978723404,
                    0.1259365800703758,
                    -0.3646038632435105,
                    0.5,
                    0.3333333333333333,
                    0.4411764705882353,
                    0.1492087415222306,
                    0.7818675889328063,
                    0.40540540540540543,
                    0.3620689655172414,
                    0.419686610883794,
                    0.8796992481203008,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "tostitos",
                "fritos",
                "doritos"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "fritos": 0.7143571702777655,
                "doritos": 0.55136854588975,
                "tostitos": 0.23170358478596229
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.4512086853636146,
                    2.7716118166822397,
                    3.7771794979541458
                ],
                "result_count_important_words": [
                    25.0,
                    32.0,
                    45.0
                ],
                "wikipedia_search": [
                    0.9507575757575758,
                    0.3580368906455863,
                    4.691205533596838
                ],
                "word_count_appended_bing": [
                    36.0,
                    38.0,
                    42.0
                ],
                "answer_relation_to_question_bing": [
                    0.3617021276595745,
                    4.382978723404255,
                    0.2553191489361702
                ],
                "cosine_similarity_raw": [
                    0.0478006936609745,
                    0.07048177719116211,
                    0.01704234443604946
                ],
                "result_count_noun_chunks": [
                    12900.0,
                    100000.0,
                    19800.0
                ],
                "question_answer_similarity": [
                    -0.835018546320498,
                    -0.210132060572505,
                    0.2792502345982939
                ],
                "word_count_noun_chunks": [
                    0.0,
                    16.0,
                    117.0
                ],
                "result_count_bing": [
                    308000.0,
                    308000.0,
                    308000.0
                ],
                "word_count_raw": [
                    0.0,
                    3.0,
                    0.0
                ],
                "word_count_appended": [
                    20.0,
                    24.0,
                    30.0
                ],
                "answer_relation_to_question": [
                    1.5455890761107185,
                    4.387002656980459,
                    1.0674082669088218
                ],
                "result_count": [
                    0,
                    3.0,
                    3.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which Canadian clothing brand was founded by Americans?": {
        "raw_data": {
            "data": {
                "word_count_appended_relation_to_question": [
                    1.5515406943809995,
                    1.715424282135551,
                    1.7330350234834495
                ],
                "result_count_important_words": [
                    273000.0,
                    254000.0,
                    2660000.0
                ],
                "wikipedia_search": [
                    0.8397129186602871,
                    2.160287081339713,
                    0.0
                ],
                "word_count_appended_bing": [
                    91.0,
                    82.0,
                    43.0
                ],
                "answer_relation_to_question_bing": [
                    1.4705882352941178,
                    1.4852941176470589,
                    1.0441176470588236
                ],
                "cosine_similarity_raw": [
                    0.33197325468063354,
                    0.06069965660572052,
                    0.33734774589538574
                ],
                "result_count_noun_chunks": [
                    452000.0,
                    232000.0,
                    5230000.0
                ],
                "question_answer_similarity": [
                    -0.3637452907860279,
                    0.08247894048690796,
                    2.002967096865177
                ],
                "word_count_noun_chunks": [
                    6.0,
                    3.0,
                    9.0
                ],
                "word_count_raw": [
                    9.0,
                    0.0,
                    44.0
                ],
                "result_count_bing": [
                    7850000.0,
                    12800000.0,
                    12800000.0
                ],
                "word_count_appended": [
                    546.0,
                    544.0,
                    454.0
                ],
                "answer_relation_to_question": [
                    2.1680428005729215,
                    1.5539570870896173,
                    0.27800011233746175
                ],
                "result_count": [
                    860000.0,
                    1940000.0,
                    2080000.0
                ]
            },
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "z-best_answer_by_ml": [
                "roots",
                "bottom"
            ],
            "lines": [
                [
                    0.5420107001432303,
                    0.36764705882352944,
                    0.4547450149728959,
                    -0.21127091424657865,
                    0.1762295081967213,
                    0.23467862481315396,
                    0.08566049576404142,
                    0.07642881298613459,
                    0.2799043062200957,
                    0.3536269430051813,
                    0.4212962962962963,
                    0.3103081388761999,
                    0.3333333333333333,
                    0.16981132075471697,
                    -1.0
                ],
                [
                    0.3884892717724042,
                    0.3713235294117647,
                    0.08314786165100153,
                    0.04790550312033767,
                    0.3975409836065574,
                    0.38266068759342303,
                    0.07969877627863194,
                    0.03922894825836997,
                    0.7200956937799043,
                    0.35233160621761656,
                    0.37962962962962965,
                    0.3430848564271102,
                    0.16666666666666666,
                    0.0,
                    -1.0
                ],
                [
                    0.06950002808436542,
                    0.2610294117647059,
                    0.4621071233761026,
                    1.163365411126241,
                    0.4262295081967213,
                    0.38266068759342303,
                    0.8346407279573267,
                    0.8843422387554954,
                    0.0,
                    0.29404145077720206,
                    0.19907407407407407,
                    0.3466070046966899,
                    0.5,
                    0.8301886792452831,
                    -1.0
                ]
            ],
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "question": "which canadian clothing brand was founded by americans?",
            "rate_limited": false,
            "answers": [
                "aritzia",
                "lululemon",
                "roots"
            ],
            "ml_answers": {
                "lululemon": 0.07179859305743636,
                "aritzia": 0.26109048605398383,
                "roots": 0.6658797713555316
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these instruments is considered a woodwind?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "clarinet"
            ],
            "lines": [
                [
                    0.6907551775730373,
                    0.6785714285714286,
                    0.7678421442422232,
                    0.29790399908216253,
                    0.3568521031207598,
                    0.02351295543861986,
                    0.878341516639389,
                    0.3287760416666667,
                    0.6588693957115009,
                    0.4081237911025145,
                    0.37719298245614036,
                    0.35471272489320366,
                    0.23076923076923078,
                    0.9166666666666666,
                    -1.0
                ],
                [
                    0.22885266556421768,
                    0.21428571428571427,
                    0.15857742017740364,
                    0.3463386996564129,
                    0.29579375848032563,
                    0.9525078265503231,
                    0.058919803600654665,
                    0.3216145833333333,
                    0.16680590364800887,
                    0.28433268858800775,
                    0.2631578947368421,
                    0.32978828739966176,
                    0.15384615384615385,
                    0.08333333333333333,
                    -1.0
                ],
                [
                    0.0803921568627451,
                    0.10714285714285714,
                    0.07358043558037321,
                    0.3557573012614246,
                    0.3473541383989145,
                    0.023979218011057082,
                    0.06273867975995635,
                    0.349609375,
                    0.1743247006404901,
                    0.30754352030947774,
                    0.35964912280701755,
                    0.3154989877071346,
                    0.6153846153846154,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "violin": 0.22663965063329025,
                "clarinet": 0.4977778684238246,
                "trumpet": 0.2755824809428852
            },
            "question": "which of these instruments is considered a woodwind?",
            "rate_limited": false,
            "answers": [
                "clarinet",
                "trumpet",
                "violin"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "violin": 0.1486469829039032,
                "clarinet": 0.7420902821268696,
                "trumpet": 0.03578236720362811
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.064138174679611,
                    0.9893648621989852,
                    0.9464969631214037
                ],
                "result_count_important_words": [
                    16100000.0,
                    1080000.0,
                    1150000.0
                ],
                "wikipedia_search": [
                    1.976608187134503,
                    0.5004177109440267,
                    0.5229741019214703
                ],
                "word_count_appended_bing": [
                    43.0,
                    30.0,
                    41.0
                ],
                "answer_relation_to_question_bing": [
                    1.3571428571428572,
                    0.42857142857142855,
                    0.21428571428571427
                ],
                "cosine_similarity_raw": [
                    0.32002708315849304,
                    0.06609310209751129,
                    0.030667413026094437
                ],
                "result_count_noun_chunks": [
                    5050000.0,
                    4940000.0,
                    5370000.0
                ],
                "question_answer_similarity": [
                    1.703945191577077,
                    1.9809809997677803,
                    2.0348533242940903
                ],
                "word_count_noun_chunks": [
                    3.0,
                    2.0,
                    8.0
                ],
                "word_count_raw": [
                    11.0,
                    1.0,
                    0.0
                ],
                "result_count_bing": [
                    353000.0,
                    14300000.0,
                    360000.0
                ],
                "result_count": [
                    2630000.0,
                    2180000.0,
                    2560000.0
                ],
                "answer_relation_to_question": [
                    2.0722655327191117,
                    0.686557996692653,
                    0.2411764705882353
                ],
                "word_count_appended": [
                    422.0,
                    294.0,
                    318.0
                ]
            },
            "integer_answers": {
                "violin": 3,
                "clarinet": 10,
                "trumpet": 1
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these Summer Olympics events uses the longest playing surface?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "badminton"
            ],
            "question": "which of these summer olympics events uses the longest playing surface?",
            "lines": [
                [
                    0.11805555555555554,
                    0.4455882352941176,
                    0.26059633152753403,
                    0.464416557118236,
                    0.17310087173100872,
                    0.26375321336760926,
                    0.1872085276482345,
                    0.160099076784508,
                    0.4777622377622378,
                    0.3458083832335329,
                    0.3113207547169811,
                    0.34784080148931784,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.5440705128205129,
                    0.3088235294117647,
                    0.4974631662898553,
                    0.3854009773893707,
                    0.5429638854296388,
                    0.45141388174807195,
                    0.5485232067510548,
                    0.5696915109209637,
                    0.3258741258741259,
                    0.36826347305389223,
                    0.41509433962264153,
                    0.3367202242608952,
                    1.0,
                    0,
                    -1.0
                ],
                [
                    0.33787393162393164,
                    0.24558823529411763,
                    0.24194050218261065,
                    0.15018246549239336,
                    0.28393524283935245,
                    0.2848329048843188,
                    0.26426826560071065,
                    0.2702094122945283,
                    0.19636363636363635,
                    0.28592814371257486,
                    0.27358490566037735,
                    0.3154389742497869,
                    0.0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "fencing",
                "badminton",
                "taekwondo"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "fencing": 0.3953169398288925,
                "badminton": 0.44288168632685504,
                "taekwondo": 0.07557485469802164
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.434885610425225,
                    2.3570415698262663,
                    2.208072819748508
                ],
                "result_count_important_words": [
                    84300.0,
                    247000.0,
                    119000.0
                ],
                "wikipedia_search": [
                    2.388811188811189,
                    1.6293706293706294,
                    0.9818181818181817
                ],
                "answer_relation_to_question": [
                    0.7083333333333333,
                    3.264423076923077,
                    2.02724358974359
                ],
                "result_count": [
                    69500.0,
                    218000.0,
                    114000.0
                ],
                "answer_relation_to_question_bing": [
                    1.7823529411764705,
                    1.2352941176470589,
                    0.9823529411764705
                ],
                "cosine_similarity_raw": [
                    0.03996945545077324,
                    0.07629935443401337,
                    0.037108082324266434
                ],
                "result_count_noun_chunks": [
                    71100.0,
                    253000.0,
                    120000.0
                ],
                "question_answer_similarity": [
                    2.037695363163948,
                    1.6910029854625463,
                    0.6589474661741406
                ],
                "word_count_noun_chunks": [
                    0.0,
                    2.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    51300.0,
                    87800.0,
                    55400.0
                ],
                "word_count_appended": [
                    231.0,
                    246.0,
                    191.0
                ],
                "word_count_appended_bing": [
                    33.0,
                    44.0,
                    29.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "In Harry Potter's Quidditch, what ALWAYS happens when one team catches the snitch?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "the game ends"
            ],
            "lines": [
                [
                    0.25995222929936307,
                    0.6182795698924731,
                    0.5593625366402655,
                    0.3338759673731476,
                    0.1794871794871795,
                    0.26026200873362443,
                    0.025879917184265012,
                    0.012534818941504178,
                    0.0625,
                    0.11666666666666667,
                    0.3333333333333333,
                    0.1597846980283918,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.5300955414012739,
                    0.3763440860215054,
                    0.1679551768897467,
                    0.34585338582943714,
                    0.042735042735042736,
                    0.26026200873362443,
                    0.006211180124223602,
                    0.0032497678737233053,
                    0.9375,
                    0.08333333333333333,
                    0.3333333333333333,
                    0.09806138661387281,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.20995222929936308,
                    0.005376344086021506,
                    0.2726822864699878,
                    0.32027064679741524,
                    0.7777777777777778,
                    0.4794759825327511,
                    0.9679089026915114,
                    0.9842154131847726,
                    0.0,
                    0.8,
                    0.3333333333333333,
                    0.7421539153577354,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "the game ends": 0.5637962022521906,
                "that team loses": 0.2274953030635083,
                "that team wins": 0.208708494684301
            },
            "question": "in harry potter's quidditch, what always happens when one team catches the snitch?",
            "rate_limited": false,
            "answers": [
                "that team wins",
                "that team loses",
                "the game ends"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "the game ends": 0.4869861986582607,
                "that team loses": 0.16466010493570699,
                "that team wins": 0.11306868011177049
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1184928861987424,
                    0.6864297062971096,
                    5.195077407504147
                ],
                "result_count_important_words": [
                    25.0,
                    6.0,
                    935.0
                ],
                "wikipedia_search": [
                    0.0625,
                    0.9375,
                    0.0
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.6182795698924731,
                    0.3763440860215054,
                    0.005376344086021506
                ],
                "cosine_similarity_raw": [
                    0.2583844065666199,
                    0.0775829553604126,
                    0.12595918774604797
                ],
                "result_count_noun_chunks": [
                    27.0,
                    7.0,
                    2120.0
                ],
                "question_answer_similarity": [
                    15.678421485237777,
                    16.240866923704743,
                    15.039531684014946
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    4.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    2.0
                ],
                "result_count_bing": [
                    29800.0,
                    29800.0,
                    54900.0
                ],
                "word_count_appended": [
                    7.0,
                    5.0,
                    48.0
                ],
                "answer_relation_to_question": [
                    0.5199044585987261,
                    1.0601910828025478,
                    0.41990445859872616
                ],
                "result_count": [
                    21.0,
                    5.0,
                    91.0
                ]
            },
            "integer_answers": {
                "the game ends": 8,
                "that team loses": 3,
                "that team wins": 3
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "One of the Gal\u00e1pagos Islands is named for which revolutionary?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "guy fawkes"
            ],
            "question": "one of the gal\u00e1pagos islands is named for which revolutionary?",
            "lines": [
                [
                    0.3627450980392157,
                    0.1,
                    0.07493039657870097,
                    0.0,
                    0.12488760115895695,
                    0.3493408662900188,
                    0.0018810537961086546,
                    0.009948738099916051,
                    0.20710784313725492,
                    0.0,
                    0.0,
                    0.36939010173429004,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.4632352941176471,
                    0.9,
                    0.1928939819207314,
                    -0.6267736127725421,
                    0.8522329903087221,
                    0.3192090395480226,
                    0.9980411328274336,
                    0.9698680050726062,
                    0.7720588235294118,
                    0.5125,
                    0.26229508196721313,
                    0.32205894572314836,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.17401960784313725,
                    0.0,
                    0.7321756215005676,
                    1.626773612772542,
                    0.02287940853232091,
                    0.3314500941619586,
                    7.781337645773212e-05,
                    0.02018325682747781,
                    0.020833333333333332,
                    0.4875,
                    0.7377049180327869,
                    0.30855095254256154,
                    0,
                    1.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "sim\u00f3n bol\u00edvar",
                "che guevara",
                "guy fawkes"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sim\u00f3n bol\u00edvar": 0.1582493641715445,
                "guy fawkes": 0.773208014653815,
                "che guevara": 0.2154498575976433
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1081703052028702,
                    0.9661768371694451,
                    0.9256528576276846
                ],
                "result_count_important_words": [
                    55600.0,
                    29500000.0,
                    2300.0
                ],
                "wikipedia_search": [
                    0.6213235294117647,
                    2.3161764705882355,
                    0.0625
                ],
                "word_count_appended_bing": [
                    0.0,
                    16.0,
                    45.0
                ],
                "answer_relation_to_question_bing": [
                    0.1,
                    0.9,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.020696565508842468,
                    0.05327935144305229,
                    0.20223462581634521
                ],
                "result_count_noun_chunks": [
                    55700.0,
                    5430000.0,
                    113000.0
                ],
                "question_answer_similarity": [
                    0.0,
                    -0.839610724709928,
                    2.1791864624246955
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    371000.0,
                    339000.0,
                    352000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    4.0
                ],
                "result_count": [
                    12500.0,
                    85300.0,
                    2290.0
                ],
                "answer_relation_to_question": [
                    0.7254901960784313,
                    0.9264705882352942,
                    0.3480392156862745
                ],
                "word_count_appended": [
                    0.0,
                    82.0,
                    78.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "In 2017, what did Twitter do to its 140-character limit on tweets?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "double it"
            ],
            "question": "in 2017, what did twitter do to its 140-character limit on tweets?",
            "lines": [
                [
                    0.38795518207282914,
                    0.63625,
                    0.35692369472458624,
                    0.26315130271662407,
                    0.0012130630817161188,
                    0.4744939271255061,
                    0.008004361180177904,
                    0.003438592469482492,
                    0.0,
                    0.7105263157894737,
                    0.42857142857142855,
                    0.3808691914675183,
                    0,
                    0,
                    1.0
                ],
                [
                    0.37640056022408963,
                    0.2075,
                    0.15260034378753215,
                    0.21347668983099993,
                    8.038369818600787e-05,
                    0.16761133603238867,
                    9.307396721137098e-05,
                    0.000251281757385259,
                    0.0,
                    0.13157894736842105,
                    0.2857142857142857,
                    0.08317265026353712,
                    0,
                    0,
                    1.0
                ],
                [
                    0.23564425770308123,
                    0.15625,
                    0.49047596148788164,
                    0.523372007452376,
                    0.9987065532200978,
                    0.35789473684210527,
                    0.9919025648526107,
                    0.9963101257731323,
                    1.0,
                    0.15789473684210525,
                    0.2857142857142857,
                    0.5359581582689444,
                    0,
                    0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "double it",
                "halve it",
                "reduce it to 15"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "halve it": 0.03638313833413193,
                "reduce it to 15": 0.4398774832506264,
                "double it": 0.5503066231979791
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9043459573375916,
                    0.41586325131768564,
                    2.6797907913447223
                ],
                "result_count_important_words": [
                    3010.0,
                    35.0,
                    373000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_appended_bing": [
                    3.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.2725,
                    0.415,
                    0.3125
                ],
                "cosine_similarity_raw": [
                    0.042402032762765884,
                    0.018128706142306328,
                    0.058267854154109955
                ],
                "result_count_noun_chunks": [
                    780.0,
                    57.0,
                    226000.0
                ],
                "question_answer_similarity": [
                    8.710780024528503,
                    7.0664612574037164,
                    17.324552000500262
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    58600.0,
                    20700.0,
                    44200.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    498.0,
                    33.0,
                    410000.0
                ],
                "answer_relation_to_question": [
                    1.5518207282913166,
                    1.5056022408963585,
                    0.9425770308123249
                ],
                "word_count_appended": [
                    27.0,
                    5.0,
                    6.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Who is Robert Downey, Jr.\u2019s father?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "robert downey, sr."
            ],
            "question": "who is robert downey, jr.\u2019s father?",
            "lines": [
                [
                    0.5504892593882645,
                    0.5726350076148463,
                    0.8366514679943136,
                    0.4423872086547876,
                    0.5786846168837984,
                    0.24177529484792054,
                    0.6009635283015744,
                    0.6256834605368476,
                    0.37409090909090903,
                    0.2631578947368421,
                    0.5238095238095238,
                    0.5617978735612813,
                    0,
                    0,
                    0.0
                ],
                [
                    0.08050681253535355,
                    0.03009259259259259,
                    0.10932600443195088,
                    0.14603287319449662,
                    4.497548835884443e-05,
                    0.5524518932340161,
                    4.966640729765078e-05,
                    3.382072759658636e-05,
                    0.14,
                    0.15789473684210525,
                    0.14285714285714285,
                    0.03120573430613975,
                    0,
                    0,
                    0.0
                ],
                [
                    0.369003928076382,
                    0.39727239979256107,
                    0.05402252757373551,
                    0.4115799181507158,
                    0.42127040762784285,
                    0.20577281191806332,
                    0.3989868052911279,
                    0.37428271873555574,
                    0.4859090909090909,
                    0.5789473684210527,
                    0.3333333333333333,
                    0.406996392132579,
                    0,
                    0,
                    0.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "robert downey, sr.",
                "chet downey iii",
                "morton downey, jr."
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "chet downey iii": 0.08696322797556474,
                "robert downey, sr.": 0.6831492953446435,
                "morton downey, jr.": 0.4587015466078629
            },
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.247191494245125,
                    0.124822937224559,
                    1.627985568530316
                ],
                "result_count_important_words": [
                    36300.0,
                    3.0,
                    24100.0
                ],
                "wikipedia_search": [
                    1.4963636363636361,
                    0.56,
                    1.9436363636363636
                ],
                "answer_relation_to_question": [
                    2.2019570375530577,
                    0.32202725014141415,
                    1.4760157123055277
                ],
                "word_count_appended_bing": [
                    22.0,
                    6.0,
                    14.0
                ],
                "answer_relation_to_question_bing": [
                    2.2905400304593853,
                    0.12037037037037036,
                    1.5890895991702443
                ],
                "cosine_similarity_raw": [
                    0.6147797703742981,
                    0.08033382892608643,
                    0.039696287363767624
                ],
                "result_count_noun_chunks": [
                    55500.0,
                    3.0,
                    33200.0
                ],
                "question_answer_similarity": [
                    8.988621512893587,
                    2.9671613462269306,
                    8.362665181513876
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    779000.0,
                    1780000.0,
                    663000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    38600.0,
                    3.0,
                    28100.0
                ],
                "word_count_appended": [
                    5.0,
                    3.0,
                    11.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these two U.S. cities are in the same time zone?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "el paso / pierre"
            ],
            "lines": [
                [
                    0.2738816738816739,
                    0.2527322404371585,
                    0.37000025464194986,
                    0.30697650259698,
                    0.9999526088810957,
                    0.33602150537634407,
                    0.8018018018018018,
                    0.9998577727208079,
                    0.014492753623188406,
                    0.5,
                    0.3333333333333333,
                    0.7640071286527677,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.4554988662131519,
                    0.5027322404371585,
                    0.35693725360378614,
                    0.11830706248428483,
                    4.739111890431733e-05,
                    0.3279569892473118,
                    0.1981981981981982,
                    0.00014222727919214906,
                    0.7671497584541062,
                    0.25,
                    0.3333333333333333,
                    0.14671445452397658,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.27061945990517416,
                    0.24453551912568305,
                    0.273062491754264,
                    0.5747164349187351,
                    0.0,
                    0.33602150537634407,
                    0.0,
                    0.0,
                    0.2183574879227053,
                    0.25,
                    0.3333333333333333,
                    0.08927841682325573,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "el paso / pierre": 0.4960881313289252,
                "pensacola / sioux falls": 0.2158270540966246,
                "bismarck / cheyenne": 0.2880848145744503
            },
            "question": "which of these two u.s. cities are in the same time zone?",
            "rate_limited": false,
            "answers": [
                "el paso / pierre",
                "bismarck / cheyenne",
                "pensacola / sioux falls"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "el paso / pierre": 0.3647841823829532,
                "pensacola / sioux falls": 0.09118398094194266,
                "bismarck / cheyenne": 0.117972943594293
            },
            "integer_answers": {
                "el paso / pierre": 8,
                "pensacola / sioux falls": 1,
                "bismarck / cheyenne": 3
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.0560285146110706,
                    0.5868578180959063,
                    0.3571136672930229
                ],
                "result_count_important_words": [
                    89.0,
                    22.0,
                    0
                ],
                "wikipedia_search": [
                    0.043478260869565216,
                    2.3014492753623186,
                    0.6550724637681159
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.010928961748634,
                    2.010928961748634,
                    0.9781420765027322
                ],
                "cosine_similarity_raw": [
                    0.05261365324258804,
                    0.05075610801577568,
                    0.03882920369505882
                ],
                "result_count_noun_chunks": [
                    70300.0,
                    10.0,
                    0
                ],
                "question_answer_similarity": [
                    2.8946413625963032,
                    1.115578924305737,
                    5.419300663750619
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    12500000.0,
                    12200000.0,
                    12500000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    6.0,
                    3.0,
                    3.0
                ],
                "answer_relation_to_question": [
                    1.0955266955266956,
                    1.8219954648526075,
                    1.0824778396206967
                ],
                "result_count": [
                    211000.0,
                    10.0,
                    0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "When it comes to clothing design, what is a frog?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "cap sleeve"
            ],
            "lines": [
                [
                    0.34558823529411764,
                    0.5,
                    0.7167776986175926,
                    0.19333327828072294,
                    0.15,
                    0.316694954482333,
                    0.19230769230769232,
                    0.13756613756613756,
                    0.8395061728395062,
                    0.24675324675324675,
                    0.3333333333333333,
                    0.17417391288645975,
                    0,
                    0,
                    1.0
                ],
                [
                    0.22990196078431374,
                    0.26785714285714285,
                    0.1204895517456601,
                    0.2942435321215933,
                    0.21428571428571427,
                    0.035256904798642186,
                    0.2980769230769231,
                    0.3386243386243386,
                    0.0,
                    0.0,
                    0.0,
                    0.2468973907259715,
                    0,
                    0,
                    1.0
                ],
                [
                    0.4245098039215686,
                    0.23214285714285715,
                    0.16273274963674733,
                    0.5124231895976837,
                    0.6357142857142857,
                    0.6480481407190248,
                    0.5096153846153846,
                    0.5238095238095238,
                    0.16049382716049382,
                    0.7532467532467533,
                    0.6666666666666666,
                    0.5789286963875688,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "braided fastener": 0.3455028885300952,
                "cap sleeve": 0.4840276565515465,
                "crystal appliqu\u00e9": 0.1704694549183583
            },
            "question": "when it comes to clothing design, what is a frog?",
            "rate_limited": false,
            "answers": [
                "braided fastener",
                "crystal appliqu\u00e9",
                "cap sleeve"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "braided fastener": 0.3158420913899801,
                "cap sleeve": 0.3459376156709961,
                "crystal appliqu\u00e9": 0.14680338812744936
            },
            "integer_answers": {
                "braided fastener": 3,
                "cap sleeve": 9,
                "crystal appliqu\u00e9": 0
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.696695651545839,
                    0.987589562903886,
                    2.3157147855502753
                ],
                "result_count_important_words": [
                    20.0,
                    31.0,
                    53.0
                ],
                "wikipedia_search": [
                    2.5185185185185186,
                    0.0,
                    0.48148148148148145
                ],
                "word_count_appended_bing": [
                    2.0,
                    0.0,
                    4.0
                ],
                "answer_relation_to_question_bing": [
                    2.0,
                    1.0714285714285714,
                    0.9285714285714286
                ],
                "cosine_similarity_raw": [
                    0.14999347925186157,
                    0.02521374076604843,
                    0.034053586423397064
                ],
                "result_count_noun_chunks": [
                    26.0,
                    64.0,
                    99.0
                ],
                "question_answer_similarity": [
                    2.0287215458229184,
                    3.0876122240442783,
                    5.377056524157524
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    8210000.0,
                    914000.0,
                    16800000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    21.0,
                    30.0,
                    89.0
                ],
                "answer_relation_to_question": [
                    1.3823529411764706,
                    0.919607843137255,
                    1.6980392156862745
                ],
                "word_count_appended": [
                    19.0,
                    0.0,
                    58.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What was the first product to have its barcode scanned at purchase?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "wrigley's chewing gum"
            ],
            "question": "what was the first product to have its barcode scanned at purchase?",
            "lines": [
                [
                    0.2583333333333333,
                    0.14285714285714285,
                    0.03105797373548177,
                    0.35632521445243076,
                    0.993359247956392,
                    0.3382352941176471,
                    0.9798440488450787,
                    0.9904489105061984,
                    0.07142857142857142,
                    0.0,
                    0.0,
                    0.297575434476619,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.385,
                    0.7142857142857143,
                    0.929776413391596,
                    0.5152295189661107,
                    0.0008323020477124163,
                    0.33144796380090497,
                    0.0015006620567897602,
                    0.00041384954476550076,
                    0.6785714285714285,
                    0.625,
                    0.5,
                    0.4603738458350482,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.3566666666666667,
                    0.14285714285714285,
                    0.039165612872922195,
                    0.12844526658145855,
                    0.005808449995895639,
                    0.33031674208144796,
                    0.01865528909813153,
                    0.009137239949036056,
                    0.25,
                    0.375,
                    0.5,
                    0.24205071968833283,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "rubik\u2019s cube",
                "wrigley's chewing gum",
                "apple iie"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "apple iie": 0.11087482934314496,
                "wrigley's chewing gum": 0.931007854288402,
                "rubik\u2019s cube": 0.06650666252220014
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.190301737906476,
                    1.8414953833401928,
                    0.9682028787533313
                ],
                "result_count_important_words": [
                    333000.0,
                    510.0,
                    6340.0
                ],
                "wikipedia_search": [
                    0.21428571428571427,
                    2.0357142857142856,
                    0.75
                ],
                "answer_relation_to_question": [
                    0.5166666666666666,
                    0.77,
                    0.7133333333333334
                ],
                "answer_relation_to_question_bing": [
                    0.14285714285714285,
                    0.7142857142857143,
                    0.14285714285714285
                ],
                "word_count_appended": [
                    0.0,
                    60.0,
                    36.0
                ],
                "cosine_similarity_raw": [
                    0.015872150659561157,
                    0.47516143321990967,
                    0.020015552639961243
                ],
                "result_count_noun_chunks": [
                    4260000.0,
                    1780.0,
                    39300.0
                ],
                "question_answer_similarity": [
                    5.095247627934441,
                    7.3674886813387275,
                    1.8366941583808511
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count_bing": [
                    2990000.0,
                    2930000.0,
                    2920000.0
                ],
                "word_count_raw": [
                    0.0,
                    2.0,
                    0.0
                ],
                "result_count": [
                    956000.0,
                    801.0,
                    5590.0
                ],
                "word_count_appended_bing": [
                    0.0,
                    2.0,
                    2.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these phrases appears in a Shakespeare play?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "at a loss"
            ],
            "lines": [
                [
                    0.8322981366459627,
                    0.7529880478087648,
                    0.5050420399225889,
                    0.38083291225711197,
                    4.190300596927367e-05,
                    0.34015927189988626,
                    0.00011605795755849768,
                    7.962328925659623e-05,
                    0.813953488372093,
                    0.297029702970297,
                    0.3333333333333333,
                    0.33319558162198265,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.08695652173913043,
                    0.15670650730411687,
                    0.3892480458498548,
                    0.2622285306507105,
                    0.9999580969940307,
                    0.4732650739476678,
                    0.9998839420424415,
                    0.9999203767107434,
                    0.13953488372093023,
                    0.6633663366336634,
                    0.3333333333333333,
                    0.6047783541805014,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.08074534161490683,
                    0.09030544488711818,
                    0.10570991422755635,
                    0.35693855709217753,
                    0.0,
                    0.18657565415244595,
                    0.0,
                    0.0,
                    0.046511627906976744,
                    0.039603960396039604,
                    0.3333333333333333,
                    0.062026064197515905,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "at a loss": 0.5090983335922604,
                "in such a pickle": 0.3824225082570671,
                "up a dark creek": 0.10847915815067256
            },
            "question": "which of these phrases appears in a shakespeare play?",
            "rate_limited": false,
            "answers": [
                "in such a pickle",
                "at a loss",
                "up a dark creek"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "at a loss": 0.3959854503061485,
                "in such a pickle": 0.3252645019738909,
                "up a dark creek": 0.14130421857663164
            },
            "integer_answers": {
                "at a loss": 6,
                "in such a pickle": 6,
                "up a dark creek": 0
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3327823264879306,
                    2.4191134167220056,
                    0.24810425679006362
                ],
                "result_count_important_words": [
                    78.0,
                    672000.0,
                    0
                ],
                "wikipedia_search": [
                    0.813953488372093,
                    0.13953488372093023,
                    0.046511627906976744
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    2.2589641434262946,
                    0.4701195219123506,
                    0.27091633466135456
                ],
                "cosine_similarity_raw": [
                    0.11924837529659271,
                    0.09190759062767029,
                    0.024959774687886238
                ],
                "result_count_noun_chunks": [
                    86.0,
                    1080000.0,
                    0
                ],
                "question_answer_similarity": [
                    13.143948335200548,
                    9.050473706331104,
                    12.31926601473242
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    2990000.0,
                    4160000.0,
                    1640000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    44.0,
                    1050000.0,
                    0
                ],
                "answer_relation_to_question": [
                    3.329192546583851,
                    0.34782608695652173,
                    0.32298136645962733
                ],
                "word_count_appended": [
                    30.0,
                    67.0,
                    4.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these Hebrew texts does NOT form a significant part of the Christian Old Testament?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ketuvim"
            ],
            "lines": [
                [
                    0.3532616641503473,
                    0.2681588143657109,
                    0.33283671489356603,
                    0.08249714013652126,
                    0.0466643585138381,
                    0.4003104901308494,
                    0.19097337029783557,
                    0.04374224946769928,
                    0.3461312716575992,
                    0.29268292682926833,
                    0.2815533980582524,
                    0.3288293081500853,
                    0.025000000000000022,
                    0.0,
                    -1.0
                ],
                [
                    0.31528918517958754,
                    0.34038735211149007,
                    0.3801088422406022,
                    0.5,
                    0.49672170168567287,
                    0.32146817476158795,
                    0.4946969504285678,
                    0.49638503474578255,
                    0.44717858129712235,
                    0.3635307781649245,
                    0.3592233009708738,
                    0.33751814742154695,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.33144915067006514,
                    0.39145383352279906,
                    0.28705444286583176,
                    0.4175028598634788,
                    0.45661393980048903,
                    0.27822133510756264,
                    0.3143296792735967,
                    0.45987271578651817,
                    0.2066901470452785,
                    0.3437862950058072,
                    0.3592233009708738,
                    0.3336525444283678,
                    0.475,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "talmud": 0.2635928222370474,
                "ketuvim": 0.1639274215703202,
                "torah": 0.5724797561926324
            },
            "question": "which of these hebrew texts does not form a significant part of the christian old testament?",
            "rate_limited": false,
            "answers": [
                "torah",
                "ketuvim",
                "talmud"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "talmud": 0.2825069450057118,
                "ketuvim": 0.30011092591887933,
                "torah": 0.11353082233265682
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.3963896858988054,
                    2.274745936098343,
                    2.3288643780028515
                ],
                "result_count_important_words": [
                    2430000.0,
                    41700.0,
                    1460000.0
                ],
                "wikipedia_search": [
                    2.154162196793611,
                    0.7394998618402874,
                    4.106337941366101
                ],
                "answer_relation_to_question": [
                    2.0543367018951373,
                    2.5859514074857746,
                    2.359711890619088
                ],
                "answer_relation_to_question_bing": [
                    2.782094227611469,
                    1.9153517746621191,
                    1.3025539977264116
                ],
                "word_count_appended": [
                    357.0,
                    235.0,
                    269.0
                ],
                "cosine_similarity_raw": [
                    0.09564834833145142,
                    0.06859993934631348,
                    0.12184428423643112
                ],
                "result_count_noun_chunks": [
                    3900000.0,
                    30900.0,
                    343000.0
                ],
                "question_answer_similarity": [
                    2.259939356474206,
                    0.0,
                    0.4465563034755178
                ],
                "word_count_noun_chunks": [
                    19.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    8.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    899000.0,
                    1610000.0,
                    2000000.0
                ],
                "result_count": [
                    4190000.0,
                    30300.0,
                    401000.0
                ],
                "word_count_appended_bing": [
                    45.0,
                    29.0,
                    29.0
                ]
            },
            "integer_answers": {
                "talmud": 3,
                "ketuvim": 1,
                "torah": 10
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these athletes has a notably obscured glabella?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "michael phelps"
            ],
            "lines": [
                [
                    0.8666666666666667,
                    0.0,
                    0.3120464181704756,
                    0.5023348439272827,
                    0.65625,
                    0.7278358497870693,
                    0.6666666666666666,
                    0.6521739130434783,
                    0.45,
                    0.4375,
                    0.5,
                    0.41038083538083536,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.0,
                    0.5,
                    0.22852591042526785,
                    0.024076729717822265,
                    0.25,
                    0.2497096399535424,
                    0.25,
                    0.2608695652173913,
                    0.0,
                    0.3125,
                    0.42857142857142855,
                    0.3073064323064323,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.13333333333333333,
                    0.5,
                    0.4594276714042565,
                    0.4735884263548951,
                    0.09375,
                    0.02245451025938831,
                    0.08333333333333333,
                    0.08695652173913043,
                    0.55,
                    0.25,
                    0.07142857142857142,
                    0.28231273231273235,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "michael strahan": 0.23429664218265708,
                "michael phelps": 0.5151545994702063,
                "anthony davis": 0.2505487583471367
            },
            "question": "which of these athletes has a notably obscured glabella?",
            "rate_limited": false,
            "answers": [
                "michael phelps",
                "michael strahan",
                "anthony davis"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "michael strahan": 0.1230980074561317,
                "michael phelps": 0.5326835036079326,
                "anthony davis": 0.136225867234876
            },
            "integer_answers": {
                "michael strahan": 1,
                "michael phelps": 9,
                "anthony davis": 2
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6415233415233415,
                    1.2292257292257291,
                    1.1292509292509294
                ],
                "result_count_important_words": [
                    16.0,
                    6.0,
                    2.0
                ],
                "wikipedia_search": [
                    0.9,
                    0.0,
                    1.1
                ],
                "word_count_appended_bing": [
                    21.0,
                    18.0,
                    3.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    1.0,
                    1.0
                ],
                "cosine_similarity_raw": [
                    0.032896459102630615,
                    0.024091586470603943,
                    0.048433639109134674
                ],
                "result_count_noun_chunks": [
                    15.0,
                    6.0,
                    2.0
                ],
                "question_answer_similarity": [
                    0.7542836407665163,
                    0.036152545595541596,
                    0.7111192997545004
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    376000.0,
                    129000.0,
                    11600.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    7.0,
                    5.0,
                    4.0
                ],
                "answer_relation_to_question": [
                    0.8666666666666667,
                    0.0,
                    0.13333333333333333
                ],
                "result_count": [
                    21.0,
                    8.0,
                    3.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "J.K. Rowling\u2019s first book published in England was titled \u201cHarry Potter and the\u201d what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "sorcerer's stone"
            ],
            "lines": [
                [
                    0.20043083462038314,
                    0.28083827941303374,
                    0.2600049163613552,
                    0.3477555735945452,
                    0.621681946003203,
                    0.3533424283765348,
                    0.6368526894609158,
                    0.5366876464136651,
                    0.07086374695863747,
                    0.4824561403508772,
                    0.6785714285714286,
                    0.4671201651467251,
                    0.6451612903225806,
                    0.7142857142857143,
                    1.0
                ],
                [
                    0.7086298498778153,
                    0.7032887047139504,
                    0.6112269453765661,
                    0.31221793421594035,
                    0.3783054438153365,
                    0.3656207366984993,
                    0.363127443384127,
                    0.46329446399812113,
                    0.8777372262773723,
                    0.49122807017543857,
                    0.25,
                    0.47074396160861287,
                    0.3548387096774194,
                    0.2857142857142857,
                    1.0
                ],
                [
                    0.09093931550180151,
                    0.015873015873015872,
                    0.12876813826207878,
                    0.3400264921895144,
                    1.2610181460511217e-05,
                    0.2810368349249659,
                    1.986715495718628e-05,
                    1.7889588213788835e-05,
                    0.05139902676399027,
                    0.02631578947368421,
                    0.07142857142857142,
                    0.06213587324466212,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "magician's stone": 0.07628381604192255,
                "philosopher's stone": 0.4497180571342571,
                "sorcerer's stone": 0.47399812682382036
            },
            "question": "j.k. rowling\u2019s first book published in england was titled \u201charry potter and the\u201d what?",
            "rate_limited": false,
            "answers": [
                "philosopher's stone",
                "sorcerer's stone",
                "magician's stone"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "magician's stone": 0.0396006449091319,
                "philosopher's stone": 0.32470827884231823,
                "sorcerer's stone": 0.892123312056763
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.7369613211738004,
                    3.7659516928689025,
                    0.4970869859572969
                ],
                "result_count_important_words": [
                    577000.0,
                    329000.0,
                    18.0
                ],
                "wikipedia_search": [
                    0.4251824817518248,
                    5.266423357664234,
                    0.3083941605839416
                ],
                "word_count_appended_bing": [
                    19.0,
                    7.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.6850296764782025,
                    4.219732228283703,
                    0.09523809523809523
                ],
                "cosine_similarity_raw": [
                    0.0696406364440918,
                    0.16371318697929382,
                    0.0344897136092186
                ],
                "result_count_noun_chunks": [
                    1170000.0,
                    1010000.0,
                    39.0
                ],
                "question_answer_similarity": [
                    11.789356105029583,
                    10.58458494511433,
                    11.527330418117344
                ],
                "word_count_noun_chunks": [
                    20.0,
                    11.0,
                    0.0
                ],
                "word_count_raw": [
                    10.0,
                    4.0,
                    0.0
                ],
                "result_count_bing": [
                    259000.0,
                    268000.0,
                    206000.0
                ],
                "result_count": [
                    493000.0,
                    300000.0,
                    10.0
                ],
                "answer_relation_to_question": [
                    1.603446676963065,
                    5.669038799022522,
                    0.7275145240144121
                ],
                "word_count_appended": [
                    110.0,
                    112.0,
                    6.0
                ]
            },
            "integer_answers": {
                "magician's stone": 0,
                "philosopher's stone": 7,
                "sorcerer's stone": 7
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these companies purchased the music startup Napster?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "best buy"
            ],
            "question": "which of these companies purchased the music startup napster?",
            "lines": [
                [
                    0.3935810810810811,
                    0.6666666666666667,
                    0.5816710281436052,
                    0.6544640335897361,
                    0.04036517277802927,
                    0.36760752688172044,
                    0.06236192006426993,
                    0.056054355738898325,
                    0.4452661502966381,
                    0.20038910505836577,
                    0.18181818181818182,
                    0.3278753484583853,
                    0.6666666666666666,
                    0.3333333333333333,
                    -1.0
                ],
                [
                    0.17331081081081082,
                    0.16666666666666669,
                    0.2760441756123103,
                    0.15920971133029388,
                    0.05424777425682813,
                    0.5,
                    0.08405302269532035,
                    0.5435573889832565,
                    0.22778304218853,
                    0.3735408560311284,
                    0.4025974025974026,
                    0.31423428440102513,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.4331081081081081,
                    0.16666666666666669,
                    0.14228479624408452,
                    0.18632625507996994,
                    0.9053870529651425,
                    0.13239247311827956,
                    0.8535850572404097,
                    0.40038825527784516,
                    0.3269508075148319,
                    0.4260700389105058,
                    0.4155844155844156,
                    0.35789036714058947,
                    0.3333333333333333,
                    0.6666666666666666,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "best buy",
                "disney",
                "sony"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sony": 0.1656177400935703,
                "best buy": 0.6157055530869228,
                "disney": 0.049941091711540694
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6393767422919265,
                    1.5711714220051256,
                    1.7894518357029474
                ],
                "result_count_important_words": [
                    62100.0,
                    83700.0,
                    850000.0
                ],
                "wikipedia_search": [
                    1.7810646011865523,
                    0.91113216875412,
                    1.3078032300593276
                ],
                "answer_relation_to_question": [
                    1.5743243243243243,
                    0.6932432432432433,
                    1.7324324324324325
                ],
                "word_count_appended_bing": [
                    14.0,
                    31.0,
                    32.0
                ],
                "answer_relation_to_question_bing": [
                    1.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "cosine_similarity_raw": [
                    0.1567915380001068,
                    0.07440871000289917,
                    0.03835338354110718
                ],
                "result_count_noun_chunks": [
                    231000.0,
                    2240000.0,
                    1650000.0
                ],
                "question_answer_similarity": [
                    6.333364944206551,
                    1.5407007150352001,
                    1.8031123355031013
                ],
                "word_count_noun_chunks": [
                    4.0,
                    0.0,
                    2.0
                ],
                "result_count_bing": [
                    5470000.0,
                    7440000.0,
                    1970000.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    2.0
                ],
                "result_count": [
                    53500.0,
                    71900.0,
                    1200000.0
                ],
                "word_count_appended": [
                    103.0,
                    192.0,
                    219.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What does bicameral mean?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "two chambers"
            ],
            "question": "what does bicameral mean?",
            "lines": [
                [
                    0.041666666666666664,
                    0.0,
                    0.02274556087715969,
                    0.29200055592486107,
                    0.0005683432793407218,
                    0.23804226918798665,
                    0.0012620712148498985,
                    0.0024385705471603875,
                    0.75,
                    0.11627906976744186,
                    0.2,
                    0.11151887673626804,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.14583333333333334,
                    0.0,
                    0.023334722224016384,
                    0.34600395587310373,
                    0.038931514634839445,
                    0.41991101223581756,
                    0.03871297097236206,
                    0.028988629797569263,
                    0.25,
                    0.4069767441860465,
                    0.13333333333333333,
                    0.28295820904516555,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.8125,
                    1.0,
                    0.9539197168988239,
                    0.36199548820203514,
                    0.9605001420858198,
                    0.3420467185761958,
                    0.9600249578127881,
                    0.9685727996552703,
                    0.0,
                    0.47674418604651164,
                    0.6666666666666666,
                    0.6055229142185664,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "two camels",
                "two cameras",
                "two chambers"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "two camels": 0.03952346749007423,
                "two cameras": 0.04624950594922008,
                "two chambers": 0.9828168521917592
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.22303775347253607,
                    0.5659164180903311,
                    1.2110458284371328
                ],
                "result_count_important_words": [
                    89.0,
                    2730.0,
                    67700.0
                ],
                "wikipedia_search": [
                    0.75,
                    0.25,
                    0.0
                ],
                "word_count_appended_bing": [
                    3.0,
                    2.0,
                    10.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "cosine_similarity_raw": [
                    0.005378692410886288,
                    0.005518012680113316,
                    0.22557547688484192
                ],
                "result_count_noun_chunks": [
                    249.0,
                    2960.0,
                    98900.0
                ],
                "question_answer_similarity": [
                    1.9019060218706727,
                    2.2536498438566923,
                    2.357808521017432
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    9.0
                ],
                "result_count_bing": [
                    856000.0,
                    1510000.0,
                    1230000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    5.0
                ],
                "result_count": [
                    40.0,
                    2740.0,
                    67600.0
                ],
                "answer_relation_to_question": [
                    0.08333333333333333,
                    0.2916666666666667,
                    1.625
                ],
                "word_count_appended": [
                    20.0,
                    70.0,
                    82.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these is an actual Winter Olympics event?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "skeleton"
            ],
            "question": "which of these is an actual winter olympics event?",
            "lines": [
                [
                    0.7060810810810811,
                    0.578853046594982,
                    0.5287595412229754,
                    1.0,
                    0.9999527972592598,
                    0.5777007510109763,
                    0.9998799393693816,
                    0.9999503380369029,
                    0.2,
                    0.7703984819734345,
                    0.5846153846153846,
                    0.6702022684188048,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.0472972972972973,
                    0.21057347670250895,
                    0.00586782562946426,
                    0.0,
                    0.0,
                    0.21086077411900636,
                    0.0,
                    0.0,
                    0.0,
                    0.030360531309297913,
                    0.2076923076923077,
                    0.044778086316803425,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.24662162162162163,
                    0.21057347670250895,
                    0.4653726331475603,
                    0.0,
                    4.720274074018866e-05,
                    0.21143847487001732,
                    0.00012006063061846233,
                    4.9661963097134776e-05,
                    0.8,
                    0.19924098671726756,
                    0.2076923076923077,
                    0.28501964526439166,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "skeleton",
                "ghostathlon",
                "frankensteining"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "frankensteining": 0.1891977416997796,
                "skeleton": 0.9105832673213091,
                "ghostathlon": 0.05619922535914049
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.6808090736752193,
                    0.1791123452672137,
                    1.1400785810575667
                ],
                "result_count_important_words": [
                    533000.0,
                    0,
                    64.0
                ],
                "wikipedia_search": [
                    0.4,
                    0.0,
                    1.6
                ],
                "word_count_appended_bing": [
                    76.0,
                    27.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    1.736559139784946,
                    0.6317204301075269,
                    0.6317204301075269
                ],
                "cosine_similarity_raw": [
                    0.13465090095996857,
                    0.0014942671405151486,
                    0.11850915104150772
                ],
                "result_count_noun_chunks": [
                    1490000.0,
                    0,
                    74.0
                ],
                "question_answer_similarity": [
                    1.8447708636522293,
                    0.0,
                    0.0
                ],
                "word_count_noun_chunks": [
                    7.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    1000000.0,
                    365000.0,
                    366000.0
                ],
                "word_count_raw": [
                    5.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    1610000.0,
                    0,
                    76.0
                ],
                "answer_relation_to_question": [
                    2.8243243243243246,
                    0.1891891891891892,
                    0.9864864864864865
                ],
                "word_count_appended": [
                    406.0,
                    16.0,
                    105.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of the following is a dice-based game?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "farkle"
            ],
            "lines": [
                [
                    0.3883090142527342,
                    0.46782178217821785,
                    0.20038423710254116,
                    -0.0,
                    0.9976120153333752,
                    0.03831319105168424,
                    0.9976089056388656,
                    0.9972516686297606,
                    0.0,
                    0.21513513513513513,
                    0.2191780821917808,
                    0.35061828379368815,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.05788297723781594,
                    0.0,
                    0.02319628529757626,
                    -13.448183182848236,
                    0.0011782819078740652,
                    0.48084340447415785,
                    0.0011084543395987396,
                    0.001287789556340793,
                    0.0,
                    0.2972972972972973,
                    0.19863013698630136,
                    0.30327997294484654,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.5538080085094498,
                    0.5321782178217821,
                    0.7764194775998826,
                    14.448183182848236,
                    0.001209702758750707,
                    0.48084340447415785,
                    0.0012826400215356843,
                    0.0014605418138987044,
                    1.0,
                    0.4875675675675676,
                    0.5821917808219178,
                    0.3461017432614654,
                    1.0,
                    1.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "sparkle": -0.8631056130576024,
                "quirkle": 0.348016593950556,
                "farkle": 1.5150890191070463
            },
            "question": "which of the following is a dice-based game?",
            "rate_limited": false,
            "answers": [
                "quirkle",
                "sparkle",
                "farkle"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sparkle": 0.1260103156444987,
                "quirkle": 0.2911571601508031,
                "farkle": 0.776432132067992
            },
            "integer_answers": {
                "sparkle": 2,
                "quirkle": 4,
                "farkle": 8
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.4024731351747524,
                    1.213119891779386,
                    1.3844069730458615
                ],
                "result_count_important_words": [
                    63000.0,
                    70.0,
                    81.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    4.0
                ],
                "word_count_appended_bing": [
                    32.0,
                    29.0,
                    85.0
                ],
                "answer_relation_to_question_bing": [
                    1.4034653465346536,
                    0.0,
                    1.5965346534653464
                ],
                "cosine_similarity_raw": [
                    0.11361358314752579,
                    0.013151798397302628,
                    0.44021326303482056
                ],
                "result_count_noun_chunks": [
                    63500.0,
                    82.0,
                    93.0
                ],
                "question_answer_similarity": [
                    0.0,
                    1.0122998552396894,
                    -1.087573952972889
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    4.0
                ],
                "result_count_bing": [
                    149000.0,
                    1870000.0,
                    1870000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    3.0
                ],
                "result_count": [
                    63500.0,
                    75.0,
                    77.0
                ],
                "answer_relation_to_question": [
                    1.1649270427582026,
                    0.17364893171344783,
                    1.6614240255283494
                ],
                "word_count_appended": [
                    199.0,
                    275.0,
                    451.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "An extremely impolite guest at a Nixon family wedding went on to write under what nom de plume?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "miss manners"
            ],
            "question": "an extremely impolite guest at a nixon family wedding went on to write under what nom de plume?",
            "lines": [
                [
                    0.15543300653594772,
                    0.12138528138528137,
                    0.2273907976296619,
                    0.016257219829710633,
                    0.0425531914893617,
                    0.4350758853288364,
                    0.046511627906976744,
                    0.008236101578586136,
                    0.2,
                    0.21428571428571427,
                    0.3333333333333333,
                    0.24690402372802325,
                    0,
                    0,
                    1.0
                ],
                [
                    0.47198655103066867,
                    0.6511688311688312,
                    0.25694110338339105,
                    0.5891650971595644,
                    0.2553191489361702,
                    0.1973018549747049,
                    0.27906976744186046,
                    0.24365133836650651,
                    0.6444444444444445,
                    0.5714285714285714,
                    0.3333333333333333,
                    0.3323882342129658,
                    0,
                    0,
                    1.0
                ],
                [
                    0.3725804424333836,
                    0.22744588744588742,
                    0.5156680989869471,
                    0.3945776830107249,
                    0.7021276595744681,
                    0.3676222596964587,
                    0.6744186046511628,
                    0.7481125600549073,
                    0.15555555555555556,
                    0.21428571428571427,
                    0.3333333333333333,
                    0.42070774205901096,
                    0,
                    0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "dorothy dix",
                "miss manners",
                "dear abby"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "dorothy dix": 0.018791842409896915,
                "miss manners": 0.4593079293543569,
                "dear abby": 0.2760696860937856
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.4690402372802325,
                    3.323882342129658,
                    4.207077420590109
                ],
                "result_count_important_words": [
                    2.0,
                    12.0,
                    29.0
                ],
                "wikipedia_search": [
                    1.0,
                    3.2222222222222223,
                    0.7777777777777778
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.6069264069264069,
                    3.255844155844156,
                    1.1372294372294371
                ],
                "cosine_similarity_raw": [
                    0.052895039319992065,
                    0.059768952429294586,
                    0.11995333433151245
                ],
                "result_count_noun_chunks": [
                    12.0,
                    355.0,
                    1090.0
                ],
                "question_answer_similarity": [
                    0.22684586932882667,
                    8.220942451618612,
                    5.505757962178905
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    12900.0,
                    5850.0,
                    10900.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    3.0,
                    8.0,
                    3.0
                ],
                "answer_relation_to_question": [
                    1.2434640522875817,
                    3.7758924082453493,
                    2.980643539467069
                ],
                "result_count": [
                    2.0,
                    12.0,
                    33.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "The cooking technique now known as tempura originated in which of these places?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "portugal"
            ],
            "question": "the cooking technique now known as tempura originated in which of these places?",
            "lines": [
                [
                    0.0707070707070707,
                    0.1,
                    0.21026694821081435,
                    0.2245841067403735,
                    0.08612056879631484,
                    0.22310756972111553,
                    0.1414885777450258,
                    0.20006001800540163,
                    0.3144736842105263,
                    0.46439628482972134,
                    0.573170731707317,
                    0.33247644090602574,
                    1.0,
                    0,
                    -1.0
                ],
                [
                    0.4343434343434343,
                    0.1,
                    0.2303295288923134,
                    0.6767489539327955,
                    0.028640096134588424,
                    0.31274900398406374,
                    0.040530582166543844,
                    0.06772031609482845,
                    0.27850877192982454,
                    0.06501547987616099,
                    0.04878048780487805,
                    0.30739759638766423,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.494949494949495,
                    0.8,
                    0.5594035228968722,
                    0.09866693932683095,
                    0.8852393350690967,
                    0.4641434262948207,
                    0.8179808400884304,
                    0.73221966589977,
                    0.4070175438596491,
                    0.47058823529411764,
                    0.3780487804878049,
                    0.36012596270631003,
                    0.0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "hawaii",
                "south korea",
                "portugal"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "south korea": 0.2186375116405095,
                "hawaii": 0.3811672344716341,
                "portugal": 0.5420986139410431
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9948586454361545,
                    1.8443855783259853,
                    2.16075577623786
                ],
                "result_count_important_words": [
                    192000.0,
                    55000.0,
                    1110000.0
                ],
                "wikipedia_search": [
                    0.9434210526315789,
                    0.8355263157894737,
                    1.2210526315789474
                ],
                "word_count_appended_bing": [
                    47.0,
                    4.0,
                    31.0
                ],
                "answer_relation_to_question_bing": [
                    0.2,
                    0.2,
                    1.6
                ],
                "cosine_similarity_raw": [
                    0.024170804768800735,
                    0.026477057486772537,
                    0.06430508196353912
                ],
                "result_count_noun_chunks": [
                    200000.0,
                    67700.0,
                    732000.0
                ],
                "question_answer_similarity": [
                    1.5544354915618896,
                    4.684047362650745,
                    0.6829129387624562
                ],
                "word_count_noun_chunks": [
                    4.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    112000.0,
                    157000.0,
                    233000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    300.0,
                    42.0,
                    304.0
                ],
                "answer_relation_to_question": [
                    0.21212121212121213,
                    1.303030303030303,
                    1.4848484848484849
                ],
                "result_count": [
                    215000.0,
                    71500.0,
                    2210000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "According to Alexa, which of these is NOT one the top five most popular sports sites?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "yahoo sports"
            ],
            "lines": [
                [
                    0.3687467997951869,
                    0.24922600619195046,
                    0.25912978148387644,
                    0.21925303677174135,
                    0.49990614007166,
                    0.3323952470293934,
                    0.4918394353771504,
                    0.4649870322341608,
                    0.29144171779141104,
                    0.4263005780346821,
                    0.46875,
                    0.3386173191211396,
                    0.375,
                    0,
                    -1.0
                ],
                [
                    0.24646057347670247,
                    0.2786377708978328,
                    0.40437095477272944,
                    0.22396468380442386,
                    0.17761155048439886,
                    0.33364602876798,
                    0.3892319756898495,
                    0.1785846609855502,
                    0.37903374233128834,
                    0.3916184971098266,
                    0.453125,
                    0.31804858789231055,
                    0.125,
                    0,
                    -1.0
                ],
                [
                    0.3847926267281106,
                    0.47213622291021673,
                    0.33649926374339406,
                    0.5567822794238347,
                    0.32248230944394113,
                    0.3339587242026266,
                    0.11892858893300007,
                    0.35642830678028903,
                    0.3295245398773006,
                    0.18208092485549132,
                    0.078125,
                    0.3433340929865498,
                    0.5,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "deadspin": 0.3361426338638838,
                "sports illustrated": 0.4001024575057089,
                "yahoo sports": 0.2637549086304073
            },
            "question": "according to alexa, which of these is not one the top five most popular sports sites?",
            "rate_limited": false,
            "answers": [
                "yahoo sports",
                "sports illustrated",
                "deadspin"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "deadspin": 0.4550972461882418,
                "sports illustrated": 0.21524611520079845,
                "yahoo sports": 0.5241355690191698
            },
            "integer_answers": {
                "deadspin": 3,
                "sports illustrated": 5,
                "yahoo sports": 5
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6138268087886036,
                    1.8195141210768944,
                    1.5666590701345018
                ],
                "result_count_important_words": [
                    66600.0,
                    904000.0,
                    3110000.0
                ],
                "wikipedia_search": [
                    2.0855828220858896,
                    1.2096625766871165,
                    1.7047546012269938
                ],
                "word_count_appended_bing": [
                    2.0,
                    3.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    0.5015479876160991,
                    0.44272445820433437,
                    0.05572755417956656
                ],
                "cosine_similarity_raw": [
                    0.09082356095314026,
                    0.036058299243450165,
                    0.061650291085243225
                ],
                "result_count_noun_chunks": [
                    37800.0,
                    347000.0,
                    155000.0
                ],
                "question_answer_similarity": [
                    9.299221996872802,
                    9.143157435304602,
                    -1.8808075990527868
                ],
                "word_count_noun_chunks": [
                    1.0,
                    3.0,
                    0.0
                ],
                "result_count_bing": [
                    53600000.0,
                    53200000.0,
                    53100000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    92.0,
                    316000.0,
                    174000.0
                ],
                "answer_relation_to_question": [
                    1.0500256016385048,
                    2.02831541218638,
                    0.9216589861751152
                ],
                "word_count_appended": [
                    51.0,
                    75.0,
                    220.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these acclaimed authors did NOT write a book titled \u201cThe Great American Novel\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "william carlos williams"
            ],
            "question": "which of these acclaimed authors did not write a book titled \u201cthe great american novel\u201d?",
            "lines": [
                [
                    0.3021353807841731,
                    0.3052478270143444,
                    0.3470462634637338,
                    0.2960628941857492,
                    0.3806818181818182,
                    0.2824427480916031,
                    0.41304347826086957,
                    0.32876005776769135,
                    0.3333866805397055,
                    0.4661016949152542,
                    0.39285714285714285,
                    0.37476505909184066,
                    0,
                    0.5,
                    -1.0
                ],
                [
                    0.33665727008655755,
                    0.30575441262731295,
                    0.19951758657575824,
                    0.4531885753222268,
                    0.22159090909090912,
                    0.36259541984732824,
                    0.19182990922121357,
                    0.421910460078399,
                    0.2848027861817898,
                    0.19915254237288138,
                    0.25,
                    0.2165592757036514,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.3612073491292693,
                    0.38899776035834266,
                    0.45343614996050796,
                    0.250748530492024,
                    0.3977272727272727,
                    0.3549618320610687,
                    0.39512661251791686,
                    0.24932948215390965,
                    0.38181053327850484,
                    0.3347457627118644,
                    0.35714285714285715,
                    0.40867566520450793,
                    0,
                    0.5,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "e.l. doctorow",
                "philip roth",
                "william carlos williams"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "philip roth": 0.10396579408566502,
                "william carlos williams": 0.6077323995110641,
                "e.l. doctorow": 0.3500942954648251
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.003759054530549,
                    4.535051588741577,
                    1.4611893567278735
                ],
                "result_count_important_words": [
                    36400.0,
                    129000.0,
                    43900.0
                ],
                "wikipedia_search": [
                    2.6658131113647126,
                    3.4431554210913644,
                    1.891031467543923
                ],
                "word_count_appended_bing": [
                    12.0,
                    28.0,
                    16.0
                ],
                "answer_relation_to_question_bing": [
                    2.3370260758278674,
                    2.330947048472244,
                    1.3320268756998879
                ],
                "cosine_similarity_raw": [
                    0.07458346337080002,
                    0.14652155339717865,
                    0.02270551398396492
                ],
                "result_count_noun_chunks": [
                    166000.0,
                    75700.0,
                    243000.0
                ],
                "question_answer_similarity": [
                    3.31491569429636,
                    0.7609008949948475,
                    4.0514824646525085
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    228000.0,
                    144000.0,
                    152000.0
                ],
                "word_count_raw": [
                    0.0,
                    5.0,
                    0.0
                ],
                "word_count_appended": [
                    8.0,
                    71.0,
                    39.0
                ],
                "answer_relation_to_question": [
                    3.1658339074532296,
                    2.613483678615079,
                    2.2206824139316916
                ],
                "result_count": [
                    42.0,
                    98.0,
                    36.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "The \u201cAmerican Craftsman\u201d style of house was an architectural reaction to what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "industrial revolution"
            ],
            "lines": [
                [
                    0.17899091914015036,
                    0.24887935145445872,
                    0.20550140654288626,
                    0.37899132670912994,
                    0.0009731290808638875,
                    0.42342342342342343,
                    0.4986217457886677,
                    0.017676767676767676,
                    0.45637403125397025,
                    0.22807017543859648,
                    0.2,
                    0.28966956211352496,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.3212077817670221,
                    0.510824988078207,
                    0.6480408076388888,
                    0.20377879766972998,
                    0.9982420894023104,
                    0.36486486486486486,
                    0.01010719754977029,
                    0.9659090909090909,
                    0.37042391902765426,
                    0.6228070175438597,
                    0.6666666666666666,
                    0.5008923684449349,
                    0.9,
                    1.0,
                    1.0
                ],
                [
                    0.49980129909282756,
                    0.2402956604673343,
                    0.14645778581822486,
                    0.4172298756211401,
                    0.0007847815168257157,
                    0.21171171171171171,
                    0.491271056661562,
                    0.016414141414141416,
                    0.17320204971837544,
                    0.14912280701754385,
                    0.13333333333333333,
                    0.20943806944154017,
                    0.1,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "world war i": 0.22336941704445998,
                "industrial revolution": 0.5774118278259286,
                "the great depression": 0.19921875512961149
            },
            "question": "the \u201camerican craftsman\u201d style of house was an architectural reaction to what?",
            "rate_limited": false,
            "answers": [
                "world war i",
                "industrial revolution",
                "the great depression"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "world war i": 0.05937964371759157,
                "industrial revolution": 0.841973849738393,
                "the great depression": 0.06636761137326008
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7380173726811496,
                    3.0053542106696094,
                    1.256628416649241
                ],
                "result_count_important_words": [
                    4070000.0,
                    82500.0,
                    4010000.0
                ],
                "wikipedia_search": [
                    2.2818701562698513,
                    1.8521195951382712,
                    0.8660102485918773
                ],
                "word_count_appended_bing": [
                    3.0,
                    10.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.7466380543633762,
                    1.532474964234621,
                    0.7208869814020029
                ],
                "cosine_similarity_raw": [
                    0.02848064713180065,
                    0.08981262892484665,
                    0.020297732204198837
                ],
                "result_count_noun_chunks": [
                    56.0,
                    3060.0,
                    52.0
                ],
                "question_answer_similarity": [
                    13.826534636318684,
                    7.434351146221161,
                    15.22157083824277
                ],
                "word_count_noun_chunks": [
                    0.0,
                    9.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    2.0,
                    0.0
                ],
                "result_count_bing": [
                    188000.0,
                    162000.0,
                    94000.0
                ],
                "result_count": [
                    62.0,
                    63600.0,
                    50.0
                ],
                "answer_relation_to_question": [
                    0.8949545957007519,
                    1.6060389088351104,
                    2.499006495464138
                ],
                "word_count_appended": [
                    26.0,
                    71.0,
                    17.0
                ]
            },
            "integer_answers": {
                "world war i": 3,
                "industrial revolution": 9,
                "the great depression": 2
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "On a compass, what does the letter N typically stand for?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "north"
            ],
            "question": "on a compass, what does the letter n typically stand for?",
            "lines": [
                [
                    0.30000000000000004,
                    0.5,
                    0.1644172782365177,
                    -2.060070921109513,
                    1.6822146917154663e-05,
                    0.35325365205843295,
                    0.0005627834607872347,
                    0.009251233792652079,
                    0.06666666666666667,
                    0.12053571428571429,
                    0.2523364485981308,
                    0.14453782234686985,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.48333333333333334,
                    0.3333333333333333,
                    0.681968237388154,
                    2.917322096681714,
                    0.6043512040607416,
                    0.29282868525896416,
                    0.9517661469195882,
                    0.903896839014381,
                    0.09999999999999999,
                    0.5491071428571429,
                    0.4485981308411215,
                    0.4975711551073515,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.2166666666666667,
                    0.16666666666666666,
                    0.1536144843753283,
                    0.1427488244277989,
                    0.39563197379234116,
                    0.35391766268260294,
                    0.04767106961962459,
                    0.08685192719296694,
                    0.8333333333333334,
                    0.33035714285714285,
                    0.29906542056074764,
                    0.3578910225457787,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "norbit",
                "north",
                "nordic"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "norbit": 0.028510123381052657,
                "north": 1.030948961234666,
                "nordic": 0.32726036020510035
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.5781512893874793,
                    1.9902846204294058,
                    1.4315640901831146
                ],
                "result_count_important_words": [
                    6800.0,
                    11500000.0,
                    576000.0
                ],
                "wikipedia_search": [
                    0.2,
                    0.3,
                    2.5
                ],
                "word_count_appended_bing": [
                    27.0,
                    48.0,
                    32.0
                ],
                "answer_relation_to_question_bing": [
                    1.5,
                    1.0,
                    0.5
                ],
                "cosine_similarity_raw": [
                    0.025507573038339615,
                    0.10580004006624222,
                    0.02383163571357727
                ],
                "result_count_noun_chunks": [
                    52300.0,
                    5110000.0,
                    491000.0
                ],
                "question_answer_similarity": [
                    -2.054556777700782,
                    2.909513369202614,
                    0.14236673200502992
                ],
                "word_count_noun_chunks": [
                    0.0,
                    23.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    26.0,
                    0.0
                ],
                "result_count_bing": [
                    53200000.0,
                    44100000.0,
                    53300000.0
                ],
                "word_count_appended": [
                    81.0,
                    369.0,
                    222.0
                ],
                "answer_relation_to_question": [
                    1.2,
                    1.9333333333333331,
                    0.8666666666666667
                ],
                "result_count": [
                    54.0,
                    1940000.0,
                    1270000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "When did the Super Bowl-winning 1985 Chicago Bears record \u201cThe Super Bowl Shuffle\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "during regular season"
            ],
            "question": "when did the super bowl-winning 1985 chicago bears record \u201cthe super bowl shuffle\u201d?",
            "lines": [
                [
                    0.561811264058455,
                    0.15,
                    0.19707032747699327,
                    0.32138428698041366,
                    0.0,
                    0.2570514684501308,
                    0.49647160023150977,
                    0.6825196450897846,
                    0.20054945054945053,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.2539399377522556,
                    0,
                    0,
                    2.0
                ],
                [
                    0.18777394058292934,
                    0.2,
                    0.5517359999318381,
                    0.3692033414894501,
                    1.0,
                    0.05670252980517592,
                    0.00015398584392207253,
                    0.0003805198021297029,
                    0.5989010989010989,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.4921201244954888,
                    0,
                    0,
                    2.0
                ],
                [
                    0.2504147953586156,
                    0.65,
                    0.2511936725911687,
                    0.30941237153013623,
                    0.0,
                    0.6862460017446932,
                    0.5033744139245682,
                    0.31709983510808576,
                    0.20054945054945053,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.2539399377522556,
                    0,
                    0,
                    2.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "week before sb",
                "during regular season",
                "week after sb"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "week after sb": 0.19015107291173972,
                "week before sb": 0.1668984862825972,
                "during regular season": 0.6435820696314984
            },
            "categorical_data": {
                "question_type": 2
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.539399377522556,
                    4.921201244954888,
                    2.539399377522556
                ],
                "result_count_important_words": [
                    93500.0,
                    29.0,
                    94800.0
                ],
                "wikipedia_search": [
                    1.4038461538461537,
                    4.1923076923076925,
                    1.4038461538461537
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.75,
                    1.0,
                    3.25
                ],
                "cosine_similarity_raw": [
                    0.026711050420999527,
                    0.07478268444538116,
                    0.0340469665825367
                ],
                "result_count_noun_chunks": [
                    113000.0,
                    63.0,
                    52500.0
                ],
                "question_answer_similarity": [
                    12.433622437529266,
                    14.283632202073932,
                    11.970456431619823
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    884000.0,
                    195000.0,
                    2360000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    0,
                    11.0,
                    0
                ],
                "answer_relation_to_question": [
                    5.056301376526095,
                    1.689965465246364,
                    2.2537331582275404
                ],
                "word_count_appended": [
                    3.0,
                    3.0,
                    3.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "A healthy parathyroid gland is roughly the same color as a common type of what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "mustard",
                "middle"
            ],
            "question": "a healthy parathyroid gland is roughly the same color as a common type of what?",
            "lines": [
                [
                    0.5064916564916565,
                    0.5078305967242687,
                    0.25225574469212364,
                    0.26029054792135253,
                    0.13984674329501914,
                    0.3254437869822485,
                    0.3956953642384106,
                    0.363395225464191,
                    0.30338699026506577,
                    0.3034825870646766,
                    0.3176470588235294,
                    0.30480971408226404,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.2806064306064306,
                    0.2349230264641371,
                    0.3645442307041335,
                    0.3977444580427364,
                    0.7375478927203065,
                    0.3668639053254438,
                    0.22019867549668873,
                    0.3156498673740053,
                    0.42628361250559177,
                    0.3805970149253731,
                    0.36470588235294116,
                    0.36254680480441,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.21290191290191288,
                    0.2572463768115942,
                    0.3832000246037429,
                    0.3419649940359111,
                    0.12260536398467432,
                    0.3076923076923077,
                    0.3841059602649007,
                    0.3209549071618037,
                    0.27032939722934257,
                    0.31592039800995025,
                    0.3176470588235294,
                    0.3326434811133259,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "mayonnaise",
                "mustard",
                "ketchup"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "mayonnaise": 0.08279033718623172,
                "mustard": 0.7511192374016856,
                "ketchup": 0.16690510328278813
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.1336679985758487,
                    2.53782763363087,
                    2.3285043677932817
                ],
                "result_count_important_words": [
                    23900.0,
                    13300.0,
                    23200.0
                ],
                "wikipedia_search": [
                    1.5169349513253287,
                    2.131418062527959,
                    1.3516469861467129
                ],
                "word_count_appended_bing": [
                    27.0,
                    31.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    2.0313223868970747,
                    0.9396921058565484,
                    1.0289855072463767
                ],
                "cosine_similarity_raw": [
                    0.06434997171163559,
                    0.0929945558309555,
                    0.0977536141872406
                ],
                "result_count_noun_chunks": [
                    13700.0,
                    11900.0,
                    12100.0
                ],
                "question_answer_similarity": [
                    1.595263160765171,
                    2.4376877546310425,
                    2.0958277648314834
                ],
                "word_count_noun_chunks": [
                    0.0,
                    3.0,
                    0.0
                ],
                "result_count_bing": [
                    33000.0,
                    37200.0,
                    31200.0
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count": [
                    29200.0,
                    154000.0,
                    25600.0
                ],
                "answer_relation_to_question": [
                    2.5324582824582826,
                    1.403032153032153,
                    1.0645095645095644
                ],
                "word_count_appended": [
                    122.0,
                    153.0,
                    127.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "The '70s sitcom \u201cThree's Company\u201d was about three people who were what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "roommates"
            ],
            "lines": [
                [
                    0.26666666666666666,
                    0.3333333333333333,
                    0.3514713519660129,
                    0.12018798833433601,
                    0.9998579436801329,
                    0.4312796208530806,
                    0.9081290219028049,
                    0.9992956506427187,
                    0.28205128205128205,
                    0.9514563106796117,
                    0.8809523809523809,
                    0.7928649211502165,
                    1.0,
                    0,
                    1.0
                ],
                [
                    0.43703703703703706,
                    0.0,
                    0.4099379525524295,
                    0.6151645829190068,
                    0.0,
                    0.4834123222748815,
                    2.1119279579134998e-06,
                    0.0,
                    0.3333333333333333,
                    0.019417475728155338,
                    0.047619047619047616,
                    0.06437321271039383,
                    0.0,
                    0,
                    1.0
                ],
                [
                    0.2962962962962963,
                    0.6666666666666666,
                    0.23859069548155762,
                    0.2646474287466572,
                    0.0001420563198671227,
                    0.08530805687203792,
                    0.09186886616923723,
                    0.0007043493572812115,
                    0.3846153846153846,
                    0.02912621359223301,
                    0.07142857142857142,
                    0.1427618661393896,
                    0.0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "trivia show hosts": 0.18540746739248024,
                "roommates": 0.6398112670932753,
                "panda bears": 0.1747812655142446
            },
            "question": "the '70s sitcom \u201cthree's company\u201d was about three people who were what?",
            "rate_limited": false,
            "answers": [
                "roommates",
                "trivia show hosts",
                "panda bears"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "trivia show hosts": 0.2394318473248863,
                "roommates": 0.5112819522001698,
                "panda bears": 0.18469633983261868
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.3785947634506495,
                    0.1931196381311815,
                    0.4282855984181688
                ],
                "result_count_important_words": [
                    1720000.0,
                    4.0,
                    174000.0
                ],
                "wikipedia_search": [
                    0.8461538461538461,
                    1.0,
                    1.1538461538461537
                ],
                "word_count_appended_bing": [
                    37.0,
                    2.0,
                    3.0
                ],
                "answer_relation_to_question_bing": [
                    0.3333333333333333,
                    0.0,
                    0.6666666666666666
                ],
                "cosine_similarity_raw": [
                    0.06192227825522423,
                    0.07222293317317963,
                    0.042034946382045746
                ],
                "result_count_noun_chunks": [
                    90800.0,
                    0,
                    64.0
                ],
                "question_answer_similarity": [
                    2.478737026453018,
                    12.687051760964096,
                    5.458044432569295
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    45500000.0,
                    51000000.0,
                    9000000.0
                ],
                "word_count_appended": [
                    196.0,
                    4.0,
                    6.0
                ],
                "answer_relation_to_question": [
                    0.8,
                    1.3111111111111111,
                    0.8888888888888888
                ],
                "result_count": [
                    183000.0,
                    0,
                    26.0
                ]
            },
            "integer_answers": {
                "trivia show hosts": 4,
                "roommates": 7,
                "panda bears": 2
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these is NOT a pseudonym?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "george saunders"
            ],
            "question": "which of these is not a pseudonym?",
            "lines": [
                [
                    0.16666666666666669,
                    0,
                    0.2818592941739667,
                    0.34326175894206556,
                    0.3683651804670913,
                    0.3250230840258541,
                    0.3854762496891321,
                    0.32803180914512925,
                    0.5,
                    0.2948028673835126,
                    0.30180180180180183,
                    0.33213256484149856,
                    0.125,
                    0.0,
                    -1.0
                ],
                [
                    0.33333333333333337,
                    0,
                    0.26680659840995313,
                    0.214506460399297,
                    0.2760084925690021,
                    0.32456140350877194,
                    0.31720964934095996,
                    0.3339960238568589,
                    0.0,
                    0.30286738351254483,
                    0.2792792792792793,
                    0.29755043227665706,
                    0.375,
                    0.5,
                    -1.0
                ],
                [
                    0.5,
                    0,
                    0.45133410741608015,
                    0.4422317806586374,
                    0.3556263269639066,
                    0.35041551246537395,
                    0.29731410096990796,
                    0.33797216699801197,
                    0.5,
                    0.40232974910394265,
                    0.4189189189189189,
                    0.3703170028818444,
                    0.5,
                    0.5,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "george eliot",
                "george orwell",
                "george saunders"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "george eliot": 0.1140493649688756,
                "george saunders": 0.7217849555074225,
                "george orwell": 0.15078724546143288
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.3357348703170029,
                    0.4048991354466859,
                    0.25936599423631124
                ],
                "result_count_important_words": [
                    92100.0,
                    147000.0,
                    163000.0
                ],
                "wikipedia_search": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    44.0,
                    49.0,
                    18.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.07438582181930542,
                    0.07951878011226654,
                    0.016595033928751945
                ],
                "result_count_noun_chunks": [
                    173000.0,
                    167000.0,
                    163000.0
                ],
                "question_answer_similarity": [
                    0.299813250079751,
                    0.5460999524220824,
                    0.1105006504803896
                ],
                "word_count_noun_chunks": [
                    3.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    3790000.0,
                    3800000.0,
                    3240000.0
                ],
                "result_count": [
                    124000.0,
                    211000.0,
                    136000.0
                ],
                "answer_relation_to_question": [
                    0.6666666666666666,
                    0.3333333333333333,
                    0.0
                ],
                "word_count_appended": [
                    229.0,
                    220.0,
                    109.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these college mascots is NOT a bulldog?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "georgia state"
            ],
            "question": "which of these college mascots is not a bulldog?",
            "lines": [
                [
                    0.24570590699622957,
                    0.24783843873517786,
                    0.4370046103096058,
                    0.5052378299934491,
                    0.25766283524904215,
                    0.29301323484148967,
                    0.21114369501466276,
                    0.23376029277218663,
                    0.5,
                    0.15390879478827363,
                    0.11486486486486486,
                    0.32218896029562283,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.2823977098170647,
                    0.25413784584980237,
                    0.40613794893482125,
                    0.2667069878328024,
                    0.40948275862068967,
                    0.4393659587565405,
                    0.4090909090909091,
                    0.434583714547118,
                    0.33695652173913043,
                    0.38517915309446255,
                    0.39864864864864863,
                    0.33347949974336777,
                    0.5,
                    0,
                    -1.0
                ],
                [
                    0.4718963831867058,
                    0.4980237154150198,
                    0.15685744075557295,
                    0.22805518217374854,
                    0.3328544061302682,
                    0.26762080640196984,
                    0.3797653958944282,
                    0.33165599268069534,
                    0.16304347826086957,
                    0.4609120521172638,
                    0.4864864864864865,
                    0.34433153996100935,
                    0.5,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "gonzaga",
                "brooklyn college",
                "georgia state"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "gonzaga": 0.4529333829981847,
                "brooklyn college": 0.3929290134946206,
                "georgia state": 0.594749743360004
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.0668662382262628,
                    0.9991230015397932,
                    0.9340107602339439
                ],
                "result_count_important_words": [
                    98500.0,
                    31000.0,
                    41000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.32608695652173914,
                    0.6739130434782609
                ],
                "word_count_appended_bing": [
                    57.0,
                    15.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.0086462450592886,
                    0.9834486166007905,
                    0.007905138339920948
                ],
                "cosine_similarity_raw": [
                    0.053990308195352554,
                    0.08044463396072388,
                    0.2940909266471863
                ],
                "result_count_noun_chunks": [
                    58200.0,
                    14300.0,
                    36800.0
                ],
                "question_answer_similarity": [
                    -0.09593919850885868,
                    4.273133078590035,
                    4.981102459132671
                ],
                "word_count_noun_chunks": [
                    3.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    2690000.0,
                    788000.0,
                    3020000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    50600.0,
                    18900.0,
                    34900.0
                ],
                "answer_relation_to_question": [
                    1.0171763720150817,
                    0.8704091607317413,
                    0.11241446725317693
                ],
                "word_count_appended": [
                    425.0,
                    141.0,
                    48.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these is NOT a real animal?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "jackalope"
            ],
            "lines": [
                [
                    0.4005456349206349,
                    0.32333333333333336,
                    0.4032920644909005,
                    0.21366376270780446,
                    0.2712252160650737,
                    0.3372434017595308,
                    0.32577903682719545,
                    0.32521489971346706,
                    0.16379310344827586,
                    0.3179692718770875,
                    0.33088235294117646,
                    0.3518515402595749,
                    0.5,
                    0,
                    -1.0
                ],
                [
                    0.2757936507936508,
                    0.30666666666666664,
                    0.19572242734848366,
                    0.2863362372921955,
                    0.2534316217590239,
                    0.33431085043988273,
                    0.2577903682719547,
                    0.2578796561604585,
                    0.39655172413793105,
                    0.28891115564462255,
                    0.3088235294117647,
                    0.32711347988612904,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.3236607142857143,
                    0.37,
                    0.4009855081606158,
                    0.5,
                    0.4753431621759024,
                    0.32844574780058655,
                    0.4164305949008499,
                    0.4169054441260745,
                    0.4396551724137931,
                    0.3931195724782899,
                    0.3602941176470588,
                    0.3210349798542961,
                    0.5,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "liger": 0.4631797895672672,
                "wholphin": 0.1929423055625875,
                "jackalope": 0.3438779048701454
            },
            "question": "which of these is not a real animal?",
            "rate_limited": false,
            "answers": [
                "jackalope",
                "liger",
                "wholphin"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "liger": 0.07687381715380737,
                "wholphin": 0.2097234632991426,
                "jackalope": 0.35467681700370246
            },
            "integer_answers": {
                "liger": 9,
                "wholphin": 2,
                "jackalope": 1
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.5925938389617005,
                    0.6915460804554839,
                    0.7158600805828157
                ],
                "result_count_important_words": [
                    2460000.0,
                    3420000.0,
                    1180000.0
                ],
                "wikipedia_search": [
                    1.3448275862068966,
                    0.41379310344827586,
                    0.2413793103448276
                ],
                "answer_relation_to_question": [
                    0.3978174603174603,
                    0.8968253968253967,
                    0.7053571428571428
                ],
                "word_count_appended_bing": [
                    92.0,
                    104.0,
                    76.0
                ],
                "answer_relation_to_question_bing": [
                    0.7066666666666667,
                    0.7733333333333333,
                    0.52
                ],
                "cosine_similarity_raw": [
                    0.03330664709210396,
                    0.10479456186294556,
                    0.034101035445928574
                ],
                "result_count_noun_chunks": [
                    2440000.0,
                    3380000.0,
                    1160000.0
                ],
                "question_answer_similarity": [
                    -0.7042543757706881,
                    -0.5255137849599123,
                    0.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    6.0,
                    0.0
                ],
                "result_count_bing": [
                    111000000.0,
                    113000000.0,
                    117000000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    1800000.0,
                    1940000.0,
                    194000.0
                ],
                "word_count_appended": [
                    545.0,
                    632.0,
                    320.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Most humans have how many kidneys?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ten"
            ],
            "lines": [
                [
                    0.0,
                    0,
                    0.4362222221676091,
                    0.3905897319789175,
                    0.578163219924394,
                    0.37823834196891193,
                    0.09838895281933256,
                    0.5803571428571429,
                    0.0,
                    0.31019978969505785,
                    0.3723404255319149,
                    0.397393108722381,
                    0.2484076433121019,
                    0.2611464968152866,
                    5.0
                ],
                [
                    1.0,
                    0,
                    0.33473757294366,
                    0.25290540524516186,
                    0.014157586539174264,
                    0.17789291882556132,
                    0.1766398158803222,
                    0.014136904761904762,
                    0.0,
                    0.25236593059936907,
                    0.14893617021276595,
                    0.3014365343855173,
                    0.0,
                    0.0,
                    5.0
                ],
                [
                    0.0,
                    0,
                    0.22904020488873084,
                    0.3565048627759206,
                    0.40767919353643167,
                    0.4438687392055268,
                    0.7249712313003452,
                    0.4055059523809524,
                    1.0,
                    0.4374342797055731,
                    0.4787234042553192,
                    0.3011703568921017,
                    0.7515923566878981,
                    0.7388535031847133,
                    5.0
                ]
            ],
            "fraction_answers": {
                "zero": 0.20563144918411053,
                "two": 0.3116497750610039,
                "ten": 0.48271877575488564
            },
            "question": "most humans have how many kidneys?",
            "rate_limited": false,
            "answers": [
                "two",
                "zero",
                "ten"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "zero": 0.33154400644733284,
                "two": 0.3602427367210622,
                "ten": 0.5459680941164622
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 5
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.794786217444762,
                    0.6028730687710346,
                    0.6023407137842034
                ],
                "result_count_important_words": [
                    17100000.0,
                    30700000.0,
                    126000000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_appended_bing": [
                    70.0,
                    28.0,
                    90.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.11902302503585815,
                    0.09133298695087433,
                    0.06249351054430008
                ],
                "result_count_noun_chunks": [
                    156000000.0,
                    3800000.0,
                    109000000.0
                ],
                "question_answer_similarity": [
                    3.0198150128126144,
                    1.9553190395236015,
                    2.7562904208898544
                ],
                "word_count_noun_chunks": [
                    39.0,
                    0.0,
                    118.0
                ],
                "word_count_raw": [
                    41.0,
                    0.0,
                    116.0
                ],
                "result_count_bing": [
                    219000000.0,
                    103000000.0,
                    257000000.0
                ],
                "word_count_appended": [
                    295.0,
                    240.0,
                    416.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count": [
                    156000000.0,
                    3820000.0,
                    110000000.0
                ]
            },
            "integer_answers": {
                "zero": 1,
                "two": 5,
                "ten": 7
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What is the direct object in the sentence: \u201cYour cousin stole my sandwich\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "sandwich",
                "bottom"
            ],
            "question": "what is the direct object in the sentence: \u201cyour cousin stole my sandwich\u201d?",
            "lines": [
                [
                    0.5833476928108876,
                    0.4914529914529915,
                    0.20989406362552437,
                    0.28058219447031285,
                    0.9544182985025506,
                    0.032621073079241746,
                    0.33043478260869563,
                    0.3422897196261682,
                    0.2945205479452055,
                    0.30823529411764705,
                    0.31443298969072164,
                    0.3332541343594028,
                    0.2926829268292683,
                    0.2887700534759358,
                    1.0
                ],
                [
                    0.18333333333333332,
                    0.1188034188034188,
                    0.3812383123877394,
                    0.4675819451066722,
                    0.02616422576929406,
                    0.9377721213745059,
                    0.3507246376811594,
                    0.3352803738317757,
                    0.3116438356164384,
                    0.43686274509803924,
                    0.3402061855670103,
                    0.3244591486260681,
                    0.3861788617886179,
                    0.41711229946524064,
                    1.0
                ],
                [
                    0.233318973855779,
                    0.38974358974358975,
                    0.4088676239867362,
                    0.2518358604230149,
                    0.019417475728155338,
                    0.029606805546252262,
                    0.3188405797101449,
                    0.32242990654205606,
                    0.3938356164383562,
                    0.2549019607843137,
                    0.34536082474226804,
                    0.342286717014529,
                    0.32113821138211385,
                    0.29411764705882354,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "cousin",
                "you",
                "sandwich"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "cousin": 0.1252214300157074,
                "you": 0.18727614431295406,
                "sandwich": 0.5856912966727422
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9995248061564168,
                    1.9467548917564086,
                    2.053720302087174
                ],
                "result_count_important_words": [
                    228000.0,
                    242000.0,
                    220000.0
                ],
                "wikipedia_search": [
                    1.178082191780822,
                    1.2465753424657535,
                    1.5753424657534247
                ],
                "word_count_appended_bing": [
                    61.0,
                    66.0,
                    67.0
                ],
                "answer_relation_to_question_bing": [
                    2.4572649572649574,
                    0.594017094017094,
                    1.9487179487179487
                ],
                "cosine_similarity_raw": [
                    0.028969094157218933,
                    0.0526176318526268,
                    0.056430965662002563
                ],
                "result_count_noun_chunks": [
                    293000.0,
                    287000.0,
                    276000.0
                ],
                "question_answer_similarity": [
                    4.271620485931635,
                    7.118529453873634,
                    3.8339824895374477
                ],
                "word_count_noun_chunks": [
                    72.0,
                    95.0,
                    79.0
                ],
                "word_count_raw": [
                    54.0,
                    78.0,
                    55.0
                ],
                "result_count_bing": [
                    487000.0,
                    14000000.0,
                    442000.0
                ],
                "word_count_appended": [
                    393.0,
                    557.0,
                    325.0
                ],
                "answer_relation_to_question": [
                    2.916738464054438,
                    0.9166666666666666,
                    1.166594869278895
                ],
                "result_count": [
                    11600000.0,
                    318000.0,
                    236000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What\u2019s the anatomical name for the center of a hurricane?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "eye"
            ],
            "question": "what\u2019s the anatomical name for the center of a hurricane?",
            "lines": [
                [
                    0.5380434782608696,
                    0.3225806451612903,
                    0.08260538464659524,
                    -0.20634386211748887,
                    0.00973062774415569,
                    0.3319783197831978,
                    7.231195478429546e-05,
                    0.0020693523120660644,
                    0.33029893277964334,
                    0.17433751743375175,
                    0.21568627450980393,
                    0.21432648585059186,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.30434782608695654,
                    0.22580645161290322,
                    0.03825844414916527,
                    0.2870099329360067,
                    0.5114512875281833,
                    0.3346883468834688,
                    0.013855577350277593,
                    0.42160635890691783,
                    0.3633097431208861,
                    0.33751743375174337,
                    0.38562091503267976,
                    0.3084596575773779,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.15760869565217392,
                    0.45161290322580644,
                    0.8791361712042395,
                    0.9193339291814822,
                    0.4788180847276611,
                    0.3333333333333333,
                    0.9860721106949382,
                    0.5763242887810162,
                    0.3063913240994705,
                    0.4881450488145049,
                    0.39869281045751637,
                    0.47721385657203025,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "shinbone",
                "pancreas",
                "eye"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "shinbone": 0.1384244477213097,
                "eye": 0.9270373658920235,
                "pancreas": 0.07306813639472697
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.6429794575517755,
                    0.9253789727321338,
                    1.4316415697160907
                ],
                "result_count_important_words": [
                    14300.0,
                    2740000.0,
                    195000000.0
                ],
                "wikipedia_search": [
                    0.9908967983389301,
                    1.0899292293626583,
                    0.9191739722984116
                ],
                "word_count_appended_bing": [
                    33.0,
                    59.0,
                    61.0
                ],
                "answer_relation_to_question_bing": [
                    0.3225806451612903,
                    0.22580645161290322,
                    0.45161290322580644
                ],
                "cosine_similarity_raw": [
                    0.04492255300283432,
                    0.02080575004220009,
                    0.47809281945228577
                ],
                "result_count_noun_chunks": [
                    5350.0,
                    1090000.0,
                    1490000.0
                ],
                "question_answer_similarity": [
                    -0.854731080122292,
                    1.1888713696971536,
                    3.808125302195549
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    43.0
                ],
                "result_count_bing": [
                    24500000.0,
                    24700000.0,
                    24600000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    35.0
                ],
                "word_count_appended": [
                    125.0,
                    242.0,
                    350.0
                ],
                "answer_relation_to_question": [
                    1.0760869565217392,
                    0.6086956521739131,
                    0.31521739130434784
                ],
                "result_count": [
                    16400.0,
                    862000.0,
                    807000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which alien race was created by sci-fi legend Douglas Adams?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "vogons"
            ],
            "question": "which alien race was created by sci-fi legend douglas adams?",
            "lines": [
                [
                    0.49087238196663113,
                    0.6135590599876314,
                    0.8320740479556858,
                    0,
                    0.6362922230950511,
                    0.3333333333333333,
                    0.634020618556701,
                    0.9597314642996443,
                    0.2188917575062153,
                    0.6935483870967742,
                    0.38636363636363635,
                    0.6158683207294112,
                    0,
                    1.0,
                    -1.0
                ],
                [
                    0.2992944621938232,
                    0.09891774891774892,
                    0.04133570878941859,
                    0,
                    0.3587326525268395,
                    0.3333333333333333,
                    0.3556701030927835,
                    0.0320304466533502,
                    0.6602859055268694,
                    0.1774193548387097,
                    0.3068181818181818,
                    0.2579118382643037,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.20983315583954562,
                    0.2875231910946197,
                    0.12659024325489554,
                    0,
                    0.004975124378109453,
                    0.3333333333333333,
                    0.010309278350515464,
                    0.008238089047005567,
                    0.12082233696691527,
                    0.12903225806451613,
                    0.3068181818181818,
                    0.12621984100628514,
                    0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "vogons",
                "snarfs",
                "ovions"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "snarfs": 0.06584909848149813,
                "vogons": 1.004722996006491,
                "ovions": 0.016605709195842627
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    4.926946565835289,
                    2.063294706114429,
                    1.0097587280502809
                ],
                "result_count_important_words": [
                    2460.0,
                    1380.0,
                    40.0
                ],
                "wikipedia_search": [
                    1.5322423025435072,
                    4.622001338688086,
                    0.8457563587684069
                ],
                "word_count_appended_bing": [
                    34.0,
                    27.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    4.29491341991342,
                    0.6924242424242424,
                    2.012662337662338
                ],
                "cosine_similarity_raw": [
                    0.17997527122497559,
                    0.008940797299146652,
                    0.027381112799048424
                ],
                "result_count_noun_chunks": [
                    81200.0,
                    2710.0,
                    697.0
                ],
                "question_answer_similarity": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    217000.0,
                    217000.0,
                    217000.0
                ],
                "word_count_raw": [
                    3.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    2430.0,
                    1370.0,
                    19.0
                ],
                "answer_relation_to_question": [
                    3.926979055733049,
                    2.3943556975505857,
                    1.678665246716365
                ],
                "word_count_appended": [
                    215.0,
                    55.0,
                    40.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "The lyrics to \u201cThe Star-Spangled Banner\u201d were written during what conflict?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "the war of 1812"
            ],
            "lines": [
                [
                    0.11505602240896359,
                    0.28287461773700306,
                    0.09189460833883341,
                    0.3798194555148806,
                    0.4361111111111111,
                    0.3679694947569113,
                    0.45979381443298967,
                    0.5508241758241759,
                    0.19607344632768361,
                    0.3048128342245989,
                    0.38461538461538464,
                    0.32653634792600433,
                    0.1,
                    0.0,
                    1.0
                ],
                [
                    0.12686274509803921,
                    0.16258919469928645,
                    0.226469555201115,
                    0.17684281224940673,
                    0.3194444444444444,
                    0.4747378455672069,
                    0.32577319587628867,
                    0.3159340659340659,
                    0.0927401129943503,
                    0.21390374331550802,
                    0.15384615384615385,
                    0.30603759179765394,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.7580812324929972,
                    0.5545361875637105,
                    0.6816358364600515,
                    0.4433377322357126,
                    0.24444444444444444,
                    0.1572926596758818,
                    0.21443298969072164,
                    0.13324175824175824,
                    0.7111864406779661,
                    0.48128342245989303,
                    0.46153846153846156,
                    0.3674260602763417,
                    0.9,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "american revolution": 0.20679867578739428,
                "the war of 1812": 0.5077455161255672,
                "the civil war": 0.2854558080870386
            },
            "question": "the lyrics to \u201cthe star-spangled banner\u201d were written during what conflict?",
            "rate_limited": false,
            "answers": [
                "the civil war",
                "american revolution",
                "the war of 1812"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "american revolution": 0.07585346914536653,
                "the war of 1812": 0.776432132067992,
                "the civil war": 0.060160287364955066
            },
            "integer_answers": {
                "american revolution": 1,
                "the war of 1812": 10,
                "the civil war": 3
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9592180875560261,
                    1.8362255507859235,
                    2.2045563616580504
                ],
                "result_count_important_words": [
                    223000.0,
                    158000.0,
                    104000.0
                ],
                "wikipedia_search": [
                    1.1764406779661016,
                    0.5564406779661017,
                    4.267118644067796
                ],
                "word_count_appended_bing": [
                    5.0,
                    2.0,
                    6.0
                ],
                "answer_relation_to_question_bing": [
                    0.8486238532110092,
                    0.48776758409785936,
                    1.6636085626911314
                ],
                "cosine_similarity_raw": [
                    0.03359289467334747,
                    0.08278796821832657,
                    0.24917806684970856
                ],
                "result_count_noun_chunks": [
                    802000.0,
                    460000.0,
                    194000.0
                ],
                "question_answer_similarity": [
                    14.88093376904726,
                    6.928518638014793,
                    17.369514212419745
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    18.0
                ],
                "result_count_bing": [
                    386000.0,
                    498000.0,
                    165000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    8.0
                ],
                "result_count": [
                    157000.0,
                    115000.0,
                    88000.0
                ],
                "answer_relation_to_question": [
                    0.575280112044818,
                    0.634313725490196,
                    3.790406162464986
                ],
                "word_count_appended": [
                    57.0,
                    40.0,
                    90.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Catching a catfish with your bare hands is called what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "noodling"
            ],
            "lines": [
                [
                    0.03434343434343434,
                    0.0,
                    0.014583527288588062,
                    0.8138307149619655,
                    0.02710086329362911,
                    0.3318777292576419,
                    0.019532939310941327,
                    0.017388555169143217,
                    0.06666666666666667,
                    0.17766497461928935,
                    0.22875816993464052,
                    0.30416475927441833,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.15833333333333333,
                    0.0,
                    0.008060569149646845,
                    -0.19897317461342343,
                    0.0003278330237132554,
                    0.33449781659388644,
                    0.0002162318004901254,
                    0.0002709904701684657,
                    0.06666666666666667,
                    0.04060913705583756,
                    0.20261437908496732,
                    0.05542314514760412,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.8073232323232323,
                    1.0,
                    0.977355903561765,
                    0.385142459651458,
                    0.9725713036826577,
                    0.33362445414847164,
                    0.9802508288885685,
                    0.9823404543606883,
                    0.8666666666666666,
                    0.7817258883248731,
                    0.5686274509803921,
                    0.6404120955779776,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "noodling": 0.8068600527261964,
                "whiskering": 0.04771763769377791,
                "strumming": 0.14542230958002555
            },
            "question": "catching a catfish with your bare hands is called what?",
            "rate_limited": false,
            "answers": [
                "strumming",
                "whiskering",
                "noodling"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "noodling": 0.8537698832596685,
                "whiskering": 0.08752269759025946,
                "strumming": 0.06161239096497993
            },
            "integer_answers": {
                "noodling": 12,
                "whiskering": 1,
                "strumming": 1
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.5208237963720916,
                    0.2771157257380206,
                    3.202060477889888
                ],
                "result_count_important_words": [
                    542.0,
                    6.0,
                    27200.0
                ],
                "wikipedia_search": [
                    0.3333333333333333,
                    0.3333333333333333,
                    4.333333333333333
                ],
                "answer_relation_to_question": [
                    0.1717171717171717,
                    0.7916666666666666,
                    4.036616161616162
                ],
                "word_count_appended_bing": [
                    35.0,
                    31.0,
                    87.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    3.0
                ],
                "cosine_similarity_raw": [
                    0.011091290041804314,
                    0.006130348891019821,
                    0.7433138489723206
                ],
                "result_count_noun_chunks": [
                    770.0,
                    12.0,
                    43500.0
                ],
                "question_answer_similarity": [
                    1.4455587603151798,
                    -0.35342413396574557,
                    0.6841054856777191
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    144.0
                ],
                "result_count_bing": [
                    3800000.0,
                    3830000.0,
                    3820000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    73.0
                ],
                "result_count": [
                    496.0,
                    6.0,
                    17800.0
                ],
                "word_count_appended": [
                    105.0,
                    24.0,
                    462.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "By definition, which of these people is guilty of treason?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "quisling"
            ],
            "question": "by definition, which of these people is guilty of treason?",
            "lines": [
                [
                    0.13887249114521844,
                    0.38095238095238093,
                    0.17908013702702824,
                    0.4212688350474597,
                    0.1747448979591837,
                    0.41054313099041534,
                    0.15123859191655803,
                    0.12577319587628866,
                    0.0,
                    0.23539518900343642,
                    0.35353535353535354,
                    0.29133936649178743,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.20026564344746164,
                    0.38095238095238093,
                    0.18253786319738605,
                    0.3483683038470242,
                    0.1326530612244898,
                    0.4073482428115016,
                    0.1408083441981747,
                    0.1154639175257732,
                    0.0,
                    0.2027491408934708,
                    0.35353535353535354,
                    0.29293245451373323,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.66086186540732,
                    0.23809523809523808,
                    0.6383819997755857,
                    0.23036286110551607,
                    0.6926020408163265,
                    0.18210862619808307,
                    0.7079530638852672,
                    0.7587628865979381,
                    1.0,
                    0.5618556701030928,
                    0.29292929292929293,
                    0.41572817899447934,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "scullion",
                "poltroon",
                "quisling"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "scullion": 0.03661784632750234,
                "poltroon": 0.0728884431859712,
                "quisling": 0.7816620228387952
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1653574659671497,
                    1.171729818054933,
                    1.6629127159779173
                ],
                "result_count_important_words": [
                    11600.0,
                    10800.0,
                    54300.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_count_appended_bing": [
                    35.0,
                    35.0,
                    29.0
                ],
                "answer_relation_to_question_bing": [
                    0.38095238095238093,
                    0.38095238095238093,
                    0.23809523809523808
                ],
                "cosine_similarity_raw": [
                    0.021855589002370834,
                    0.022277582436800003,
                    0.07791045308113098
                ],
                "result_count_noun_chunks": [
                    12200.0,
                    11200.0,
                    73600.0
                ],
                "question_answer_similarity": [
                    -1.845580130815506,
                    -1.526202667504549,
                    -1.009220440639183
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    257000.0,
                    255000.0,
                    114000.0
                ],
                "word_count_appended": [
                    137.0,
                    118.0,
                    327.0
                ],
                "answer_relation_to_question": [
                    0.5554899645808737,
                    0.8010625737898466,
                    2.64344746162928
                ],
                "result_count": [
                    13700.0,
                    10400.0,
                    54300.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which two countries have almost perfectly identical flags?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "indonesia / monaco"
            ],
            "lines": [
                [
                    0.07814207650273224,
                    0.22727272727272727,
                    0.18648432704347195,
                    0.16941492720506374,
                    0.29508196721311475,
                    0.11865631208235507,
                    0.4010207801676996,
                    0.2857142857142857,
                    0.00315955766192733,
                    0.17647058823529413,
                    0.3333333333333333,
                    0.30020775351038576,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.15100182149362476,
                    0.045454545454545456,
                    0.2097497344410357,
                    0.48353237953530304,
                    0.39344262295081966,
                    0.6429474444645115,
                    0.5723660226029894,
                    0.38095238095238093,
                    0.00315955766192733,
                    0.17647058823529413,
                    0.3333333333333333,
                    0.32209508703240725,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.770856102003643,
                    0.7272727272727273,
                    0.6037659385154923,
                    0.3470526932596332,
                    0.3114754098360656,
                    0.23839624345313346,
                    0.026613197229310975,
                    0.3333333333333333,
                    0.9936808846761452,
                    0.6470588235294118,
                    0.3333333333333333,
                    0.3776971594572071,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "luxembourg / russia": 0.30954212651318097,
                "indonesia / monaco": 0.4758779871582864,
                "armenia / romania": 0.21457988632853262
            },
            "question": "which two countries have almost perfectly identical flags?",
            "rate_limited": false,
            "answers": [
                "armenia / romania",
                "luxembourg / russia",
                "indonesia / monaco"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "luxembourg / russia": 0.058267561365857194,
                "indonesia / monaco": 0.8770063378678692,
                "armenia / romania": 0.08858325755959118
            },
            "integer_answers": {
                "luxembourg / russia": 5,
                "indonesia / monaco": 6,
                "armenia / romania": 1
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2008310140415428,
                    1.2883803481296288,
                    1.5107886378288282
                ],
                "result_count_important_words": [
                    1100.0,
                    1570.0,
                    73.0
                ],
                "wikipedia_search": [
                    0.009478672985781991,
                    0.009478672985781991,
                    2.9810426540284363
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.45454545454545453,
                    0.09090909090909091,
                    1.4545454545454546
                ],
                "cosine_similarity_raw": [
                    0.03942738473415375,
                    0.044346265494823456,
                    0.12765100598335266
                ],
                "result_count_noun_chunks": [
                    18.0,
                    24.0,
                    21.0
                ],
                "question_answer_similarity": [
                    0.5158121178392321,
                    1.4721953067928553,
                    1.0566600456368178
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    65700.0,
                    356000.0,
                    132000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    3.0,
                    3.0,
                    11.0
                ],
                "answer_relation_to_question": [
                    0.31256830601092894,
                    0.604007285974499,
                    3.083424408014572
                ],
                "result_count": [
                    18.0,
                    24.0,
                    19.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Pope Clement VIII officially sanctioned the drinking of what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "coffee",
                "bottom"
            ],
            "question": "pope clement viii officially sanctioned the drinking of what?",
            "lines": [
                [
                    0.8518518518518517,
                    0,
                    0.1475118075640991,
                    0.40584543750059493,
                    0.48255813953488375,
                    0.3948535936113576,
                    0.6392907139524032,
                    0.1943270971635486,
                    0.43243243243243246,
                    0.31223628691983124,
                    0.11594202898550725,
                    0.4081987237796095,
                    0.025295109612141653,
                    0.0,
                    1.0
                ],
                [
                    0.037037037037037035,
                    0,
                    0.08898314697768386,
                    0.2512778676674032,
                    0.12043189368770764,
                    0.2608695652173913,
                    0.13765748950069995,
                    0.6155703077851539,
                    0.10810810810810811,
                    0.37130801687763715,
                    0.6992753623188406,
                    0.23666781968686493,
                    0.31534569983136596,
                    0.8543689320388349,
                    1.0
                ],
                [
                    0.1111111111111111,
                    0,
                    0.7635050454582171,
                    0.34287669483200184,
                    0.39700996677740863,
                    0.3442768411712511,
                    0.22305179654689689,
                    0.19010259505129753,
                    0.4594594594594595,
                    0.31645569620253167,
                    0.18478260869565216,
                    0.3551334565335256,
                    0.6593591905564924,
                    0.14563106796116504,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "wine",
                "gin",
                "coffee"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "gin": 0.6371074725945147,
                "coffee": 0.7636150768161741,
                "wine": 0.3037066332302616
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.4491923426776565,
                    1.4200069181211894,
                    2.1308007392011534
                ],
                "result_count_important_words": [
                    13700.0,
                    2950.0,
                    4780.0
                ],
                "wikipedia_search": [
                    0.43243243243243246,
                    0.10810810810810811,
                    0.4594594594594595
                ],
                "word_count_appended_bing": [
                    32.0,
                    193.0,
                    51.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.0655212551355362,
                    0.03952420875430107,
                    0.33913087844848633
                ],
                "result_count_noun_chunks": [
                    322000.0,
                    1020000.0,
                    315000.0
                ],
                "question_answer_similarity": [
                    2.069357984699309,
                    1.281236189417541,
                    1.7482877979055047
                ],
                "word_count_noun_chunks": [
                    15.0,
                    187.0,
                    391.0
                ],
                "word_count_raw": [
                    0.0,
                    88.0,
                    15.0
                ],
                "result_count_bing": [
                    44500.0,
                    29400.0,
                    38800.0
                ],
                "word_count_appended": [
                    222.0,
                    264.0,
                    225.0
                ],
                "answer_relation_to_question": [
                    2.5555555555555554,
                    0.1111111111111111,
                    0.3333333333333333
                ],
                "result_count": [
                    5810.0,
                    1450.0,
                    4780.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these music artists has NOT performed on the Super Bowl halftime show?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "van halen"
            ],
            "question": "which of these music artists has not performed on the super bowl halftime show?",
            "lines": [
                [
                    0.431924416579589,
                    0.3594006659267481,
                    0.45772050453351626,
                    0.4061425356000589,
                    0.3513777392059015,
                    0.32830076968620486,
                    0.4504310344827586,
                    0.3514422034265886,
                    0.31074767887051946,
                    0.21462264150943394,
                    0.23333333333333334,
                    0.3589691654493696,
                    0.25,
                    0.5,
                    -1.0
                ],
                [
                    0.15288915803806086,
                    0.2037513873473918,
                    0.1153347918670879,
                    0.32154712799963453,
                    0.20167064439140814,
                    0.47957371225577267,
                    0.125,
                    0.20180004337453916,
                    0.32876856953255545,
                    0.38443396226415094,
                    0.33333333333333337,
                    0.316269071796686,
                    0.25,
                    0.0,
                    -1.0
                ],
                [
                    0.4151864253823501,
                    0.43684794672586014,
                    0.42694470359939585,
                    0.27231033640030655,
                    0.4469516164026904,
                    0.19212551805802247,
                    0.4245689655172414,
                    0.44675775319887223,
                    0.3604837515969252,
                    0.40094339622641506,
                    0.43333333333333335,
                    0.32476176275394447,
                    0.5,
                    0.5,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "van halen",
                "britney spears",
                "james brown"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "van halen": 0.4296775980841843,
                "james brown": 0.3372387342412851,
                "britney spears": 0.07208183906511842
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6923700146075655,
                    2.204771138439768,
                    2.1028588469526666
                ],
                "result_count_important_words": [
                    57500.0,
                    435000.0,
                    87500.0
                ],
                "wikipedia_search": [
                    1.8925232112948058,
                    1.7123143046744458,
                    1.3951624840307484
                ],
                "word_count_appended_bing": [
                    16.0,
                    10.0,
                    4.0
                ],
                "answer_relation_to_question_bing": [
                    1.4059933407325194,
                    2.9624861265260822,
                    0.6315205327413984
                ],
                "cosine_similarity_raw": [
                    0.010404810309410095,
                    0.09466452896595001,
                    0.01797860860824585
                ],
                "result_count_noun_chunks": [
                    137000.0,
                    275000.0,
                    49100.0
                ],
                "question_answer_similarity": [
                    2.064023678191006,
                    3.9243650529533625,
                    5.007133529055864
                ],
                "word_count_noun_chunks": [
                    1.0,
                    1.0,
                    0.0
                ],
                "result_count_bing": [
                    4060000.0,
                    483000.0,
                    7280000.0
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count": [
                    137000.0,
                    275000.0,
                    48900.0
                ],
                "answer_relation_to_question": [
                    0.8169070010449321,
                    4.16533010354327,
                    1.0177628954117983
                ],
                "word_count_appended": [
                    121.0,
                    49.0,
                    42.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "The U.S. has never had a Miss America from what state?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "new mexico"
            ],
            "lines": [
                [
                    0.3564887609614261,
                    0.3236425339366516,
                    0.427871200064868,
                    0.24551597015623117,
                    0.2903225806451613,
                    0.42162733356815785,
                    0.3066581306017926,
                    0.42324641939037827,
                    0.31891853664788095,
                    0.42290748898678415,
                    0.4383561643835616,
                    0.33705449973807206,
                    0.5,
                    0.5,
                    1.0
                ],
                [
                    0.32403503305579895,
                    0.37569489334195216,
                    0.21838977672980747,
                    0.3169507826414436,
                    0.40880893300248144,
                    0.33022190912293065,
                    0.3969270166453265,
                    0.176827029012119,
                    0.29487962728531547,
                    0.37298091042584436,
                    0.4041095890410959,
                    0.3270636424162821,
                    0.01315789473684209,
                    0.0,
                    1.0
                ],
                [
                    0.319476205982775,
                    0.30066257272139624,
                    0.3537390232053246,
                    0.4375332472023252,
                    0.30086848635235736,
                    0.2481507573089116,
                    0.29641485275288093,
                    0.3999265515975028,
                    0.3862018360668036,
                    0.2041116005873715,
                    0.15753424657534248,
                    0.33588185784564584,
                    0.4868421052631579,
                    0.5,
                    1.0
                ]
            ],
            "fraction_answers": {
                "nebraska": 0.324665236648315,
                "new mexico": 0.24105576870271928,
                "north dakota": 0.4342789946489658
            },
            "question": "the u.s. has never had a miss america from what state?",
            "rate_limited": false,
            "answers": [
                "new mexico",
                "north dakota",
                "nebraska"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "nebraska": 0.12325300933012143,
                "new mexico": 0.6963123554551894,
                "north dakota": 0.6320329807090421
            },
            "integer_answers": {
                "nebraska": 6,
                "new mexico": 2,
                "north dakota": 6
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3035640020954236,
                    1.3834908606697431,
                    1.3129451372348333
                ],
                "result_count_important_words": [
                    3020000.0,
                    1610000.0,
                    3180000.0
                ],
                "wikipedia_search": [
                    1.0864887801127145,
                    1.230722236288107,
                    0.6827889835991785
                ],
                "word_count_appended_bing": [
                    9.0,
                    14.0,
                    50.0
                ],
                "answer_relation_to_question_bing": [
                    1.4108597285067872,
                    0.9944408532643826,
                    1.59469941822883
                ],
                "cosine_similarity_raw": [
                    0.023262985050678253,
                    0.09082494676113129,
                    0.047172099351882935
                ],
                "result_count_noun_chunks": [
                    836000.0,
                    3520000.0,
                    1090000.0
                ],
                "question_answer_similarity": [
                    7.735453888773918,
                    5.564077168703079,
                    1.8987780339084566
                ],
                "word_count_noun_chunks": [
                    0.0,
                    37.0,
                    1.0
                ],
                "result_count_bing": [
                    44500000.0,
                    96400000.0,
                    143000000.0
                ],
                "word_count_raw": [
                    0.0,
                    2.0,
                    0.0
                ],
                "result_count": [
                    3380000.0,
                    1470000.0,
                    3210000.0
                ],
                "answer_relation_to_question": [
                    1.1480899123085913,
                    1.4077197355536086,
                    1.4441903521378001
                ],
                "word_count_appended": [
                    105.0,
                    173.0,
                    403.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "In the game of Candy Land, which player goes first?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "the youngest"
            ],
            "question": "in the game of candy land, which player goes first?",
            "lines": [
                [
                    0.39657874465428855,
                    0.3333333333333333,
                    0.37037853666394666,
                    0.2030613014024972,
                    0.20333839150227617,
                    0.2686733556298774,
                    0.002801432255431254,
                    0.1927877947295423,
                    0.0875,
                    0.8441558441558441,
                    0.25,
                    0.4819555789033698,
                    0,
                    1.0,
                    -1.0
                ],
                [
                    0.20207951496799215,
                    0.041666666666666664,
                    0.19066187250622446,
                    0.18875789490779968,
                    0.0,
                    0.12374581939799331,
                    0.0,
                    0.0,
                    0.0125,
                    0.03896103896103896,
                    0.5,
                    0.02643207210504061,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.40134174037771936,
                    0.625,
                    0.4389595908298289,
                    0.6081808036897031,
                    0.7966616084977238,
                    0.6075808249721293,
                    0.9971985677445687,
                    0.8072122052704577,
                    0.9,
                    0.11688311688311688,
                    0.25,
                    0.4916123489915895,
                    0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "the youngest",
                "blue token holder",
                "first to draw a red card"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "first to draw a red card": 0.4065383465436521,
                "blue token holder": 0.10853619714556147,
                "the youngest": 0.6819164149858246
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.409777894516849,
                    0.13216036052520305,
                    2.4580617449579476
                ],
                "result_count_important_words": [
                    60400.0,
                    0,
                    21500000.0
                ],
                "wikipedia_search": [
                    0.35,
                    0.05,
                    3.6
                ],
                "word_count_appended_bing": [
                    2.0,
                    4.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.0,
                    0.125,
                    1.875
                ],
                "cosine_similarity_raw": [
                    0.14485405385494232,
                    0.07456734776496887,
                    0.1716759204864502
                ],
                "result_count_noun_chunks": [
                    1390000.0,
                    0,
                    5820000.0
                ],
                "question_answer_similarity": [
                    8.35710547119379,
                    7.768440492451191,
                    25.030033230781555
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    2410000.0,
                    1110000.0,
                    5450000.0
                ],
                "result_count": [
                    1340000.0,
                    0,
                    5250000.0
                ],
                "answer_relation_to_question": [
                    1.5863149786171542,
                    0.8083180598719686,
                    1.6053669615108774
                ],
                "word_count_appended": [
                    65.0,
                    3.0,
                    9.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these versions of the Old Testament typically contains the most books?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "catholic"
            ],
            "lines": [
                [
                    0.20233100233100232,
                    0.19471153846153846,
                    0.26425352567701627,
                    0.2728832663744709,
                    0.6075910621365167,
                    0.5632883862548934,
                    0.5517057569296375,
                    0.35092567007460623,
                    0.28238038277511956,
                    0.562358276643991,
                    0.5258620689655172,
                    0.3354817058408883,
                    0.7532467532467533,
                    0.6276595744680851,
                    -1.0
                ],
                [
                    0.37505827505827505,
                    0.4326923076923077,
                    0.5909460673959568,
                    0.14621734367712147,
                    0.35506580961126416,
                    0.274032187907786,
                    0.4131130063965885,
                    0.5664548217739707,
                    0.2269554568238779,
                    0.3798185941043084,
                    0.4482758620689655,
                    0.33665468795280556,
                    0.24675324675324675,
                    0.3723404255319149,
                    -1.0
                ],
                [
                    0.42261072261072263,
                    0.37259615384615385,
                    0.1448004069270269,
                    0.5808993899484076,
                    0.03734312825221916,
                    0.16267942583732056,
                    0.035181236673773986,
                    0.08261950815142305,
                    0.4906641604010026,
                    0.05782312925170068,
                    0.02586206896551724,
                    0.3278636062063061,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "catholic": 0.4353342121557168,
                "protestant": 0.3688841494820278,
                "eastern orthodox": 0.1957816383622553
            },
            "question": "which of these versions of the old testament typically contains the most books?",
            "rate_limited": false,
            "answers": [
                "catholic",
                "protestant",
                "eastern orthodox"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "catholic": 0.6512410678882583,
                "protestant": 0.43352434910603954,
                "eastern orthodox": 0.19762704271275722
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.01289023504533,
                    2.0199281277168333,
                    1.9671816372378368
                ],
                "result_count_important_words": [
                    2070000.0,
                    1550000.0,
                    132000.0
                ],
                "wikipedia_search": [
                    1.411901913875598,
                    1.1347772841193895,
                    2.453320802005013
                ],
                "word_count_appended_bing": [
                    61.0,
                    52.0,
                    3.0
                ],
                "answer_relation_to_question_bing": [
                    0.7788461538461539,
                    1.7307692307692308,
                    1.4903846153846154
                ],
                "cosine_similarity_raw": [
                    0.07316697388887405,
                    0.1636221706867218,
                    0.0400925874710083
                ],
                "result_count_noun_chunks": [
                    12700000.0,
                    20500000.0,
                    2990000.0
                ],
                "question_answer_similarity": [
                    2.474046828225255,
                    1.3256531269289553,
                    5.266619358211756
                ],
                "word_count_noun_chunks": [
                    116.0,
                    38.0,
                    0.0
                ],
                "word_count_raw": [
                    59.0,
                    35.0,
                    0.0
                ],
                "result_count_bing": [
                    2590000.0,
                    1260000.0,
                    748000.0
                ],
                "word_count_appended": [
                    496.0,
                    335.0,
                    51.0
                ],
                "answer_relation_to_question": [
                    1.0116550116550116,
                    1.8752913752913751,
                    2.113053613053613
                ],
                "result_count": [
                    3970000.0,
                    2320000.0,
                    244000.0
                ]
            },
            "integer_answers": {
                "catholic": 7,
                "protestant": 4,
                "eastern orthodox": 3
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "The Devon School is heavily featured in a famous book by which author?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "john knowles"
            ],
            "question": "the devon school is heavily featured in a famous book by which author?",
            "lines": [
                [
                    0.33127790041697325,
                    0.343903743315508,
                    0.07064161243930395,
                    0.0609578470229652,
                    0.3270558694287508,
                    0.32947551511907947,
                    0.1737563746297371,
                    0.06617281058764969,
                    0.6224599708879185,
                    0.1650485436893204,
                    0.0625,
                    0.33832965676712395,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.23293598233995585,
                    0.31403263403263404,
                    0.10075650757304479,
                    0.34872577575033903,
                    0.5034526051475204,
                    0.014918383730264918,
                    0.8061807188444743,
                    0.9135361461657834,
                    0.36459000485201365,
                    0.2815533980582524,
                    0.59375,
                    0.3448011743133487,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.43578611724307087,
                    0.34206362265185797,
                    0.8286018799876512,
                    0.5903163772266957,
                    0.1694915254237288,
                    0.6556061011506557,
                    0.020062906525788623,
                    0.020291043246566918,
                    0.012950024260067932,
                    0.5533980582524272,
                    0.34375,
                    0.3168691689195273,
                    1.0,
                    1.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "j.d. salinger",
                "william faulkner",
                "john knowles"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "john knowles": 0.8573983923671693,
                "j.d. salinger": 0.2888362965384002,
                "william faulkner": 0.30660424319474605
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.3683075973698675,
                    2.4136082201934412,
                    2.2180841824366913
                ],
                "result_count_important_words": [
                    569000.0,
                    2640000.0,
                    65700.0
                ],
                "wikipedia_search": [
                    3.7347598253275107,
                    2.1875400291120815,
                    0.07770014556040758
                ],
                "word_count_appended_bing": [
                    4.0,
                    38.0,
                    22.0
                ],
                "answer_relation_to_question_bing": [
                    1.71951871657754,
                    1.5701631701631702,
                    1.71031811325929
                ],
                "cosine_similarity_raw": [
                    0.02920038439333439,
                    0.04164866358041763,
                    0.34251049160957336
                ],
                "result_count_noun_chunks": [
                    226000.0,
                    3120000.0,
                    69300.0
                ],
                "question_answer_similarity": [
                    0.27212131582200527,
                    1.5567432511597872,
                    2.6352254413068295
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    18.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    7.0
                ],
                "result_count_bing": [
                    985000.0,
                    44600.0,
                    1960000.0
                ],
                "word_count_appended": [
                    17.0,
                    29.0,
                    57.0
                ],
                "answer_relation_to_question": [
                    1.6563895020848662,
                    1.1646799116997792,
                    2.1789305862153543
                ],
                "result_count": [
                    5210.0,
                    8020.0,
                    2700.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these websites is an online database for movies, television and video games?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "imdb"
            ],
            "question": "which of these websites is an online database for movies, television and video games?",
            "lines": [
                [
                    0.19419691419691423,
                    0.06932573599240266,
                    0.08481721242504266,
                    -1.861138382534257,
                    3.1878178380219425e-05,
                    0.1632928475033738,
                    0.002090086191909319,
                    0.004195006720156397,
                    0.024234693877551023,
                    0.17153748411689962,
                    0.2621359223300971,
                    0.26076132553278153,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.7353846153846154,
                    0.8105413105413105,
                    0.8864752373087098,
                    2.2380058045525146,
                    0.2718034156629235,
                    0.6734143049932524,
                    0.2947755571953439,
                    0.8278010833706675,
                    0.7138605442176871,
                    0.5362134688691232,
                    0.4368932038834951,
                    0.3864134401347336,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.07041847041847044,
                    0.12013295346628679,
                    0.028707550266247557,
                    0.623132577981742,
                    0.7281647061586963,
                    0.1632928475033738,
                    0.7031343566127468,
                    0.16800390990917607,
                    0.2619047619047619,
                    0.29224904701397714,
                    0.30097087378640774,
                    0.35282523433248486,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "petfinder",
                "imdb",
                "yelp"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "petfinder": 0.026195713532190284,
                "imdb": 0.940021800537566,
                "yelp": 0.09943926812761175
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.8253292787294708,
                    2.704894080943135,
                    2.469776640327394
                ],
                "result_count_important_words": [
                    5410.0,
                    763000.0,
                    1820000.0
                ],
                "wikipedia_search": [
                    0.16964285714285715,
                    4.997023809523809,
                    1.8333333333333335
                ],
                "word_count_appended_bing": [
                    27.0,
                    45.0,
                    31.0
                ],
                "answer_relation_to_question_bing": [
                    0.41595441595441596,
                    4.863247863247863,
                    0.7207977207977208
                ],
                "cosine_similarity_raw": [
                    0.028307706117630005,
                    0.2958607077598572,
                    0.009581131860613823
                ],
                "result_count_noun_chunks": [
                    4120.0,
                    813000.0,
                    165000.0
                ],
                "question_answer_similarity": [
                    -1.1989778960123658,
                    1.4417624804191291,
                    0.4014329048804939
                ],
                "word_count_noun_chunks": [
                    0.0,
                    25.0,
                    0.0
                ],
                "result_count_bing": [
                    12100000.0,
                    49900000.0,
                    12100000.0
                ],
                "word_count_raw": [
                    0.0,
                    25.0,
                    0.0
                ],
                "result_count": [
                    95.0,
                    810000.0,
                    2170000.0
                ],
                "answer_relation_to_question": [
                    1.3593783993783994,
                    5.147692307692307,
                    0.49292929292929294
                ],
                "word_count_appended": [
                    135.0,
                    422.0,
                    230.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "In July of 2017, Joey Chestnut set a world record by consuming 72 what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "hot dogs"
            ],
            "question": "in july of 2017, joey chestnut set a world record by consuming 72 what?",
            "lines": [
                [
                    0.11294768157513256,
                    0.15876906318082787,
                    0.017501722400097574,
                    0.11168697480037908,
                    0.13040291645646732,
                    0.1310200418785522,
                    0.013473288422371926,
                    0.21059653135124834,
                    0.08118379952558773,
                    0.39090909090909093,
                    0.7297297297297297,
                    0.27440678632531196,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.6146823761202846,
                    0.40890522875816987,
                    0.9714497611384235,
                    0.5109434943953564,
                    0.8690354943412749,
                    0.8196230930302124,
                    0.9854300485665047,
                    0.7813988946064417,
                    0.43835869794969357,
                    0.55,
                    0.21621621621621623,
                    0.5695738269934233,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.27236994230458283,
                    0.4323257080610022,
                    0.01104851646147894,
                    0.37736953080426455,
                    0.0005615892022577789,
                    0.049356865091235415,
                    0.0010966630111232962,
                    0.008004574042309892,
                    0.4804575025247187,
                    0.05909090909090909,
                    0.05405405405405406,
                    0.15601938668126467,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "pickles",
                "hot dogs",
                "raw oysters"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "pickles": 0.06882735594973156,
                "hot dogs": 0.8888101250740732,
                "raw oysters": 0.0787270008795942
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.469661076927808,
                    5.12616444294081,
                    1.4041744801313822
                ],
                "result_count_important_words": [
                    86.0,
                    6290.0,
                    7.0
                ],
                "wikipedia_search": [
                    0.4871027971535264,
                    2.6301521876981613,
                    2.8827450151483123
                ],
                "word_count_appended_bing": [
                    27.0,
                    8.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.9526143790849673,
                    2.4534313725490193,
                    2.593954248366013
                ],
                "cosine_similarity_raw": [
                    0.006958669517189264,
                    0.38624757528305054,
                    0.004392880480736494
                ],
                "result_count_noun_chunks": [
                    22100.0,
                    82000.0,
                    840.0
                ],
                "question_answer_similarity": [
                    1.3877592366188765,
                    6.348695136606693,
                    4.688980545208324
                ],
                "word_count_noun_chunks": [
                    0.0,
                    95.0,
                    0.0
                ],
                "result_count_bing": [
                    87600.0,
                    548000.0,
                    33000.0
                ],
                "word_count_raw": [
                    0.0,
                    21.0,
                    0.0
                ],
                "result_count": [
                    13700.0,
                    91300.0,
                    59.0
                ],
                "answer_relation_to_question": [
                    1.016529134176193,
                    5.532141385082562,
                    2.4513294807412453
                ],
                "word_count_appended": [
                    86.0,
                    121.0,
                    13.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What five-letter acronym helps Americans remember the names of their Great Lakes?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "homes"
            ],
            "question": "what five-letter acronym helps americans remember the names of their great lakes?",
            "lines": [
                [
                    0.3235294117647059,
                    0.4166666666666667,
                    0.25936511538365403,
                    0.0780091712298646,
                    0.0033407572383073497,
                    0.21724709784411278,
                    0.07574349442379182,
                    0.0006175046312847346,
                    0.0,
                    0.20430107526881722,
                    0.27835051546391754,
                    0.21171336635512625,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.029411764705882353,
                    0.16666666666666669,
                    0.4481760561540991,
                    0.5862764114130131,
                    0.8709831371301304,
                    0.4295190713101161,
                    0.46143122676579923,
                    0.7653807403555527,
                    0.5,
                    0.47491039426523296,
                    0.38144329896907214,
                    0.479123942938306,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.6470588235294118,
                    0.4166666666666667,
                    0.2924588284622468,
                    0.33571441735712226,
                    0.1256761056315622,
                    0.35323383084577115,
                    0.4628252788104089,
                    0.2340017550131626,
                    0.5,
                    0.3207885304659498,
                    0.3402061855670103,
                    0.3091626907065678,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "soaks",
                "homes",
                "mound"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "soaks": 0.04845384332929994,
                "homes": 0.7250009858609001,
                "mound": 0.11175092122901986
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6937069308410098,
                    3.8329915435064477,
                    2.473301525652542
                ],
                "result_count_important_words": [
                    16300.0,
                    99300.0,
                    99600.0
                ],
                "wikipedia_search": [
                    0.0,
                    1.0,
                    1.0
                ],
                "answer_relation_to_question": [
                    1.2941176470588236,
                    0.11764705882352941,
                    2.588235294117647
                ],
                "word_count_appended_bing": [
                    27.0,
                    37.0,
                    33.0
                ],
                "answer_relation_to_question_bing": [
                    0.8333333333333333,
                    0.3333333333333333,
                    0.8333333333333333
                ],
                "cosine_similarity_raw": [
                    0.034404557198286057,
                    0.059450164437294006,
                    0.03879440948367119
                ],
                "result_count_noun_chunks": [
                    380.0,
                    471000.0,
                    144000.0
                ],
                "question_answer_similarity": [
                    0.4418391026556492,
                    3.3206331953406334,
                    1.901465617120266
                ],
                "word_count_noun_chunks": [
                    0.0,
                    10.0,
                    0.0
                ],
                "result_count_bing": [
                    131000.0,
                    259000.0,
                    213000.0
                ],
                "word_count_raw": [
                    0.0,
                    13.0,
                    0.0
                ],
                "result_count": [
                    84.0,
                    21900.0,
                    3160.0
                ],
                "word_count_appended": [
                    114.0,
                    265.0,
                    179.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which color is NOT represented in the original electronic Simon game?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "orange"
            ],
            "lines": [
                [
                    0.29876957494407164,
                    0.16168146987112503,
                    0.36215639074047185,
                    0.32260869715949814,
                    0.4503454231433506,
                    0.3492537313432836,
                    0.46136977058029693,
                    0.3652207591014717,
                    0.3572197484054904,
                    0.31991525423728817,
                    0.27941176470588236,
                    0.32402292489224727,
                    0.23076923076923078,
                    0.5,
                    -1.0
                ],
                [
                    0.41181208053691276,
                    0.4283133054684779,
                    0.3559571241684453,
                    0.3519548682225229,
                    0.0984455958549223,
                    0.30149253731343284,
                    0.07658569500674763,
                    0.2618125484120837,
                    0.4315209564118391,
                    0.336864406779661,
                    0.36764705882352944,
                    0.34765352688125983,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.28941834451901566,
                    0.41000522466039707,
                    0.28188648509108283,
                    0.32543643461797894,
                    0.4512089810017271,
                    0.3492537313432836,
                    0.4620445344129555,
                    0.37296669248644465,
                    0.21125929518267056,
                    0.3432203389830508,
                    0.3529411764705882,
                    0.3283235482264929,
                    0.2692307692307692,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "blue": 0.3167507514437559,
                "orange": 0.31856289944573785,
                "green": 0.3646863491105061
            },
            "question": "which color is not represented in the original electronic simon game?",
            "rate_limited": false,
            "answers": [
                "blue",
                "orange",
                "green"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "blue": 0.18968499210616951,
                "orange": 0.460608549387696,
                "green": 0.20600199105557998
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.1117249012930324,
                    1.8281576774248824,
                    2.0601174212820847
                ],
                "result_count_important_words": [
                    2290000.0,
                    25100000.0,
                    2250000.0
                ],
                "wikipedia_search": [
                    1.1422420127560768,
                    0.5478323487052876,
                    2.3099256385386355
                ],
                "word_count_appended_bing": [
                    45.0,
                    27.0,
                    30.0
                ],
                "answer_relation_to_question_bing": [
                    2.706548241031,
                    0.5734935562521769,
                    0.7199582027168234
                ],
                "cosine_similarity_raw": [
                    0.03717388957738876,
                    0.038845717906951904,
                    0.05882120877504349
                ],
                "result_count_noun_chunks": [
                    3480000.0,
                    6150000.0,
                    3280000.0
                ],
                "question_answer_similarity": [
                    3.33958138525486,
                    2.787108264863491,
                    3.286346197128296
                ],
                "word_count_noun_chunks": [
                    7.0,
                    0.0,
                    6.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    1010000.0,
                    1330000.0,
                    1010000.0
                ],
                "word_count_appended": [
                    255.0,
                    231.0,
                    222.0
                ],
                "answer_relation_to_question": [
                    2.012304250559284,
                    0.8818791946308724,
                    2.1058165548098433
                ],
                "result_count": [
                    2300000.0,
                    18600000.0,
                    2260000.0
                ]
            },
            "integer_answers": {
                "blue": 6,
                "orange": 4,
                "green": 4
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these universities has the oldest marching band?": {
        "raw_data": {
            "data": {
                "word_count_appended_relation_to_question": [
                    1.5027414449576129,
                    1.2567805661652207,
                    1.2404779888771664
                ],
                "result_count_important_words": [
                    523000.0,
                    146000.0,
                    340000.0
                ],
                "wikipedia_search": [
                    0.3333333333333333,
                    1.5457831325301203,
                    1.1208835341365462
                ],
                "word_count_appended_bing": [
                    25.0,
                    66.0,
                    48.0
                ],
                "answer_relation_to_question_bing": [
                    0.14285714285714285,
                    0.5,
                    1.3571428571428572
                ],
                "cosine_similarity_raw": [
                    0.030612101778388023,
                    0.0344487726688385,
                    0.026329820975661278
                ],
                "result_count_noun_chunks": [
                    607000.0,
                    141000.0,
                    286000.0
                ],
                "question_answer_similarity": [
                    1.5973724671639502,
                    0.3935061083175242,
                    0.6409848049515858
                ],
                "word_count_noun_chunks": [
                    4.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    1240000.0,
                    710000.0,
                    606000.0
                ],
                "word_count_appended": [
                    154.0,
                    391.0,
                    346.0
                ],
                "answer_relation_to_question": [
                    1.6193181818181819,
                    0.6352272727272728,
                    1.7454545454545454
                ],
                "result_count": [
                    348000.0,
                    172000.0,
                    283000.0
                ]
            },
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "z-best_answer_by_ml": [
                "notre dame",
                "top"
            ],
            "lines": [
                [
                    0.40482954545454547,
                    0.07142857142857142,
                    0.33495862611328325,
                    0.6069359371158204,
                    0.43337484433374845,
                    0.48513302034428796,
                    0.5183349851337958,
                    0.5870406189555126,
                    0.1111111111111111,
                    0.1728395061728395,
                    0.17985611510791366,
                    0.3756853612394032,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.1588068181818182,
                    0.25,
                    0.37693960538800303,
                    0.1495161607715271,
                    0.21419676214196762,
                    0.2777777777777778,
                    0.14469772051536176,
                    0.13636363636363635,
                    0.5152610441767068,
                    0.4388327721661055,
                    0.4748201438848921,
                    0.3141951415413052,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.43636363636363634,
                    0.6785714285714286,
                    0.2881017684987137,
                    0.24354790211265256,
                    0.35242839352428396,
                    0.23708920187793428,
                    0.33696729435084244,
                    0.2765957446808511,
                    0.3736278447121821,
                    0.388327721661055,
                    0.34532374100719426,
                    0.3101194972192916,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "question": "which of these universities has the oldest marching band?",
            "rate_limited": false,
            "answers": [
                "notre dame",
                "princeton",
                "harvard"
            ],
            "ml_answers": {
                "harvard": 0.42872845147744276,
                "notre dame": 0.9117454376128077,
                "princeton": 0.37780575978036424
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "In the original Angry Birds game, what did the pigs do that made the birds so angry?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "steal their eggs"
            ],
            "question": "in the original angry birds game, what did the pigs do that made the birds so angry?",
            "lines": [
                [
                    0.026455026455026454,
                    0.3333333333333333,
                    0.058306735929637164,
                    0.41168828543950503,
                    0.9989799148756552,
                    0.28857715430861725,
                    0.004374134285922578,
                    0.9987354720233254,
                    0.2,
                    0.1864406779661017,
                    0.3333333333333333,
                    0.2851576041037584,
                    0,
                    0,
                    1.0
                ],
                [
                    0.8915343915343915,
                    0.5541125541125541,
                    0.874549581520976,
                    0.24105594950681863,
                    0.0006894368426606634,
                    0.3777555110220441,
                    0.991470438142451,
                    0.0007007256940808814,
                    0.8,
                    0.711864406779661,
                    0.3333333333333333,
                    0.44387931384708984,
                    0,
                    0,
                    1.0
                ],
                [
                    0.082010582010582,
                    0.11255411255411255,
                    0.06714368254938688,
                    0.34725576505367634,
                    0.0003306482816841957,
                    0.3336673346693387,
                    0.004155427571626449,
                    0.0005638022825938127,
                    0.0,
                    0.1016949152542373,
                    0.3333333333333333,
                    0.2709630820491518,
                    0,
                    0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "make too much noise",
                "steal their eggs",
                "knock down their tree"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "steal their eggs": 0.6826851169024626,
                "make too much noise": 0.07896891312850449,
                "knock down their tree": 0.07587283668186315
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9961032287263087,
                    3.1071551969296287,
                    1.8967415743440628
                ],
                "result_count_important_words": [
                    60.0,
                    13600.0,
                    57.0
                ],
                "wikipedia_search": [
                    0.4,
                    1.6,
                    0.0
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    2.333333333333333,
                    3.878787878787879,
                    0.7878787878787878
                ],
                "cosine_similarity_raw": [
                    0.01271515991538763,
                    0.19071617722511292,
                    0.014642264693975449
                ],
                "result_count_noun_chunks": [
                    124000.0,
                    87.0,
                    70.0
                ],
                "question_answer_similarity": [
                    32.83790588378906,
                    19.22758762538433,
                    27.698510095477104
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    288000.0,
                    377000.0,
                    333000.0
                ],
                "word_count_appended": [
                    11.0,
                    42.0,
                    6.0
                ],
                "answer_relation_to_question": [
                    0.18518518518518517,
                    6.2407407407407405,
                    0.5740740740740741
                ],
                "result_count": [
                    142000.0,
                    98.0,
                    47.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "In a 2017 report, what country did the UN name as the world\u2019s happiest?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "norway"
            ],
            "question": "in a 2017 report, what country did the un name as the world\u2019s happiest?",
            "lines": [
                [
                    0.4142937812966977,
                    0.38378707333225937,
                    0.11710988916276709,
                    0.3510428671640295,
                    0.1504950495049505,
                    0.32577720207253885,
                    0.43805165086629616,
                    0.35181975736568455,
                    0.27379677250233875,
                    0.2857142857142857,
                    0.2631578947368421,
                    0.2899270898245983,
                    0.045454545454545456,
                    0.0,
                    1.0
                ],
                [
                    0.2504394317741376,
                    0.24494263366088526,
                    0.6525626891414307,
                    0.8168723728440077,
                    0.15742574257425743,
                    0.34196891191709844,
                    0.4968944099378882,
                    0.36048526863084923,
                    0.5302742719295983,
                    0.4773809523809524,
                    0.4824561403508772,
                    0.4318948503852316,
                    0.9545454545454546,
                    1.0,
                    1.0
                ],
                [
                    0.3352667869291647,
                    0.3712702930068553,
                    0.23032742169580228,
                    -0.16791524000803726,
                    0.692079207920792,
                    0.3322538860103627,
                    0.06505393919581563,
                    0.2876949740034662,
                    0.19592895556806292,
                    0.2369047619047619,
                    0.2543859649122807,
                    0.27817805979017013,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "nepal",
                "norway",
                "nicaragua"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "norway": 0.8474588156456981,
                "nepal": 0.050320676001852874,
                "nicaragua": 0.12746140109447912
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.73956253894759,
                    2.59136910231139,
                    1.6690683587410207
                ],
                "result_count_important_words": [
                    1340000.0,
                    1520000.0,
                    199000.0
                ],
                "wikipedia_search": [
                    1.6427806350140324,
                    3.18164563157759,
                    1.1755737334083776
                ],
                "word_count_appended_bing": [
                    30.0,
                    55.0,
                    29.0
                ],
                "answer_relation_to_question_bing": [
                    1.918935366661297,
                    1.2247131683044263,
                    1.8563514650342765
                ],
                "cosine_similarity_raw": [
                    0.01776442490518093,
                    0.09898737818002701,
                    0.03493841737508774
                ],
                "result_count_noun_chunks": [
                    6090000.0,
                    6240000.0,
                    4980000.0
                ],
                "question_answer_similarity": [
                    0.5069574299268425,
                    1.1796836154535413,
                    -0.24249425492598675
                ],
                "word_count_noun_chunks": [
                    1.0,
                    21.0,
                    0.0
                ],
                "result_count_bing": [
                    50300000.0,
                    52800000.0,
                    51300000.0
                ],
                "word_count_raw": [
                    0.0,
                    47.0,
                    0.0
                ],
                "word_count_appended": [
                    240.0,
                    401.0,
                    199.0
                ],
                "answer_relation_to_question": [
                    2.0714689064834886,
                    1.252197158870688,
                    1.6763339346458233
                ],
                "result_count": [
                    1520000.0,
                    1590000.0,
                    6990000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these is NOT a Slavic language?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "serbian"
            ],
            "lines": [
                [
                    0.2805944055944056,
                    0.2809224318658281,
                    0.34664560477454076,
                    0.3406021580808316,
                    0.4350254524886878,
                    0.327755905511811,
                    0.4743980738362761,
                    0.4315270935960591,
                    0.1857976653696498,
                    0.33437744714173845,
                    0.3218390804597701,
                    0.3236600370477425,
                    0.13157894736842107,
                    0.25,
                    -1.0
                ],
                [
                    0.30157342657342656,
                    0.31813417190775684,
                    0.3703479882766807,
                    0.3457162185637367,
                    0.13518099547511314,
                    0.375,
                    0.09069020866773675,
                    0.18103448275862066,
                    0.43093385214007784,
                    0.33555207517619423,
                    0.3247126436781609,
                    0.3249196472438972,
                    0.368421052631579,
                    0.25,
                    -1.0
                ],
                [
                    0.4178321678321678,
                    0.40094339622641506,
                    0.28300640694877854,
                    0.3136816233554317,
                    0.42979355203619907,
                    0.297244094488189,
                    0.43491171749598717,
                    0.3874384236453202,
                    0.38326848249027234,
                    0.3300704776820673,
                    0.35344827586206895,
                    0.3514203157083603,
                    0.5,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "bulgarian": 0.36218224240917685,
                "hungarian": 0.23099158088982039,
                "serbian": 0.4068261767010029
            },
            "question": "which of these is not a slavic language?",
            "rate_limited": false,
            "answers": [
                "bulgarian",
                "serbian",
                "hungarian"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "bulgarian": 0.10870923980965118,
                "hungarian": 0.3542047625882654,
                "serbian": 0.5054717125249937
            },
            "integer_answers": {
                "bulgarian": 7,
                "hungarian": 4,
                "serbian": 3
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.7053598518090299,
                    0.7003214110244111,
                    0.5943187371665588
                ],
                "result_count_important_words": [
                    319000.0,
                    5100000.0,
                    811000.0
                ],
                "wikipedia_search": [
                    1.2568093385214008,
                    0.27626459143968873,
                    0.4669260700389105
                ],
                "word_count_appended_bing": [
                    62.0,
                    61.0,
                    51.0
                ],
                "answer_relation_to_question_bing": [
                    0.8763102725366876,
                    0.7274633123689727,
                    0.39622641509433965
                ],
                "cosine_similarity_raw": [
                    0.051789019256830215,
                    0.04378453269600868,
                    0.07328049093484879
                ],
                "result_count_noun_chunks": [
                    556000.0,
                    2590000.0,
                    914000.0
                ],
                "question_answer_similarity": [
                    0.840626840479672,
                    0.81365648470819,
                    0.9825994279235601
                ],
                "word_count_noun_chunks": [
                    14.0,
                    5.0,
                    0.0
                ],
                "result_count_bing": [
                    17500000.0,
                    12700000.0,
                    20600000.0
                ],
                "word_count_raw": [
                    4.0,
                    4.0,
                    0.0
                ],
                "result_count": [
                    919000.0,
                    5160000.0,
                    993000.0
                ],
                "answer_relation_to_question": [
                    0.8776223776223776,
                    0.7937062937062938,
                    0.32867132867132864
                ],
                "word_count_appended": [
                    423.0,
                    420.0,
                    434.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these activities has been an Olympic event?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "tug of war"
            ],
            "question": "which of these activities has been an olympic event?",
            "lines": [
                [
                    0.15873015873015872,
                    0.16666666666666666,
                    0.080520834288799,
                    0.09869051793654235,
                    0.44687989919561616,
                    0.3940754039497307,
                    0.2542787863055239,
                    0.44352233807526575,
                    0.15555555555555556,
                    0.7035175879396985,
                    0.7169811320754716,
                    0.4013791583232507,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.22220426632191337,
                    0.16666666666666666,
                    0.18742085543204176,
                    0.30918259399775344,
                    1.465179997362676e-05,
                    0.2118491921005386,
                    0.0023539961887680755,
                    0.0002570248303436171,
                    0.7222222222222222,
                    0.0829145728643216,
                    0.03773584905660377,
                    0.12820708935210678,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.6190655749479279,
                    0.6666666666666666,
                    0.7320583102791592,
                    0.5921268880657042,
                    0.5531054490044102,
                    0.3940754039497307,
                    0.743367217505708,
                    0.5562206370943906,
                    0.12222222222222222,
                    0.2135678391959799,
                    0.24528301886792453,
                    0.4704137523246425,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "juggling",
                "synchronized walking",
                "tug of war"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "synchronized walking": 0.12476672870889893,
                "juggling": 0.20435610152884542,
                "tug of war": 0.7018779439897715
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2041374749697522,
                    0.38462126805632035,
                    1.4112412569739274
                ],
                "result_count_important_words": [
                    431000.0,
                    3990.0,
                    1260000.0
                ],
                "wikipedia_search": [
                    0.3111111111111111,
                    1.4444444444444444,
                    0.24444444444444444
                ],
                "word_count_appended_bing": [
                    38.0,
                    2.0,
                    13.0
                ],
                "answer_relation_to_question_bing": [
                    0.3333333333333333,
                    0.3333333333333333,
                    1.3333333333333333
                ],
                "cosine_similarity_raw": [
                    0.019327227026224136,
                    0.044986188411712646,
                    0.17571423947811127
                ],
                "result_count_noun_chunks": [
                    1220000.0,
                    707.0,
                    1530000.0
                ],
                "question_answer_similarity": [
                    1.3961935862898827,
                    4.374065145850182,
                    8.376932056620717
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    43900000.0,
                    23600000.0,
                    43900000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    1220000.0,
                    40.0,
                    1510000.0
                ],
                "answer_relation_to_question": [
                    0.47619047619047616,
                    0.6666127989657401,
                    1.8571967248437837
                ],
                "word_count_appended": [
                    280.0,
                    33.0,
                    85.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What 1930s movie monster famously climbed the Empire State Building?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "king kong",
                "top"
            ],
            "question": "what 1930s movie monster famously climbed the empire state building?",
            "lines": [
                [
                    0.6094579815734191,
                    0.6049511481956925,
                    0.9463492704096009,
                    0.3877416216858661,
                    0.2939793038570085,
                    0.29935992122107336,
                    0.25882352941176473,
                    0.35893980233602873,
                    0.7806324110671937,
                    0.37966101694915255,
                    0.4782608695652174,
                    0.35036971337073214,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.290297327280309,
                    0.32647803451718727,
                    0.025689365002467206,
                    0.5392311363619444,
                    0.4797742238946378,
                    0.3126538650910881,
                    0.49176470588235294,
                    0.192722371967655,
                    0.0,
                    0.13220338983050847,
                    0.028985507246376812,
                    0.32555984546054706,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.10024469114627181,
                    0.06857081728712024,
                    0.027961364587931907,
                    0.07302724195218942,
                    0.2262464722483537,
                    0.3879862136878385,
                    0.24941176470588236,
                    0.44833782569631625,
                    0.2193675889328063,
                    0.488135593220339,
                    0.4927536231884058,
                    0.32407044116872086,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "king kong",
                "the mummy",
                "dracula"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "dracula": 0.0380520092235266,
                "king kong": 0.8291624339329228,
                "the mummy": 0.0982215751699864
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.802957706965857,
                    2.6044787636843765,
                    2.592563529349767
                ],
                "result_count_important_words": [
                    11000.0,
                    20900.0,
                    10600.0
                ],
                "wikipedia_search": [
                    4.683794466403162,
                    0.0,
                    1.316205533596838
                ],
                "word_count_appended_bing": [
                    33.0,
                    2.0,
                    34.0
                ],
                "answer_relation_to_question_bing": [
                    3.629706889174155,
                    1.9588682071031236,
                    0.41142490372272145
                ],
                "cosine_similarity_raw": [
                    0.5144217014312744,
                    0.013964365236461163,
                    0.015199391171336174
                ],
                "result_count_noun_chunks": [
                    79900.0,
                    42900.0,
                    99800.0
                ],
                "question_answer_similarity": [
                    4.3282579984515905,
                    6.019295707345009,
                    0.8151839431375265
                ],
                "word_count_noun_chunks": [
                    75.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    608000.0,
                    635000.0,
                    788000.0
                ],
                "word_count_raw": [
                    48.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    12500.0,
                    20400.0,
                    9620.0
                ],
                "answer_relation_to_question": [
                    3.047289907867096,
                    1.451486636401545,
                    0.5012234557313591
                ],
                "word_count_appended": [
                    112.0,
                    39.0,
                    144.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Napa cabbage is named after what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "city in california"
            ],
            "lines": [
                [
                    0.3181818181818182,
                    0.0,
                    0.04559913817466954,
                    0.31364525355347306,
                    0.7333333333333333,
                    0.5239096163951655,
                    0.7567567567567568,
                    0.5111111111111111,
                    0.0,
                    0.8125,
                    0.5,
                    0.5214486638537271,
                    0,
                    1.0,
                    1.0
                ],
                [
                    0.3246753246753247,
                    1.0,
                    0.6300719691543386,
                    0.21419926224198277,
                    0.05,
                    0.28218602207041515,
                    0.04054054054054054,
                    0.06666666666666667,
                    0.0,
                    0.1875,
                    0.5,
                    0.16877637130801687,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.35714285714285715,
                    0.0,
                    0.32432889267099185,
                    0.4721554842045442,
                    0.21666666666666667,
                    0.19390436153441934,
                    0.20270270270270271,
                    0.4222222222222222,
                    1.0,
                    0.0,
                    0.0,
                    0.30977496483825595,
                    0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "japanese for \u201cvegetable\u201d": 0.26914601169097385,
                "scientist edward napa": 0.26650893512748347,
                "city in california": 0.4643450531815426
            },
            "question": "napa cabbage is named after what?",
            "rate_limited": false,
            "answers": [
                "city in california",
                "scientist edward napa",
                "japanese for \u201cvegetable\u201d"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "japanese for \u201cvegetable\u201d": 0.1226139585438765,
                "scientist edward napa": 0.36705948320355886,
                "city in california": 0.5757220238212634
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.0428973277074542,
                    0.33755274261603374,
                    0.6195499296765119
                ],
                "result_count_important_words": [
                    56.0,
                    3.0,
                    15.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    1.0
                ],
                "answer_relation_to_question": [
                    0.6363636363636364,
                    0.6493506493506493,
                    0.7142857142857143
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    0.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    1.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.026053015142679214,
                    0.3599908947944641,
                    0.1853049397468567
                ],
                "result_count_noun_chunks": [
                    23.0,
                    3.0,
                    19.0
                ],
                "question_answer_similarity": [
                    5.399515964090824,
                    3.6875174193410203,
                    8.128326654434204
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    997000.0,
                    537000.0,
                    369000.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    44.0,
                    3.0,
                    13.0
                ],
                "word_count_appended": [
                    26.0,
                    6.0,
                    0.0
                ]
            },
            "integer_answers": {
                "japanese for \u201cvegetable\u201d": 3,
                "scientist edward napa": 2,
                "city in california": 8
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these grape varieties is typically white?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "riesling"
            ],
            "lines": [
                [
                    0.5826687574671446,
                    0.6956521739130436,
                    0.3668847392777292,
                    0.2126182900314809,
                    0.43727598566308246,
                    0.527729130180969,
                    0.14986251145737856,
                    0.8035714285714286,
                    0.646306572358468,
                    0.3544165757906216,
                    0.4827586206896552,
                    0.3589324879107484,
                    0.3103448275862069,
                    0.3333333333333333,
                    -1.0
                ],
                [
                    0.37126642771804064,
                    0.24235104669887278,
                    0.5987790309443232,
                    0.41102817302529326,
                    0.28136200716845877,
                    0.021599532983070636,
                    0.10311640696608616,
                    0.1736111111111111,
                    0.3536934276415321,
                    0.3446019629225736,
                    0.1724137931034483,
                    0.32097361672778635,
                    0.3793103448275862,
                    0.5,
                    -1.0
                ],
                [
                    0.046064814814814815,
                    0.06199677938808374,
                    0.03433622977794762,
                    0.3763535369432258,
                    0.28136200716845877,
                    0.4506713368359603,
                    0.7470210815765352,
                    0.022817460317460316,
                    0.0,
                    0.3009814612868048,
                    0.3448275862068966,
                    0.32009389536146526,
                    0.3103448275862069,
                    0.16666666666666666,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "riesling": 0.4473111024450922,
                "merlot": 0.30529334870272734,
                "concord": 0.24739554885218046
            },
            "question": "which of these grape varieties is typically white?",
            "rate_limited": false,
            "answers": [
                "riesling",
                "merlot",
                "concord"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "riesling": 0.4833428494178032,
                "merlot": 0.3900449659487445,
                "concord": 0.210513268384477
            },
            "integer_answers": {
                "riesling": 9,
                "merlot": 4,
                "concord": 1
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.4357299516429936,
                    1.2838944669111454,
                    1.280375581445861
                ],
                "result_count_important_words": [
                    327000.0,
                    225000.0,
                    1630000.0
                ],
                "wikipedia_search": [
                    2.585226289433872,
                    1.4147737105661284,
                    0.0
                ],
                "word_count_appended_bing": [
                    84.0,
                    30.0,
                    60.0
                ],
                "answer_relation_to_question_bing": [
                    2.0869565217391304,
                    0.7270531400966183,
                    0.1859903381642512
                ],
                "cosine_similarity_raw": [
                    0.20458167791366577,
                    0.3338901996612549,
                    0.01914651319384575
                ],
                "result_count_noun_chunks": [
                    4860000.0,
                    1050000.0,
                    138000.0
                ],
                "question_answer_similarity": [
                    0.5918847410939634,
                    1.1442162559833378,
                    1.047689339146018
                ],
                "word_count_noun_chunks": [
                    9.0,
                    11.0,
                    9.0
                ],
                "result_count_bing": [
                    4520000.0,
                    185000.0,
                    3860000.0
                ],
                "word_count_raw": [
                    2.0,
                    3.0,
                    1.0
                ],
                "result_count": [
                    244000.0,
                    157000.0,
                    157000.0
                ],
                "answer_relation_to_question": [
                    1.7480062724014338,
                    1.113799283154122,
                    0.13819444444444445
                ],
                "word_count_appended": [
                    325.0,
                    316.0,
                    276.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these is an alveolar disorder?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "pneumonia"
            ],
            "question": "which of these is an alveolar disorder?",
            "lines": [
                [
                    0.5897435897435898,
                    0.5,
                    0.8723663702964939,
                    0.276605489621037,
                    0.5596723868954758,
                    0.39386602098466506,
                    0.7940904893813481,
                    0.8580802557415664,
                    0.3029556650246305,
                    0.568019093078759,
                    0.358974358974359,
                    0.4125381974350201,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.14102564102564102,
                    0.0,
                    0.0707411480449518,
                    0.48747470143114985,
                    0.3451638065522621,
                    0.19047619047619047,
                    0.04739919975377039,
                    0.025826533187515772,
                    0.21428571428571427,
                    0.08353221957040573,
                    0.11965811965811966,
                    0.2823675330651156,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.2692307692307692,
                    0.5,
                    0.05689248165855429,
                    0.2359198089478132,
                    0.09516380655226209,
                    0.4156577885391445,
                    0.15851031086488152,
                    0.11609321107091781,
                    0.4827586206896552,
                    0.34844868735083534,
                    0.5213675213675214,
                    0.3050942694998643,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "pneumonia",
                "strep throat",
                "psoriasis"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "psoriasis": 0.2891959614238134,
                "pneumonia": 1.012743914285031,
                "strep throat": -0.004820480620503645
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.8250763948700403,
                    0.5647350661302312,
                    0.6101885389997286
                ],
                "result_count_important_words": [
                    516000.0,
                    30800.0,
                    103000.0
                ],
                "wikipedia_search": [
                    0.605911330049261,
                    0.42857142857142855,
                    0.9655172413793104
                ],
                "word_count_appended_bing": [
                    42.0,
                    14.0,
                    61.0
                ],
                "answer_relation_to_question_bing": [
                    0.5,
                    0.0,
                    0.5
                ],
                "cosine_similarity_raw": [
                    0.1930895745754242,
                    0.01565784588456154,
                    0.012592582032084465
                ],
                "result_count_noun_chunks": [
                    1020000.0,
                    30700.0,
                    138000.0
                ],
                "question_answer_similarity": [
                    1.4998830556869507,
                    2.6433135718107224,
                    1.2792664542794228
                ],
                "word_count_noun_chunks": [
                    15.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    9760000.0,
                    4720000.0,
                    10300000.0
                ],
                "word_count_appended": [
                    476.0,
                    70.0,
                    292.0
                ],
                "answer_relation_to_question": [
                    1.1794871794871795,
                    0.28205128205128205,
                    0.5384615384615384
                ],
                "result_count": [
                    287000.0,
                    177000.0,
                    48800.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What is something often said after a sneeze?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "gesundheit"
            ],
            "question": "what is something often said after a sneeze?",
            "lines": [
                [
                    0.13636363636363635,
                    0.0,
                    0.4163142312660304,
                    0.4801098122072088,
                    0.00073947404908259,
                    0.028798411122144985,
                    1.3999619210357479e-05,
                    0.0001943580630830742,
                    0,
                    0.027548209366391185,
                    0.03773584905660377,
                    0.086067430272439,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.2727272727272727,
                    0.0,
                    0.37510025409537556,
                    0.5282436807617193,
                    0.0009705596894208993,
                    0.48659384309831183,
                    1.3199640969765622e-05,
                    0.0002498889382496668,
                    0,
                    0.027548209366391185,
                    0.03773584905660377,
                    0.1270303500938329,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.5909090909090909,
                    1.0,
                    0.20858551463859407,
                    -0.008353492968928122,
                    0.9982899662614965,
                    0.4846077457795432,
                    0.9999728007398199,
                    0.9995557529986673,
                    0,
                    0.9449035812672176,
                    0.9245283018867925,
                    0.7869022196337281,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "what are thoooose?",
                "not the mama!",
                "gesundheit"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "what are thoooose?": 0.33788739380158356,
                "not the mama!": 0.13248520225469856,
                "gesundheit": 0.887941383301401
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.172134860544878,
                    0.2540607001876658,
                    1.5738044392674562
                ],
                "result_count_important_words": [
                    35.0,
                    33.0,
                    2500000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    0.0
                ],
                "answer_relation_to_question": [
                    0.2727272727272727,
                    0.5454545454545454,
                    1.1818181818181819
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    49.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "cosine_similarity_raw": [
                    0.10288254916667938,
                    0.09269745647907257,
                    0.05154714360833168
                ],
                "result_count_noun_chunks": [
                    28.0,
                    36.0,
                    144000.0
                ],
                "question_answer_similarity": [
                    12.318324383348227,
                    13.55330977961421,
                    -0.21432812558487058
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    3.0
                ],
                "result_count_bing": [
                    1450000.0,
                    24500000.0,
                    24400000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    20.0
                ],
                "result_count": [
                    16.0,
                    21.0,
                    21600.0
                ],
                "word_count_appended": [
                    10.0,
                    10.0,
                    343.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of the following is NOT the name of a RuPaul song?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "jealous of my boogie"
            ],
            "lines": [
                [
                    0.13820097192915148,
                    0.16666666666666669,
                    0.1801377842951825,
                    0.030878462450571575,
                    0.49990688178124804,
                    0.49632507349853006,
                    0.49991052185211987,
                    0.412751677852349,
                    0.0,
                    0.4548254620123203,
                    0.3605769230769231,
                    0.37338812423450596,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.4,
                    0.33333333333333337,
                    0.4252936264164583,
                    0.4347467175699773,
                    0.14477123957593274,
                    0.4856152876942461,
                    0.08178691751670908,
                    0.21588366890380312,
                    0.5,
                    0.26899383983572894,
                    0.2980769230769231,
                    0.314840483265326,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.4617990280708485,
                    0.5,
                    0.3945685892883592,
                    0.5343748199794511,
                    0.35532187864281917,
                    0.018059638807223832,
                    0.4183025606311711,
                    0.3713646532438479,
                    0.5,
                    0.27618069815195073,
                    0.34134615384615385,
                    0.31177139250016817,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "jealous of my boogie": 0.39773857505840526,
                "snapshot": 0.34944299380192706,
                "werk": 0.25281843113966773
            },
            "question": "which of the following is not the name of a rupaul song?",
            "rate_limited": false,
            "answers": [
                "jealous of my boogie",
                "snapshot",
                "werk"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "jealous of my boogie": 0.35831548559175835,
                "snapshot": 0.12997825150715375,
                "werk": 0.14304549007289052
            },
            "integer_answers": {
                "jealous of my boogie": 5,
                "snapshot": 5,
                "werk": 2
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.7596712545929643,
                    1.1109571004080443,
                    1.1293716449989912
                ],
                "result_count_important_words": [
                    92.0,
                    430000.0,
                    84000.0
                ],
                "wikipedia_search": [
                    1.0,
                    0.0,
                    0.0
                ],
                "answer_relation_to_question": [
                    2.170794168425091,
                    0.6,
                    0.22920583157490887
                ],
                "word_count_appended_bing": [
                    29.0,
                    42.0,
                    33.0
                ],
                "answer_relation_to_question_bing": [
                    2.0,
                    1.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.1416553556919098,
                    0.03308473899960518,
                    0.04669174179434776
                ],
                "result_count_noun_chunks": [
                    39000.0,
                    127000.0,
                    57500.0
                ],
                "question_answer_similarity": [
                    14.626556504517794,
                    2.0345065109431744,
                    -1.0717590358108282
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    350000.0,
                    1370000.0,
                    45900000.0
                ],
                "word_count_appended": [
                    44.0,
                    225.0,
                    218.0
                ],
                "result_count": [
                    54.0,
                    206000.0,
                    83900.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "The U.S. government once paid what composer to write a pro-IRS song called \u201cI Paid My Income Tax Today\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "irving berlin"
            ],
            "lines": [
                [
                    0.28015066390343857,
                    0.2583618345225023,
                    0.026243189771084138,
                    -0.07760693555683054,
                    0.0,
                    0.18253968253968253,
                    0.3333333333333333,
                    0.9781021897810219,
                    0.25093425234063155,
                    0.17647058823529413,
                    0.3,
                    0.18479084227337608,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.45123654518563566,
                    0.41588497459650303,
                    0.951473554185036,
                    0.4801909397261631,
                    0.09615384615384616,
                    0.2619047619047619,
                    0.3825136612021858,
                    0.010381184103811841,
                    0.4741721937084036,
                    0.4117647058823529,
                    0.5,
                    0.2503211207778855,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.26861279091092577,
                    0.32575319088099475,
                    0.022283256043879866,
                    0.5974159958306674,
                    0.9038461538461539,
                    0.5555555555555556,
                    0.28415300546448086,
                    0.01151662611516626,
                    0.2748935539509649,
                    0.4117647058823529,
                    0.2,
                    0.5648880369487383,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "irving berlin": 0.47757124910189896,
                "woody guthrie": 0.20666568865310955,
                "carole king": 0.31576306224499145
            },
            "question": "the u.s. government once paid what composer to write a pro-irs song called \u201ci paid my income tax today\u201d?",
            "rate_limited": false,
            "answers": [
                "woody guthrie",
                "irving berlin",
                "carole king"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "irving berlin": 0.7573851840267674,
                "woody guthrie": 0.09974918843942385,
                "carole king": 0.37430793030533077
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.402280949553889,
                    3.2541745701125113,
                    7.343544480333599
                ],
                "result_count_important_words": [
                    61.0,
                    70.0,
                    52.0
                ],
                "wikipedia_search": [
                    3.011211028087579,
                    5.690066324500843,
                    3.2987226474115787
                ],
                "word_count_appended_bing": [
                    3.0,
                    5.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    2.3252565107025203,
                    3.7429647713685275,
                    2.9317787179289527
                ],
                "cosine_similarity_raw": [
                    0.016130901873111725,
                    0.5848422646522522,
                    0.013696849346160889
                ],
                "result_count_noun_chunks": [
                    6030.0,
                    64.0,
                    71.0
                ],
                "question_answer_similarity": [
                    -0.564964628778398,
                    3.4957042699679732,
                    4.349081739783287
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    4.0,
                    0.0
                ],
                "result_count_bing": [
                    39100.0,
                    56100.0,
                    119000.0
                ],
                "result_count": [
                    0,
                    5.0,
                    47.0
                ],
                "answer_relation_to_question": [
                    2.5213559751309473,
                    4.061128906670721,
                    2.417515118198332
                ],
                "word_count_appended": [
                    3.0,
                    7.0,
                    7.0
                ]
            },
            "integer_answers": {
                "irving berlin": 9,
                "woody guthrie": 1,
                "carole king": 4
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "A high-tech store with no cashiers recently opened in Seattle under what name?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "amazon go"
            ],
            "question": "a high-tech store with no cashiers recently opened in seattle under what name?",
            "lines": [
                [
                    0.6765913944941405,
                    0.7453441295546559,
                    0.9242980565852148,
                    0.39043960681970147,
                    0.05742097043459529,
                    0.1743878226340172,
                    0.18709292671803926,
                    0.011264916390412827,
                    0.8389454819845312,
                    0.9213483146067416,
                    0.75,
                    0.5057722093757808,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.19299000960900273,
                    0.1270505323136902,
                    0.05130265364838271,
                    0.40360665808502477,
                    0.0001480962895147827,
                    0.2399073461283918,
                    0.00022217285047767163,
                    1.5803299972161878e-05,
                    0.03508771929824561,
                    0.0449438202247191,
                    0.125,
                    0.09419868035643005,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.13041859589685675,
                    0.12760533813165392,
                    0.024399289766402477,
                    0.2059537350952738,
                    0.9424309332758899,
                    0.585704831237591,
                    0.8126849004314831,
                    0.988719280309615,
                    0.12596679871722316,
                    0.033707865168539325,
                    0.125,
                    0.4000291102677892,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "amazon go",
                "google shop",
                "microsoft zip"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "google shop": 0.029558510997022242,
                "microsoft zip": 0.11692750065039147,
                "amazon go": 0.9135040837136864
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.540405465630466,
                    0.6593907624950103,
                    2.8002037718745245
                ],
                "result_count_important_words": [
                    32000.0,
                    38.0,
                    139000.0
                ],
                "wikipedia_search": [
                    5.033672891907187,
                    0.21052631578947367,
                    0.755800792303339
                ],
                "word_count_appended_bing": [
                    12.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    3.7267206477732793,
                    0.6352526615684511,
                    0.6380266906582697
                ],
                "cosine_similarity_raw": [
                    0.7281864881515503,
                    0.040417589247226715,
                    0.019222406670451164
                ],
                "result_count_noun_chunks": [
                    27800.0,
                    39.0,
                    2440000.0
                ],
                "question_answer_similarity": [
                    8.234383163042367,
                    8.512076674029231,
                    4.343570526689291
                ],
                "word_count_noun_chunks": [
                    102.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    527000.0,
                    725000.0,
                    1770000.0
                ],
                "word_count_raw": [
                    16.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    8530.0,
                    22.0,
                    140000.0
                ],
                "answer_relation_to_question": [
                    4.7361397614589835,
                    1.3509300672630191,
                    0.9129301712779974
                ],
                "word_count_appended": [
                    164.0,
                    8.0,
                    6.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Until it was banned, lithium was a key ingredient in which of these brands?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "7up"
            ],
            "lines": [
                [
                    0.06428571428571428,
                    0.14655172413793102,
                    0.07135812956154025,
                    0.21936787856822487,
                    0.5499243570347958,
                    0.26897470039946736,
                    0.21641144177983243,
                    0.585432266848196,
                    0.13513513513513514,
                    0.056962025316455694,
                    0.2391304347826087,
                    0.36500183957063986,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.7579365079365079,
                    0.625,
                    0.7992065896515486,
                    -0.05531387488783091,
                    0.4485627836611195,
                    0.2756324900133156,
                    0.7830106905518637,
                    0.4118447923757658,
                    0.8648648648648649,
                    0.9177215189873418,
                    0.717391304347826,
                    0.5194425694283371,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.1777777777777778,
                    0.22844827586206898,
                    0.12943528078691116,
                    0.8359459963196061,
                    0.0015128593040847202,
                    0.45539280958721706,
                    0.0005778676683039584,
                    0.002722940776038121,
                    0.0,
                    0.02531645569620253,
                    0.043478260869565216,
                    0.11555559100102308,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "cracker jack": 0.2084668319586101,
                "7up": 0.6475214454950472,
                "good and plenty": 0.1440117225463427
            },
            "question": "until it was banned, lithium was a key ingredient in which of these brands?",
            "rate_limited": false,
            "answers": [
                "cracker jack",
                "7up",
                "good and plenty"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "cracker jack": 0.20862312472465147,
                "7up": 0.8232899199413797,
                "good and plenty": 0.13092685556472403
            },
            "integer_answers": {
                "cracker jack": 2,
                "7up": 10,
                "good and plenty": 2
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.8250091978531993,
                    2.5972128471416855,
                    0.5777779550051154
                ],
                "result_count_important_words": [
                    749.0,
                    2710.0,
                    2.0
                ],
                "wikipedia_search": [
                    0.40540540540540543,
                    2.5945945945945947,
                    0.0
                ],
                "word_count_appended_bing": [
                    11.0,
                    33.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.29310344827586204,
                    1.25,
                    0.45689655172413796
                ],
                "cosine_similarity_raw": [
                    0.013775791972875595,
                    0.15428800880908966,
                    0.02498767152428627
                ],
                "result_count_noun_chunks": [
                    1720.0,
                    1210.0,
                    8.0
                ],
                "question_answer_similarity": [
                    4.679841212928295,
                    -1.180027600377798,
                    17.833488434553146
                ],
                "word_count_noun_chunks": [
                    0.0,
                    8.0,
                    0.0
                ],
                "result_count_bing": [
                    20200.0,
                    20700.0,
                    34200.0
                ],
                "word_count_raw": [
                    0.0,
                    9.0,
                    0.0
                ],
                "result_count": [
                    727.0,
                    593.0,
                    2.0
                ],
                "answer_relation_to_question": [
                    0.2571428571428571,
                    3.0317460317460316,
                    0.7111111111111112
                ],
                "word_count_appended": [
                    9.0,
                    145.0,
                    4.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these phrases, written backwards, is a hip hop group?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "beat chefs"
            ],
            "lines": [
                [
                    0.3867880485527544,
                    0.5873015873015873,
                    0.43304426099506443,
                    0.3006900407745902,
                    0.007735204173412484,
                    0.1198005919925222,
                    0.001029738858225554,
                    2.4377030071802792e-05,
                    0.03571428571428571,
                    0.37209302325581395,
                    0.3333333333333333,
                    0.2358692240465259,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.5373093681917211,
                    0.18253968253968253,
                    0.3729286237440798,
                    0.30967054954573076,
                    0.0010793308148947653,
                    0.3053435114503817,
                    0.00012356866298706648,
                    1.9899616385145134e-05,
                    0.0,
                    0.2558139534883721,
                    0.3333333333333333,
                    0.1325731991006469,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.07590258325552442,
                    0.23015873015873015,
                    0.19402711526085573,
                    0.389639409679679,
                    0.9911854650116928,
                    0.5748558965570961,
                    0.9988466924787874,
                    0.9999557233535431,
                    0.9642857142857143,
                    0.37209302325581395,
                    0.3333333333333333,
                    0.6315575768528271,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "beat chefs": 0.2025612517073513,
                "drummers ear": 0.23445197633568227,
                "blues rhythm": 0.5629867719569664
            },
            "question": "which of these phrases, written backwards, is a hip hop group?",
            "rate_limited": false,
            "answers": [
                "drummers ear",
                "beat chefs",
                "blues rhythm"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "beat chefs": 0.31693190505617475,
                "drummers ear": 0.290991099710818,
                "blues rhythm": 0.23526724641060173
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.4152153442791555,
                    0.7954391946038815,
                    3.789345461116963
                ],
                "result_count_important_words": [
                    50.0,
                    6.0,
                    48500.0
                ],
                "wikipedia_search": [
                    0.14285714285714285,
                    0.0,
                    3.857142857142857
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.7619047619047619,
                    0.5476190476190476,
                    0.6904761904761905
                ],
                "cosine_similarity_raw": [
                    0.058199070394039154,
                    0.05011981725692749,
                    0.026076313108205795
                ],
                "result_count_noun_chunks": [
                    49.0,
                    40.0,
                    2010000.0
                ],
                "question_answer_similarity": [
                    4.85761278308928,
                    5.0026918621733785,
                    6.2945795357227325
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    76900.0,
                    196000.0,
                    369000.0
                ],
                "word_count_appended": [
                    16.0,
                    11.0,
                    16.0
                ],
                "answer_relation_to_question": [
                    2.3207282913165264,
                    3.223856209150327,
                    0.45541549953314653
                ],
                "result_count": [
                    43.0,
                    6.0,
                    5510.0
                ]
            },
            "integer_answers": {
                "beat chefs": 1,
                "drummers ear": 4,
                "blues rhythm": 7
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What term describes a person from the state between New York and Rhode Island?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "hoosier"
            ],
            "lines": [
                [
                    0.31622791335180717,
                    0.3278957286432161,
                    0.2761783223035961,
                    0.8662525025803617,
                    0.008923431203223949,
                    0.3345367027677497,
                    0.02101915110694041,
                    0.5128636287203632,
                    0.2922962962962963,
                    0.2638580931263858,
                    0.32926829268292684,
                    0.28454492549048427,
                    0,
                    0,
                    1.0
                ],
                [
                    0.462582809411718,
                    0.2591520100502513,
                    0.41484585400109436,
                    -0.0,
                    0.005604659825933828,
                    0.33574007220216606,
                    0.002569656999026512,
                    0.005801244324869682,
                    0.0654074074074074,
                    0.29933481152993346,
                    0.32926829268292684,
                    0.3234356651036388,
                    0,
                    0,
                    1.0
                ],
                [
                    0.22118927723647486,
                    0.4129522613065327,
                    0.3089758236953095,
                    0.13374749741963826,
                    0.9854719089708422,
                    0.3297232250300842,
                    0.976411191894033,
                    0.4813351269547671,
                    0.6422962962962963,
                    0.43680709534368073,
                    0.34146341463414637,
                    0.39201940940587693,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "hoosier": 0.47186604401564014,
                "nutmegger": 0.2086452069615805,
                "cheesehead": 0.31948874902277924
            },
            "question": "what term describes a person from the state between new york and rhode island?",
            "rate_limited": false,
            "answers": [
                "cheesehead",
                "nutmegger",
                "hoosier"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "hoosier": 0.32731553267653396,
                "nutmegger": 0.16516141950389213,
                "cheesehead": 0.06305124328207959
            },
            "integer_answers": {
                "hoosier": 7,
                "nutmegger": 3,
                "cheesehead": 1
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.276359403923874,
                    2.5874853208291104,
                    3.1361552752470154
                ],
                "result_count_important_words": [
                    7190.0,
                    879.0,
                    334000.0
                ],
                "wikipedia_search": [
                    1.4614814814814814,
                    0.327037037037037,
                    3.2114814814814814
                ],
                "word_count_appended_bing": [
                    27.0,
                    27.0,
                    28.0
                ],
                "answer_relation_to_question_bing": [
                    1.3115829145728644,
                    1.0366080402010052,
                    1.6518090452261307
                ],
                "cosine_similarity_raw": [
                    0.03117109090089798,
                    0.04682191461324692,
                    0.03487280756235123
                ],
                "result_count_noun_chunks": [
                    244000.0,
                    2760.0,
                    229000.0
                ],
                "question_answer_similarity": [
                    -1.2163297208026052,
                    0.0,
                    -0.1877986565232277
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    27800000.0,
                    27900000.0,
                    27400000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    5270.0,
                    3310.0,
                    582000.0
                ],
                "answer_relation_to_question": [
                    1.8973674801108429,
                    2.775496856470308,
                    1.3271356634188491
                ],
                "word_count_appended": [
                    119.0,
                    135.0,
                    197.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Who just became the youngest woman to win gold in Olympics snowboarding?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "chloe kim"
            ],
            "question": "who just became the youngest woman to win gold in olympics snowboarding?",
            "lines": [
                [
                    0.07297838722419728,
                    0.14479166666666665,
                    0.016756293335886917,
                    0.17715392863188717,
                    0.04948875255623722,
                    0.3161953727506427,
                    0.007578371451104101,
                    0.317907444668008,
                    0.08304195804195805,
                    0.0,
                    0.0,
                    0.2560388766572195,
                    0.0,
                    0.0,
                    0.0
                ],
                [
                    0.024055770424485503,
                    0.04583333333333334,
                    0.027683202262805247,
                    0.2820268265502115,
                    0.588957055214724,
                    0.2249357326478149,
                    0.0004559345425867508,
                    0.3118712273641851,
                    0.06439393939393939,
                    0.0,
                    0.0,
                    0.25853450503442127,
                    0.0,
                    0.0,
                    0.0
                ],
                [
                    0.9029658423513172,
                    0.8093750000000001,
                    0.9555605044013078,
                    0.5408192448179013,
                    0.36155419222903884,
                    0.4588688946015424,
                    0.9919656940063092,
                    0.3702213279678068,
                    0.8525641025641025,
                    1.0,
                    1.0,
                    0.48542661830835926,
                    1.0,
                    1.0,
                    0.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "khlo\u00e9 kardashian",
                "chlo\u00eb grace moretz",
                "chloe kim"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "chlo\u00eb grace moretz": 0.07859308003079309,
                "khlo\u00e9 kardashian": 0.08799802879216438,
                "chloe kim": 0.9017931566680056
            },
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.5362332599433173,
                    1.5512070302065277,
                    2.9125597098501554
                ],
                "result_count_important_words": [
                    1230.0,
                    74.0,
                    161000.0
                ],
                "wikipedia_search": [
                    0.4982517482517483,
                    0.38636363636363635,
                    5.115384615384615
                ],
                "word_count_appended_bing": [
                    0.0,
                    0.0,
                    34.0
                ],
                "answer_relation_to_question_bing": [
                    0.8687499999999999,
                    0.275,
                    4.85625
                ],
                "cosine_similarity_raw": [
                    0.012952199205756187,
                    0.021398428827524185,
                    0.7386245727539062
                ],
                "result_count_noun_chunks": [
                    158000.0,
                    155000.0,
                    184000.0
                ],
                "question_answer_similarity": [
                    0.9560651862993836,
                    1.5220437534153461,
                    2.9186959388607647
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    47.0
                ],
                "result_count_bing": [
                    246000.0,
                    175000.0,
                    357000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    27.0
                ],
                "result_count": [
                    12100.0,
                    144000.0,
                    88400.0
                ],
                "answer_relation_to_question": [
                    0.4378703233451837,
                    0.14433462254691304,
                    5.417795054107904
                ],
                "word_count_appended": [
                    0.0,
                    0.0,
                    188.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which animal relies on a tomial tooth to attack prey?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "falcon"
            ],
            "question": "which animal relies on a tomial tooth to attack prey?",
            "lines": [
                [
                    0.07698412698412699,
                    0.6884920634920635,
                    0.7899531054664196,
                    0.17442056316644955,
                    0.35714285714285715,
                    0.0953262378528459,
                    0.9772535804549284,
                    0.547486033519553,
                    0.18,
                    0.7030567685589519,
                    0.39552238805970147,
                    0.45329740725820306,
                    0.9896373056994818,
                    1.0,
                    -1.0
                ],
                [
                    0.35100732600732604,
                    0.16666666666666669,
                    0.11494871108682674,
                    0.1779422979889269,
                    0.41836734693877553,
                    0.8144377602961592,
                    0.01825329963493401,
                    0.3128491620111732,
                    0.42500000000000004,
                    0.23580786026200873,
                    0.4552238805970149,
                    0.3690013900176817,
                    0.010362694300518135,
                    0.0,
                    -1.0
                ],
                [
                    0.572008547008547,
                    0.14484126984126985,
                    0.09509818344675366,
                    0.6476371388446235,
                    0.22448979591836735,
                    0.09023600185099491,
                    0.004493119910137602,
                    0.13966480446927373,
                    0.39499999999999996,
                    0.0611353711790393,
                    0.14925373134328357,
                    0.17770120272411527,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "falcon",
                "rattlesnake",
                "vampire bat"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "falcon": 0.9452155093587431,
                "vampire bat": 0.008892938529159112,
                "rattlesnake": 0.23714135284774737
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.7197844435492184,
                    2.21400834010609,
                    1.0662072163446916
                ],
                "result_count_important_words": [
                    3480.0,
                    65.0,
                    16.0
                ],
                "wikipedia_search": [
                    0.72,
                    1.7000000000000002,
                    1.5799999999999998
                ],
                "word_count_appended_bing": [
                    53.0,
                    61.0,
                    20.0
                ],
                "answer_relation_to_question_bing": [
                    2.0654761904761902,
                    0.5,
                    0.4345238095238095
                ],
                "cosine_similarity_raw": [
                    0.32139360904693604,
                    0.04676705598831177,
                    0.03869083896279335
                ],
                "result_count_noun_chunks": [
                    98.0,
                    56.0,
                    25.0
                ],
                "question_answer_similarity": [
                    1.115691434359178,
                    1.1382184191606939,
                    4.142649210989475
                ],
                "word_count_noun_chunks": [
                    191.0,
                    2.0,
                    0.0
                ],
                "word_count_raw": [
                    35.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    20600.0,
                    176000.0,
                    19500.0
                ],
                "word_count_appended": [
                    161.0,
                    54.0,
                    14.0
                ],
                "answer_relation_to_question": [
                    0.30793650793650795,
                    1.4040293040293041,
                    2.288034188034188
                ],
                "result_count": [
                    35.0,
                    41.0,
                    22.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "The first person to lead an expedition to the South Pole came from what country?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "norway"
            ],
            "lines": [
                [
                    0.18134920634920634,
                    0.1564102564102564,
                    0.47738855806418595,
                    0.24177599908722341,
                    0.2912552891396333,
                    0.19661563255439163,
                    0.2733812949640288,
                    0.2917369308600337,
                    0.3841568718276065,
                    0.3048327137546468,
                    0.3333333333333333,
                    0.3443073112576064,
                    1.0,
                    0.25,
                    1.0
                ],
                [
                    0.25396825396825395,
                    0.5056410256410258,
                    0.15151608208606487,
                    0.12069222672759346,
                    0.15867418899858957,
                    0.1385979049153908,
                    0.21007194244604316,
                    0.18549747048903878,
                    0.3842689900612428,
                    0.3828996282527881,
                    0.3125,
                    0.31897436866404494,
                    0.0,
                    0.25,
                    1.0
                ],
                [
                    0.5646825396825397,
                    0.33794871794871795,
                    0.37109535984974923,
                    0.6375317741851831,
                    0.5500705218617772,
                    0.6647864625302176,
                    0.516546762589928,
                    0.5227655986509275,
                    0.23157413811115066,
                    0.31226765799256506,
                    0.3541666666666667,
                    0.33671832007834873,
                    0.0,
                    0.5,
                    1.0
                ]
            ],
            "fraction_answers": {
                "canada": 0.4214396085819837,
                "iceland": 0.24095014873214832,
                "norway": 0.33761024268586803
            },
            "question": "the first person to lead an expedition to the south pole came from what country?",
            "rate_limited": false,
            "answers": [
                "norway",
                "iceland",
                "canada"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "canada": 0.22638971214216727,
                "iceland": 0.1979473130238837,
                "norway": 0.4537116745438933
            },
            "integer_answers": {
                "canada": 8,
                "iceland": 3,
                "norway": 3
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.4101511788032446,
                    2.2328205806483146,
                    2.3570282405484413
                ],
                "result_count_important_words": [
                    190000.0,
                    146000.0,
                    359000.0
                ],
                "wikipedia_search": [
                    2.6890981027932455,
                    2.6898829304286997,
                    1.6210189667780546
                ],
                "word_count_appended_bing": [
                    32.0,
                    30.0,
                    34.0
                ],
                "answer_relation_to_question_bing": [
                    0.782051282051282,
                    2.5282051282051285,
                    1.6897435897435897
                ],
                "cosine_similarity_raw": [
                    0.06393487006425858,
                    0.02029198408126831,
                    0.04969941824674606
                ],
                "result_count_noun_chunks": [
                    173000.0,
                    110000.0,
                    310000.0
                ],
                "question_answer_similarity": [
                    1.361483957618475,
                    0.6796395470155403,
                    3.590055614709854
                ],
                "word_count_noun_chunks": [
                    6.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    488000.0,
                    344000.0,
                    1650000.0
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    2.0
                ],
                "result_count": [
                    82600.0,
                    45000.0,
                    156000.0
                ],
                "answer_relation_to_question": [
                    1.088095238095238,
                    1.5238095238095237,
                    3.388095238095238
                ],
                "word_count_appended": [
                    246.0,
                    309.0,
                    252.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these is a U.S. postage stamp?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "forever stamp"
            ],
            "lines": [
                [
                    0.09170591313448456,
                    0.20782312925170068,
                    0.021421084261492605,
                    0.19778120577025707,
                    0.010238224038789044,
                    0.4454609429978888,
                    3.1763246708964083e-05,
                    6.19083928189399e-05,
                    0.046468401486988845,
                    0.28308823529411764,
                    0.3888888888888889,
                    0.23965737444036073,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.7736787022501308,
                    0.5792517006802721,
                    0.8400866635238591,
                    0.4370629939845183,
                    0.2885135541263449,
                    0.09289232934553132,
                    0.054724629872070656,
                    0.04388443035266625,
                    0.6988847583643123,
                    0.33455882352941174,
                    0.4444444444444444,
                    0.39091027805334017,
                    1.0,
                    0,
                    -1.0
                ],
                [
                    0.1346153846153846,
                    0.21292517006802722,
                    0.13849225221464834,
                    0.3651558002452246,
                    0.701248221834866,
                    0.4616467276565799,
                    0.9452436068812203,
                    0.9560536612545149,
                    0.25464684014869887,
                    0.38235294117647056,
                    0.16666666666666666,
                    0.36943234750629905,
                    0.0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "terence stamp": 0.14866362086188437,
                "forever stamp": 0.4599148698866848,
                "rubber stamp": 0.3914215092514309
            },
            "question": "which of these is a u.s. postage stamp?",
            "rate_limited": false,
            "answers": [
                "terence stamp",
                "forever stamp",
                "rubber stamp"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "terence stamp": 0.1260103156444987,
                "forever stamp": 0.5377733909217588,
                "rubber stamp": 0.25373361824932844
            },
            "integer_answers": {
                "terence stamp": 0,
                "forever stamp": 8,
                "rubber stamp": 5
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.7189721233210822,
                    1.1727308341600204,
                    1.1082970425188972
                ],
                "result_count_important_words": [
                    83.0,
                    143000.0,
                    2470000.0
                ],
                "wikipedia_search": [
                    0.09293680297397769,
                    1.3977695167286246,
                    0.5092936802973977
                ],
                "word_count_appended_bing": [
                    7.0,
                    8.0,
                    3.0
                ],
                "answer_relation_to_question_bing": [
                    0.6234693877551021,
                    1.7377551020408162,
                    0.6387755102040816
                ],
                "cosine_similarity_raw": [
                    0.016785720363259315,
                    0.658298134803772,
                    0.10852355509996414
                ],
                "result_count_noun_chunks": [
                    79.0,
                    56000.0,
                    1220000.0
                ],
                "question_answer_similarity": [
                    2.799171890132129,
                    6.185696169734001,
                    5.168002933263779
                ],
                "word_count_noun_chunks": [
                    0.0,
                    5.0,
                    0.0
                ],
                "result_count_bing": [
                    6330000.0,
                    1320000.0,
                    6560000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    51100.0,
                    1440000.0,
                    3500000.0
                ],
                "answer_relation_to_question": [
                    0.2751177394034537,
                    2.3210361067503924,
                    0.40384615384615385
                ],
                "word_count_appended": [
                    77.0,
                    91.0,
                    104.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Made famous in a documentary, where is the \u201cGrey Gardens\u201d home located?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "east hampton, ny"
            ],
            "lines": [
                [
                    0.1996732026143791,
                    0.425,
                    0.11839124318058028,
                    0.3674592436367138,
                    0.22972972972972974,
                    0.07704972011853803,
                    0.000474861305752771,
                    0.7539742619227857,
                    0.3482142857142857,
                    0.12121212121212122,
                    0.2857142857142857,
                    0.1761709863152494,
                    0.0,
                    0,
                    3.0
                ],
                [
                    0.6416258169934641,
                    0.2625,
                    0.7604082370831856,
                    0.39010097015012135,
                    0.5945945945945946,
                    0.06848864010536714,
                    0.3497758886276508,
                    0.22649507948523845,
                    0.38125,
                    0.696969696969697,
                    0.2857142857142857,
                    0.6665708212135255,
                    1.0,
                    0,
                    3.0
                ],
                [
                    0.15870098039215685,
                    0.3125,
                    0.12120051973623412,
                    0.24243978621316487,
                    0.17567567567567569,
                    0.8544616397760948,
                    0.6497492500665965,
                    0.019530658591975777,
                    0.27053571428571427,
                    0.18181818181818182,
                    0.42857142857142855,
                    0.15725819247122505,
                    0.0,
                    0,
                    3.0
                ]
            ],
            "fraction_answers": {
                "east hampton, ny": 0.48649954084131775,
                "savannah, ga": 0.2748032328921883,
                "cape cod, ma": 0.238697226266494
            },
            "question": "made famous in a documentary, where is the \u201cgrey gardens\u201d home located?",
            "rate_limited": false,
            "answers": [
                "cape cod, ma",
                "east hampton, ny",
                "savannah, ga"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "east hampton, ny": 0.6087735524260762,
                "savannah, ga": 0.12853168771410994,
                "cape cod, ma": 0.12023459203783665
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 3
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.0570259178914965,
                    3.999424927281153,
                    0.9435491548273502
                ],
                "result_count_important_words": [
                    41.0,
                    30200.0,
                    56100.0
                ],
                "wikipedia_search": [
                    1.3928571428571428,
                    1.525,
                    1.082142857142857
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    3.0
                ],
                "answer_relation_to_question_bing": [
                    1.7,
                    1.05,
                    1.25
                ],
                "cosine_similarity_raw": [
                    0.020820097997784615,
                    0.13372419774532318,
                    0.02131413295865059
                ],
                "result_count_noun_chunks": [
                    249000.0,
                    74800.0,
                    6450.0
                ],
                "question_answer_similarity": [
                    10.66745613887906,
                    11.324752502143383,
                    7.038102403283119
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    468000.0,
                    416000.0,
                    5190000.0
                ],
                "word_count_appended": [
                    4.0,
                    23.0,
                    6.0
                ],
                "answer_relation_to_question": [
                    0.7986928104575164,
                    2.5665032679738564,
                    0.6348039215686274
                ],
                "result_count": [
                    34.0,
                    88.0,
                    26.0
                ]
            },
            "integer_answers": {
                "east hampton, ny": 8,
                "savannah, ga": 3,
                "cape cod, ma": 2
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Bart's age on \u201cThe Simpsons\u201d is NOT the same as which of these characters?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ash from pok\u00e9mon"
            ],
            "question": "bart's age on \u201cthe simpsons\u201d is not the same as which of these characters?",
            "lines": [
                [
                    0.132996632996633,
                    0.2786098310291859,
                    0.3330598399868425,
                    0.3034687864651192,
                    0.49906085649887305,
                    0.33926218708827405,
                    0.4995638728094521,
                    0.4993057272744565,
                    0.3194444444444444,
                    0.5,
                    0.5,
                    0.40855772632386633,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.44388327721661053,
                    0.39343317972350234,
                    0.3834227437417439,
                    0.3474516383750068,
                    0.438580015026296,
                    0.3695652173913043,
                    0.2938307826500674,
                    0.1994655051071843,
                    0.4322649572649573,
                    0.26724137931034486,
                    0.25,
                    0.33249281364726635,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.42312008978675647,
                    0.3279569892473118,
                    0.2835174162714136,
                    0.349079575159874,
                    0.06235912847483094,
                    0.29117259552042163,
                    0.20660534454048052,
                    0.30122876761835926,
                    0.2482905982905983,
                    0.2327586206896552,
                    0.25,
                    0.2589494600288673,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "ash from pok\u00e9mon",
                "encyclopedia brown",
                "nancy drew"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "nancy drew": -0.05361842105668617,
                "encyclopedia brown": 0.04606901076443263,
                "ash from pok\u00e9mon": 0.23296146865320003
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.7315381894090691,
                    1.3400574908218694,
                    1.9284043197690615
                ],
                "result_count_important_words": [
                    77.0,
                    36400.0,
                    51800.0
                ],
                "wikipedia_search": [
                    0.7222222222222222,
                    0.270940170940171,
                    1.0068376068376068
                ],
                "word_count_appended_bing": [
                    0.0,
                    3.0,
                    3.0
                ],
                "answer_relation_to_question_bing": [
                    0.8855606758832565,
                    0.42626728110599077,
                    0.6881720430107527
                ],
                "cosine_similarity_raw": [
                    0.04222925007343292,
                    0.02948942966759205,
                    0.05476152151823044
                ],
                "result_count_noun_chunks": [
                    73.0,
                    31600.0,
                    20900.0
                ],
                "question_answer_similarity": [
                    7.819983180612326,
                    6.069904116913676,
                    6.005128460936248
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    48800.0,
                    39600.0,
                    63400.0
                ],
                "word_count_appended": [
                    0.0,
                    27.0,
                    31.0
                ],
                "answer_relation_to_question": [
                    2.202020202020202,
                    0.3367003367003367,
                    0.4612794612794613
                ],
                "result_count": [
                    50.0,
                    3270.0,
                    23300.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which person is most famous for being a children\u2019s book writer?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "dr. seuss"
            ],
            "lines": [
                [
                    0.3275591075591076,
                    0.19458389563652723,
                    0.06795113494960646,
                    0.21003198744933138,
                    0.11862857142857143,
                    0.3474320241691843,
                    0.3140916808149406,
                    0.18459877653832313,
                    0.43396549975696663,
                    0.28493150684931506,
                    0.4166666666666667,
                    0.2881767476202425,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.5948651348651348,
                    0.5449212775528565,
                    0.8841390889721493,
                    0.444350695785928,
                    0.7588571428571429,
                    0.3496978851963746,
                    0.4923599320882852,
                    0.6297229219143576,
                    0.3450155786116677,
                    0.3643835616438356,
                    0.4166666666666667,
                    0.4059557457839256,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.07757575757575758,
                    0.2604948268106163,
                    0.04790977607824424,
                    0.3456173167647406,
                    0.12251428571428571,
                    0.3028700906344411,
                    0.1935483870967742,
                    0.18567830154731918,
                    0.2210189216313656,
                    0.3506849315068493,
                    0.16666666666666666,
                    0.30586750659583195,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "tiger woods": 0.18431762633020662,
                "paris hilton": 0.2277583999599131,
                "dr. seuss": 0.5879239737098804
            },
            "question": "which person is most famous for being a children\u2019s book writer?",
            "rate_limited": false,
            "answers": [
                "paris hilton",
                "dr. seuss",
                "tiger woods"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "tiger woods": 0.15416646071314272,
                "paris hilton": 0.09362944612562255,
                "dr. seuss": 0.693692889839821
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.4408837381012125,
                    2.029778728919628,
                    1.5293375329791599
                ],
                "result_count_important_words": [
                    1850000.0,
                    2900000.0,
                    1140000.0
                ],
                "wikipedia_search": [
                    2.169827498784833,
                    1.7250778930583386,
                    1.105094608156828
                ],
                "word_count_appended_bing": [
                    30.0,
                    30.0,
                    12.0
                ],
                "answer_relation_to_question_bing": [
                    0.9729194781826361,
                    2.7246063877642825,
                    1.3024741340530814
                ],
                "cosine_similarity_raw": [
                    0.016191523522138596,
                    0.21067431569099426,
                    0.011416031047701836
                ],
                "result_count_noun_chunks": [
                    513000.0,
                    1750000.0,
                    516000.0
                ],
                "question_answer_similarity": [
                    2.633950650691986,
                    5.572474071756005,
                    4.334287207573652
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    2.0,
                    0.0
                ],
                "result_count_bing": [
                    46000000.0,
                    46300000.0,
                    40100000.0
                ],
                "word_count_appended": [
                    104.0,
                    133.0,
                    128.0
                ],
                "answer_relation_to_question": [
                    1.637795537795538,
                    2.9743256743256743,
                    0.3878787878787879
                ],
                "result_count": [
                    519000.0,
                    3320000.0,
                    536000.0
                ]
            },
            "integer_answers": {
                "tiger woods": 0,
                "paris hilton": 2,
                "dr. seuss": 12
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these elements would earn you the highest score in Scrabble?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "oxygen"
            ],
            "question": "which of these elements would earn you the highest score in scrabble?",
            "lines": [
                [
                    0.29251355013550134,
                    0.18115942028985507,
                    0.546338901334228,
                    0.5474390729476273,
                    0.4488200376429709,
                    0.38769230769230767,
                    0.006641062570011202,
                    0.4570990806945863,
                    0.5664794007490637,
                    0.37450980392156863,
                    0.37755102040816324,
                    0.385547954734964,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.48204607046070463,
                    0.13768115942028986,
                    0.18145074566814454,
                    0.05241256827694494,
                    0.002219970078664157,
                    0.4153846153846154,
                    0.0052008321331413025,
                    0.03983656792645557,
                    0.12890137328339576,
                    0.29215686274509806,
                    0.29591836734693877,
                    0.2930454376671805,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.22544037940379402,
                    0.6811594202898551,
                    0.2722103529976275,
                    0.40014835877542776,
                    0.5489599922783649,
                    0.19692307692307692,
                    0.9881581052968474,
                    0.5030643513789581,
                    0.3046192259675406,
                    0.3333333333333333,
                    0.32653061224489793,
                    0.32140660759785555,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "oxygen",
                "xenon",
                "zinc"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "oxygen": 0.5504480696088643,
                "zinc": 0.20833639973478155,
                "xenon": 0.10575629902989722
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.92773977367482,
                    1.4652271883359025,
                    1.6070330379892777
                ],
                "result_count_important_words": [
                    166000.0,
                    130000.0,
                    24700000.0
                ],
                "wikipedia_search": [
                    1.6994382022471908,
                    0.38670411985018727,
                    0.9138576779026217
                ],
                "word_count_appended_bing": [
                    37.0,
                    29.0,
                    32.0
                ],
                "answer_relation_to_question_bing": [
                    0.36231884057971014,
                    0.2753623188405797,
                    1.3623188405797102
                ],
                "cosine_similarity_raw": [
                    0.053580109030008316,
                    0.01779509149491787,
                    0.026695994660258293
                ],
                "result_count_noun_chunks": [
                    1790000.0,
                    156000.0,
                    1970000.0
                ],
                "question_answer_similarity": [
                    2.3608030527830124,
                    0.22602652478963137,
                    1.725619367789477
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    252000.0,
                    270000.0,
                    128000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    3720000.0,
                    18400.0,
                    4550000.0
                ],
                "answer_relation_to_question": [
                    0.8775406504065041,
                    1.446138211382114,
                    0.6763211382113821
                ],
                "word_count_appended": [
                    191.0,
                    149.0,
                    170.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which famous math equation remains unsolved?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "fermat's last theorem"
            ],
            "question": "which famous math equation remains unsolved?",
            "lines": [
                [
                    0.4873555962644886,
                    0.34780982905982905,
                    0.3858528262780158,
                    0.20896068202496634,
                    0.5,
                    0.2228429546865301,
                    0.44359255202628695,
                    0.27009273570324577,
                    0.32946457747859614,
                    0.016666666666666666,
                    0.4,
                    0.34359151578007185,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.10609134860478535,
                    0.25544871794871793,
                    0.2144560503979861,
                    0.18803050539594593,
                    0.08620689655172414,
                    0.2153941651148355,
                    0.18729463307776562,
                    0.48299845440494593,
                    0.3357566498921639,
                    0.0,
                    0.0,
                    0.33893110594983394,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.4065530551307261,
                    0.39674145299145297,
                    0.3996911233239981,
                    0.6030088125790877,
                    0.41379310344827586,
                    0.5617628801986344,
                    0.36911281489594744,
                    0.24690880989180836,
                    0.3347787726292399,
                    0.9833333333333333,
                    0.6,
                    0.31747737827009415,
                    1.0,
                    1.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "reimann hypothesis",
                "poincar\u00e9 conjecture",
                "fermat's last theorem"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "reimann hypothesis": 0.5433490951142239,
                "poincar\u00e9 conjecture": 0.08626331860235377,
                "fermat's last theorem": 0.6917769203799655
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7179575789003594,
                    1.6946555297491697,
                    1.5873868913504707
                ],
                "result_count_important_words": [
                    8100.0,
                    3420.0,
                    6740.0
                ],
                "wikipedia_search": [
                    1.6473228873929808,
                    1.6787832494608197,
                    1.6738938631461995
                ],
                "word_count_appended_bing": [
                    6.0,
                    0.0,
                    9.0
                ],
                "answer_relation_to_question_bing": [
                    1.3912393162393162,
                    1.0217948717948717,
                    1.5869658119658119
                ],
                "cosine_similarity_raw": [
                    0.18747636675834656,
                    0.10419890284538269,
                    0.1942000538110733
                ],
                "result_count_noun_chunks": [
                    69900.0,
                    125000.0,
                    63900.0
                ],
                "question_answer_similarity": [
                    1.8765289783477783,
                    1.6885697767138481,
                    5.415198208764195
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    71800.0,
                    69400.0,
                    181000.0
                ],
                "word_count_appended": [
                    2.0,
                    0.0,
                    118.0
                ],
                "answer_relation_to_question": [
                    2.436777981322443,
                    0.5304567430239268,
                    2.0327652756536305
                ],
                "result_count": [
                    8120.0,
                    1400.0,
                    6720.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What does a rattlesnake typically do when it feels threatened?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "rattles its tail"
            ],
            "lines": [
                [
                    0.4841269841269842,
                    1.0,
                    0.8441031028169786,
                    0.25338692452378664,
                    7.417032253102105e-05,
                    0.04995044598612488,
                    0.00029905583799718033,
                    0.002845269543608324,
                    0.3333333333333333,
                    0.6666666666666666,
                    0.42857142857142855,
                    0.22200920379288755,
                    0,
                    0,
                    1.0
                ],
                [
                    0.29166666666666663,
                    0.0,
                    0.08290045471694302,
                    0.3386160289864902,
                    0.999925829677469,
                    0.08780971258671952,
                    0.9997009441620028,
                    0.9971495097783301,
                    0.3333333333333333,
                    0.2222222222222222,
                    0.2857142857142857,
                    0.7024397889388905,
                    0,
                    0,
                    1.0
                ],
                [
                    0.22420634920634921,
                    0.0,
                    0.07299644246607835,
                    0.40799704648972324,
                    0.0,
                    0.8622398414271556,
                    0.0,
                    5.2206780616666494e-06,
                    0.3333333333333333,
                    0.1111111111111111,
                    0.2857142857142857,
                    0.07555100726822203,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "sends an angry email": 0.19776288647452667,
                "rattles its tail": 0.3571138821268606,
                "eats its feelings": 0.4451232313986127
            },
            "question": "what does a rattlesnake typically do when it feels threatened?",
            "rate_limited": false,
            "answers": [
                "rattles its tail",
                "eats its feelings",
                "sends an angry email"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sends an angry email": 0.1340598123239047,
                "rattles its tail": 0.5727286961733392,
                "eats its feelings": 0.22003344740209582
            },
            "integer_answers": {
                "sends an angry email": 2,
                "rattles its tail": 6,
                "eats its feelings": 4
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.8880368151715502,
                    2.809759155755562,
                    0.3022040290728881
                ],
                "result_count_important_words": [
                    28.0,
                    93600.0,
                    0
                ],
                "wikipedia_search": [
                    1.0,
                    1.0,
                    1.0
                ],
                "answer_relation_to_question": [
                    1.9365079365079367,
                    1.1666666666666665,
                    0.8968253968253969
                ],
                "word_count_appended_bing": [
                    3.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    3.0,
                    0.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.5034751892089844,
                    0.0494469478726387,
                    0.043539583683013916
                ],
                "result_count_noun_chunks": [
                    1090.0,
                    382000.0,
                    2.0
                ],
                "question_answer_similarity": [
                    8.451388319954276,
                    11.294093243777752,
                    13.608206026256084
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    25200.0,
                    44300.0,
                    435000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    27.0,
                    364000.0,
                    0
                ],
                "word_count_appended": [
                    18.0,
                    6.0,
                    3.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these was a name the ancient Greeks gave the planet Venus?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "phosphorus"
            ],
            "lines": [
                [
                    0.16666666666666666,
                    0.0,
                    0.1814252335545699,
                    2.7138827596323836,
                    0.005840565203447974,
                    0.33228346456692914,
                    0.018299656881433472,
                    0.0179405718557279,
                    0.14285714285714288,
                    0.1495176848874598,
                    0.27,
                    0.2106727386126816,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.16666666666666666,
                    0.0,
                    0.10822112267953117,
                    -1.7518389244612156,
                    0.5160299705542221,
                    0.33070866141732286,
                    0.6052230270682425,
                    0.6139039431881891,
                    0.634920634920635,
                    0.5868167202572347,
                    0.46,
                    0.4386959378330034,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.6666666666666666,
                    1.0,
                    0.7103536437658989,
                    0.037956164828831886,
                    0.47812946424233,
                    0.33700787401574805,
                    0.37647731605032403,
                    0.36815548495608297,
                    0.22222222222222224,
                    0.26366559485530544,
                    0.27,
                    0.350631323554315,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "antimony": 0.36294755393983746,
                "flourine": 0.3006704631941745,
                "phosphorus": 0.33638198286598797
            },
            "question": "which of these was a name the ancient greeks gave the planet venus?",
            "rate_limited": false,
            "answers": [
                "flourine",
                "phosphorus",
                "antimony"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "antimony": 0.5455694793689033,
                "flourine": 0.1828073842602098,
                "phosphorus": 0.6580514912745106
            },
            "integer_answers": {
                "antimony": 4,
                "flourine": 0,
                "phosphorus": 10
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.053363693063408,
                    2.193479689165017,
                    1.753156617771575
                ],
                "result_count_important_words": [
                    1920.0,
                    63500.0,
                    39500.0
                ],
                "wikipedia_search": [
                    0.42857142857142855,
                    1.9047619047619047,
                    0.6666666666666666
                ],
                "answer_relation_to_question": [
                    0.5,
                    0.5,
                    2.0
                ],
                "word_count_appended_bing": [
                    27.0,
                    46.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "cosine_similarity_raw": [
                    0.014865988865494728,
                    0.008867641910910606,
                    0.05820639804005623
                ],
                "result_count_noun_chunks": [
                    1920.0,
                    65700.0,
                    39400.0
                ],
                "question_answer_similarity": [
                    -1.692743442952633,
                    1.0926831094548106,
                    -0.023674585390836
                ],
                "word_count_noun_chunks": [
                    0.0,
                    4.0,
                    0.0
                ],
                "result_count_bing": [
                    211000.0,
                    210000.0,
                    214000.0
                ],
                "word_count_raw": [
                    0.0,
                    3.0,
                    0.0
                ],
                "word_count_appended": [
                    93.0,
                    365.0,
                    164.0
                ],
                "result_count": [
                    601.0,
                    53100.0,
                    49200.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "The lunar module for the Apollo 10 moon mission was nicknamed what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "snoopy"
            ],
            "question": "the lunar module for the apollo 10 moon mission was nicknamed what?",
            "lines": [
                [
                    0.2160112359550562,
                    0.3298245614035088,
                    0.09024212013459425,
                    0.5779109256645693,
                    0.44096728307254623,
                    0.6911636045494314,
                    0.47834645669291337,
                    0.10508474576271186,
                    0.05614035087719298,
                    0.3395784543325527,
                    0.3559322033898305,
                    0.35863345880535175,
                    0.10204081632653061,
                    0.029411764705882353,
                    1.0
                ],
                [
                    0.0775749063670412,
                    0.3371929824561404,
                    0.27822641070505744,
                    0.3029948919079589,
                    0.1939307728781413,
                    0.08398950131233596,
                    0.21850393700787402,
                    0.051694915254237285,
                    0.20087719298245615,
                    0.22482435597189696,
                    0.2542372881355932,
                    0.28094435755003644,
                    0.02040816326530612,
                    0.029411764705882353,
                    1.0
                ],
                [
                    0.7064138576779025,
                    0.3329824561403509,
                    0.6315314691603483,
                    0.11909418242747183,
                    0.36510194404931245,
                    0.22484689413823272,
                    0.3031496062992126,
                    0.8432203389830508,
                    0.7429824561403509,
                    0.43559718969555034,
                    0.3898305084745763,
                    0.36042218364461176,
                    0.8775510204081632,
                    0.9411764705882353,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "eagle",
                "beagle",
                "snoopy"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "beagle": -0.037301941334125205,
                "eagle": 0.14811433828640042,
                "snoopy": 1.000001050992092
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.5104342116374623,
                    1.966610502850255,
                    2.5229552855122823
                ],
                "result_count_important_words": [
                    243000.0,
                    111000.0,
                    154000.0
                ],
                "wikipedia_search": [
                    0.2807017543859649,
                    1.0043859649122808,
                    3.7149122807017543
                ],
                "word_count_appended_bing": [
                    42.0,
                    30.0,
                    46.0
                ],
                "answer_relation_to_question_bing": [
                    0.6596491228070176,
                    0.6743859649122808,
                    0.6659649122807018
                ],
                "cosine_similarity_raw": [
                    0.011938029900193214,
                    0.03680626302957535,
                    0.08354459702968597
                ],
                "result_count_noun_chunks": [
                    248000.0,
                    122000.0,
                    1990000.0
                ],
                "question_answer_similarity": [
                    2.332615192979574,
                    1.2229747818782926,
                    0.48069847270380706
                ],
                "word_count_noun_chunks": [
                    5.0,
                    1.0,
                    43.0
                ],
                "result_count_bing": [
                    158000.0,
                    19200.0,
                    51400.0
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    32.0
                ],
                "result_count": [
                    186000.0,
                    81800.0,
                    154000.0
                ],
                "answer_relation_to_question": [
                    1.2960674157303371,
                    0.4654494382022472,
                    4.2384831460674155
                ],
                "word_count_appended": [
                    290.0,
                    192.0,
                    372.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Who was the first U.S. president to enjoy indoor plumbing at the White House?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "john adams"
            ],
            "question": "who was the first u.s. president to enjoy indoor plumbing at the white house?",
            "lines": [
                [
                    0.416554219707685,
                    0.4048936647829864,
                    0.5317907955701634,
                    -0.2639528980728003,
                    0.018354144726944487,
                    0.19907407407407407,
                    0.06203995793901157,
                    0.0698051948051948,
                    0.2907556165099269,
                    0.4074074074074074,
                    0.2413793103448276,
                    0.30612672385300677,
                    0.2,
                    0.5,
                    0.0
                ],
                [
                    0.30997336162187644,
                    0.27483188397310143,
                    0.20169308790054072,
                    0.6047680861431558,
                    0.5761997893786671,
                    0.4367283950617284,
                    0.5520504731861199,
                    0.5681818181818182,
                    0.390168344586448,
                    0.2962962962962963,
                    0.4827586206896552,
                    0.3398228203992759,
                    0.3333333333333333,
                    0.5,
                    0.0
                ],
                [
                    0.2734724186704385,
                    0.3202744512439122,
                    0.2665161165292959,
                    0.6591848119296445,
                    0.4054460658943884,
                    0.36419753086419754,
                    0.38590956887486855,
                    0.362012987012987,
                    0.3190760389036251,
                    0.2962962962962963,
                    0.27586206896551724,
                    0.3540504557477173,
                    0.4666666666666667,
                    0.0,
                    0.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "millard fillmore",
                "andrew jackson",
                "john adams"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "millard fillmore": 0.16300391163500166,
                "john adams": 0.4206043957842009,
                "andrew jackson": 0.2728384695422925
            },
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.1428870669710474,
                    2.3787597427949314,
                    2.4783531902340212
                ],
                "result_count_important_words": [
                    11800.0,
                    105000.0,
                    73400.0
                ],
                "wikipedia_search": [
                    1.1630224660397075,
                    1.560673378345792,
                    1.2763041556145005
                ],
                "word_count_appended_bing": [
                    7.0,
                    14.0,
                    8.0
                ],
                "answer_relation_to_question_bing": [
                    1.6195746591319455,
                    1.0993275358924057,
                    1.2810978049756487
                ],
                "cosine_similarity_raw": [
                    0.21618503332138062,
                    0.08199281990528107,
                    0.10834485292434692
                ],
                "result_count_noun_chunks": [
                    12900.0,
                    105000.0,
                    66900.0
                ],
                "question_answer_similarity": [
                    -1.917939600534737,
                    4.394377443939447,
                    4.7897812984883785
                ],
                "word_count_noun_chunks": [
                    3.0,
                    5.0,
                    7.0
                ],
                "result_count_bing": [
                    129000.0,
                    283000.0,
                    236000.0
                ],
                "word_count_raw": [
                    2.0,
                    2.0,
                    0.0
                ],
                "word_count_appended": [
                    66.0,
                    48.0,
                    48.0
                ],
                "answer_relation_to_question": [
                    2.082771098538425,
                    1.5498668081093823,
                    1.3673620933521924
                ],
                "result_count": [
                    2440.0,
                    76600.0,
                    53900.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Romaine, Iceberg and Butterhead are all varieties of what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "lettuce"
            ],
            "lines": [
                [
                    1.0,
                    1.0,
                    0.9861240000704937,
                    0.33551107167246474,
                    0.9987031403396075,
                    0.968307055263577,
                    0.998104864181933,
                    0.9982284001090216,
                    0.9895833333333334,
                    0.972707423580786,
                    0.7894736842105263,
                    0.6622008405981982,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.0,
                    0.0,
                    0.006726421257933772,
                    0.251214040558982,
                    0.0011271770880046542,
                    0.003663003663003663,
                    0.0016244021297716813,
                    0.0015331152902698284,
                    0.0,
                    0.014192139737991267,
                    0.11403508771929824,
                    0.25692494756061846,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.0,
                    0.0,
                    0.0071495786715725105,
                    0.41327488776855326,
                    0.0001696825723877974,
                    0.028029941073419336,
                    0.0002707336882952802,
                    0.00023848460070863996,
                    0.010416666666666666,
                    0.013100436681222707,
                    0.09649122807017543,
                    0.0808742118411835,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "lettuce": 0.9070674152399958,
                "race cars": 0.04642970368815608,
                "disney dwarfs": 0.046502881071848114
            },
            "question": "romaine, iceberg and butterhead are all varieties of what?",
            "rate_limited": false,
            "answers": [
                "lettuce",
                "disney dwarfs",
                "race cars"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "lettuce": 0.8577516943870164,
                "race cars": 0.1066707556515485,
                "disney dwarfs": 0.16374123926444278
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.6488033623927922,
                    1.0276997902424736,
                    0.32349684736473394
                ],
                "result_count_important_words": [
                    55300.0,
                    90.0,
                    15.0
                ],
                "wikipedia_search": [
                    3.9583333333333335,
                    0.0,
                    0.041666666666666664
                ],
                "word_count_appended_bing": [
                    90.0,
                    13.0,
                    11.0
                ],
                "answer_relation_to_question_bing": [
                    3.0,
                    0.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.7813736200332642,
                    0.005329804494976997,
                    0.005665101110935211
                ],
                "result_count_noun_chunks": [
                    58600.0,
                    90.0,
                    14.0
                ],
                "question_answer_similarity": [
                    3.1902155205607414,
                    2.3886750657111406,
                    3.9296347349882126
                ],
                "word_count_noun_chunks": [
                    676.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    182.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    304000.0,
                    1150.0,
                    8800.0
                ],
                "word_count_appended": [
                    891.0,
                    13.0,
                    12.0
                ],
                "answer_relation_to_question": [
                    4.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    82400.0,
                    93.0,
                    14.0
                ]
            },
            "integer_answers": {
                "lettuce": 13,
                "race cars": 1,
                "disney dwarfs": 0
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which writer has stated that his/her trademark series of books would never be adapted for film?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "jeff kinney"
            ],
            "lines": [
                [
                    0.3182736552847357,
                    0.30799391385767794,
                    0.3017970395698326,
                    0.15618529231671863,
                    0.09667174796747968,
                    0.3128054740957967,
                    0.31494140625,
                    0.09267110841913989,
                    0.1946872342902522,
                    0.27238805970149255,
                    0.20370370370370372,
                    0.3157407023851537,
                    0.5,
                    0,
                    -1.0
                ],
                [
                    0.29432387836655927,
                    0.32568594502407705,
                    0.3751335519982128,
                    0.24287744907648756,
                    0.4731961382113821,
                    0.3299120234604106,
                    0.34033203125,
                    0.4613870381586917,
                    0.30531276570974775,
                    0.3694029850746269,
                    0.33333333333333337,
                    0.34348864591074935,
                    0.04999999999999999,
                    0,
                    -1.0
                ],
                [
                    0.387402466348705,
                    0.36632014111824507,
                    0.32306940843195453,
                    0.6009372586067938,
                    0.4301321138211382,
                    0.3572825024437928,
                    0.3447265625,
                    0.4459418534221684,
                    0.5,
                    0.35820895522388063,
                    0.46296296296296297,
                    0.3407706517040969,
                    0.45,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "jeff kinney": 0.17419155744865567,
                "james patterson": 0.47879087110123336,
                "sue grafton": 0.347017571450111
            },
            "question": "which writer has stated that his/her trademark series of books would never be adapted for film?",
            "rate_limited": false,
            "answers": [
                "james patterson",
                "sue grafton",
                "jeff kinney"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "jeff kinney": 0.8509479433682784,
                "james patterson": 0.1389559861347159,
                "sue grafton": 0.7406694485263279
            },
            "integer_answers": {
                "jeff kinney": 0,
                "james patterson": 11,
                "sue grafton": 2
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.579630166607848,
                    2.1911589572495087,
                    2.229210876142643
                ],
                "result_count_important_words": [
                    75800.0,
                    65400.0,
                    63600.0
                ],
                "wikipedia_search": [
                    3.6637531885169734,
                    2.3362468114830266,
                    0.0
                ],
                "word_count_appended_bing": [
                    16.0,
                    9.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.5360486891385767,
                    1.3945124398073836,
                    1.0694388710540397
                ],
                "cosine_similarity_raw": [
                    0.10679435729980469,
                    0.06727968156337738,
                    0.09533252567052841
                ],
                "result_count_noun_chunks": [
                    26900.0,
                    2550.0,
                    3570.0
                ],
                "question_answer_similarity": [
                    1.9795182928210124,
                    1.4803869109600782,
                    -0.5811477676033974
                ],
                "word_count_noun_chunks": [
                    0.0,
                    9.0,
                    1.0
                ],
                "result_count_bing": [
                    38300.0,
                    34800.0,
                    29200.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    61.0,
                    35.0,
                    38.0
                ],
                "answer_relation_to_question": [
                    2.1807161365831718,
                    2.4681134596012884,
                    1.35117040381554
                ],
                "result_count": [
                    6350.0,
                    422.0,
                    1100.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these is usually found on the ocean floor?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "sea cucumber"
            ],
            "lines": [
                [
                    0.05133928571428571,
                    0.05555555555555555,
                    0.04305281270090645,
                    0.37064613271934366,
                    0.7552708833733653,
                    0.42508917954815695,
                    0.7791458253174298,
                    0.7039372762911198,
                    0.28108974358974365,
                    0.35127478753541075,
                    0.6944444444444444,
                    0.31004563255875844,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.05357142857142857,
                    0.3055555555555556,
                    0.04959235392139037,
                    0.2753523892912719,
                    0.06965572457966374,
                    0.3388822829964328,
                    0.06810311658330127,
                    0.06084881540821544,
                    0.03541666666666667,
                    0.14730878186968838,
                    0.05555555555555555,
                    0.28335719220528505,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.8950892857142858,
                    0.6388888888888888,
                    0.9073548333777032,
                    0.3540014779893844,
                    0.17507339204697092,
                    0.23602853745541022,
                    0.15275105809926895,
                    0.23521390830066474,
                    0.6834935897435898,
                    0.5014164305949008,
                    0.25,
                    0.40659717523595645,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "sweet potato": 0.4017409632790434,
                "cherry tomato": 0.14526665526703794,
                "sea cucumber": 0.45299238145391874
            },
            "question": "which of these is usually found on the ocean floor?",
            "rate_limited": false,
            "answers": [
                "sweet potato",
                "cherry tomato",
                "sea cucumber"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sweet potato": 0.1114017696671088,
                "cherry tomato": 0.12598017450453997,
                "sea cucumber": 0.5979859450826331
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2401825302350338,
                    1.1334287688211402,
                    1.6263887009438258
                ],
                "result_count_important_words": [
                    405000.0,
                    35400.0,
                    79400.0
                ],
                "wikipedia_search": [
                    1.1243589743589744,
                    0.14166666666666666,
                    2.7339743589743586
                ],
                "word_count_appended_bing": [
                    50.0,
                    4.0,
                    18.0
                ],
                "answer_relation_to_question_bing": [
                    0.2222222222222222,
                    1.2222222222222223,
                    2.5555555555555554
                ],
                "cosine_similarity_raw": [
                    0.008864780887961388,
                    0.01021130383014679,
                    0.18682871758937836
                ],
                "result_count_noun_chunks": [
                    413000.0,
                    35700.0,
                    138000.0
                ],
                "question_answer_similarity": [
                    5.230855330824852,
                    3.8859936371445656,
                    4.995952621102333
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    1430000.0,
                    1140000.0,
                    794000.0
                ],
                "word_count_appended": [
                    124.0,
                    52.0,
                    177.0
                ],
                "answer_relation_to_question": [
                    0.20535714285714285,
                    0.21428571428571427,
                    3.5803571428571432
                ],
                "result_count": [
                    283000.0,
                    26100.0,
                    65600.0
                ]
            },
            "integer_answers": {
                "sweet potato": 6,
                "cherry tomato": 0,
                "sea cucumber": 6
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these video games was NOT produced by FromSoftware?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "beyond: two souls"
            ],
            "lines": [
                [
                    0.3038575596690104,
                    0.3200320512820513,
                    0.2297547951764642,
                    0.39252312988145766,
                    0.14295485636114913,
                    0.13852234989860995,
                    0.05246366536689118,
                    0.16152019002375295,
                    0.3576882323610072,
                    0.23831775700934582,
                    0.17543859649122806,
                    0.3198293397021855,
                    0.04375000000000001,
                    0.08333333333333331,
                    -1.0
                ],
                [
                    0.351972697269727,
                    0.4256410256410256,
                    0.4610706491008428,
                    0.25859885981407776,
                    0.44856361149110807,
                    0.36260910453463424,
                    0.4936192839418646,
                    0.4251781472684085,
                    0.4148591373722264,
                    0.44042056074766356,
                    0.38596491228070173,
                    0.3386460703497186,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.3441697430612627,
                    0.25432692307692306,
                    0.30917455572269303,
                    0.3488780103044646,
                    0.4084815321477428,
                    0.4988685455667558,
                    0.4539170506912442,
                    0.41330166270783847,
                    0.22745263026676638,
                    0.3212616822429907,
                    0.4385964912280702,
                    0.34152458994809587,
                    0.45625,
                    0.4166666666666667,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "dark souls": 0.5771448776347876,
                "beyond: two souls": 0.17040799145542876,
                "demon's souls": 0.25244713090978366
            },
            "question": "which of these video games was not produced by fromsoftware?",
            "rate_limited": false,
            "answers": [
                "dark souls",
                "beyond: two souls",
                "demon's souls"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "dark souls": 0.1490037922980894,
                "beyond: two souls": 0.6189866885092548,
                "demon's souls": 0.4276415149469218
            },
            "integer_answers": {
                "dark souls": 11,
                "beyond: two souls": 1,
                "demon's souls": 2
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.441365282382516,
                    1.290831437202251,
                    1.267803280415233
                ],
                "result_count_important_words": [
                    1010000.0,
                    14400.0,
                    104000.0
                ],
                "wikipedia_search": [
                    0.8538706058339567,
                    0.5108451757666417,
                    1.6352842183994016
                ],
                "word_count_appended_bing": [
                    37.0,
                    13.0,
                    7.0
                ],
                "answer_relation_to_question_bing": [
                    1.4397435897435895,
                    0.5948717948717949,
                    1.9653846153846153
                ],
                "cosine_similarity_raw": [
                    0.25428271293640137,
                    0.036629922688007355,
                    0.1795540153980255
                ],
                "result_count_noun_chunks": [
                    171000.0,
                    37800.0,
                    43800.0
                ],
                "question_answer_similarity": [
                    4.964029252529144,
                    11.149583349004388,
                    6.979864381253719
                ],
                "word_count_noun_chunks": [
                    73.0,
                    0.0,
                    7.0
                ],
                "result_count_bing": [
                    49200000.0,
                    18700000.0,
                    154000.0
                ],
                "word_count_raw": [
                    10.0,
                    0.0,
                    2.0
                ],
                "word_count_appended": [
                    224.0,
                    51.0,
                    153.0
                ],
                "answer_relation_to_question": [
                    1.1768546419859378,
                    0.8881638163816381,
                    0.9349815416324241
                ],
                "result_count": [
                    261000.0,
                    37600.0,
                    66900.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Low-income college students may qualify for a federal grant program named for a senator from what state?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "rhode island"
            ],
            "question": "low-income college students may qualify for a federal grant program named for a senator from what state?",
            "lines": [
                [
                    0.25203166245771264,
                    0.08354343655796109,
                    0.7507155154775257,
                    0.4439706046133003,
                    0.3321733628598545,
                    0.11246786632390746,
                    0.2935123042505593,
                    0.1893278604412519,
                    0.39285810631167023,
                    0.07325383304940375,
                    0.06896551724137931,
                    0.3254268191445577,
                    0.05555555555555555,
                    1.0,
                    1.0
                ],
                [
                    0.4028929224098817,
                    0.6404540056636522,
                    0.1172543705617435,
                    0.28065513193257513,
                    0.243910155014236,
                    0.3264781491002571,
                    0.3472035794183445,
                    0.45151359671626473,
                    0.39104281442554734,
                    0.5672913117546848,
                    0.4482758620689655,
                    0.3377132607423609,
                    0.2777777777777778,
                    0.0,
                    1.0
                ],
                [
                    0.3450754151324056,
                    0.2760025577783868,
                    0.1320301139607308,
                    0.2753742634541246,
                    0.42391648212590954,
                    0.5610539845758354,
                    0.3592841163310962,
                    0.35915854284248333,
                    0.2160990792627824,
                    0.3594548551959114,
                    0.4827586206896552,
                    0.33685992011308136,
                    0.6666666666666666,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "rhode island",
                "illinois",
                "washington"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "illinois": 0.22581296064302553,
                "washington": 0.16104361384440555,
                "rhode island": 0.6432329164692799
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.579695010590134,
                    3.7148458681659697,
                    3.705459121243895
                ],
                "result_count_important_words": [
                    656000.0,
                    776000.0,
                    803000.0
                ],
                "wikipedia_search": [
                    4.3214391694283725,
                    4.301470958681021,
                    2.3770898718906066
                ],
                "word_count_appended_bing": [
                    4.0,
                    26.0,
                    28.0
                ],
                "answer_relation_to_question_bing": [
                    0.6683474924636887,
                    5.1236320453092175,
                    2.2080204622270942
                ],
                "cosine_similarity_raw": [
                    0.24335068464279175,
                    0.038008980453014374,
                    0.042798660695552826
                ],
                "result_count_noun_chunks": [
                    738000.0,
                    1760000.0,
                    1400000.0
                ],
                "question_answer_similarity": [
                    6.296387231443077,
                    3.980248626321554,
                    3.9053553957492113
                ],
                "word_count_noun_chunks": [
                    1.0,
                    5.0,
                    12.0
                ],
                "word_count_raw": [
                    4.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    175000.0,
                    508000.0,
                    873000.0
                ],
                "result_count": [
                    1050000.0,
                    771000.0,
                    1340000.0
                ],
                "answer_relation_to_question": [
                    2.772348287034839,
                    4.431822146508699,
                    3.7958295664564616
                ],
                "word_count_appended": [
                    43.0,
                    333.0,
                    211.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Parmesan cheese is named for an Italian province called what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "parma"
            ],
            "question": "parmesan cheese is named for an italian province called what?",
            "lines": [
                [
                    0.14856902356902357,
                    0.1,
                    0.07014943478116492,
                    -0.16691407086727675,
                    0.09756081697428134,
                    0.24597701149425288,
                    0.6587763651310233,
                    0.20837376533436838,
                    0.0,
                    0.3994211287988423,
                    0.29473684210526313,
                    0.4647720572935388,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.07575757575757576,
                    0.0,
                    0.05808792921528038,
                    0.0,
                    1.6260136162380224e-06,
                    0.18850574712643678,
                    1.1261134446684159e-05,
                    1.0214400261488647e-05,
                    0.05555555555555555,
                    0.02894356005788712,
                    0.29473684210526313,
                    0.03876440481755826,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.7756734006734006,
                    0.9,
                    0.8717626360035547,
                    1.1669140708672767,
                    0.9024375570121024,
                    0.5655172413793104,
                    0.34121237373453,
                    0.7916160202653701,
                    0.9444444444444445,
                    0.5716353111432706,
                    0.4105263157894737,
                    0.49646353788890285,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "romana",
                "yomamma",
                "parma"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "romana": 0.1735204498871643,
                "yomamma": 0.12382071998690954,
                "parma": 0.8018631657688969
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.788632343761233,
                    0.23258642890534956,
                    2.978781227333417
                ],
                "result_count_important_words": [
                    117000.0,
                    2.0,
                    60600.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.3333333333333333,
                    5.666666666666667
                ],
                "word_count_appended_bing": [
                    28.0,
                    28.0,
                    39.0
                ],
                "answer_relation_to_question_bing": [
                    0.5,
                    0.0,
                    4.5
                ],
                "cosine_similarity_raw": [
                    0.02295972779393196,
                    0.019012028351426125,
                    0.28532564640045166
                ],
                "result_count_noun_chunks": [
                    40800.0,
                    2.0,
                    155000.0
                ],
                "question_answer_similarity": [
                    -0.10602040775120258,
                    0.0,
                    0.7411999776959419
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    87.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    14.0
                ],
                "result_count_bing": [
                    214000.0,
                    164000.0,
                    492000.0
                ],
                "word_count_appended": [
                    276.0,
                    20.0,
                    395.0
                ],
                "answer_relation_to_question": [
                    0.8914141414141414,
                    0.45454545454545453,
                    4.654040404040404
                ],
                "result_count": [
                    120000.0,
                    2.0,
                    1110000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these is NOT the title of a current TV show?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "chicago police"
            ],
            "lines": [
                [
                    0.2633317737575379,
                    0.3437700352176415,
                    0.3122553638915616,
                    0.37008623613989455,
                    0.4163905456151977,
                    0.3606632888070014,
                    0.4407812220032252,
                    0.27601156069364163,
                    0.36744209692349494,
                    0.19534883720930235,
                    0.14473684210526316,
                    0.3313825088869794,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.3050634227490123,
                    0.31583301902983774,
                    0.26008438694996977,
                    0.3478136998114085,
                    0.23381930638391873,
                    0.2759097190234915,
                    0.22227199426626054,
                    0.252528901734104,
                    0.30074818079327664,
                    0.49534883720930234,
                    0.4605263157894737,
                    0.3283556146155717,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.43160480349344976,
                    0.3403969457525208,
                    0.4276602491584686,
                    0.2821000640486969,
                    0.3497901480008836,
                    0.36342699216950713,
                    0.33694678373051423,
                    0.47145953757225434,
                    0.3318097222832285,
                    0.30930232558139537,
                    0.39473684210526316,
                    0.3402618764974489,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "chicago med": 0.36296661479154313,
                "chicago police": 0.2700839516010614,
                "chicage fire": 0.36694943360739546
            },
            "question": "which of these is not the title of a current tv show?",
            "rate_limited": false,
            "answers": [
                "chicago med",
                "chicage fire",
                "chicago police"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "chicago med": 0.10321615604592634,
                "chicago police": 0.3124954578553677,
                "chicage fire": 0.2587560772791508
            },
            "integer_answers": {
                "chicago med": 3,
                "chicago police": 1,
                "chicage fire": 8
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.0117049466781236,
                    1.0298663123065699,
                    0.9584287410153065
                ],
                "result_count_important_words": [
                    66100.0,
                    310000.0,
                    182000.0
                ],
                "wikipedia_search": [
                    0.7953474184590307,
                    1.1955109152403403,
                    1.0091416663006292
                ],
                "answer_relation_to_question": [
                    1.4200093574547723,
                    1.1696194635059263,
                    0.41037117903930137
                ],
                "word_count_appended_bing": [
                    27.0,
                    3.0,
                    8.0
                ],
                "answer_relation_to_question_bing": [
                    0.9373797886941511,
                    1.1050018858209738,
                    0.9576183254848751
                ],
                "cosine_similarity_raw": [
                    0.0888412669301033,
                    0.11352871358394623,
                    0.034231364727020264
                ],
                "result_count_noun_chunks": [
                    1240000.0,
                    1370000.0,
                    158000.0
                ],
                "question_answer_similarity": [
                    3.1205909717828035,
                    3.655587986111641,
                    5.234061062335968
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    605000.0,
                    973000.0,
                    593000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    131.0,
                    2.0,
                    82.0
                ],
                "result_count": [
                    75700.0,
                    241000.0,
                    136000.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these things is NOT found inside an atom?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "wonton"
            ],
            "lines": [
                [
                    0.3517464833254307,
                    0.23653846153846153,
                    0.3547603394099659,
                    0.22103760256197258,
                    0.4339752407152682,
                    0.3331189710610932,
                    0.2675276752767528,
                    0.2536295644522657,
                    0.32073643410852715,
                    0.313126709206928,
                    0.37371134020618557,
                    0.3021292706021742,
                    0.21774193548387094,
                    0.17857142857142855,
                    -1.0
                ],
                [
                    0.3070175438596491,
                    0.4,
                    0.4664008564073633,
                    0.4966159294720301,
                    0.1389270976616231,
                    0.3334405144694534,
                    0.48093480934809346,
                    0.4564452265728113,
                    0.29761904761904767,
                    0.37146763901549684,
                    0.4020618556701031,
                    0.39381296909621694,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.3412359728149202,
                    0.36346153846153845,
                    0.17883880418267079,
                    0.28234646796599727,
                    0.4270976616231087,
                    0.3334405144694534,
                    0.2515375153751538,
                    0.289925208974923,
                    0.38164451827242524,
                    0.3154056517775752,
                    0.22422680412371132,
                    0.3040577603016089,
                    0.282258064516129,
                    0.3214285714285714,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "wonton": 0.20789378725830168,
                "neutron": 0.3861564208160305,
                "proton": 0.4059497919256678
            },
            "question": "which of these things is not found inside an atom?",
            "rate_limited": false,
            "answers": [
                "proton",
                "wonton",
                "neutron"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "wonton": 0.4535024340847416,
                "neutron": 0.217686292498845,
                "proton": 0.17147661632941785
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1872243763869545,
                    0.6371221854226985,
                    1.1756534381903467
                ],
                "result_count_important_words": [
                    1890000.0,
                    155000.0,
                    2020000.0
                ],
                "wikipedia_search": [
                    1.0755813953488373,
                    1.2142857142857142,
                    0.7101328903654485
                ],
                "word_count_appended_bing": [
                    49.0,
                    38.0,
                    107.0
                ],
                "answer_relation_to_question_bing": [
                    1.0538461538461539,
                    0.4,
                    0.5461538461538462
                ],
                "cosine_similarity_raw": [
                    0.09750467538833618,
                    0.022556329146027565,
                    0.21560721099376678
                ],
                "result_count_noun_chunks": [
                    224000.0,
                    39600.0,
                    191000.0
                ],
                "question_answer_similarity": [
                    1.5770087577402592,
                    0.019130567088723183,
                    1.2304221978411078
                ],
                "word_count_noun_chunks": [
                    35.0,
                    0.0,
                    27.0
                ],
                "word_count_raw": [
                    18.0,
                    0.0,
                    10.0
                ],
                "result_count_bing": [
                    51900000.0,
                    51800000.0,
                    51800000.0
                ],
                "result_count": [
                    192000.0,
                    1050000.0,
                    212000.0
                ],
                "answer_relation_to_question": [
                    0.8895211000474158,
                    1.1578947368421053,
                    0.9525841631104789
                ],
                "word_count_appended": [
                    410.0,
                    282.0,
                    405.0
                ]
            },
            "integer_answers": {
                "wonton": 3,
                "neutron": 3,
                "proton": 8
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these words is NOT a synonym for a fight?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "galumph",
                "top"
            ],
            "question": "which of these words is not a synonym for a fight?",
            "lines": [
                [
                    0.2634803921568628,
                    0.2777777777777778,
                    0.28277664541933856,
                    0.5,
                    0.3192307692307692,
                    0.2966738197424893,
                    0.3163841807909604,
                    0.3418604651162791,
                    0.26973684210526316,
                    0.3672839506172839,
                    0.4065934065934066,
                    0.3555500391573776,
                    0.5,
                    0,
                    -1.0
                ],
                [
                    0.32704248366013067,
                    0.31481481481481477,
                    0.334811944244721,
                    0.6638234338110539,
                    0.32307692307692304,
                    0.29533261802575106,
                    0.3587570621468926,
                    0.32093023255813957,
                    0.5,
                    0.312962962962963,
                    0.30219780219780223,
                    0.3141116966240143,
                    0.16666666666666669,
                    0,
                    -1.0
                ],
                [
                    0.4094771241830065,
                    0.4074074074074074,
                    0.38241141033594045,
                    -0.1638234338110539,
                    0.3576923076923077,
                    0.40799356223175964,
                    0.3248587570621469,
                    0.3372093023255814,
                    0.23026315789473684,
                    0.3197530864197531,
                    0.2912087912087912,
                    0.3303382642186081,
                    0.33333333333333337,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "galumph",
                "fracas",
                "donnybrook"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "donnybrook": 0.3368467725373118,
                "galumph": 0.5343952626244127,
                "fracas": 0.22662414650590765
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.8666997650557345,
                    1.1153298202559143,
                    1.0179704146883513
                ],
                "result_count_important_words": [
                    65.0,
                    50.0,
                    62.0
                ],
                "wikipedia_search": [
                    0.9210526315789473,
                    0.0,
                    1.0789473684210527
                ],
                "word_count_appended_bing": [
                    34.0,
                    72.0,
                    76.0
                ],
                "answer_relation_to_question_bing": [
                    1.3333333333333333,
                    1.1111111111111112,
                    0.5555555555555556
                ],
                "cosine_similarity_raw": [
                    0.07085977494716644,
                    0.05388549715280533,
                    0.03835821896791458
                ],
                "result_count_noun_chunks": [
                    68.0,
                    77.0,
                    70.0
                ],
                "question_answer_similarity": [
                    0.0,
                    0.17303355364128947,
                    -0.7011434510350227
                ],
                "word_count_noun_chunks": [
                    0.0,
                    2.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    75800000.0,
                    76300000.0,
                    34300000.0
                ],
                "result_count": [
                    94.0,
                    92.0,
                    74.0
                ],
                "answer_relation_to_question": [
                    1.4191176470588234,
                    1.0377450980392158,
                    0.5431372549019609
                ],
                "word_count_appended": [
                    215.0,
                    303.0,
                    292.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these universities is named after a former owner of the Staten Island Ferry?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "vanderbilt"
            ],
            "lines": [
                [
                    0.09411764705882353,
                    0.22916666666666666,
                    0.08325810743630245,
                    0.34088983427100966,
                    0.1580226904376013,
                    0.3154875717017208,
                    0.39796747967479673,
                    0.1149843912591051,
                    0.5925925925925926,
                    0.05,
                    0.045454545454545456,
                    0.2701491914179058,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.17294117647058824,
                    0.16666666666666669,
                    0.06116102621641002,
                    0.6633045211824571,
                    0.1774716369529984,
                    0.3422562141491396,
                    0.012601626016260163,
                    0.4172736732570239,
                    0.07407407407407407,
                    0.2980769230769231,
                    0.17532467532467533,
                    0.29761337371863744,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.7329411764705883,
                    0.6041666666666667,
                    0.8555808663472875,
                    -0.00419435545346676,
                    0.6645056726094003,
                    0.3422562141491396,
                    0.5894308943089431,
                    0.46774193548387094,
                    0.3333333333333333,
                    0.6519230769230769,
                    0.7792207792207793,
                    0.4322374348634567,
                    1.0,
                    1.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "carnegie mellon": 0.1922921941407907,
                "stetson": 0.20419754193613243,
                "vanderbilt": 0.6035102639230769
            },
            "question": "which of these universities is named after a former owner of the staten island ferry?",
            "rate_limited": false,
            "answers": [
                "carnegie mellon",
                "stetson",
                "vanderbilt"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "carnegie mellon": 0.13856824379534213,
                "stetson": 0.10498133419932126,
                "vanderbilt": 0.7906221929744528
            },
            "integer_answers": {
                "carnegie mellon": 1,
                "stetson": 1,
                "vanderbilt": 12
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6208951485074348,
                    1.7856802423118245,
                    2.5934246091807402
                ],
                "result_count_important_words": [
                    979000.0,
                    31000.0,
                    1450000.0
                ],
                "wikipedia_search": [
                    1.7777777777777777,
                    0.2222222222222222,
                    1.0
                ],
                "word_count_appended_bing": [
                    7.0,
                    27.0,
                    120.0
                ],
                "answer_relation_to_question_bing": [
                    0.9166666666666665,
                    0.6666666666666666,
                    2.4166666666666665
                ],
                "cosine_similarity_raw": [
                    0.006080477498471737,
                    0.004466691054403782,
                    0.062484487891197205
                ],
                "result_count_noun_chunks": [
                    22100.0,
                    80200.0,
                    89900.0
                ],
                "question_answer_similarity": [
                    -0.8124950705096126,
                    -1.580955486278981,
                    0.00999705120921135
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    2.0
                ],
                "result_count_bing": [
                    165000.0,
                    179000.0,
                    179000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    5.0
                ],
                "result_count": [
                    19500.0,
                    21900.0,
                    82000.0
                ],
                "answer_relation_to_question": [
                    0.47058823529411764,
                    0.8647058823529412,
                    3.6647058823529415
                ],
                "word_count_appended": [
                    26.0,
                    155.0,
                    339.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "By definition, an Anglophile would be most interested in which of these things?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "geometry"
            ],
            "lines": [
                [
                    0.3846153846153846,
                    1.0,
                    0.4353186990502371,
                    0.3023327550698545,
                    0.41313620105961785,
                    0.9471673810211197,
                    0.09572935319514939,
                    0.41552019037703575,
                    0.26666666666666666,
                    0.6735537190082644,
                    0.7575757575757576,
                    0.40635178845756753,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.5384615384615384,
                    0.0,
                    0.40309302202234426,
                    0.011893953872856482,
                    0.5862326186332077,
                    0.006418086122276755,
                    0.903840593619177,
                    0.5837733323417863,
                    0.7333333333333334,
                    0.2768595041322314,
                    0.18181818181818182,
                    0.4200617336577158,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.07692307692307693,
                    0.0,
                    0.1615882789274186,
                    0.685773291057289,
                    0.0006311803071744162,
                    0.046414532856603526,
                    0.00043005318567364064,
                    0.0007064772811779579,
                    0.0,
                    0.049586776859504134,
                    0.06060606060606061,
                    0.17358647788471665,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "geometry": 0.5081639913413879,
                "downton abbey": 0.3871488248345541,
                "trout fishing": 0.10468718382405795
            },
            "question": "by definition, an anglophile would be most interested in which of these things?",
            "rate_limited": false,
            "answers": [
                "geometry",
                "downton abbey",
                "trout fishing"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "geometry": 0.7970866997107331,
                "downton abbey": 0.2861712414301517,
                "trout fishing": 0.12191230027485045
            },
            "integer_answers": {
                "geometry": 5,
                "downton abbey": 6,
                "trout fishing": 1
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6254071538302701,
                    1.6802469346308633,
                    0.6943459115388666
                ],
                "result_count_important_words": [
                    197000.0,
                    1860000.0,
                    885.0
                ],
                "wikipedia_search": [
                    0.8,
                    2.2,
                    0.0
                ],
                "word_count_appended_bing": [
                    50.0,
                    12.0,
                    4.0
                ],
                "answer_relation_to_question_bing": [
                    1.0,
                    0.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.05387365072965622,
                    0.04988550394773483,
                    0.019997648894786835
                ],
                "result_count_noun_chunks": [
                    44700.0,
                    62800.0,
                    76.0
                ],
                "question_answer_similarity": [
                    2.0908243283629417,
                    0.08225429663434625,
                    4.74256082624197
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    8530000.0,
                    57800.0,
                    418000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    43200.0,
                    61300.0,
                    66.0
                ],
                "answer_relation_to_question": [
                    1.1538461538461537,
                    1.6153846153846154,
                    0.23076923076923078
                ],
                "word_count_appended": [
                    163.0,
                    67.0,
                    12.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What Japanese word means \u201cempty orchestra\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "karaoke"
            ],
            "lines": [
                [
                    0.27706766917293235,
                    0.46266166040884443,
                    0.019554620348090957,
                    0.44658484516116204,
                    0.04940970703979012,
                    0.23235613463626492,
                    0.3717884130982368,
                    0.15374677002583978,
                    0.15625,
                    0.19955898566703417,
                    0.3076923076923077,
                    0.29997514686541915,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.3902255639097744,
                    0.1118064246975386,
                    0.11970479896181072,
                    0.29825212549289426,
                    0.05421950153038916,
                    0.6514657980456026,
                    0.29974811083123426,
                    0.3695090439276486,
                    0.4270833333333333,
                    0.22601984564498345,
                    0.24786324786324787,
                    0.31240709193252547,
                    0.017605633802816902,
                    0.025,
                    1.0
                ],
                [
                    0.33270676691729323,
                    0.425531914893617,
                    0.8607405806900983,
                    0.2551630293459437,
                    0.8963707914298207,
                    0.11617806731813246,
                    0.32846347607052895,
                    0.47674418604651164,
                    0.4166666666666667,
                    0.5744211686879823,
                    0.4444444444444444,
                    0.3876177612020554,
                    0.9823943661971831,
                    0.975,
                    1.0
                ]
            ],
            "fraction_answers": {
                "sake": 0.2126175900082802,
                "anime": 0.2536364657124142,
                "karaoke": 0.5337459442793056
            },
            "question": "what japanese word means \u201cempty orchestra\u201d?",
            "rate_limited": false,
            "answers": [
                "sake",
                "anime",
                "karaoke"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sake": 0.04758609181201683,
                "anime": 0.06971040142761664,
                "karaoke": 0.767716294146466
            },
            "integer_answers": {
                "sake": 3,
                "anime": 3,
                "karaoke": 8
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1999005874616766,
                    1.2496283677301019,
                    1.5504710448082215
                ],
                "result_count_important_words": [
                    738000.0,
                    595000.0,
                    652000.0
                ],
                "wikipedia_search": [
                    0.625,
                    1.7083333333333333,
                    1.6666666666666667
                ],
                "word_count_appended_bing": [
                    36.0,
                    29.0,
                    52.0
                ],
                "answer_relation_to_question_bing": [
                    1.3879849812265332,
                    0.3354192740926158,
                    1.2765957446808511
                ],
                "cosine_similarity_raw": [
                    0.010764405131340027,
                    0.0658949613571167,
                    0.4738194942474365
                ],
                "result_count_noun_chunks": [
                    11900.0,
                    28600.0,
                    36900.0
                ],
                "question_answer_similarity": [
                    2.953303724527359,
                    1.9723667800426483,
                    1.6874149069190025
                ],
                "word_count_noun_chunks": [
                    0.0,
                    5.0,
                    279.0
                ],
                "result_count_bing": [
                    4280000.0,
                    12000000.0,
                    2140000.0
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    39.0
                ],
                "result_count": [
                    1130.0,
                    1240.0,
                    20500.0
                ],
                "answer_relation_to_question": [
                    1.1082706766917294,
                    1.5609022556390977,
                    1.330827067669173
                ],
                "word_count_appended": [
                    181.0,
                    205.0,
                    521.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which company named an iconic sports car after an untranslatable local expression?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "porsche"
            ],
            "question": "which company named an iconic sports car after an untranslatable local expression?",
            "lines": [
                [
                    0.46550559737372926,
                    0.32325485336637755,
                    0.5601214348190032,
                    0.4181473416329346,
                    0.5004433060782189,
                    0.3645224171539961,
                    0.5022591967731301,
                    0.5013807555964669,
                    0.3363833434160113,
                    0.3848797250859107,
                    0.38181818181818183,
                    0.37012637293154094,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.24524988355443617,
                    0.3958364312267658,
                    0.3020355906730387,
                    0.41247114506604515,
                    0.49847305684169047,
                    0.3333333333333333,
                    0.49644152267923286,
                    0.49746371844336945,
                    0.23772097869194056,
                    0.38831615120274915,
                    0.26666666666666666,
                    0.40935090964582765,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.28924451907183457,
                    0.28090871540685663,
                    0.1378429745079581,
                    0.16938151330102022,
                    0.0010836370800906315,
                    0.30214424951267055,
                    0.0012992805476370547,
                    0.001155525960163732,
                    0.42589567789204813,
                    0.2268041237113402,
                    0.3515151515151515,
                    0.22052271742263144,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "porsche",
                "ferrari",
                "lamborghini"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "porsche": 0.774688303286301,
                "ferrari": 0.25581529317364143,
                "lamborghini": 0.09977118142716283
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.961010983452327,
                    3.274807277166621,
                    1.7641817393810513
                ],
                "result_count_important_words": [
                    25900.0,
                    25600.0,
                    67.0
                ],
                "wikipedia_search": [
                    2.0183000604960677,
                    1.4263258721516434,
                    2.5553740673522887
                ],
                "word_count_appended_bing": [
                    63.0,
                    44.0,
                    58.0
                ],
                "answer_relation_to_question_bing": [
                    1.6162742668318877,
                    1.979182156133829,
                    1.4045435770342831
                ],
                "cosine_similarity_raw": [
                    0.1252407282590866,
                    0.06753385066986084,
                    0.030821092426776886
                ],
                "result_count_noun_chunks": [
                    25600.0,
                    25400.0,
                    59.0
                ],
                "question_answer_similarity": [
                    1.267041533254087,
                    1.2498419098556042,
                    0.5132483001798391
                ],
                "word_count_noun_chunks": [
                    5.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    7.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    187000.0,
                    171000.0,
                    155000.0
                ],
                "word_count_appended": [
                    112.0,
                    113.0,
                    66.0
                ],
                "answer_relation_to_question": [
                    3.2585391816161047,
                    1.7167491848810532,
                    2.024711633502842
                ],
                "result_count": [
                    25400.0,
                    25300.0,
                    55.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Table tennis is also known as what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ping-pong"
            ],
            "lines": [
                [
                    0.6816993464052287,
                    0.8666666666666667,
                    0.9367748736424744,
                    0.7563207514471897,
                    0.9877157055138339,
                    0.4334862385321101,
                    0.9885883331654598,
                    0.9886928648715577,
                    0.20535714285714285,
                    0.494195688225539,
                    0.43157894736842106,
                    0.6217619681172665,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.14901960784313725,
                    0.09824561403508773,
                    0.020094665425424006,
                    0.3266652415397998,
                    0.012206579264846557,
                    0.13302752293577982,
                    0.011191566035835394,
                    0.011088144278933357,
                    0.6785714285714286,
                    0.2902155887230514,
                    0.28421052631578947,
                    0.2122935390297178,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.16928104575163397,
                    0.03508771929824561,
                    0.04313046093210163,
                    -0.08298599298698954,
                    7.771522131952308e-05,
                    0.4334862385321101,
                    0.00022010079870476277,
                    0.0002189908495089338,
                    0.11607142857142858,
                    0.2155887230514096,
                    0.28421052631578947,
                    0.1659444928530156,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "hunky-dory": 0.1590592874284879,
                "argle-bargle": 0.09859510351344847,
                "ping-pong": 0.7423456090580637
            },
            "question": "table tennis is also known as what?",
            "rate_limited": false,
            "answers": [
                "ping-pong",
                "hunky-dory",
                "argle-bargle"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "hunky-dory": 0.1331820401012603,
                "argle-bargle": 0.06903399818773011,
                "ping-pong": 0.6230036992319048
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.8652859043517998,
                    0.6368806170891534,
                    0.4978334785590469
                ],
                "result_count_important_words": [
                    10600000.0,
                    120000.0,
                    2360.0
                ],
                "wikipedia_search": [
                    0.4107142857142857,
                    1.3571428571428572,
                    0.23214285714285715
                ],
                "word_count_appended_bing": [
                    41.0,
                    27.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    2.6,
                    0.2947368421052632,
                    0.10526315789473684
                ],
                "cosine_similarity_raw": [
                    0.501369833946228,
                    0.010754834860563278,
                    0.02308378741145134
                ],
                "result_count_noun_chunks": [
                    10700000.0,
                    120000.0,
                    2370.0
                ],
                "question_answer_similarity": [
                    3.905524665489793,
                    1.6868493370711803,
                    -0.4285269733518362
                ],
                "word_count_noun_chunks": [
                    24.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    36.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    3780000.0,
                    1160000.0,
                    3780000.0
                ],
                "word_count_appended": [
                    298.0,
                    175.0,
                    130.0
                ],
                "answer_relation_to_question": [
                    2.045098039215686,
                    0.44705882352941173,
                    0.5078431372549019
                ],
                "result_count": [
                    9710000.0,
                    120000.0,
                    764.0
                ]
            },
            "integer_answers": {
                "hunky-dory": 1,
                "argle-bargle": 0,
                "ping-pong": 13
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these sharks is NOT a Lamniforme?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "hammerhead shark"
            ],
            "lines": [
                [
                    0.3455703883495146,
                    0.3695652173913043,
                    0.3603311769488582,
                    0.39621993430121516,
                    0.49098135126872516,
                    0.47912176724137934,
                    0.47888829502939606,
                    0.4821342054263566,
                    0.4612794612794613,
                    0.28661616161616166,
                    0.3805970149253731,
                    0.3304070930353294,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.247876213592233,
                    0.32608695652173914,
                    0.17910223178531676,
                    0.1984494618644107,
                    0.03531030265973706,
                    0.10668103448275862,
                    0.10288615713522181,
                    0.07000968992248063,
                    0.14814814814814814,
                    0.3017676767676768,
                    0.26865671641791045,
                    0.33262894379222485,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.4065533980582524,
                    0.30434782608695654,
                    0.46056659126582505,
                    0.40533060383437414,
                    0.4737083460715378,
                    0.4141971982758621,
                    0.4182255478353821,
                    0.44785610465116277,
                    0.39057239057239057,
                    0.4116161616161616,
                    0.35074626865671643,
                    0.33696396317244576,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "goblin shark": 0.18971465553115419,
                "great white shark": 0.6137327444850237,
                "hammerhead shark": 0.1965525999838221
            },
            "question": "which of these sharks is not a lamniforme?",
            "rate_limited": false,
            "answers": [
                "goblin shark",
                "great white shark",
                "hammerhead shark"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "goblin shark": 0.11986154376177419,
                "great white shark": 0.15369216210848952,
                "hammerhead shark": 0.29867154909798527
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.6783716278586824,
                    0.6694842248311006,
                    0.652144147310217
                ],
                "result_count_important_words": [
                    39500.0,
                    743000.0,
                    153000.0
                ],
                "wikipedia_search": [
                    0.07744107744107744,
                    0.7037037037037037,
                    0.21885521885521886
                ],
                "word_count_appended_bing": [
                    16.0,
                    31.0,
                    20.0
                ],
                "answer_relation_to_question_bing": [
                    0.2608695652173913,
                    0.34782608695652173,
                    0.391304347826087
                ],
                "cosine_similarity_raw": [
                    0.09988401085138321,
                    0.22948969900608063,
                    0.02820076048374176
                ],
                "result_count_noun_chunks": [
                    29500.0,
                    710000.0,
                    86100.0
                ],
                "question_answer_similarity": [
                    2.786467866972089,
                    8.096553795039654,
                    2.5418487512506545
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    155000.0,
                    2920000.0,
                    637000.0
                ],
                "word_count_appended": [
                    169.0,
                    157.0,
                    70.0
                ],
                "answer_relation_to_question": [
                    0.6177184466019418,
                    1.008495145631068,
                    0.3737864077669903
                ],
                "result_count": [
                    29500.0,
                    1520000.0,
                    86000.0
                ]
            },
            "integer_answers": {
                "goblin shark": 2,
                "great white shark": 9,
                "hammerhead shark": 1
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which golfer shares his nickname with the symbol of a major California university?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "jack nicklaus",
                "bottom"
            ],
            "question": "which golfer shares his nickname with the symbol of a major california university?",
            "lines": [
                [
                    0.4631494618836391,
                    0.2332689832689833,
                    0.33125503876168183,
                    0.45996617995623845,
                    0.4635036496350365,
                    0.3486590038314176,
                    0.030637254901960783,
                    0.04533869885982562,
                    0.18711908449785253,
                    0.4230769230769231,
                    0.2857142857142857,
                    0.32107841153927075,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.17691096994894465,
                    0.2404118404118404,
                    0.30113145963404114,
                    0.22999390956809726,
                    0.23722627737226276,
                    0.32567049808429116,
                    0.031862745098039214,
                    0.02910798122065728,
                    0.09765677747329124,
                    0.27884615384615385,
                    0.2,
                    0.29864131508776603,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.35993956816741624,
                    0.5263191763191764,
                    0.367613501604277,
                    0.31003991047566426,
                    0.29927007299270075,
                    0.32567049808429116,
                    0.9375,
                    0.9255533199195171,
                    0.7152241380288562,
                    0.2980769230769231,
                    0.5142857142857142,
                    0.3802802733729632,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "greg norman",
                "byron nelson",
                "jack nicklaus"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "greg norman": 0.24237436703653634,
                "jack nicklaus": 0.4048886197653066,
                "byron nelson": 0.0796165087772863
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.2475488807748953,
                    2.0904892056143622,
                    2.6619619136107424
                ],
                "result_count_important_words": [
                    150000.0,
                    156000.0,
                    4590000.0
                ],
                "wikipedia_search": [
                    0.9355954224892626,
                    0.48828388736645617,
                    3.5761206901442812
                ],
                "word_count_appended_bing": [
                    10.0,
                    7.0,
                    18.0
                ],
                "answer_relation_to_question_bing": [
                    1.1663449163449164,
                    1.202059202059202,
                    2.6315958815958815
                ],
                "cosine_similarity_raw": [
                    0.03985992446541786,
                    0.036235153675079346,
                    0.04423493891954422
                ],
                "result_count_noun_chunks": [
                    6760.0,
                    4340.0,
                    138000.0
                ],
                "question_answer_similarity": [
                    2.2196512180380523,
                    1.1098778209361626,
                    1.4961544889956713
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    182000.0,
                    170000.0,
                    170000.0
                ],
                "word_count_appended": [
                    44.0,
                    29.0,
                    31.0
                ],
                "answer_relation_to_question": [
                    2.7788967713018344,
                    1.061465819693668,
                    2.1596374090044974
                ],
                "result_count": [
                    25400.0,
                    13000.0,
                    16400.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Traditionally, an \u201camuse-bouche\u201d arrives right before what part of the meal?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "dessert"
            ],
            "question": "traditionally, an \u201camuse-bouche\u201d arrives right before what part of the meal?",
            "lines": [
                [
                    0.19495005549389566,
                    0.1062874251497006,
                    0.41297466660303817,
                    0.314079863600959,
                    0.3494132985658409,
                    0.2937937937937938,
                    0.3418803418803419,
                    0.9998527508307221,
                    0.46359268650740687,
                    0.515970515970516,
                    0.5571428571428572,
                    0.3297285339950779,
                    0.6666666666666666,
                    0.0,
                    1.0
                ],
                [
                    0.6619866814650388,
                    0.7139649272882806,
                    0.2735294802261939,
                    0.2233335916676549,
                    0.14471968709256844,
                    0.26426426426426425,
                    0.14846470402025957,
                    7.310972740368823e-05,
                    0.170470216646648,
                    0.0,
                    0.0,
                    0.315628180335398,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.14306326304106548,
                    0.17974764756201883,
                    0.31349585317076795,
                    0.4625865447313861,
                    0.5058670143415906,
                    0.4419419419419419,
                    0.5096549540993985,
                    7.413944187416271e-05,
                    0.3659370968459451,
                    0.48402948402948404,
                    0.44285714285714284,
                    0.3546432856695241,
                    0.3333333333333333,
                    1.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "appetizers",
                "entr\u00e9e",
                "dessert"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "dessert": 0.5008476108277418,
                "appetizers": 0.3457898325365812,
                "entr\u00e9e": 0.10959829621476558
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9783712039704673,
                    1.8937690820123882,
                    2.1278597140171445
                ],
                "result_count_important_words": [
                    108000.0,
                    46900.0,
                    161000.0
                ],
                "wikipedia_search": [
                    2.3179634325370344,
                    0.8523510832332399,
                    1.8296854842297257
                ],
                "word_count_appended_bing": [
                    39.0,
                    0.0,
                    31.0
                ],
                "answer_relation_to_question_bing": [
                    0.4251497005988024,
                    2.8558597091531226,
                    0.7189905902480753
                ],
                "cosine_similarity_raw": [
                    0.12385908514261246,
                    0.08203677833080292,
                    0.09402346611022949
                ],
                "result_count_noun_chunks": [
                    971000.0,
                    71.0,
                    72.0
                ],
                "question_answer_similarity": [
                    2.7654968285933137,
                    1.966469076985959,
                    4.073109328746796
                ],
                "word_count_noun_chunks": [
                    18.0,
                    0.0,
                    9.0
                ],
                "result_count_bing": [
                    58700.0,
                    52800.0,
                    88300.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    2.0
                ],
                "result_count": [
                    53600.0,
                    22200.0,
                    77600.0
                ],
                "answer_relation_to_question": [
                    0.7798002219755826,
                    2.6479467258601552,
                    0.5722530521642619
                ],
                "word_count_appended": [
                    210.0,
                    0.0,
                    197.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What Ivy League school is located in a city with the same name?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "cornell"
            ],
            "lines": [
                [
                    0.2826873758291473,
                    0.4459287054409006,
                    0.47222055885406344,
                    0.39747811540313027,
                    0.4258804258804259,
                    0.2925311203319502,
                    0.2671957671957672,
                    0.3447098976109215,
                    0.6660353130016051,
                    0.35403726708074534,
                    0.34394904458598724,
                    0.34544495837007483,
                    0.42424242424242425,
                    0.5263157894736842,
                    1.0
                ],
                [
                    0.3985636373795022,
                    0.31129455909943715,
                    0.12675445363353802,
                    -0.40157997214475094,
                    0.13185913185913187,
                    0.25311203319502074,
                    0.3201058201058201,
                    0.2525597269624573,
                    0.05523809523809524,
                    0.3007985803016859,
                    0.34394904458598724,
                    0.32988130656591746,
                    0.30303030303030304,
                    0.15789473684210525,
                    1.0
                ],
                [
                    0.31874898679135055,
                    0.2427767354596623,
                    0.4010249875123985,
                    1.0041018567416207,
                    0.44226044226044225,
                    0.45435684647302904,
                    0.4126984126984127,
                    0.40273037542662116,
                    0.27872659176029957,
                    0.3451641526175688,
                    0.31210191082802546,
                    0.3246737350640077,
                    0.2727272727272727,
                    0.3157894736842105,
                    1.0
                ]
            ],
            "fraction_answers": {
                "cornell": 0.399189768807202,
                "dartmouth": 0.20596153261816072,
                "princeton": 0.3948486985746373
            },
            "question": "what ivy league school is located in a city with the same name?",
            "rate_limited": false,
            "answers": [
                "cornell",
                "dartmouth",
                "princeton"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "cornell": 0.460608549387696,
                "dartmouth": 0.1691569715695715,
                "princeton": 0.21285173058259743
            },
            "integer_answers": {
                "cornell": 8,
                "dartmouth": 1,
                "princeton": 5
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7272247918503743,
                    1.6494065328295873,
                    1.6233686753200385
                ],
                "result_count_important_words": [
                    1010000.0,
                    1210000.0,
                    1560000.0
                ],
                "wikipedia_search": [
                    3.3301765650080255,
                    0.2761904761904762,
                    1.393632958801498
                ],
                "word_count_appended_bing": [
                    54.0,
                    54.0,
                    49.0
                ],
                "answer_relation_to_question_bing": [
                    2.229643527204503,
                    1.5564727954971858,
                    1.2138836772983115
                ],
                "cosine_similarity_raw": [
                    0.1520576924085617,
                    0.040815651416778564,
                    0.12913231551647186
                ],
                "result_count_noun_chunks": [
                    2020000.0,
                    1480000.0,
                    2360000.0
                ],
                "question_answer_similarity": [
                    0.3621192193822935,
                    -0.36585618276149035,
                    0.9147788682021201
                ],
                "word_count_noun_chunks": [
                    14.0,
                    10.0,
                    9.0
                ],
                "result_count_bing": [
                    1410000.0,
                    1220000.0,
                    2190000.0
                ],
                "word_count_raw": [
                    10.0,
                    3.0,
                    6.0
                ],
                "result_count": [
                    1040000.0,
                    322000.0,
                    1080000.0
                ],
                "answer_relation_to_question": [
                    1.4134368791457366,
                    1.9928181868975108,
                    1.5937449339567527
                ],
                "word_count_appended": [
                    399.0,
                    339.0,
                    389.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Who is NOT considered an official member of the Eagles?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "j.d. souther"
            ],
            "lines": [
                [
                    0.36151646919959024,
                    0.41406810035842295,
                    0.40449237177288755,
                    0.6971256234132697,
                    0.3385964912280702,
                    0.315,
                    0.4998632970541206,
                    0.3861702127659574,
                    0.33333333333333337,
                    0.43607954545454547,
                    0.4666666666666667,
                    0.3609486919861699,
                    0.5,
                    0.5,
                    0.0
                ],
                [
                    0.2918350890086559,
                    0.3153449820788531,
                    0.34157071393291344,
                    0.0861530535134809,
                    0.33333333333333337,
                    0.3416666666666667,
                    0.21830908121811993,
                    0.28191489361702127,
                    0.2691658223573117,
                    0.26846590909090906,
                    0.18,
                    0.3130254311201448,
                    0.25,
                    0.125,
                    0.0
                ],
                [
                    0.3466484417917539,
                    0.270586917562724,
                    0.253936914294199,
                    0.21672132307324943,
                    0.3280701754385965,
                    0.3433333333333333,
                    0.28182762172775955,
                    0.3319148936170213,
                    0.39750084430935495,
                    0.2954545454545454,
                    0.35333333333333333,
                    0.3260258768936853,
                    0.25,
                    0.375,
                    0.0
                ]
            ],
            "fraction_answers": {
                "j.d. souther": 0.14087702810956657,
                "randy meisner": 0.4834592891517985,
                "bernie leadon": 0.3756636827386349
            },
            "question": "who is not considered an official member of the eagles?",
            "rate_limited": false,
            "answers": [
                "j.d. souther",
                "randy meisner",
                "bernie leadon"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "j.d. souther": 0.5255211406635183,
                "randy meisner": 0.15532879528263782,
                "bernie leadon": 0.1890288279777383
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1124104641106407,
                    1.4957965510388416,
                    1.3917929848505177
                ],
                "result_count_important_words": [
                    99.0,
                    204000.0,
                    158000.0
                ],
                "wikipedia_search": [
                    1.0,
                    1.3850050658561297,
                    0.6149949341438703
                ],
                "word_count_appended_bing": [
                    5.0,
                    48.0,
                    22.0
                ],
                "answer_relation_to_question_bing": [
                    0.5155913978494624,
                    1.1079301075268817,
                    1.376478494623656
                ],
                "cosine_similarity_raw": [
                    0.057853784412145615,
                    0.09596860408782959,
                    0.14905281364917755
                ],
                "result_count_noun_chunks": [
                    107000.0,
                    205000.0,
                    158000.0
                ],
                "question_answer_similarity": [
                    0.4345264742150903,
                    -0.9122479939833283,
                    -0.6244347263127565
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    3.0,
                    1.0
                ],
                "result_count_bing": [
                    3330000.0,
                    2850000.0,
                    2820000.0
                ],
                "word_count_appended": [
                    45.0,
                    163.0,
                    144.0
                ],
                "answer_relation_to_question": [
                    1.1078682464032783,
                    1.6653192879307528,
                    1.226812465665969
                ],
                "result_count": [
                    92.0,
                    95.0,
                    98.0
                ]
            },
            "integer_answers": {
                "j.d. souther": 2,
                "randy meisner": 9,
                "bernie leadon": 3
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these Adam Sandler movies has a female director?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "billy madison"
            ],
            "question": "which of these adam sandler movies has a female director?",
            "lines": [
                [
                    0.5421801339613181,
                    0.32719468950008646,
                    0.4159509790332164,
                    0.19429722510676048,
                    0.32860998650472334,
                    0.31348314606741573,
                    0.22433460076045628,
                    0.27487179487179486,
                    0.3888888888888889,
                    0.3543859649122807,
                    0.3898305084745763,
                    0.3500034503328547,
                    0.5,
                    0.25,
                    -1.0
                ],
                [
                    0.3195088559423165,
                    0.3030768898114513,
                    0.3243781487134328,
                    0.47562762276482273,
                    0.43859649122807015,
                    0.32808988764044944,
                    0.48098859315589354,
                    0.46153846153846156,
                    0.6111111111111112,
                    0.3684210526315789,
                    0.22033898305084745,
                    0.31920862219186497,
                    0.4,
                    0.5,
                    -1.0
                ],
                [
                    0.13831101009636546,
                    0.3697284206884622,
                    0.25967087225335084,
                    0.33007515212841676,
                    0.23279352226720648,
                    0.35842696629213483,
                    0.2946768060836502,
                    0.2635897435897436,
                    0.0,
                    0.2771929824561403,
                    0.3898305084745763,
                    0.33078792747528035,
                    0.1,
                    0.25,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "billy madison",
                "the wedding singer",
                "big daddy"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "big daddy": 0.2215213731253631,
                "billy madison": 0.7236727451841062,
                "the wedding singer": 0.18186761487605746
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7500172516642734,
                    1.5960431109593247,
                    1.6539396373764017
                ],
                "result_count_important_words": [
                    118000.0,
                    253000.0,
                    155000.0
                ],
                "wikipedia_search": [
                    0.7777777777777778,
                    1.2222222222222223,
                    0.0
                ],
                "word_count_appended_bing": [
                    23.0,
                    13.0,
                    23.0
                ],
                "answer_relation_to_question_bing": [
                    1.3087787580003458,
                    1.2123075592458052,
                    1.4789136827538487
                ],
                "cosine_similarity_raw": [
                    0.1209564208984375,
                    0.09432750940322876,
                    0.07551096379756927
                ],
                "result_count_noun_chunks": [
                    80400.0,
                    135000.0,
                    77100.0
                ],
                "question_answer_similarity": [
                    3.5366709362715483,
                    8.657552309334278,
                    6.0081516690552235
                ],
                "word_count_noun_chunks": [
                    5.0,
                    4.0,
                    1.0
                ],
                "word_count_raw": [
                    1.0,
                    2.0,
                    1.0
                ],
                "result_count_bing": [
                    558000.0,
                    584000.0,
                    638000.0
                ],
                "result_count": [
                    97400.0,
                    130000.0,
                    69000.0
                ],
                "answer_relation_to_question": [
                    2.1687205358452726,
                    1.278035423769266,
                    0.5532440403854618
                ],
                "word_count_appended": [
                    101.0,
                    105.0,
                    79.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What nickname is stamped on official NFL footballs?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "the duke"
            ],
            "question": "what nickname is stamped on official nfl footballs?",
            "lines": [
                [
                    0.2272727272727273,
                    0.09523809523809523,
                    0.11529837151065579,
                    0.3177726795330138,
                    0.00989694651292593,
                    0.23885918003565063,
                    0.006309716964124752,
                    0.14603616133518776,
                    0.25,
                    0.1751412429378531,
                    0.07142857142857142,
                    0.2593905571216243,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.3529292929292929,
                    0.5714285714285715,
                    0.17323303519049474,
                    0.3224354140623553,
                    0.11067699470438488,
                    0.26916221033868093,
                    0.47016405264106725,
                    0.5945757997218358,
                    0.0,
                    0.4632768361581921,
                    0.8571428571428571,
                    0.3706060171301718,
                    0.9736842105263158,
                    1.0,
                    1.0
                ],
                [
                    0.4197979797979798,
                    0.3333333333333333,
                    0.7114685932988495,
                    0.3597919064046309,
                    0.8794260587826892,
                    0.4919786096256685,
                    0.5235262303948081,
                    0.25938803894297635,
                    0.75,
                    0.3615819209039548,
                    0.07142857142857142,
                    0.3700034257482039,
                    0.02631578947368421,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "the champ",
                "the duke",
                "the pro"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "the duke": 0.8882594897636996,
                "the pro": 0.6576331025465756,
                "the champ": 0.0391776239531341
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2969527856081215,
                    1.853030085650859,
                    1.8500171287410194
                ],
                "result_count_important_words": [
                    8750.0,
                    652000.0,
                    726000.0
                ],
                "wikipedia_search": [
                    0.5,
                    0.0,
                    1.5
                ],
                "word_count_appended_bing": [
                    2.0,
                    24.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.2857142857142857,
                    1.7142857142857144,
                    1.0
                ],
                "cosine_similarity_raw": [
                    0.04566202312707901,
                    0.06860609352588654,
                    0.2817654311656952
                ],
                "result_count_noun_chunks": [
                    210000.0,
                    855000.0,
                    373000.0
                ],
                "question_answer_similarity": [
                    4.18517349101603,
                    4.24658327922225,
                    4.738580897450447
                ],
                "word_count_noun_chunks": [
                    0.0,
                    37.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    22.0,
                    0.0
                ],
                "result_count_bing": [
                    1340000.0,
                    1510000.0,
                    2760000.0
                ],
                "word_count_appended": [
                    31.0,
                    82.0,
                    64.0
                ],
                "answer_relation_to_question": [
                    0.6818181818181819,
                    1.0587878787878788,
                    1.2593939393939393
                ],
                "result_count": [
                    7270.0,
                    81300.0,
                    646000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which public radio program is hosted by the cousin of a multiple Oscar-nominated composer?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "this american life"
            ],
            "question": "which public radio program is hosted by the cousin of a multiple oscar-nominated composer?",
            "lines": [
                [
                    0.3811218200583641,
                    0.4629585326953748,
                    0.39144596974703294,
                    0.3850984543935382,
                    0.0008953966080269672,
                    0.2443754848719938,
                    0.011990407673860911,
                    0.023055881203595155,
                    0.5607056451612903,
                    0.3076923076923077,
                    0.21052631578947367,
                    0.23993746717689618,
                    1.0,
                    0,
                    -1.0
                ],
                [
                    0.327396037720552,
                    0.28010366826156297,
                    0.22641415641436935,
                    0.1812542170490517,
                    0.9981038660065311,
                    0.4491854150504267,
                    0.9664268585131894,
                    0.5431809300508011,
                    0.12375672043010753,
                    0.4230769230769231,
                    0.5263157894736842,
                    0.4891794479083979,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.29148214222108393,
                    0.2569377990430622,
                    0.3821398738385977,
                    0.43364732855741006,
                    0.0010007373854419045,
                    0.3064391000775795,
                    0.02158273381294964,
                    0.43376318874560377,
                    0.31553763440860216,
                    0.2692307692307692,
                    0.2631578947368421,
                    0.2708830849147059,
                    0.0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "this american life",
                "fresh air",
                "all things considered"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "fresh air": 0.2221465686053774,
                "this american life": 0.3152728955015684,
                "all things considered": 0.14701576275997275
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.1594372045920656,
                    4.402615031175581,
                    2.4379477642323533
                ],
                "result_count_important_words": [
                    50.0,
                    4030.0,
                    90.0
                ],
                "wikipedia_search": [
                    2.242822580645161,
                    0.49502688172043013,
                    1.2621505376344087
                ],
                "word_count_appended_bing": [
                    4.0,
                    10.0,
                    5.0
                ],
                "answer_relation_to_question_bing": [
                    1.8518341307814992,
                    1.1204146730462519,
                    1.0277511961722487
                ],
                "cosine_similarity_raw": [
                    0.10683850198984146,
                    0.061795882880687714,
                    0.10429856181144714
                ],
                "result_count_noun_chunks": [
                    59.0,
                    1390.0,
                    1110.0
                ],
                "question_answer_similarity": [
                    13.97685657441616,
                    6.578484453260899,
                    15.738901171833277
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    31500.0,
                    57900.0,
                    39500.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    34.0,
                    37900.0,
                    38.0
                ],
                "answer_relation_to_question": [
                    1.5244872802334564,
                    1.309584150882208,
                    1.1659285688843357
                ],
                "word_count_appended": [
                    16.0,
                    22.0,
                    14.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "How do you let someone on Tinder know you're interested?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "swipe right"
            ],
            "lines": [
                [
                    0.5175868593051256,
                    0.5357142857142857,
                    0.806155830002902,
                    0.2599849600624926,
                    0.999567116603046,
                    0.4589028936623469,
                    0.09090523846562894,
                    0.5343855784977872,
                    0.9128571428571428,
                    0.8333333333333334,
                    0.7142857142857143,
                    0.7148147806196916,
                    1.0,
                    1.0,
                    5.0
                ],
                [
                    0.16546267629858963,
                    0.11904761904761905,
                    0.09372904866618545,
                    0.5174167924346847,
                    0.0,
                    0.0857447186224037,
                    0.9090523846562893,
                    0.46530151131556385,
                    0.06469387755102039,
                    0.027777777777777776,
                    0.14285714285714285,
                    0.07023841214029874,
                    0.0,
                    0.0,
                    5.0
                ],
                [
                    0.3169504643962849,
                    0.3452380952380953,
                    0.10011512133091251,
                    0.22259824750282267,
                    0.00043288339695407503,
                    0.45535238771524944,
                    4.2376878081721756e-05,
                    0.00031291018664889443,
                    0.02244897959183673,
                    0.1388888888888889,
                    0.14285714285714285,
                    0.21494680724000959,
                    0.0,
                    0.0,
                    5.0
                ]
            ],
            "fraction_answers": {
                "draw circle around face": 0.19009442581196967,
                "shake phone": 0.14001316465878053,
                "swipe right": 0.6698924095292498
            },
            "question": "how do you let someone on tinder know you're interested?",
            "rate_limited": false,
            "answers": [
                "swipe right",
                "draw circle around face",
                "shake phone"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "draw circle around face": 0.10333065815224987,
                "shake phone": 0.11315285427801795,
                "swipe right": 0.7906221929744528
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 5
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.574073903098458,
                    0.3511920607014937,
                    1.074734036200048
                ],
                "result_count_important_words": [
                    133000.0,
                    1330000.0,
                    62.0
                ],
                "wikipedia_search": [
                    4.564285714285715,
                    0.32346938775510203,
                    0.11224489795918367
                ],
                "answer_relation_to_question": [
                    2.070347437220502,
                    0.6618507051943584,
                    1.2678018575851393
                ],
                "answer_relation_to_question_bing": [
                    1.6071428571428572,
                    0.35714285714285715,
                    1.0357142857142858
                ],
                "word_count_appended": [
                    90.0,
                    3.0,
                    15.0
                ],
                "cosine_similarity_raw": [
                    0.25540950894355774,
                    0.02969561144709587,
                    0.03171887248754501
                ],
                "result_count_noun_chunks": [
                    2630000.0,
                    2290000.0,
                    1540.0
                ],
                "question_answer_similarity": [
                    8.666833512485027,
                    17.248556206934154,
                    7.420513674383983
                ],
                "word_count_noun_chunks": [
                    14.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    3.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    51700000.0,
                    9660000.0,
                    51300000.0
                ],
                "result_count": [
                    127000.0,
                    0,
                    55.0
                ],
                "word_count_appended_bing": [
                    10.0,
                    2.0,
                    2.0
                ]
            },
            "integer_answers": {
                "draw circle around face": 2,
                "shake phone": 0,
                "swipe right": 12
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Albert Einstein won a Nobel Prize for his work on what scientific phenomenon?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "photoelectric effect"
            ],
            "question": "albert einstein won a nobel prize for his work on what scientific phenomenon?",
            "lines": [
                [
                    0.24420961207163305,
                    0.18571170606873705,
                    0.2414393050938356,
                    0.3884370563351825,
                    0.33016627078384797,
                    0.6264119942493325,
                    0.014077425842131725,
                    0.9835947876655777,
                    0.37366452991453,
                    0.22058823529411764,
                    0.09090909090909091,
                    0.33345313660053405,
                    0.1891891891891892,
                    0.16666666666666666,
                    1.0
                ],
                [
                    0.43826859466842716,
                    0.6338330187374972,
                    0.6278431142090111,
                    0.23097750770975134,
                    0.24782264449722882,
                    0.18874512220168413,
                    0.08597285067873303,
                    0.005276950138205837,
                    0.43643162393162394,
                    0.5833333333333334,
                    0.8181818181818182,
                    0.3392285411048448,
                    0.7837837837837838,
                    0.75,
                    1.0
                ],
                [
                    0.3175217932599398,
                    0.18045527519376564,
                    0.13071758069715336,
                    0.38058543595506616,
                    0.4220110847189232,
                    0.18484288354898337,
                    0.8999497234791353,
                    0.01112826219621639,
                    0.18990384615384617,
                    0.19607843137254902,
                    0.09090909090909091,
                    0.32731832229462116,
                    0.02702702702702703,
                    0.08333333333333333,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "general relativity",
                "photoelectric effect",
                "special relativity"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "special relativity": 0.14673835108995562,
                "general relativity": 0.23311381279711046,
                "photoelectric effect": 0.8941198245920544
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.6676250928042724,
                    2.7138283288387584,
                    2.6185465783569692
                ],
                "result_count_important_words": [
                    28000.0,
                    171000.0,
                    1790000.0
                ],
                "wikipedia_search": [
                    2.9893162393162394,
                    3.491452991452991,
                    1.5192307692307692
                ],
                "answer_relation_to_question": [
                    1.9536768965730644,
                    3.5061487573474173,
                    2.5401743460795183
                ],
                "word_count_appended_bing": [
                    2.0,
                    18.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.1142702364124224,
                    3.802998112424984,
                    1.082731651162594
                ],
                "cosine_similarity_raw": [
                    0.11090030521154404,
                    0.28838714957237244,
                    0.060042500495910645
                ],
                "result_count_noun_chunks": [
                    2740000.0,
                    14700.0,
                    31000.0
                ],
                "question_answer_similarity": [
                    6.774048951454461,
                    4.028073322027922,
                    6.63712262082845
                ],
                "word_count_noun_chunks": [
                    7.0,
                    29.0,
                    1.0
                ],
                "result_count_bing": [
                    305000.0,
                    91900.0,
                    90000.0
                ],
                "word_count_raw": [
                    2.0,
                    9.0,
                    1.0
                ],
                "result_count": [
                    41700.0,
                    31300.0,
                    53300.0
                ],
                "word_count_appended": [
                    45.0,
                    119.0,
                    40.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which band featured a regular didgeridoo player?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "jamiroquai"
            ],
            "question": "which band featured a regular didgeridoo player?",
            "lines": [
                [
                    0.4957107843137255,
                    0.3361111111111111,
                    0.19203180241402967,
                    0.5218417716532806,
                    0.002579584431782748,
                    0.34521158129175944,
                    0.00546651757982358,
                    0.6830985915492958,
                    0.43397807865892973,
                    0.037366548042704624,
                    0.05,
                    0.2513930601990528,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.3762254901960785,
                    0.3194444444444444,
                    0.5686396897284026,
                    -0.12235679816163257,
                    0.9949825665447742,
                    0.0467706013363029,
                    0.5553484904957138,
                    0.0892018779342723,
                    0.5021921341070278,
                    0.7544483985765125,
                    0.78125,
                    0.3887577897749258,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.12806372549019607,
                    0.3444444444444444,
                    0.23932850785756768,
                    0.600515026508352,
                    0.0024378490234430368,
                    0.6080178173719376,
                    0.43918499192446264,
                    0.22769953051643194,
                    0.06382978723404256,
                    0.20818505338078291,
                    0.16875,
                    0.35984915002602147,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "dexys midnight runners",
                "jamiroquai",
                "o.a.r."
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "o.a.r.": 0.26081725710053577,
                "dexys midnight runners": 0.08015313988745036,
                "jamiroquai": 0.7274325959063712
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.256965300995264,
                    1.943788948874629,
                    1.7992457501301073
                ],
                "result_count_important_words": [
                    88.0,
                    8940.0,
                    7070.0
                ],
                "wikipedia_search": [
                    1.7359123146357187,
                    2.0087685364281107,
                    0.2553191489361702
                ],
                "word_count_appended_bing": [
                    8.0,
                    125.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    1.0083333333333333,
                    0.9583333333333333,
                    1.0333333333333332
                ],
                "cosine_similarity_raw": [
                    0.032340917736291885,
                    0.09576710313558578,
                    0.04030636325478554
                ],
                "result_count_noun_chunks": [
                    291.0,
                    38.0,
                    97.0
                ],
                "question_answer_similarity": [
                    1.7505669547244906,
                    -0.4104573056101799,
                    2.0144837349653244
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count_bing": [
                    155000.0,
                    21000.0,
                    273000.0
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_appended": [
                    21.0,
                    424.0,
                    117.0
                ],
                "answer_relation_to_question": [
                    1.982843137254902,
                    1.504901960784314,
                    0.5122549019607843
                ],
                "result_count": [
                    91.0,
                    35100.0,
                    86.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which monster is NOT named in the Kanye West song \u201cMonster\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "vampire"
            ],
            "question": "which monster is not named in the kanye west song \u201cmonster\u201d?",
            "lines": [
                [
                    0.24095238095238097,
                    0.0,
                    0.3994895527268562,
                    0.34483125462368547,
                    0.3680733944954129,
                    0.37017543859649127,
                    0.33665966386554624,
                    0.3280282935455349,
                    0.275,
                    0.3411497730711044,
                    0.3313253012048193,
                    0.3547502414626442,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.4057142857142857,
                    0.5,
                    0.19688182043524327,
                    0.30953140760140674,
                    0.36678899082568805,
                    0.32105263157894737,
                    0.3471638655462185,
                    0.33620689655172414,
                    0.45,
                    0.31164901664145234,
                    0.3313253012048193,
                    0.3049243500366795,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.35333333333333333,
                    0.5,
                    0.4036286268379005,
                    0.3456373377749078,
                    0.26513761467889907,
                    0.30877192982456136,
                    0.3161764705882353,
                    0.33576480990274093,
                    0.275,
                    0.3472012102874433,
                    0.3373493975903614,
                    0.3403254085006763,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "dragon",
                "devil",
                "vampire"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "devil": 0.28627500794290767,
                "vampire": 0.47510636202453566,
                "dragon": 0.42336080631915235
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7429971024482696,
                    2.340907799559846,
                    1.9160950979918845
                ],
                "result_count_important_words": [
                    933000.0,
                    873000.0,
                    1050000.0
                ],
                "wikipedia_search": [
                    1.8,
                    0.4,
                    1.8
                ],
                "word_count_appended_bing": [
                    28.0,
                    28.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    3.0,
                    0.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.02625187300145626,
                    0.07917007803916931,
                    0.025170806795358658
                ],
                "result_count_noun_chunks": [
                    778000.0,
                    741000.0,
                    743000.0
                ],
                "question_answer_similarity": [
                    3.2229075022041798,
                    3.9560973048210144,
                    3.2061648815870285
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    1480000.0,
                    2040000.0,
                    2180000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    719000.0,
                    726000.0,
                    1280000.0
                ],
                "answer_relation_to_question": [
                    2.59047619047619,
                    0.9428571428571428,
                    1.4666666666666668
                ],
                "word_count_appended": [
                    210.0,
                    249.0,
                    202.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which Oscar-winning actress has NOT won the award for playing a real person?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "emma thompson"
            ],
            "lines": [
                [
                    0.34125077547282784,
                    0.3373940497487009,
                    0.3630508052358993,
                    0.2218545548243634,
                    0.33223374175306314,
                    0.4207161125319693,
                    0.33017241379310347,
                    0.3409893992932862,
                    0.1568236451048951,
                    0.28805970149253735,
                    0.3717948717948718,
                    0.3441857087565661,
                    0.5,
                    0,
                    -1.0
                ],
                [
                    0.3624155682640805,
                    0.3554135786693926,
                    0.27741659906780924,
                    0.50371470938339,
                    0.28416588124410935,
                    0.14897698209718668,
                    0.3172413793103448,
                    0.2676678445229682,
                    0.39623397435897434,
                    0.3537313432835821,
                    0.34615384615384615,
                    0.3318666357784548,
                    0.5,
                    0,
                    -1.0
                ],
                [
                    0.29633365626309177,
                    0.3071923715819065,
                    0.3595325956962915,
                    0.27443073579224664,
                    0.3836003770028275,
                    0.430306905370844,
                    0.35258620689655173,
                    0.3913427561837456,
                    0.44694238053613056,
                    0.35820895522388063,
                    0.28205128205128205,
                    0.32394765546497906,
                    0.0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "hilary swank": 0.3528498649132649,
                "emma thompson": 0.3309960338766025,
                "susan sarandon": 0.31615410121013254
            },
            "question": "which oscar-winning actress has not won the award for playing a real person?",
            "rate_limited": false,
            "answers": [
                "emma thompson",
                "susan sarandon",
                "hilary swank"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "hilary swank": 0.0661150920366537,
                "emma thompson": 0.23800605278106207,
                "susan sarandon": 0.18496430264656322
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.493028659894942,
                    2.690133827544723,
                    2.816837512560335
                ],
                "result_count_important_words": [
                    1970000.0,
                    2120000.0,
                    1710000.0
                ],
                "wikipedia_search": [
                    5.490821678321678,
                    1.6602564102564104,
                    0.8489219114219113
                ],
                "answer_relation_to_question": [
                    2.5399875924347555,
                    2.2013509077747124,
                    3.2586614997905317
                ],
                "answer_relation_to_question_bing": [
                    2.6016952040207855,
                    2.313382741289718,
                    3.0849220546894967
                ],
                "word_count_appended": [
                    142.0,
                    98.0,
                    95.0
                ],
                "cosine_similarity_raw": [
                    0.03219468146562576,
                    0.052325986325740814,
                    0.03302175924181938
                ],
                "result_count_noun_chunks": [
                    360000.0,
                    526000.0,
                    246000.0
                ],
                "question_answer_similarity": [
                    2.248649863333412,
                    -0.03003134112805128,
                    1.823600939475
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    1240000.0,
                    5490000.0,
                    1090000.0
                ],
                "result_count": [
                    356000.0,
                    458000.0,
                    247000.0
                ],
                "word_count_appended_bing": [
                    10.0,
                    12.0,
                    17.0
                ]
            },
            "integer_answers": {
                "hilary swank": 5,
                "emma thompson": 3,
                "susan sarandon": 5
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "In which of these movies is the title NOT spoken by any character?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "gravity"
            ],
            "lines": [
                [
                    0.19723895582329315,
                    0.16753393665158367,
                    0.18580023072053725,
                    0.35123211291886114,
                    0.41085423197492166,
                    0.29486023444544635,
                    0.3724007561436673,
                    0.2791828793774319,
                    0.10241739211831885,
                    0.3138913624220837,
                    0.22535211267605632,
                    0.3261281676018138,
                    0.5,
                    0.0,
                    -1.0
                ],
                [
                    0.4156124497991968,
                    0.40486425339366516,
                    0.40048114583837374,
                    0.34398438693097644,
                    0.3947884012539185,
                    0.4102795311091073,
                    0.3402646502835539,
                    0.3438715953307393,
                    0.42334784236637646,
                    0.3192341941228851,
                    0.3427230046948357,
                    0.3374832560074836,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.38714859437751004,
                    0.4276018099547511,
                    0.413718623441089,
                    0.3047835001501624,
                    0.1943573667711599,
                    0.29486023444544635,
                    0.2873345935727788,
                    0.3769455252918288,
                    0.4742347655153047,
                    0.3668744434550312,
                    0.431924882629108,
                    0.3363885763907027,
                    0.0,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "inception": 0.4675868038751407,
                "speed": 0.31483244057216103,
                "gravity": 0.2175807555526983
            },
            "question": "in which of these movies is the title not spoken by any character?",
            "rate_limited": false,
            "answers": [
                "inception",
                "gravity",
                "speed"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "inception": 0.15720197078447753,
                "speed": 0.24462968968857096,
                "gravity": 0.3075486066324411
            },
            "integer_answers": {
                "inception": 10,
                "speed": 4,
                "gravity": 0
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.39097465918549,
                    1.3001339519401314,
                    1.3088913888743785
                ],
                "result_count_important_words": [
                    1350000.0,
                    1690000.0,
                    2250000.0
                ],
                "wikipedia_search": [
                    2.385495647290087,
                    0.4599129458017411,
                    0.15459140690817186
                ],
                "word_count_appended_bing": [
                    117.0,
                    67.0,
                    29.0
                ],
                "answer_relation_to_question_bing": [
                    1.3298642533936653,
                    0.38054298642533935,
                    0.2895927601809955
                ],
                "cosine_similarity_raw": [
                    0.14731481671333313,
                    0.046660128980875015,
                    0.04045364260673523
                ],
                "result_count_noun_chunks": [
                    4540000.0,
                    3210000.0,
                    2530000.0
                ],
                "question_answer_similarity": [
                    2.5318306535482407,
                    2.655177265405655,
                    3.322323977947235
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    18200000.0,
                    7960000.0,
                    18200000.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    4550000.0,
                    5370000.0,
                    15600000.0
                ],
                "answer_relation_to_question": [
                    1.816566265060241,
                    0.5063253012048192,
                    0.6771084337349398
                ],
                "word_count_appended": [
                    418.0,
                    406.0,
                    299.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What is a common remedy for dehydration?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "drinking water"
            ],
            "question": "what is a common remedy for dehydration?",
            "lines": [
                [
                    0.6666666666666667,
                    0.75,
                    0.6890297927465077,
                    0.2919506975172218,
                    0.9156588660279567,
                    0.4256691389874234,
                    0.9419283887846411,
                    0.19458020731636633,
                    0.051560379918588875,
                    0.49700598802395207,
                    0.6363636363636364,
                    0.5623308323473859,
                    1.0,
                    0,
                    1.0
                ],
                [
                    0.04761904761904762,
                    0.0,
                    0.1985195875173137,
                    0.42230236193350773,
                    0.08434113397204335,
                    0.2944211544663012,
                    0.058069145434236386,
                    0.805419039955559,
                    0.9226594301221167,
                    0.47904191616766467,
                    0.18181818181818182,
                    0.3952591608052051,
                    0.0,
                    0,
                    1.0
                ],
                [
                    0.28571428571428575,
                    0.25,
                    0.11245061973617862,
                    0.2857469405492704,
                    0.0,
                    0.2799097065462754,
                    2.4657811224728825e-06,
                    7.527280747248214e-07,
                    0.025780189959294438,
                    0.023952095808383235,
                    0.18181818181818182,
                    0.042410006847408976,
                    0.0,
                    0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "drinking water",
                "running a marathon",
                "slaying a hydra"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "running a marathon": 0.19208186532751098,
                "drinking water": 0.8425220969566196,
                "slaying a hydra": 0.09112195263430593
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6869924970421577,
                    1.1857774824156153,
                    0.12723002054222693
                ],
                "result_count_important_words": [
                    764000.0,
                    47100.0,
                    2.0
                ],
                "wikipedia_search": [
                    0.10312075983717775,
                    1.8453188602442334,
                    0.051560379918588875
                ],
                "word_count_appended_bing": [
                    7.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.5,
                    0.0,
                    0.5
                ],
                "cosine_similarity_raw": [
                    0.07389344274997711,
                    0.021289784461259842,
                    0.012059512548148632
                ],
                "result_count_noun_chunks": [
                    517000.0,
                    2140000.0,
                    2.0
                ],
                "question_answer_similarity": [
                    4.972012221813202,
                    7.191942073404789,
                    4.866360289044678
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    13200000.0,
                    9130000.0,
                    8680000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    583000.0,
                    53700.0,
                    0
                ],
                "answer_relation_to_question": [
                    1.3333333333333333,
                    0.09523809523809523,
                    0.5714285714285714
                ],
                "word_count_appended": [
                    83.0,
                    80.0,
                    4.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which author's Pulitzer Prize-winning novel was adapted into a Broadway musical?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "alice walker"
            ],
            "question": "which author's pulitzer prize-winning novel was adapted into a broadway musical?",
            "lines": [
                [
                    0.28010316932040685,
                    0.4070149658582951,
                    0.02176063094905409,
                    0.5192382258526766,
                    0.5493055326449144,
                    0.286734693877551,
                    0.16548463356973994,
                    0.5220742817098809,
                    0.19982183349530286,
                    0.2361111111111111,
                    0.07317073170731707,
                    0.38737163839754485,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.17244068300198123,
                    0.4060736217406964,
                    0.025098837752003264,
                    -0.10365563494249905,
                    0.00036290455610174527,
                    0.29897959183673467,
                    0.6524822695035462,
                    0.10651716888577435,
                    0.23705863297700036,
                    0.1388888888888889,
                    0.07317073170731707,
                    0.1556675781442188,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.5474561476776119,
                    0.18691141240100848,
                    0.9531405312989426,
                    0.5844174090898224,
                    0.45033156279898384,
                    0.4142857142857143,
                    0.18203309692671396,
                    0.37140854940434476,
                    0.5631195335276968,
                    0.625,
                    0.8536585365853658,
                    0.45696078345823626,
                    1.0,
                    1.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "jane smiley",
                "annie proulx",
                "alice walker"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "jane smiley": 0.22639376723891066,
                "alice walker": 0.9250868892566856,
                "annie proulx": -0.004717172399557287
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.098973107180359,
                    1.2453406251537504,
                    3.65568626766589
                ],
                "result_count_important_words": [
                    14000.0,
                    55200.0,
                    15400.0
                ],
                "wikipedia_search": [
                    1.39875283446712,
                    1.6594104308390025,
                    3.9418367346938776
                ],
                "word_count_appended_bing": [
                    3.0,
                    3.0,
                    35.0
                ],
                "answer_relation_to_question_bing": [
                    2.4420897951497706,
                    2.4364417304441783,
                    1.1214684744060508
                ],
                "cosine_similarity_raw": [
                    0.009832974523305893,
                    0.011341409757733345,
                    0.4306955337524414
                ],
                "result_count_noun_chunks": [
                    44700.0,
                    9120.0,
                    31800.0
                ],
                "question_answer_similarity": [
                    3.12354756006971,
                    -0.6235544485971332,
                    3.515641725389287
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    9.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    8.0
                ],
                "result_count_bing": [
                    28100.0,
                    29300.0,
                    40600.0
                ],
                "result_count": [
                    33300.0,
                    22.0,
                    27300.0
                ],
                "answer_relation_to_question": [
                    2.240825354563255,
                    1.3795254640158499,
                    4.379649181420895
                ],
                "word_count_appended": [
                    51.0,
                    30.0,
                    135.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What form of transportation counts Jay-Z as a prominent investor?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "boats"
            ],
            "lines": [
                [
                    0.3796718724313661,
                    0.3510876538399474,
                    0.30944513530966794,
                    0.21880172288869149,
                    0.47813777917584144,
                    0.12349183818310859,
                    0.5104895104895105,
                    0.575013068478829,
                    0.16511018786127168,
                    0.4396984924623116,
                    0.49557522123893805,
                    0.3818280258385283,
                    0.3333333333333333,
                    0.3333333333333333,
                    1.0
                ],
                [
                    0.2757722614937805,
                    0.33317196619948913,
                    0.38933913702850587,
                    0.6017750880559021,
                    0.06259830135262662,
                    0.30872959545777146,
                    0.08251748251748252,
                    0.014874304994535,
                    0.4782153179190752,
                    0.1984924623115578,
                    0.23893805309734514,
                    0.26684015787744364,
                    0.3333333333333333,
                    0.3333333333333333,
                    1.0
                ],
                [
                    0.34455586607485345,
                    0.3157403799605634,
                    0.30121572766182625,
                    0.17942318905540647,
                    0.45926391947153195,
                    0.5677785663591199,
                    0.406993006993007,
                    0.41011262652663594,
                    0.35667449421965314,
                    0.36180904522613067,
                    0.26548672566371684,
                    0.35133181628402793,
                    0.3333333333333333,
                    0.3333333333333333,
                    1.0
                ]
            ],
            "fraction_answers": {
                "boats": 0.35621800215451,
                "aviation": 0.36392979820461996,
                "e-bikes": 0.27985219964087016
            },
            "question": "what form of transportation counts jay-z as a prominent investor?",
            "rate_limited": false,
            "answers": [
                "aviation",
                "e-bikes",
                "boats"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "boats": 0.5498699963744416,
                "aviation": 0.39387107934162624,
                "e-bikes": 0.08691115724032182
            },
            "integer_answers": {
                "boats": 1,
                "aviation": 10,
                "e-bikes": 3
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.6727961808696983,
                    1.8678811051421056,
                    2.4593227139881955
                ],
                "result_count_important_words": [
                    365000.0,
                    59000.0,
                    291000.0
                ],
                "wikipedia_search": [
                    0.6604407514450867,
                    1.9128612716763007,
                    1.4266979768786126
                ],
                "word_count_appended_bing": [
                    56.0,
                    27.0,
                    30.0
                ],
                "answer_relation_to_question_bing": [
                    1.0532629615198421,
                    0.9995158985984673,
                    0.9472211398816903
                ],
                "cosine_similarity_raw": [
                    0.053956739604473114,
                    0.06788754463195801,
                    0.05252180993556976
                ],
                "result_count_noun_chunks": [
                    1210000.0,
                    31300.0,
                    863000.0
                ],
                "question_answer_similarity": [
                    2.1645172073040158,
                    5.953118265373632,
                    1.7749612524639815
                ],
                "word_count_noun_chunks": [
                    1.0,
                    1.0,
                    1.0
                ],
                "result_count_bing": [
                    34800.0,
                    87000.0,
                    160000.0
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    1.0
                ],
                "word_count_appended": [
                    175.0,
                    79.0,
                    144.0
                ],
                "answer_relation_to_question": [
                    1.8983593621568304,
                    1.3788613074689025,
                    1.7227793303742671
                ],
                "result_count": [
                    456000.0,
                    59700.0,
                    438000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these best-selling authors uses his/her given last name?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "j.d. robb"
            ],
            "lines": [
                [
                    0.377009100837277,
                    0.34841269841269845,
                    0.23115252245263493,
                    0.1085123983363008,
                    0.19486404833836857,
                    0.34330708661417325,
                    0.09015256588072122,
                    0.20891434262948208,
                    0.265625,
                    0.2288135593220339,
                    0.2222222222222222,
                    0.3394473166589414,
                    0.25,
                    0,
                    -1.0
                ],
                [
                    0.41143162237592523,
                    0.37857142857142856,
                    0.2644053368671957,
                    -0.062449521878158214,
                    0.2084592145015106,
                    0.3401574803149606,
                    0.09847434119278779,
                    0.20094621513944222,
                    0.5587121212121212,
                    0.4152542372881356,
                    0.5833333333333334,
                    0.31162624545662854,
                    0.75,
                    0,
                    -1.0
                ],
                [
                    0.2115592767867977,
                    0.273015873015873,
                    0.5044421406801693,
                    0.9539371235418574,
                    0.5966767371601208,
                    0.3165354330708661,
                    0.811373092926491,
                    0.5901394422310757,
                    0.17566287878787878,
                    0.3559322033898305,
                    0.19444444444444445,
                    0.3489264378844301,
                    0.0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "r.l. stine": 0.3429940041827163,
                "e.l. james": 0.4102034679938334,
                "j.d. robb": 0.24680252782345027
            },
            "question": "which of these best-selling authors uses his/her given last name?",
            "rate_limited": false,
            "answers": [
                "j.d. robb",
                "r.l. stine",
                "e.l. james"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "r.l. stine": 0.30254693022365486,
                "e.l. james": 0.24753409188915657,
                "j.d. robb": 0.48602254485623625
            },
            "integer_answers": {
                "r.l. stine": 6,
                "e.l. james": 6,
                "j.d. robb": 1
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.697236583294707,
                    1.5581312272831427,
                    1.7446321894221506
                ],
                "result_count_important_words": [
                    130000.0,
                    142000.0,
                    1170000.0
                ],
                "wikipedia_search": [
                    1.0625,
                    2.234848484848485,
                    0.7026515151515151
                ],
                "word_count_appended_bing": [
                    8.0,
                    21.0,
                    7.0
                ],
                "answer_relation_to_question_bing": [
                    1.0452380952380953,
                    1.1357142857142857,
                    0.819047619047619
                ],
                "cosine_similarity_raw": [
                    0.05698461830615997,
                    0.06518223136663437,
                    0.12435703724622726
                ],
                "result_count_noun_chunks": [
                    83900.0,
                    80700.0,
                    237000.0
                ],
                "question_answer_similarity": [
                    0.731094989401754,
                    -0.4207494556903839,
                    6.427087244577706
                ],
                "word_count_noun_chunks": [
                    1.0,
                    3.0,
                    0.0
                ],
                "result_count_bing": [
                    2180000.0,
                    2160000.0,
                    2010000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    12900.0,
                    13800.0,
                    39500.0
                ],
                "answer_relation_to_question": [
                    1.885045504186385,
                    2.0571581118796263,
                    1.0577963839339886
                ],
                "word_count_appended": [
                    27.0,
                    49.0,
                    42.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these things was created by a person who chose to remain anonymous?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "bitcoin"
            ],
            "lines": [
                [
                    0.5504761904761905,
                    0.6444444444444445,
                    0.5006813433715943,
                    0.24620562545757632,
                    0.9232929612076531,
                    0.4629433000637078,
                    0.8390255009107468,
                    0.9700889248181084,
                    0.8973684210526316,
                    0.7586854460093897,
                    0.6293103448275862,
                    0.3950380167633712,
                    0,
                    0,
                    0.0
                ],
                [
                    0.17857142857142855,
                    0.2518518518518518,
                    0.2761173352090164,
                    0.7823076718961841,
                    0.042127435492364404,
                    0.4586961138245912,
                    0.053506375227686705,
                    0.00669823305231551,
                    0.08947368421052632,
                    0.18779342723004694,
                    0.25,
                    0.3233623000707564,
                    0,
                    0,
                    0.0
                ],
                [
                    0.27095238095238094,
                    0.1037037037037037,
                    0.22320132141938925,
                    -0.02851329735376043,
                    0.03457960329998245,
                    0.07836058611170099,
                    0.10746812386156648,
                    0.023212842129576163,
                    0.013157894736842105,
                    0.05352112676056338,
                    0.1206896551724138,
                    0.2815996831658724,
                    0,
                    0,
                    0.0
                ]
            ],
            "fraction_answers": {
                "hoverboards": 0.24170882138639738,
                "fidget spinners": 0.10682780199668591,
                "bitcoin": 0.6514633766169168
            },
            "question": "which of these things was created by a person who chose to remain anonymous?",
            "rate_limited": false,
            "answers": [
                "bitcoin",
                "hoverboards",
                "fidget spinners"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "hoverboards": 0.2406346588261331,
                "fidget spinners": 0.14760358985098584,
                "bitcoin": 0.4908365697962334
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.3702281005802273,
                    1.9401738004245384,
                    1.6895980989952346
                ],
                "result_count_important_words": [
                    737000.0,
                    47000.0,
                    94400.0
                ],
                "wikipedia_search": [
                    3.5894736842105264,
                    0.35789473684210527,
                    0.05263157894736842
                ],
                "answer_relation_to_question": [
                    2.7523809523809524,
                    0.8928571428571428,
                    1.3547619047619048
                ],
                "answer_relation_to_question_bing": [
                    1.9333333333333333,
                    0.7555555555555555,
                    0.3111111111111111
                ],
                "word_count_appended": [
                    808.0,
                    200.0,
                    57.0
                ],
                "cosine_similarity_raw": [
                    0.04489924758672714,
                    0.024761179462075233,
                    0.020015867426991463
                ],
                "result_count_noun_chunks": [
                    16800000.0,
                    116000.0,
                    402000.0
                ],
                "question_answer_similarity": [
                    -0.5448476430028677,
                    -1.7312297001481056,
                    0.0630993009544909
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    2180000.0,
                    2160000.0,
                    369000.0
                ],
                "result_count": [
                    263000.0,
                    12000.0,
                    9850.0
                ],
                "word_count_appended_bing": [
                    73.0,
                    29.0,
                    14.0
                ]
            },
            "integer_answers": {
                "hoverboards": 0,
                "fidget spinners": 1,
                "bitcoin": 11
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which actor has NOT played Inspector Clouseau?": {
        "raw_data": {
            "data": {
                "word_count_appended_relation_to_question": [
                    1.018453193845667,
                    1.613833925906217,
                    1.3677128802481155
                ],
                "result_count_important_words": [
                    19700.0,
                    12400.0,
                    169000.0
                ],
                "wikipedia_search": [
                    0.5921052631578947,
                    0.4925320056899004,
                    0.9153627311522048
                ],
                "answer_relation_to_question": [
                    0.2835820895522388,
                    0.49985367281240856,
                    1.2165642376353527
                ],
                "answer_relation_to_question_bing": [
                    0.3833333333333333,
                    0.5916666666666667,
                    1.025
                ],
                "word_count_appended": [
                    40.0,
                    134.0,
                    106.0
                ],
                "cosine_similarity_raw": [
                    0.05365971475839615,
                    0.29055020213127136,
                    0.10678545385599136
                ],
                "result_count_noun_chunks": [
                    22500.0,
                    11800.0,
                    15600.0
                ],
                "question_answer_similarity": [
                    1.1316075846552849,
                    0.16048925649374723,
                    1.62329174997285
                ],
                "word_count_noun_chunks": [
                    0.0,
                    46.0,
                    13.0
                ],
                "word_count_raw": [
                    0.0,
                    13.0,
                    7.0
                ],
                "result_count_bing": [
                    542000.0,
                    351000.0,
                    462000.0
                ],
                "result_count": [
                    220000.0,
                    159000.0,
                    14000.0
                ],
                "word_count_appended_bing": [
                    2.0,
                    21.0,
                    12.0
                ]
            },
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "z-best_answer_by_ml": [
                "michael caine",
                "top"
            ],
            "lines": [
                [
                    0.4291044776119403,
                    0.4041666666666667,
                    0.44050968342571106,
                    0.3059250852354083,
                    0.22010178117048346,
                    0.30000000000000004,
                    0.4510193933366484,
                    0.2745490981963928,
                    0.3519736842105264,
                    0.4285714285714286,
                    0.4714285714285714,
                    0.3726933507692916,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.37503658179689786,
                    0.3520833333333333,
                    0.17787895289159342,
                    0.4724754948649935,
                    0.29770992366412213,
                    0.370479704797048,
                    0.46916956737941323,
                    0.3817635270541082,
                    0.3768669985775249,
                    0.2607142857142857,
                    0.2,
                    0.2982707592617228,
                    0.11016949152542371,
                    0.175,
                    -1.0
                ],
                [
                    0.19585894059116182,
                    0.24375000000000002,
                    0.3816113636826956,
                    0.22159941989959825,
                    0.4821882951653944,
                    0.32952029520295206,
                    0.07981103928393835,
                    0.343687374749499,
                    0.2711593172119488,
                    0.3107142857142857,
                    0.32857142857142857,
                    0.32903588996898553,
                    0.3898305084745763,
                    0.325,
                    -1.0
                ]
            ],
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "question": "which actor has not played inspector clouseau?",
            "rate_limited": false,
            "answers": [
                "michael caine",
                "alan arkin",
                "roger moore"
            ],
            "ml_answers": {
                "michael caine": 0.931619950181935,
                "alan arkin": 0.04310812573551636,
                "roger moore": 0.3160363099794269
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "The band that recorded \u201cSeptember\u201d is named \u201cEarth, Wind &\u201d what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "fire"
            ],
            "question": "the band that recorded \u201cseptember\u201d is named \u201cearth, wind &\u201d what?",
            "lines": [
                [
                    0.2333333333333333,
                    0.40789473684210525,
                    0.11248478457988331,
                    -0.030343745646368537,
                    0.34428152492668623,
                    0.3325485579752796,
                    0.651285249081965,
                    0.1626260572903317,
                    0.0,
                    0.0034602076124567475,
                    0.11267605633802817,
                    0.33063081042615605,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.5777777777777777,
                    0.42105263157894735,
                    0.76782494349756,
                    1.3276585598343733,
                    0.3114369501466276,
                    0.33078281341965865,
                    0.3381140442042541,
                    0.8366992802618515,
                    0.625,
                    0.9930795847750865,
                    0.7464788732394366,
                    0.33873837914768795,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.18888888888888888,
                    0.17105263157894735,
                    0.11969027192255666,
                    -0.29731481418800476,
                    0.34428152492668623,
                    0.3366686286050618,
                    0.01060070671378092,
                    0.0006746624478167747,
                    0.375,
                    0.0034602076124567475,
                    0.14084507042253522,
                    0.33063081042615605,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "frederick",
                "fire",
                "flagellum"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "fire": 0.8894836071317621,
                "frederick": 0.15323028041459902,
                "flagellum": 0.18502155926703584
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9837848625569363,
                    2.032430274886128,
                    1.9837848625569363
                ],
                "result_count_important_words": [
                    940000.0,
                    488000.0,
                    15300.0
                ],
                "wikipedia_search": [
                    0.0,
                    2.5,
                    1.5
                ],
                "answer_relation_to_question": [
                    1.1666666666666665,
                    2.888888888888889,
                    0.9444444444444444
                ],
                "word_count_appended_bing": [
                    8.0,
                    53.0,
                    10.0
                ],
                "answer_relation_to_question_bing": [
                    1.631578947368421,
                    1.6842105263157894,
                    0.6842105263157894
                ],
                "cosine_similarity_raw": [
                    0.03448537737131119,
                    0.23539835214614868,
                    0.036694422364234924
                ],
                "result_count_noun_chunks": [
                    55200000.0,
                    284000000.0,
                    229000.0
                ],
                "question_answer_similarity": [
                    -0.09428846323862672,
                    4.125492177903652,
                    -0.9238594751805067
                ],
                "word_count_noun_chunks": [
                    0.0,
                    3.0,
                    0.0
                ],
                "result_count_bing": [
                    5650000.0,
                    5620000.0,
                    5720000.0
                ],
                "word_count_raw": [
                    0.0,
                    112.0,
                    0.0
                ],
                "result_count": [
                    587000.0,
                    531000.0,
                    587000.0
                ],
                "word_count_appended": [
                    1.0,
                    287.0,
                    1.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these fruits comes from a cactus?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "dragon fruit"
            ],
            "question": "which of these fruits comes from a cactus?",
            "lines": [
                [
                    0.4218289085545723,
                    0.14285714285714285,
                    0.08379116378936398,
                    0.11423698797474816,
                    0.34904641957538685,
                    0.2605820105820106,
                    0.3700440528634361,
                    0.3491720662347012,
                    0.1623931623931624,
                    0.35436241610738256,
                    0.37719298245614036,
                    0.3372688503222494,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.13864306784660765,
                    0.047619047619047616,
                    0.09685132740366188,
                    0.11469572093861156,
                    0.1759625764663548,
                    0.4775132275132275,
                    0.20374449339207049,
                    0.17566594672426206,
                    0.4615384615384615,
                    0.35436241610738256,
                    0.42105263157894735,
                    0.3471868621939012,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.4395280235988201,
                    0.8095238095238096,
                    0.8193575088069741,
                    0.7710672910866403,
                    0.4749910039582584,
                    0.2619047619047619,
                    0.4262114537444934,
                    0.47516198704103674,
                    0.37606837606837606,
                    0.2912751677852349,
                    0.20175438596491227,
                    0.3155442874838494,
                    0,
                    1.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "lychee",
                "mangosteen",
                "dragon fruit"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "lychee": 0.08077227639929256,
                "mangosteen": 0.19369093567005777,
                "dragon fruit": 0.7120682661888902
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.0118065509667482,
                    1.0415605865817037,
                    0.9466328624515481
                ],
                "result_count_important_words": [
                    336000.0,
                    185000.0,
                    387000.0
                ],
                "wikipedia_search": [
                    0.48717948717948717,
                    1.3846153846153846,
                    1.1282051282051282
                ],
                "word_count_appended_bing": [
                    43.0,
                    48.0,
                    23.0
                ],
                "answer_relation_to_question_bing": [
                    0.42857142857142855,
                    0.14285714285714285,
                    2.428571428571429
                ],
                "cosine_similarity_raw": [
                    0.01857461780309677,
                    0.021469762548804283,
                    0.1816331446170807
                ],
                "result_count_noun_chunks": [
                    194000.0,
                    97600.0,
                    264000.0
                ],
                "question_answer_similarity": [
                    0.7636873228475451,
                    0.7667540051479591,
                    5.154672980308533
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    3940000.0,
                    7220000.0,
                    3960000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    4.0
                ],
                "result_count": [
                    194000.0,
                    97800.0,
                    264000.0
                ],
                "answer_relation_to_question": [
                    1.2654867256637168,
                    0.415929203539823,
                    1.3185840707964602
                ],
                "word_count_appended": [
                    264.0,
                    264.0,
                    217.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What gargantuan fruit is the subject of a Roald Dahl children's book?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "peach"
            ],
            "lines": [
                [
                    0.3961882306709893,
                    0.4006700167504188,
                    0.12663841414988747,
                    -0.798390738273242,
                    0.004606172270842929,
                    0.011715797430083144,
                    0.0013271400132714001,
                    0.005522319374137137,
                    0.18108318034175963,
                    0.22040816326530613,
                    0.15,
                    0.1652210650193729,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.04383534728362314,
                    0.20845896147403686,
                    0.2331953660256818,
                    -0.6644885296576125,
                    0.005066789497927222,
                    0.1946334089191232,
                    0.0033178500331785005,
                    0.0050621260929590425,
                    0.33319799463352634,
                    0.12244897959183673,
                    0.15555555555555556,
                    0.148452136854994,
                    0.14285714285714285,
                    0.0,
                    1.0
                ],
                [
                    0.5599764220453876,
                    0.39087102177554434,
                    0.6401662198244308,
                    2.4628792679308544,
                    0.9903270382312298,
                    0.7936507936507936,
                    0.9953550099535501,
                    0.9894155545329039,
                    0.48571882502471403,
                    0.6571428571428571,
                    0.6944444444444444,
                    0.6863267981256331,
                    0.8571428571428571,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "loquat": 0.06654236636871234,
                "dragonfruit": 0.06178498292948763,
                "peach": 0.8716726507018001
            },
            "question": "what gargantuan fruit is the subject of a roald dahl children's book?",
            "rate_limited": false,
            "answers": [
                "dragonfruit",
                "loquat",
                "peach"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "loquat": 0.18348756255580995,
                "dragonfruit": 0.13283455919738485,
                "peach": 0.7906221929744528
            },
            "integer_answers": {
                "loquat": 0,
                "dragonfruit": 1,
                "peach": 13
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1565474551356103,
                    1.039164957984958,
                    4.804287586879432
                ],
                "result_count_important_words": [
                    14.0,
                    35.0,
                    10500.0
                ],
                "wikipedia_search": [
                    0.7243327213670385,
                    1.3327919785341054,
                    1.9428753000988561
                ],
                "word_count_appended_bing": [
                    27.0,
                    28.0,
                    125.0
                ],
                "answer_relation_to_question_bing": [
                    1.6026800670016752,
                    0.8338358458961475,
                    1.5634840871021773
                ],
                "cosine_similarity_raw": [
                    0.016420908272266388,
                    0.030237900093197823,
                    0.08300886303186417
                ],
                "result_count_noun_chunks": [
                    12.0,
                    11.0,
                    2150.0
                ],
                "question_answer_similarity": [
                    -0.7061498463153839,
                    -0.5877178311347961,
                    2.1783341579139233
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    6.0
                ],
                "result_count_bing": [
                    1860.0,
                    30900.0,
                    126000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    2.0
                ],
                "result_count": [
                    10.0,
                    11.0,
                    2150.0
                ],
                "answer_relation_to_question": [
                    1.9809411533549464,
                    0.21917673641811572,
                    2.799882110226938
                ],
                "word_count_appended": [
                    54.0,
                    30.0,
                    161.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "The only person who owns more U.S. land than Ted Turner made his fortune in what business?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "cable tv"
            ],
            "lines": [
                [
                    0.26963307763498107,
                    0.3969607843137255,
                    0.13856251083272977,
                    0.13011617462424702,
                    0.4096204434423149,
                    0.44957290573954745,
                    0.3826491092176607,
                    0.36607142857142855,
                    0.34123238463663996,
                    0.8352272727272727,
                    0.8125,
                    0.3162403781825374,
                    0,
                    0,
                    1.0
                ],
                [
                    0.5933827317743366,
                    0.23284313725490194,
                    0.6780507750846306,
                    0.34588049508251706,
                    0.23412251033446074,
                    0.09635845946350967,
                    0.2877614252517428,
                    0.20535714285714285,
                    0.3936787326149028,
                    0.09090909090909091,
                    0.125,
                    0.3688726546300934,
                    0,
                    0,
                    1.0
                ],
                [
                    0.13698419059068226,
                    0.3701960784313726,
                    0.18338671408263957,
                    0.524003330293236,
                    0.35625704622322435,
                    0.4540686347969429,
                    0.3295894655305964,
                    0.42857142857142855,
                    0.2650888827484572,
                    0.07386363636363637,
                    0.0625,
                    0.3148869671873692,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "pharmaceuticals": 0.40403220582692373,
                "cable tv": 0.30435142960477746,
                "fast food": 0.2916163645682988
            },
            "question": "the only person who owns more u.s. land than ted turner made his fortune in what business?",
            "rate_limited": false,
            "answers": [
                "pharmaceuticals",
                "cable tv",
                "fast food"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "pharmaceuticals": 0.29979428918187434,
                "cable tv": 0.48102098213170347,
                "fast food": 0.13183604734444246
            },
            "integer_answers": {
                "pharmaceuticals": 5,
                "cable tv": 4,
                "fast food": 3
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.213682647277762,
                    2.5821085824106538,
                    2.2042087703115842
                ],
                "result_count_important_words": [
                    98800.0,
                    74300.0,
                    85100.0
                ],
                "wikipedia_search": [
                    1.7061619231831997,
                    1.968393663074514,
                    1.325444413742286
                ],
                "word_count_appended_bing": [
                    26.0,
                    4.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.9848039215686275,
                    1.1642156862745097,
                    1.8509803921568628
                ],
                "cosine_similarity_raw": [
                    0.03379317745566368,
                    0.16536572575569153,
                    0.04472508281469345
                ],
                "result_count_noun_chunks": [
                    123000.0,
                    69000.0,
                    144000.0
                ],
                "question_answer_similarity": [
                    2.6720909513533115,
                    7.103068806231022,
                    10.761033833026886
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    300000.0,
                    64300.0,
                    303000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    147.0,
                    16.0,
                    13.0
                ],
                "answer_relation_to_question": [
                    1.6177984658098863,
                    3.5602963906460197,
                    0.8219051435440936
                ],
                "result_count": [
                    109000.0,
                    62300.0,
                    94800.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Bugs Bunny often chomps on what food?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "carrots"
            ],
            "question": "bugs bunny often chomps on what food?",
            "lines": [
                [
                    0.25853658536585367,
                    0.2916666666666667,
                    0.046032103146783486,
                    0.00714361089533562,
                    0.00035400318602867427,
                    0.248582995951417,
                    0.0002882675122513693,
                    0.141999141999142,
                    0.4107142857142857,
                    0.11538461538461539,
                    0.3783783783783784,
                    0.14212895863060124,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.25365853658536586,
                    0.3958333333333333,
                    0.07073503585148425,
                    0.7079650923314221,
                    0.005065617019124601,
                    0.20620782726045883,
                    0.05995964254828481,
                    0.2059202059202059,
                    0.5,
                    0.06108597285067873,
                    0.02702702702702703,
                    0.2969304178847729,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.4878048780487805,
                    0.3125,
                    0.8832328610017323,
                    0.28489129677324226,
                    0.9945803797948467,
                    0.5452091767881242,
                    0.9397520899394638,
                    0.6520806520806521,
                    0.08928571428571429,
                    0.8235294117647058,
                    0.5945945945945946,
                    0.5609406234846259,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "chimichangas",
                "candy corn",
                "carrots"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "candy corn": 0.18778389611689925,
                "chimichangas": 0.12261450690365217,
                "carrots": 0.8888277091488292
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.568515834522405,
                    1.1877216715390917,
                    2.2437624939385037
                ],
                "result_count_important_words": [
                    50.0,
                    10400.0,
                    163000.0
                ],
                "wikipedia_search": [
                    0.8214285714285714,
                    1.0,
                    0.17857142857142858
                ],
                "word_count_appended_bing": [
                    28.0,
                    2.0,
                    44.0
                ],
                "answer_relation_to_question_bing": [
                    0.2916666666666667,
                    0.3958333333333333,
                    0.3125
                ],
                "cosine_similarity_raw": [
                    0.0283212810754776,
                    0.043519776314496994,
                    0.5434095859527588
                ],
                "result_count_noun_chunks": [
                    99300.0,
                    144000.0,
                    456000.0
                ],
                "question_answer_similarity": [
                    0.0424124039709568,
                    4.203266657888889,
                    1.6914309784770012
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    24.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    20.0
                ],
                "result_count_bing": [
                    92100.0,
                    76400.0,
                    202000.0
                ],
                "word_count_appended": [
                    51.0,
                    27.0,
                    364.0
                ],
                "answer_relation_to_question": [
                    0.25853658536585367,
                    0.25365853658536586,
                    0.4878048780487805
                ],
                "result_count": [
                    42.0,
                    601.0,
                    118000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "The political practice known as gerrymandering is named for a governor of what state?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "massachusetts"
            ],
            "question": "the political practice known as gerrymandering is named for a governor of what state?",
            "lines": [
                [
                    0.519345238095238,
                    0.4074603174603175,
                    0.7724659785295726,
                    0.29558404045661757,
                    0.9678263144127649,
                    0.3258064516129032,
                    0.3191881918819188,
                    0.01474051460968164,
                    0.30564192777421273,
                    0.36257309941520466,
                    0.3867924528301887,
                    0.3326011184122643,
                    0.7083333333333334,
                    1.0,
                    1.0
                ],
                [
                    0.35019841269841273,
                    0.5411111111111111,
                    0.14064491447596086,
                    0.39835612865888514,
                    0.017525503531258174,
                    0.33548387096774196,
                    0.37084870848708484,
                    0.017095508068033144,
                    0.284124662863485,
                    0.3435672514619883,
                    0.33962264150943394,
                    0.3275069063764647,
                    0.2916666666666667,
                    0.0,
                    1.0
                ],
                [
                    0.1304563492063492,
                    0.051428571428571435,
                    0.08688910699446654,
                    0.3060598308844973,
                    0.014648182055976981,
                    0.3387096774193548,
                    0.30996309963099633,
                    0.9681639773222852,
                    0.41023340936230235,
                    0.29385964912280704,
                    0.27358490566037735,
                    0.339891975211271,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "massachusetts",
                "virginia",
                "alabama"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "alabama": 0.010517325141006152,
                "massachusetts": 0.9979126011306716,
                "virginia": 0.20033734714555146
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.32820782888585,
                    2.292548344635253,
                    2.379243826478897
                ],
                "result_count_important_words": [
                    173000.0,
                    201000.0,
                    168000.0
                ],
                "wikipedia_search": [
                    1.8338515666452762,
                    1.7047479771809095,
                    2.4614004561738136
                ],
                "word_count_appended_bing": [
                    41.0,
                    36.0,
                    29.0
                ],
                "answer_relation_to_question_bing": [
                    2.037301587301587,
                    2.7055555555555553,
                    0.2571428571428571
                ],
                "cosine_similarity_raw": [
                    0.16196520626544952,
                    0.029489431530237198,
                    0.01821829378604889
                ],
                "result_count_noun_chunks": [
                    169000.0,
                    196000.0,
                    11100000.0
                ],
                "question_answer_similarity": [
                    1.7056816490367055,
                    2.2987328320741653,
                    1.7661326918751001
                ],
                "word_count_noun_chunks": [
                    34.0,
                    14.0,
                    0.0
                ],
                "result_count_bing": [
                    1010000.0,
                    1040000.0,
                    1050000.0
                ],
                "word_count_raw": [
                    13.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    11100000.0,
                    201000.0,
                    168000.0
                ],
                "answer_relation_to_question": [
                    3.1160714285714284,
                    2.1011904761904763,
                    0.7827380952380951
                ],
                "word_count_appended": [
                    248.0,
                    235.0,
                    201.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which has been the last name of a U.S. president AND a British prime minister?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "wilson"
            ],
            "question": "which has been the last name of a u.s. president and a british prime minister?",
            "lines": [
                [
                    0.24744897959183673,
                    0.5416666666666667,
                    0.4629721749200566,
                    0.11028653253446033,
                    0.30697674418604654,
                    0.33712984054669703,
                    0.1988472622478386,
                    0.2657739906674782,
                    1.0,
                    0.2628696604600219,
                    0.26851851851851855,
                    0.3234512866107575,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.4260204081632653,
                    0.16666666666666669,
                    0.2888913102794935,
                    0.34598324083911763,
                    0.31937984496124033,
                    0.3365603644646925,
                    0.2132564841498559,
                    0.20267802799756543,
                    0.0,
                    0.33406352683461116,
                    0.2777777777777778,
                    0.32203552404848157,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.32653061224489793,
                    0.2916666666666667,
                    0.2481365148004499,
                    0.543730226626422,
                    0.3736434108527132,
                    0.32630979498861046,
                    0.5878962536023055,
                    0.5315479813349564,
                    0.0,
                    0.40306681270536693,
                    0.4537037037037037,
                    0.3545131893407609,
                    1.0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "hayes",
                "arthur",
                "wilson"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "arthur": 0.12254243125368253,
                "wilson": 0.5508558197017942,
                "hayes": 0.45869320323172674
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6172564330537875,
                    1.6101776202424078,
                    1.7725659467038046
                ],
                "result_count_important_words": [
                    1380000.0,
                    1480000.0,
                    4080000.0
                ],
                "wikipedia_search": [
                    1.0,
                    0.0,
                    0.0
                ],
                "answer_relation_to_question": [
                    0.9897959183673469,
                    1.7040816326530612,
                    1.3061224489795917
                ],
                "word_count_appended_bing": [
                    29.0,
                    30.0,
                    49.0
                ],
                "answer_relation_to_question_bing": [
                    1.0833333333333333,
                    0.3333333333333333,
                    0.5833333333333333
                ],
                "cosine_similarity_raw": [
                    0.038304783403873444,
                    0.023901909589767456,
                    0.020529992878437042
                ],
                "result_count_noun_chunks": [
                    1310000.0,
                    999000.0,
                    2620000.0
                ],
                "question_answer_similarity": [
                    0.3579774908721447,
                    1.123022091574967,
                    1.7648862265050411
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    3.0
                ],
                "result_count_bing": [
                    5920000.0,
                    5910000.0,
                    5730000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    1980000.0,
                    2060000.0,
                    2410000.0
                ],
                "word_count_appended": [
                    240.0,
                    305.0,
                    368.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What kind of song likely gets part of its name from the French word for \u201csing\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "sea shanty",
                "top"
            ],
            "question": "what kind of song likely gets part of its name from the french word for \u201csing\u201d?",
            "lines": [
                [
                    0.47183638224449626,
                    0.26199584199584197,
                    0.4058132104062899,
                    0.25584482710087536,
                    0.41721854304635764,
                    0.602068456045309,
                    0.29347826086956524,
                    0.3293963254593176,
                    0.5929092409487022,
                    0.5185185185185185,
                    0.4,
                    0.36912844611561585,
                    0.0,
                    0,
                    1.0
                ],
                [
                    0.3385451785596678,
                    0.183003003003003,
                    0.4748143260245087,
                    0.42672643388760284,
                    0.33112582781456956,
                    0.11228761388820488,
                    0.45652173913043476,
                    0.6692913385826772,
                    0.19013658349862547,
                    0.17592592592592593,
                    0.2,
                    0.3201083989122175,
                    0.0,
                    0,
                    1.0
                ],
                [
                    0.18961843919583596,
                    0.555001155001155,
                    0.11937246356920136,
                    0.3174287390115218,
                    0.25165562913907286,
                    0.2856439300664861,
                    0.25,
                    0.0013123359580052493,
                    0.21695417555267227,
                    0.3055555555555556,
                    0.4,
                    0.31076315497216667,
                    1.0,
                    0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "sea shanty",
                "power ballad",
                "christmas carol"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "christmas carol": 0.1805710137279658,
                "sea shanty": 0.5290859804339196,
                "power ballad": 0.0355600174483117
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.583899122809311,
                    2.2407587923855226,
                    2.1753420848051666
                ],
                "result_count_important_words": [
                    27.0,
                    42.0,
                    23.0
                ],
                "wikipedia_search": [
                    3.5574554456922134,
                    1.1408195009917528,
                    1.3017250533160336
                ],
                "word_count_appended_bing": [
                    4.0,
                    2.0,
                    4.0
                ],
                "answer_relation_to_question_bing": [
                    1.571975051975052,
                    1.0980180180180181,
                    3.3300069300069297
                ],
                "cosine_similarity_raw": [
                    0.05751848965883255,
                    0.06729845702648163,
                    0.016919419169425964
                ],
                "result_count_noun_chunks": [
                    25100.0,
                    51000.0,
                    100.0
                ],
                "question_answer_similarity": [
                    5.2156966113252565,
                    8.699318412691355,
                    6.471156822517514
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    4890000.0,
                    912000.0,
                    2320000.0
                ],
                "word_count_appended": [
                    56.0,
                    19.0,
                    33.0
                ],
                "answer_relation_to_question": [
                    3.302854675711474,
                    2.3698162499176747,
                    1.3273290743708517
                ],
                "result_count": [
                    63.0,
                    50.0,
                    38.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Who was NOT a wife of Henry VIII?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "catherine of york"
            ],
            "lines": [
                [
                    0.30416878387533874,
                    0.273227969348659,
                    0.24627353963669912,
                    0.45652423034116846,
                    0.24540162716018565,
                    0.48688658351789255,
                    0.24504072246510822,
                    0.2510118876884342,
                    0.4230769230769231,
                    0.26884422110552764,
                    0.24285714285714288,
                    0.2775194125226105,
                    0.13636363636363635,
                    0.2647058823529412,
                    0.0
                ],
                [
                    0.3618342193037315,
                    0.4308634373289546,
                    0.41860003039952504,
                    0.18501086754974566,
                    0.49997730599462153,
                    0.026168551113183336,
                    0.4999758534388964,
                    0.4999681295216241,
                    0.3205128205128205,
                    0.4623115577889447,
                    0.4857142857142857,
                    0.45132491706825284,
                    0.5,
                    0.5,
                    0.0
                ],
                [
                    0.33399699682092976,
                    0.2959085933223864,
                    0.33512642996377584,
                    0.3584649021090859,
                    0.2546210668451928,
                    0.4869448653689241,
                    0.2549834240959954,
                    0.2490199827899417,
                    0.2564102564102564,
                    0.26884422110552764,
                    0.27142857142857146,
                    0.2711556704091367,
                    0.36363636363636365,
                    0.23529411764705882,
                    0.0
                ]
            ],
            "fraction_answers": {
                "catherine of york": 0.19396257489505914,
                "catherine parr": 0.4111567768125332,
                "catherine howard": 0.39488064829240765
            },
            "question": "who was not a wife of henry viii?",
            "rate_limited": false,
            "answers": [
                "catherine parr",
                "catherine of york",
                "catherine howard"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "catherine of york": 0.5570992134828493,
                "catherine parr": 0.10179986881572885,
                "catherine howard": 0.16192750036978845
            },
            "integer_answers": {
                "catherine of york": 2,
                "catherine parr": 8,
                "catherine howard": 4
            },
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3348835248643371,
                    0.2920504975904829,
                    1.37306597754518
                ],
                "result_count_important_words": [
                    359000.0,
                    34.0,
                    345000.0
                ],
                "wikipedia_search": [
                    0.46153846153846156,
                    1.0769230769230769,
                    1.4615384615384617
                ],
                "word_count_appended_bing": [
                    36.0,
                    2.0,
                    32.0
                ],
                "answer_relation_to_question_bing": [
                    1.360632183908046,
                    0.41481937602627256,
                    1.2245484400656814
                ],
                "cosine_similarity_raw": [
                    0.22217632830142975,
                    0.07127812504768372,
                    0.14437203109264374
                ],
                "result_count_noun_chunks": [
                    375000.0,
                    48.0,
                    378000.0
                ],
                "question_answer_similarity": [
                    0.8619754314422607,
                    6.24515438079834,
                    2.8061556592583656
                ],
                "word_count_noun_chunks": [
                    8.0,
                    0.0,
                    3.0
                ],
                "result_count_bing": [
                    2250000.0,
                    81300000.0,
                    2240000.0
                ],
                "word_count_raw": [
                    8.0,
                    0.0,
                    9.0
                ],
                "result_count": [
                    359000.0,
                    32.0,
                    346000.0
                ],
                "answer_relation_to_question": [
                    1.1749872967479675,
                    0.828994684177611,
                    0.9960180190744216
                ],
                "word_count_appended": [
                    184.0,
                    30.0,
                    184.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "The author who created Tarzan once worked for which of these publications?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "sports illustrated"
            ],
            "question": "the author who created tarzan once worked for which of these publications?",
            "lines": [
                [
                    0.2583333333333333,
                    0.5,
                    0.1655355929452231,
                    0.4460939943065525,
                    0.2920517560073937,
                    0.07059736229635376,
                    0.29505582137161085,
                    0.28598130841121494,
                    0.29166666666666663,
                    0.3972602739726027,
                    0.18181818181818182,
                    0.3595669353246989,
                    0,
                    0,
                    0.0
                ],
                [
                    0.35,
                    0.25,
                    0.3796799826826019,
                    0.3265693592597802,
                    0.49168207024029575,
                    0.7551073183346263,
                    0.5151515151515151,
                    0.4953271028037383,
                    0.625,
                    0.3287671232876712,
                    0.5454545454545454,
                    0.36742688394315753,
                    0,
                    0,
                    0.0
                ],
                [
                    0.39166666666666666,
                    0.25,
                    0.45478442437217503,
                    0.22733664643366733,
                    0.21626617375231053,
                    0.1742953193690199,
                    0.189792663476874,
                    0.21869158878504674,
                    0.08333333333333333,
                    0.273972602739726,
                    0.2727272727272727,
                    0.27300618073214356,
                    0,
                    0,
                    0.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "sports illustrated",
                "paris review",
                "sears catalog"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sears catalog": 0.2275628238719517,
                "sports illustrated": 0.5289976242728175,
                "paris review": 0.042372621766740795
            },
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7978346766234945,
                    1.8371344197157877,
                    1.3650309036607178
                ],
                "result_count_important_words": [
                    18500.0,
                    32300.0,
                    11900.0
                ],
                "wikipedia_search": [
                    1.1666666666666665,
                    2.5,
                    0.3333333333333333
                ],
                "word_count_appended_bing": [
                    2.0,
                    6.0,
                    3.0
                ],
                "answer_relation_to_question_bing": [
                    0.5,
                    0.25,
                    0.25
                ],
                "cosine_similarity_raw": [
                    0.011957752518355846,
                    0.02742684632539749,
                    0.03285214677453041
                ],
                "result_count_noun_chunks": [
                    15300.0,
                    26500.0,
                    11700.0
                ],
                "question_answer_similarity": [
                    5.7964269034564495,
                    4.24335553497076,
                    2.953952015377581
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    27300.0,
                    292000.0,
                    67400.0
                ],
                "word_count_appended": [
                    29.0,
                    24.0,
                    20.0
                ],
                "answer_relation_to_question": [
                    1.0333333333333332,
                    1.4,
                    1.5666666666666667
                ],
                "result_count": [
                    15800.0,
                    26600.0,
                    11700.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Buying a can of soda will incur a higher tax in which city?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "seattle, wa"
            ],
            "question": "buying a can of soda will incur a higher tax in which city?",
            "lines": [
                [
                    0.4216417910447761,
                    0.2542372881355932,
                    0.2529294057713967,
                    0.44241537538975556,
                    0.1412536259301299,
                    0.1505488761108207,
                    0.2912256742915671,
                    0.0007211748016769296,
                    0.11364227877385773,
                    0.3387096774193548,
                    0.5263157894736842,
                    0.2852641526215574,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.23400852878464817,
                    0.3728813559322034,
                    0.323776209793746,
                    0.2886675495227158,
                    0.8134695421869088,
                    0.4547830632514375,
                    0.3789689313758962,
                    0.0007290997994975552,
                    0.31582562174667445,
                    0.3387096774193548,
                    0.21052631578947367,
                    0.35504795453041793,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.34434968017057566,
                    0.3728813559322034,
                    0.4232943844348573,
                    0.26891707508752866,
                    0.045276831882961284,
                    0.3946680606377418,
                    0.3298053943325367,
                    0.9985497253988255,
                    0.5705320994794679,
                    0.3225806451612903,
                    0.2631578947368421,
                    0.3596878928480247,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "santa barbara, ca",
                "denver, co",
                "seattle, wa"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "denver, co": 0.3462605807467021,
                "santa barbara, ca": 0.05206766096814396,
                "seattle, wa": 0.6605688207902691
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7115849157293443,
                    2.1302877271825076,
                    2.1581273570881483
                ],
                "result_count_important_words": [
                    8530.0,
                    11100.0,
                    9660.0
                ],
                "wikipedia_search": [
                    0.45456911509543085,
                    1.2633024869866976,
                    2.2821283979178713
                ],
                "answer_relation_to_question": [
                    0.8432835820895522,
                    0.46801705756929635,
                    0.6886993603411513
                ],
                "word_count_appended_bing": [
                    10.0,
                    4.0,
                    5.0
                ],
                "answer_relation_to_question_bing": [
                    0.2542372881355932,
                    0.3728813559322034,
                    0.3728813559322034
                ],
                "cosine_similarity_raw": [
                    0.03213835507631302,
                    0.041140470653772354,
                    0.05378570035099983
                ],
                "result_count_noun_chunks": [
                    91.0,
                    92.0,
                    126000.0
                ],
                "question_answer_similarity": [
                    11.573573343455791,
                    7.551534693688154,
                    7.034862857311964
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    576000.0,
                    1740000.0,
                    1510000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    112000.0,
                    645000.0,
                    35900.0
                ],
                "word_count_appended": [
                    21.0,
                    21.0,
                    20.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What author has the most books in Modern Library\u2019s 100 Best Novels of the 20th Century?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "james joyce"
            ],
            "question": "what author has the most books in modern library\u2019s 100 best novels of the 20th century?",
            "lines": [
                [
                    0.3959147979345026,
                    0.33066094549834385,
                    0.5653435255625224,
                    0.5100095953250813,
                    0.4464730290456432,
                    0.344954128440367,
                    0.21665788086452292,
                    0.44711538461538464,
                    0.28913972173231434,
                    0.36538461538461536,
                    0.2222222222222222,
                    0.34018368350223116,
                    0.7058823529411765,
                    0.5,
                    1.0
                ],
                [
                    0.35314080995978764,
                    0.2473913093831793,
                    0.2613470885399989,
                    0.20993861254807428,
                    0.25477178423236513,
                    0.318348623853211,
                    0.1191354770690564,
                    0.25240384615384615,
                    0.4388279621612955,
                    0.27884615384615385,
                    0.2962962962962963,
                    0.32397773876820435,
                    0.0,
                    0.16666666666666666,
                    1.0
                ],
                [
                    0.25094439210570985,
                    0.4219477451184768,
                    0.1733093858974787,
                    0.2800517921268445,
                    0.2987551867219917,
                    0.336697247706422,
                    0.6642066420664207,
                    0.3004807692307692,
                    0.27203231610639017,
                    0.3557692307692308,
                    0.48148148148148145,
                    0.33583857772956444,
                    0.29411764705882354,
                    0.3333333333333333,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "james joyce",
                "joseph conrad",
                "william faulkner"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "william faulkner": 0.2999918152494669,
                "james joyce": 0.5271662717831875,
                "joseph conrad": 0.0022115432960682097
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.0616531515200807,
                    2.9157996489138394,
                    3.02254719956608
                ],
                "result_count_important_words": [
                    41100.0,
                    22600.0,
                    126000.0
                ],
                "wikipedia_search": [
                    2.602257495590829,
                    3.9494516594516593,
                    2.4482908449575116
                ],
                "word_count_appended_bing": [
                    6.0,
                    8.0,
                    13.0
                ],
                "answer_relation_to_question_bing": [
                    2.975948509485095,
                    2.2265217844486136,
                    3.7975297060662916
                ],
                "cosine_similarity_raw": [
                    0.19631072878837585,
                    0.090750552713871,
                    0.06018020957708359
                ],
                "result_count_noun_chunks": [
                    37200.0,
                    21000.0,
                    25000.0
                ],
                "question_answer_similarity": [
                    2.7340611396357417,
                    1.1254396143485792,
                    1.50130258128047
                ],
                "word_count_noun_chunks": [
                    12.0,
                    0.0,
                    5.0
                ],
                "result_count_bing": [
                    752000.0,
                    694000.0,
                    734000.0
                ],
                "word_count_raw": [
                    3.0,
                    1.0,
                    2.0
                ],
                "result_count": [
                    53800.0,
                    30700.0,
                    36000.0
                ],
                "answer_relation_to_question": [
                    3.563233181410524,
                    3.1782672896380886,
                    2.2584995289513885
                ],
                "word_count_appended": [
                    76.0,
                    58.0,
                    74.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What male singing voice type is higher than a bass, but lower than a tenor?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "baritone"
            ],
            "question": "what male singing voice type is higher than a bass, but lower than a tenor?",
            "lines": [
                [
                    0.4096549375624534,
                    0.39072399011915143,
                    0.49569348773075655,
                    0.8881779034650438,
                    0.7974529412938899,
                    0.3333333333333333,
                    0.6557249533291848,
                    0.9243985184724081,
                    0.4654169831774517,
                    0.4609756097560976,
                    0.4537037037037037,
                    0.361081860975138,
                    0.8263888888888888,
                    0.75,
                    1.0
                ],
                [
                    0.41202479529940034,
                    0.35401502885373853,
                    0.4207132978234216,
                    0.1118220965349562,
                    0.20160887877517616,
                    0.3333333333333333,
                    0.3352520224019913,
                    0.07003019079336426,
                    0.4251797781259396,
                    0.33414634146341465,
                    0.2962962962962963,
                    0.37570085514234103,
                    0.1736111111111111,
                    0.25,
                    1.0
                ],
                [
                    0.17832026713814628,
                    0.25526098102711003,
                    0.08359321444582188,
                    0.0,
                    0.000938179930933988,
                    0.3333333333333333,
                    0.009023024268823895,
                    0.005571290734227644,
                    0.10940323869660858,
                    0.2048780487804878,
                    0.25,
                    0.26321728388252097,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "baritone",
                "countertenor",
                "sopranist"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "countertenor": 0.34490368762787527,
                "baritone": 0.7582099467942913,
                "sopranist": 0.02973743802582335
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.888654887801104,
                    3.0056068411387282,
                    2.1057382710601678
                ],
                "result_count_important_words": [
                    84300.0,
                    43100.0,
                    1160.0
                ],
                "wikipedia_search": [
                    3.723335865419614,
                    3.401438225007517,
                    0.8752259095728686
                ],
                "word_count_appended_bing": [
                    49.0,
                    32.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    3.1257919209532115,
                    2.8321202308299083,
                    2.0420878482168803
                ],
                "cosine_similarity_raw": [
                    0.2548410892486572,
                    0.21629300713539124,
                    0.042976126074790955
                ],
                "result_count_noun_chunks": [
                    297000.0,
                    22500.0,
                    1790.0
                ],
                "question_answer_similarity": [
                    3.4617759995162487,
                    0.43583954125642776,
                    0.0
                ],
                "word_count_noun_chunks": [
                    119.0,
                    25.0,
                    0.0
                ],
                "result_count_bing": [
                    1120000.0,
                    1120000.0,
                    1120000.0
                ],
                "word_count_raw": [
                    27.0,
                    9.0,
                    0.0
                ],
                "result_count": [
                    79900.0,
                    20200.0,
                    94.0
                ],
                "answer_relation_to_question": [
                    3.2772395004996273,
                    3.2961983623952027,
                    1.4265621371051702
                ],
                "word_count_appended": [
                    378.0,
                    274.0,
                    168.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What\u2019s another name for a garbanzo bean?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "chickpea"
            ],
            "lines": [
                [
                    0.4696969696969697,
                    0.4738095238095238,
                    0.0756876236539241,
                    0.31820440511081716,
                    0.08891389983117615,
                    0.2731535756154748,
                    0.05610724925521351,
                    0.01748367821179595,
                    0.25751485694843307,
                    0.1092896174863388,
                    0.047619047619047616,
                    0.38281645768353184,
                    0.007380073800738007,
                    0.0,
                    1.0
                ],
                [
                    0.509926854754441,
                    0.5071428571428571,
                    0.855025534685342,
                    0.09901777012832769,
                    0.7428249859313449,
                    0.45076201641266117,
                    0.8192651439920556,
                    0.9494301206152485,
                    0.5519706721810582,
                    0.7639344262295082,
                    0.9206349206349206,
                    0.37158422473885827,
                    0.988929889298893,
                    1.0,
                    1.0
                ],
                [
                    0.02037617554858934,
                    0.01904761904761905,
                    0.06928684166073387,
                    0.5827778247608552,
                    0.1682611142374789,
                    0.276084407971864,
                    0.12462760675273088,
                    0.033086201172955625,
                    0.19051447087050874,
                    0.126775956284153,
                    0.031746031746031744,
                    0.24559931757760983,
                    0.0036900369003690036,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "lima bean": 0.1841197841944989,
                "chickpea": 0.680746386910394,
                "black-eyed pea": 0.13513382889510708
            },
            "question": "what\u2019s another name for a garbanzo bean?",
            "rate_limited": false,
            "answers": [
                "lima bean",
                "chickpea",
                "black-eyed pea"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "lima bean": 0.42149974196037354,
                "chickpea": 0.7906221929744528,
                "black-eyed pea": 0.10789242084873253
            },
            "integer_answers": {
                "lima bean": 1,
                "chickpea": 12,
                "black-eyed pea": 1
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.7656329153670637,
                    0.7431684494777165,
                    0.49119863515521967
                ],
                "result_count_important_words": [
                    113000.0,
                    1650000.0,
                    251000.0
                ],
                "wikipedia_search": [
                    0.7725445708452992,
                    1.6559120165431747,
                    0.5715434126115262
                ],
                "word_count_appended_bing": [
                    6.0,
                    116.0,
                    4.0
                ],
                "answer_relation_to_question_bing": [
                    0.9476190476190476,
                    1.0142857142857142,
                    0.0380952380952381
                ],
                "cosine_similarity_raw": [
                    0.024287283420562744,
                    0.2743678092956543,
                    0.022233346477150917
                ],
                "result_count_noun_chunks": [
                    158000.0,
                    8580000.0,
                    299000.0
                ],
                "question_answer_similarity": [
                    3.672154745378066,
                    1.1426886888220906,
                    6.7253951244056225
                ],
                "word_count_noun_chunks": [
                    2.0,
                    268.0,
                    1.0
                ],
                "result_count_bing": [
                    466000.0,
                    769000.0,
                    471000.0
                ],
                "word_count_raw": [
                    0.0,
                    78.0,
                    0.0
                ],
                "result_count": [
                    158000.0,
                    1320000.0,
                    299000.0
                ],
                "answer_relation_to_question": [
                    0.9393939393939394,
                    1.019853709508882,
                    0.04075235109717868
                ],
                "word_count_appended": [
                    100.0,
                    699.0,
                    116.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which one of these was Georgia O\u2019Keeffe most likely to paint?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "cynomorphism"
            ],
            "lines": [
                [
                    0.0,
                    0.3333333333333333,
                    0.00399286694906737,
                    1.0,
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.0,
                    0,
                    0.06153846153846154,
                    0.03571428571428571,
                    0.19033958094266648,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.9333333333333332,
                    0.3333333333333333,
                    0.481354231467438,
                    -0.0,
                    0.075,
                    0.3333333333333333,
                    0.15384615384615385,
                    0.05128205128205128,
                    0,
                    0.35384615384615387,
                    0.48214285714285715,
                    0.30665821374096275,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.06666666666666667,
                    0.3333333333333333,
                    0.5146529015834946,
                    -0.0,
                    0.925,
                    0.3333333333333333,
                    0.8461538461538461,
                    0.9487179487179487,
                    0,
                    0.5846153846153846,
                    0.48214285714285715,
                    0.5030022053163709,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "ongle du doigt": 0.17802289652828615,
                "cynomorphism": 0.31855724193869245,
                "bromeliaceae": 0.5034198615330214
            },
            "question": "which one of these was georgia o\u2019keeffe most likely to paint?",
            "rate_limited": false,
            "answers": [
                "ongle du doigt",
                "cynomorphism",
                "bromeliaceae"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "ongle du doigt": 0.0890295821093803,
                "cynomorphism": 0.7160875612916177,
                "bromeliaceae": 0.5328584689211768
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.5710187428279995,
                    0.9199746412228882,
                    1.5090066159491125
                ],
                "result_count_important_words": [
                    0,
                    2.0,
                    11.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    2.0,
                    27.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "cosine_similarity_raw": [
                    0.0030207252129912376,
                    0.36415910720825195,
                    0.389350563287735
                ],
                "result_count_noun_chunks": [
                    0,
                    2.0,
                    37.0
                ],
                "question_answer_similarity": [
                    -1.3974651565076783,
                    0.0,
                    0.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    172000.0,
                    172000.0,
                    172000.0
                ],
                "word_count_appended": [
                    4.0,
                    23.0,
                    38.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    2.8,
                    0.2
                ],
                "result_count": [
                    0,
                    3.0,
                    37.0
                ]
            },
            "integer_answers": {
                "ongle du doigt": 2,
                "cynomorphism": 2,
                "bromeliaceae": 6
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "In the U.K., who appoints the Prime Minister?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "the queen"
            ],
            "lines": [
                [
                    0.08333333333333333,
                    0.0,
                    0.23007680203898415,
                    0.3687968130790466,
                    0.4738562091503268,
                    0.4454225352112676,
                    0.35352730171383023,
                    0.9238802972484434,
                    0.0,
                    0.25,
                    0.22727272727272727,
                    0.3332990332799033,
                    0.4,
                    0.0,
                    0.0
                ],
                [
                    0.7833333333333333,
                    0.0,
                    0.29401264765789115,
                    0.33006858967631847,
                    0.2995642701525055,
                    0.25264084507042256,
                    0.34834595456357115,
                    0.04800160674834304,
                    0.393939393939394,
                    0.24193548387096775,
                    0.22727272727272727,
                    0.32112897193316814,
                    0.0,
                    0.0,
                    0.0
                ],
                [
                    0.13333333333333333,
                    1.0,
                    0.4759105503031247,
                    0.301134597244635,
                    0.22657952069716775,
                    0.3019366197183099,
                    0.2981267437225986,
                    0.028118096003213498,
                    0.6060606060606061,
                    0.5080645161290323,
                    0.5454545454545454,
                    0.34557199478692857,
                    0.6,
                    1.0,
                    0.0
                ]
            ],
            "fraction_answers": {
                "the people": 0.2921046465948473,
                "the parliament": 0.25287455887276017,
                "the queen": 0.4550207945323925
            },
            "question": "in the u.k., who appoints the prime minister?",
            "rate_limited": false,
            "answers": [
                "the people",
                "the parliament",
                "the queen"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "the people": 0.16896246226856335,
                "the parliament": 0.33732834831409336,
                "the queen": 0.7362868318204314
            },
            "integer_answers": {
                "the people": 5,
                "the parliament": 1,
                "the queen": 8
            },
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3331961331196132,
                    1.2845158877326726,
                    1.3822879791477143
                ],
                "result_count_important_words": [
                    887000.0,
                    874000.0,
                    748000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.7878787878787878,
                    1.212121212121212
                ],
                "word_count_appended_bing": [
                    5.0,
                    5.0,
                    12.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    2.0
                ],
                "cosine_similarity_raw": [
                    0.07257325947284698,
                    0.09274058043956757,
                    0.15011674165725708
                ],
                "result_count_noun_chunks": [
                    46000000.0,
                    2390000.0,
                    1400000.0
                ],
                "question_answer_similarity": [
                    7.725611565634608,
                    6.914326868951321,
                    6.308213207870722
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    3.0
                ],
                "result_count_bing": [
                    5060000.0,
                    2870000.0,
                    3430000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    5.0
                ],
                "result_count": [
                    1740000.0,
                    1100000.0,
                    832000.0
                ],
                "answer_relation_to_question": [
                    0.16666666666666666,
                    1.5666666666666667,
                    0.26666666666666666
                ],
                "word_count_appended": [
                    62.0,
                    60.0,
                    126.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What sport is often played with an item resembling a Buckminsterfullerene molecule?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "soccer"
            ],
            "question": "what sport is often played with an item resembling a buckminsterfullerene molecule?",
            "lines": [
                [
                    0.37027417027417026,
                    0.3435972310586905,
                    0.24843343040371152,
                    0.14035855331122526,
                    0.5614439324116743,
                    0.23318385650224216,
                    0.5611028315946349,
                    0.6173126015316779,
                    0.32260580097177405,
                    0.375,
                    0.4166666666666667,
                    0.31746208626048306,
                    0.7068965517241379,
                    0.4,
                    1.0
                ],
                [
                    0.36869889369889375,
                    0.31308014812639684,
                    0.3926062067852669,
                    0.5711411901188566,
                    0.11674347158218126,
                    0.3004484304932735,
                    0.11475409836065574,
                    0.10187978649338594,
                    0.31091066726011646,
                    0.09935897435897435,
                    0.041666666666666664,
                    0.27075804402826853,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.261026936026936,
                    0.34332262081491266,
                    0.35896036281102156,
                    0.28850025656991823,
                    0.32181259600614437,
                    0.4663677130044843,
                    0.3241430700447094,
                    0.2808076119749362,
                    0.36648353176810944,
                    0.5256410256410257,
                    0.5416666666666666,
                    0.41177986971124847,
                    0.29310344827586204,
                    0.6,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "badminton",
                "american football",
                "soccer"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "american football": 0.07428085846904917,
                "soccer": 0.5831667336646107,
                "badminton": 0.06169558672011393
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9047725175628982,
                    1.6245482641696112,
                    2.470679218267491
                ],
                "result_count_important_words": [
                    7530.0,
                    1540.0,
                    4350.0
                ],
                "wikipedia_search": [
                    0.9678174029153221,
                    0.9327320017803493,
                    1.0994505953043283
                ],
                "word_count_appended_bing": [
                    30.0,
                    3.0,
                    39.0
                ],
                "answer_relation_to_question_bing": [
                    1.0307916931760714,
                    0.9392404443791905,
                    1.029967862444738
                ],
                "cosine_similarity_raw": [
                    0.07327590882778168,
                    0.11579994112253189,
                    0.10587603598833084
                ],
                "result_count_noun_chunks": [
                    26600.0,
                    4390.0,
                    12100.0
                ],
                "question_answer_similarity": [
                    1.3319926364347339,
                    5.420089062303305,
                    2.737846809439361
                ],
                "word_count_noun_chunks": [
                    41.0,
                    0.0,
                    17.0
                ],
                "word_count_raw": [
                    4.0,
                    0.0,
                    6.0
                ],
                "result_count_bing": [
                    10400.0,
                    13400.0,
                    20800.0
                ],
                "word_count_appended": [
                    117.0,
                    31.0,
                    164.0
                ],
                "answer_relation_to_question": [
                    1.1108225108225107,
                    1.1060966810966812,
                    0.783080808080808
                ],
                "result_count": [
                    7310.0,
                    1520.0,
                    4190.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "In gardening, what word refers to overall health and usability of soil?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "tilth"
            ],
            "question": "in gardening, what word refers to overall health and usability of soil?",
            "lines": [
                [
                    0.27358191423282474,
                    0.16139817629179332,
                    0.20667322793529933,
                    0.325572441072789,
                    0.7255661501787842,
                    0.24790419161676647,
                    0.5130560293924682,
                    0.5854077253218885,
                    0.3042929292929293,
                    0.3821292775665399,
                    0.3490566037735849,
                    0.3810715745951501,
                    0.0,
                    0,
                    1.0
                ],
                [
                    0.23521238845531753,
                    0.11387320885801129,
                    0.2497465906874346,
                    0.16247430273619828,
                    0.15941597139451727,
                    0.3461077844311377,
                    0.10379215326072694,
                    0.08927038626609442,
                    0.07575757575757576,
                    0.3517110266159696,
                    0.32075471698113206,
                    0.3505130151835071,
                    1.0,
                    0,
                    1.0
                ],
                [
                    0.49120569731185765,
                    0.7247286148501954,
                    0.543580181377266,
                    0.5119532561910127,
                    0.11501787842669844,
                    0.4059880239520958,
                    0.38315181734680487,
                    0.32532188841201715,
                    0.6199494949494949,
                    0.2661596958174905,
                    0.330188679245283,
                    0.26841541022134285,
                    0.0,
                    0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "loam",
                "tilth",
                "humus"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "loam": 0.16213096001758603,
                "tilth": 0.6130185961818516,
                "humus": 0.5322838156518898
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.667501022166051,
                    2.4535911062845495,
                    1.8789078715493999
                ],
                "result_count_important_words": [
                    39100.0,
                    7910.0,
                    29200.0
                ],
                "wikipedia_search": [
                    1.2171717171717171,
                    0.30303030303030304,
                    2.4797979797979797
                ],
                "answer_relation_to_question": [
                    1.3679095711641236,
                    1.1760619422765877,
                    2.4560284865592883
                ],
                "answer_relation_to_question_bing": [
                    0.8069908814589666,
                    0.5693660442900564,
                    3.623643074250977
                ],
                "word_count_appended": [
                    201.0,
                    185.0,
                    140.0
                ],
                "cosine_similarity_raw": [
                    0.04047388583421707,
                    0.04890916496515274,
                    0.10645211488008499
                ],
                "result_count_noun_chunks": [
                    68200.0,
                    10400.0,
                    37900.0
                ],
                "question_answer_similarity": [
                    0.6804847000166774,
                    0.3395904051139951,
                    1.0700425282120705
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    20700.0,
                    28900.0,
                    33900.0
                ],
                "result_count": [
                    48700.0,
                    10700.0,
                    7720.0
                ],
                "word_count_appended_bing": [
                    37.0,
                    34.0,
                    35.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these is a common piece of advice from a dentist?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "floss",
                "bottom"
            ],
            "question": "which of these is a common piece of advice from a dentist?",
            "lines": [
                [
                    0.31666666666666665,
                    0.16666666666666669,
                    0.1071885040592946,
                    0.5416016244963663,
                    0.6762972551868964,
                    0.3184931506849315,
                    0.7766465596395624,
                    0.8004883389374359,
                    0.1,
                    0.015625,
                    0.03508771929824561,
                    0.4068440030616322,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.24583333333333332,
                    0.0,
                    0.1438837505259066,
                    0.42902880200859417,
                    4.913270657340701e-06,
                    0.3184931506849315,
                    1.0110191900633357e-05,
                    5.336588926249573e-06,
                    0.0,
                    0.026785714285714284,
                    0.05263157894736842,
                    0.10813024323570135,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.4375,
                    0.8333333333333334,
                    0.7489277454147988,
                    0.029369573495039568,
                    0.3236978315424462,
                    0.363013698630137,
                    0.22334333016853689,
                    0.19950632447363786,
                    0.9,
                    0.9575892857142857,
                    0.9122807017543859,
                    0.48502575370266643,
                    1.0,
                    1.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "flop around on your bed",
                "flunk out of school",
                "floss"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "flop around on your bed": 0.04774683524133881,
                "floss": 0.9839885708506974,
                "flunk out of school": 0.07187425437973691
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6273760122465288,
                    0.4325209729428054,
                    1.9401030148106657
                ],
                "result_count_important_words": [
                    1690000.0,
                    22.0,
                    486000.0
                ],
                "wikipedia_search": [
                    0.1,
                    0.0,
                    0.9
                ],
                "answer_relation_to_question": [
                    1.2666666666666666,
                    0.9833333333333333,
                    1.75
                ],
                "result_count": [
                    2340000.0,
                    17.0,
                    1120000.0
                ],
                "answer_relation_to_question_bing": [
                    0.3333333333333333,
                    0.0,
                    1.6666666666666665
                ],
                "cosine_similarity_raw": [
                    0.028461463749408722,
                    0.03820504993200302,
                    0.19886068999767303
                ],
                "result_count_noun_chunks": [
                    1950000.0,
                    13.0,
                    486000.0
                ],
                "question_answer_similarity": [
                    19.615235418081284,
                    15.538175241556019,
                    1.0636805212125182
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    15.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    2.0
                ],
                "result_count_bing": [
                    279000.0,
                    279000.0,
                    318000.0
                ],
                "word_count_appended": [
                    7.0,
                    12.0,
                    429.0
                ],
                "word_count_appended_bing": [
                    2.0,
                    3.0,
                    52.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "In what medium was Superman\u2019s Kryptonite weakness introduced?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "action comics #64"
            ],
            "question": "in what medium was superman\u2019s kryptonite weakness introduced?",
            "lines": [
                [
                    0.528595988538682,
                    0.1111111111111111,
                    0.3199974317334616,
                    0.36130086985512666,
                    0.00012540102201832946,
                    0.852516582130316,
                    0.00022780884633191015,
                    0.00044692505170818914,
                    0.0,
                    0.5952380952380952,
                    0.5,
                    0.2057216283838355,
                    0,
                    0,
                    1.0
                ],
                [
                    0.1265329512893983,
                    0.2222222222222222,
                    0.14832114023809925,
                    0.32801193094915465,
                    0.00014978455407744907,
                    0.053452984783456885,
                    0.0003527362781913447,
                    0.0007275524097575172,
                    0.6111111111111112,
                    0.40476190476190477,
                    0.5,
                    0.22923171958408553,
                    0,
                    0,
                    1.0
                ],
                [
                    0.34487106017191976,
                    0.6666666666666666,
                    0.5316814280284391,
                    0.3106871991957187,
                    0.9997248144239043,
                    0.09403043308622708,
                    0.9994194548754768,
                    0.9988255225385343,
                    0.3888888888888889,
                    0.0,
                    0.0,
                    0.5650466520320789,
                    0,
                    0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "the radio series",
                "the 1978 movie",
                "action comics #64"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "action comics #64": 0.3932313165289848,
                "the 1978 movie": 0.10491225658319125,
                "the radio series": 0.09244420387557
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.0286081419191775,
                    1.1461585979204276,
                    2.825233260160395
                ],
                "result_count_important_words": [
                    31.0,
                    48.0,
                    136000.0
                ],
                "wikipedia_search": [
                    0.0,
                    1.8333333333333335,
                    1.1666666666666667
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    0.0
                ],
                "answer_relation_to_question_bing": [
                    0.3333333333333333,
                    0.6666666666666666,
                    2.0
                ],
                "cosine_similarity_raw": [
                    0.10560539364814758,
                    0.04894886910915375,
                    0.17546524107456207
                ],
                "result_count_noun_chunks": [
                    43.0,
                    70.0,
                    96100.0
                ],
                "question_answer_similarity": [
                    6.9841719581745565,
                    6.340675933053717,
                    6.005778024438769
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    437000.0,
                    27400.0,
                    48200.0
                ],
                "word_count_appended": [
                    25.0,
                    17.0,
                    0.0
                ],
                "answer_relation_to_question": [
                    2.64297994269341,
                    0.6326647564469914,
                    1.7243553008595989
                ],
                "result_count": [
                    36.0,
                    43.0,
                    287000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What zodiac sign features a creature that defends itself with an aculeus?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "scorpio"
            ],
            "question": "what zodiac sign features a creature that defends itself with an aculeus?",
            "lines": [
                [
                    0.022139713971397138,
                    0.047244094488188976,
                    0.3264634010054119,
                    0.6305505454466864,
                    0.9612908763196292,
                    0.6491803278688525,
                    0.9581355200690548,
                    0.9638721309201859,
                    0.14725274725274726,
                    0.3631578947368421,
                    0.2982456140350877,
                    0.3715530282413995,
                    0.1111111111111111,
                    0.0,
                    1.0
                ],
                [
                    0.5824700525608116,
                    0.41604212124086903,
                    0.39110448567633044,
                    0.12440305015352211,
                    0.00866878379538237,
                    0.17540983606557378,
                    0.017695295640914977,
                    0.02614243117675612,
                    0.8296703296703297,
                    0.2649122807017544,
                    0.2631578947368421,
                    0.271128105269488,
                    0.1111111111111111,
                    0.0,
                    1.0
                ],
                [
                    0.39539023346779123,
                    0.5367137842709421,
                    0.2824321133182577,
                    0.24504640439979145,
                    0.03004033988498841,
                    0.17540983606557378,
                    0.02416918429003021,
                    0.00998543790305804,
                    0.023076923076923078,
                    0.3719298245614035,
                    0.43859649122807015,
                    0.3573188664891125,
                    0.7777777777777778,
                    1.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "cancer",
                "sagittarius",
                "scorpio"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "scorpio": 0.8522540831031689,
                "sagittarius": 0.05105459835430173,
                "cancer": 0.07244822204177791
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.2293181694483972,
                    1.626768631616928,
                    2.143913198934675
                ],
                "result_count_important_words": [
                    1110000.0,
                    20500.0,
                    28000.0
                ],
                "wikipedia_search": [
                    0.589010989010989,
                    3.318681318681319,
                    0.09230769230769231
                ],
                "answer_relation_to_question": [
                    0.08855885588558855,
                    2.3298802102432465,
                    1.581560933871165
                ],
                "word_count_appended_bing": [
                    34.0,
                    30.0,
                    50.0
                ],
                "answer_relation_to_question_bing": [
                    0.09448818897637795,
                    0.8320842424817381,
                    1.0734275685418841
                ],
                "cosine_similarity_raw": [
                    0.0625738799571991,
                    0.0749637633562088,
                    0.0541343167424202
                ],
                "result_count_noun_chunks": [
                    1390000.0,
                    37700.0,
                    14400.0
                ],
                "question_answer_similarity": [
                    2.565511330962181,
                    0.5061567816883326,
                    0.9970165463164449
                ],
                "word_count_noun_chunks": [
                    1.0,
                    1.0,
                    7.0
                ],
                "result_count_bing": [
                    3960.0,
                    1070.0,
                    1070.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    2.0
                ],
                "result_count": [
                    1120000.0,
                    10100.0,
                    35000.0
                ],
                "word_count_appended": [
                    207.0,
                    151.0,
                    212.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Laurie Metcalf, Amy Morton and Tracy Letts are members of a theatre company from what city?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "chicago"
            ],
            "lines": [
                [
                    0.4354618473895583,
                    0.2949685534591195,
                    0.1970899495652382,
                    0.43169498387416644,
                    0.3888419273034658,
                    0.38092909535452324,
                    0.3898690950483779,
                    0.38513513513513514,
                    0.23854824469347158,
                    0.14402173913043478,
                    0.10416666666666667,
                    0.3436545663355241,
                    0.0962962962962963,
                    0.06451612903225806,
                    1.0
                ],
                [
                    0.25850066934404287,
                    0.44716981132075473,
                    0.06519089273322184,
                    0.3406846974583432,
                    0.2589461820231051,
                    0.4078239608801956,
                    0.2686397268070575,
                    0.26576576576576577,
                    0.28420241234874344,
                    0.07880434782608696,
                    0.10416666666666667,
                    0.3143246362966611,
                    0.022222222222222223,
                    0.0,
                    1.0
                ],
                [
                    0.3060374832663989,
                    0.2578616352201258,
                    0.73771915770154,
                    0.2276203186674904,
                    0.3522118906734291,
                    0.21124694376528116,
                    0.3414911781445646,
                    0.3490990990990991,
                    0.47724934295778504,
                    0.7771739130434783,
                    0.7916666666666666,
                    0.3420207973678148,
                    0.8814814814814815,
                    0.9354838709677419,
                    1.0
                ]
            ],
            "fraction_answers": {
                "new york": 0.2782281592345882,
                "los angeles": 0.22260299940663333,
                "chicago": 0.4991688413587784
            },
            "question": "laurie metcalf, amy morton and tracy letts are members of a theatre company from what city?",
            "rate_limited": false,
            "answers": [
                "new york",
                "los angeles",
                "chicago"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "new york": 0.10333804451297891,
                "los angeles": 0.10616857723550685,
                "chicago": 0.8232899199413797
            },
            "integer_answers": {
                "new york": 6,
                "los angeles": 2,
                "chicago": 6
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.436545663355241,
                    3.1432463629666105,
                    3.4202079736781483
                ],
                "result_count_important_words": [
                    1370.0,
                    944.0,
                    1200.0
                ],
                "wikipedia_search": [
                    0.9541929787738863,
                    1.1368096493949738,
                    1.9089973718311402
                ],
                "word_count_appended_bing": [
                    5.0,
                    5.0,
                    38.0
                ],
                "answer_relation_to_question_bing": [
                    1.4748427672955975,
                    2.2358490566037736,
                    1.2893081761006289
                ],
                "cosine_similarity_raw": [
                    0.04374731332063675,
                    0.01447017677128315,
                    0.16374874114990234
                ],
                "result_count_noun_chunks": [
                    1710.0,
                    1180.0,
                    1550.0
                ],
                "question_answer_similarity": [
                    8.457883653230965,
                    6.6747857658192515,
                    4.459598198533058
                ],
                "word_count_noun_chunks": [
                    13.0,
                    3.0,
                    119.0
                ],
                "result_count_bing": [
                    77900.0,
                    83400.0,
                    43200.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    29.0
                ],
                "word_count_appended": [
                    53.0,
                    29.0,
                    286.0
                ],
                "answer_relation_to_question": [
                    2.1773092369477913,
                    1.2925033467202143,
                    1.5301874163319946
                ],
                "result_count": [
                    1380.0,
                    919.0,
                    1250.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "For professional reasons, a phlebotomist should NOT be afraid of what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "thunder"
            ],
            "question": "for professional reasons, a phlebotomist should not be afraid of what?",
            "lines": [
                [
                    0.08333333333333331,
                    0.5,
                    0.25029383670004896,
                    0.3335373401602195,
                    0.2093295827473043,
                    0.3085289066971952,
                    0.18504479669193657,
                    0.44733475479744134,
                    0.5,
                    0.24959612277867527,
                    0.3412698412698413,
                    0.28338770345298886,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.41666666666666663,
                    0.0,
                    0.3832754285217247,
                    0.3511920890316521,
                    0.38396624472573837,
                    0.32942186605609614,
                    0.44038594073053067,
                    0.45778251599147124,
                    0.5,
                    0.38206785137318255,
                    0.35317460317460314,
                    0.36422136192998794,
                    0.5,
                    0.5,
                    1.0
                ],
                [
                    0.5,
                    0.5,
                    0.3664307347782263,
                    0.31527057080812837,
                    0.40670417252695734,
                    0.3620492272467086,
                    0.37456926257753276,
                    0.09488272921108742,
                    0.0,
                    0.3683360258481422,
                    0.3055555555555556,
                    0.35239093461702325,
                    0.5,
                    0.5,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "needles",
                "thunder",
                "snakes"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "thunder": 0.7784925821989044,
                "needles": 0.054978268691088114,
                "snakes": 0.7426740745844145
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7328983723760893,
                    1.0862291045600967,
                    1.1808725230638142
                ],
                "result_count_important_words": [
                    4570000.0,
                    865000.0,
                    1820000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_count_appended_bing": [
                    40.0,
                    37.0,
                    49.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    1.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.061258211731910706,
                    0.02863501012325287,
                    0.03276737034320831
                ],
                "result_count_noun_chunks": [
                    247000.0,
                    198000.0,
                    1900000.0
                ],
                "question_answer_similarity": [
                    2.038867197930813,
                    1.8226283825933933,
                    2.262602159753442
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    669000.0,
                    596000.0,
                    482000.0
                ],
                "word_count_appended": [
                    310.0,
                    146.0,
                    163.0
                ],
                "answer_relation_to_question": [
                    1.6666666666666665,
                    0.3333333333333333,
                    0.0
                ],
                "result_count": [
                    124000.0,
                    49500.0,
                    39800.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What 20th-century comic strip spun off a theme park in Arkansas?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "li'l abner",
                "top"
            ],
            "question": "what 20th-century comic strip spun off a theme park in arkansas?",
            "lines": [
                [
                    0.2584995423174943,
                    0.46078431372549017,
                    0.8076643655371737,
                    -0.628938336813255,
                    0.10472972972972973,
                    0.11491260691706955,
                    0.12178170905070289,
                    0.1480817497310864,
                    0.3171675082455881,
                    0.14814814814814814,
                    0.2247191011235955,
                    0.34923286895637684,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.2126215019704274,
                    0.19963057686842853,
                    0.08932720670489355,
                    0.5976645411872036,
                    0.31672297297297297,
                    0.44254369654146525,
                    0.3490759753593429,
                    0.1312298314808175,
                    0.0008250825082508251,
                    0.3789173789173789,
                    0.3258426966292135,
                    0.3030587454314051,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.5288789557120783,
                    0.3395851094060813,
                    0.1030084277579328,
                    1.0312737956260514,
                    0.5785472972972973,
                    0.44254369654146525,
                    0.5291423155899542,
                    0.7206884187880961,
                    0.682007409246161,
                    0.47293447293447294,
                    0.449438202247191,
                    0.347708385612218,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "li'l abner",
                "pogo",
                "peanuts"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "peanuts": 0.13790929378666988,
                "li'l abner": 0.9822883963697759,
                "pogo": 0.042518397389605354
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.7938629516510147,
                    2.4244699634512408,
                    2.781667084897744
                ],
                "result_count_important_words": [
                    7710.0,
                    22100.0,
                    33500.0
                ],
                "wikipedia_search": [
                    1.9030050494735287,
                    0.0049504950495049506,
                    4.092044455476966
                ],
                "word_count_appended_bing": [
                    20.0,
                    29.0,
                    40.0
                ],
                "answer_relation_to_question_bing": [
                    1.3823529411764706,
                    0.5988917306052856,
                    1.0187553282182438
                ],
                "cosine_similarity_raw": [
                    0.2890022397041321,
                    0.03196347877383232,
                    0.03685895726084709
                ],
                "result_count_noun_chunks": [
                    41300.0,
                    36600.0,
                    201000.0
                ],
                "question_answer_similarity": [
                    -0.9952419217443094,
                    0.9457537753914949,
                    1.6319038833025843
                ],
                "word_count_noun_chunks": [
                    12.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    30900.0,
                    119000.0,
                    119000.0
                ],
                "word_count_appended": [
                    52.0,
                    133.0,
                    166.0
                ],
                "answer_relation_to_question": [
                    1.5509972539049657,
                    1.2757290118225644,
                    3.17327373427247
                ],
                "result_count": [
                    24800.0,
                    75000.0,
                    137000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these Kentucky Derby winners was named for its trainer?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "clyde van dusen"
            ],
            "lines": [
                [
                    0.8251539164582643,
                    0.5916666666666667,
                    0.47604286124139544,
                    -0.07717610215685004,
                    0.009058771542200618,
                    0.36471830310010556,
                    0.3710777626193724,
                    0.008776458440887971,
                    0.7333333333333334,
                    0.5677966101694916,
                    0.39285714285714285,
                    0.31441719635028254,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.03513513513513514,
                    0.125,
                    0.303059475963187,
                    0.6356224678306841,
                    0.8395934600088378,
                    0.5681927248296381,
                    0.6275579809004093,
                    0.8384099122354156,
                    0.0,
                    0.3474576271186441,
                    0.42857142857142855,
                    0.38515423088123035,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.13971094840660062,
                    0.2833333333333333,
                    0.2208976627954176,
                    0.4415536343261659,
                    0.15134776844896156,
                    0.06708897207025626,
                    0.001364256480218281,
                    0.15281362932369644,
                    0.26666666666666666,
                    0.0847457627118644,
                    0.17857142857142858,
                    0.3004285727684871,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "lieut. gibson": 0.1907102196585914,
                "paul jones": 0.4278128702895509,
                "clyde van dusen": 0.3814769100518578
            },
            "question": "which of these kentucky derby winners was named for its trainer?",
            "rate_limited": false,
            "answers": [
                "clyde van dusen",
                "paul jones",
                "lieut. gibson"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "lieut. gibson": 0.12056407770616383,
                "paul jones": 0.20995528245031492,
                "clyde van dusen": 0.4465518914930251
            },
            "integer_answers": {
                "lieut. gibson": 0,
                "paul jones": 7,
                "clyde van dusen": 5
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.5720859817514126,
                    1.9257711544061518,
                    1.5021428638424357
                ],
                "result_count_important_words": [
                    27200.0,
                    46000.0,
                    100.0
                ],
                "wikipedia_search": [
                    2.2,
                    0.0,
                    0.8
                ],
                "answer_relation_to_question": [
                    4.125769582291321,
                    0.17567567567567566,
                    0.6985547420330029
                ],
                "word_count_appended_bing": [
                    11.0,
                    12.0,
                    5.0
                ],
                "answer_relation_to_question_bing": [
                    2.3666666666666667,
                    0.5,
                    1.1333333333333333
                ],
                "cosine_similarity_raw": [
                    0.11097888648509979,
                    0.07065162807703018,
                    0.05149741470813751
                ],
                "result_count_noun_chunks": [
                    85.0,
                    8120.0,
                    1480.0
                ],
                "question_answer_similarity": [
                    -0.440953366458416,
                    3.6316924430429935,
                    2.5228607831522822
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    380000.0,
                    592000.0,
                    69900.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    82.0,
                    7600.0,
                    1370.0
                ],
                "word_count_appended": [
                    67.0,
                    41.0,
                    10.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "The inventor of the mood ring was also behind what \u201890s fad?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "thighmaster"
            ],
            "question": "the inventor of the mood ring was also behind what \u201890s fad?",
            "lines": [
                [
                    0.3817235772357724,
                    0.2796875,
                    0.5697415100381881,
                    0.0,
                    0.46,
                    0.23951352692975925,
                    0.35802469135802467,
                    0.0024513809445987906,
                    0.36470362371010323,
                    0.7266666666666667,
                    0.7021276595744681,
                    0.40902043455155346,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.28866124661246617,
                    0.371875,
                    0.3374427630892949,
                    0.5290392607579214,
                    0.22,
                    0.09282700421940929,
                    0.30864197530864196,
                    0.0006537015852263442,
                    0.17445850046282013,
                    0.1,
                    0.0851063829787234,
                    0.26415706543998424,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.3296151761517615,
                    0.3484375,
                    0.09281572687251709,
                    0.4709607392420786,
                    0.32,
                    0.6676594688508315,
                    0.3333333333333333,
                    0.9968949174701749,
                    0.4608378758270767,
                    0.17333333333333334,
                    0.2127659574468085,
                    0.32682250000846225,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "thighmaster",
                "slap bracelet",
                "troll dolls"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "thighmaster": 0.8131681804349941,
                "slap bracelet": 0.11702191227204317,
                "troll dolls": 0.07869694159535148
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.045102172757767,
                    1.3207853271999213,
                    1.6341125000423111
                ],
                "result_count_important_words": [
                    29.0,
                    25.0,
                    27.0
                ],
                "wikipedia_search": [
                    1.823518118550516,
                    0.8722925023141006,
                    2.3041893791353836
                ],
                "word_count_appended_bing": [
                    33.0,
                    4.0,
                    10.0
                ],
                "answer_relation_to_question_bing": [
                    0.559375,
                    0.74375,
                    0.696875
                ],
                "cosine_similarity_raw": [
                    0.05774715542793274,
                    0.034202106297016144,
                    0.00940750166773796
                ],
                "result_count_noun_chunks": [
                    300.0,
                    80.0,
                    122000.0
                ],
                "question_answer_similarity": [
                    0.0,
                    5.286474524065852,
                    4.706119440495968
                ],
                "word_count_noun_chunks": [
                    8.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    96500.0,
                    37400.0,
                    269000.0
                ],
                "word_count_appended": [
                    109.0,
                    15.0,
                    26.0
                ],
                "answer_relation_to_question": [
                    1.9086178861788619,
                    1.4433062330623307,
                    1.6480758807588076
                ],
                "result_count": [
                    46.0,
                    22.0,
                    32.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Who is considered the father of scientific classification?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "carl linnaeus"
            ],
            "question": "who is considered the father of scientific classification?",
            "lines": [
                [
                    0.021276595744680854,
                    0.0,
                    0.004893462952709654,
                    0.14213288719367054,
                    0.1939286074079049,
                    0.09529627367135003,
                    0.31964091403699674,
                    0.06925276761959803,
                    0.08333333333333333,
                    0.25316455696202533,
                    0.15789473684210525,
                    0.3492524814181582,
                    0.0,
                    0.0,
                    0.0
                ],
                [
                    0.9648345153664303,
                    1.0,
                    0.9874293073994811,
                    -0.12235054068575997,
                    0.8059370697471372,
                    0.26328649969456325,
                    0.6800870511425462,
                    0.9291828173420887,
                    0.6288759689922481,
                    0.6624472573839663,
                    0.7368421052631579,
                    0.5194243635590828,
                    1.0,
                    1.0,
                    0.0
                ],
                [
                    0.01388888888888889,
                    0.0,
                    0.007677229647809222,
                    0.9802176534920894,
                    0.0001343228449578562,
                    0.6414172266340867,
                    0.0002720348204570185,
                    0.0015644150383132217,
                    0.2877906976744186,
                    0.08438818565400844,
                    0.10526315789473684,
                    0.13132315502275907,
                    0.0,
                    0.0,
                    0.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "jose reyes",
                "carl linnaeus",
                "doctor zaius"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "carl linnaeus": 0.9560199305994769,
                "jose reyes": 0.1528596935402898,
                "doctor zaius": 0.02495092804725218
            },
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3970099256726325,
                    2.077697454236331,
                    0.5252926200910362
                ],
                "result_count_important_words": [
                    94000.0,
                    200000.0,
                    80.0
                ],
                "wikipedia_search": [
                    0.3333333333333333,
                    2.5155038759689923,
                    1.1511627906976745
                ],
                "word_count_appended_bing": [
                    3.0,
                    14.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    2.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.0039000932592898607,
                    0.7869818210601807,
                    0.00611875718459487
                ],
                "result_count_noun_chunks": [
                    27800.0,
                    373000.0,
                    628.0
                ],
                "question_answer_similarity": [
                    0.41381156048737466,
                    -0.3562164194881916,
                    2.853846177458763
                ],
                "word_count_noun_chunks": [
                    0.0,
                    5.0,
                    0.0
                ],
                "result_count_bing": [
                    312000.0,
                    862000.0,
                    2100000.0
                ],
                "word_count_raw": [
                    0.0,
                    16.0,
                    0.0
                ],
                "result_count": [
                    46200.0,
                    192000.0,
                    32.0
                ],
                "answer_relation_to_question": [
                    0.0851063829787234,
                    3.8593380614657207,
                    0.05555555555555555
                ],
                "word_count_appended": [
                    60.0,
                    157.0,
                    20.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "The TSA allows which of these as a carry-on item?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "bowling ball"
            ],
            "question": "the tsa allows which of these as a carry-on item?",
            "lines": [
                [
                    0.08783783783783784,
                    0.19047619047619047,
                    0.5184736303866666,
                    0.39142781623603307,
                    0.007357244377560405,
                    0.5120578778135049,
                    0.005791390530777957,
                    0.5639189918109155,
                    0.3,
                    0.3333333333333333,
                    0.5,
                    0.41817655531799347,
                    0,
                    1.0,
                    -1.0
                ],
                [
                    0.7618243243243243,
                    0.6587301587301587,
                    0.41861224402160874,
                    0.2936735653919483,
                    0.0061031686313853355,
                    0.24758842443729903,
                    0.0031046629649531315,
                    0.002108566665032119,
                    0.0,
                    0.25396825396825395,
                    0.25,
                    0.21922543926169258,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.15033783783783783,
                    0.15079365079365079,
                    0.06291412559172467,
                    0.31489861837201866,
                    0.9865395869910543,
                    0.24035369774919615,
                    0.9911039465042689,
                    0.43397244152405234,
                    0.7,
                    0.4126984126984127,
                    0.25,
                    0.3625980054203139,
                    0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "bowling ball",
                "kayak paddle",
                "cricket bat"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "kayak paddle": 0.5403381919653253,
                "bowling ball": 0.6567002338661903,
                "cricket bat": 0.10264073518269452
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6727062212719739,
                    0.8769017570467703,
                    1.4503920216812556
                ],
                "result_count_important_words": [
                    97.0,
                    52.0,
                    16600.0
                ],
                "wikipedia_search": [
                    0.6,
                    0.0,
                    1.4
                ],
                "word_count_appended_bing": [
                    4.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.38095238095238093,
                    1.3174603174603174,
                    0.30158730158730157
                ],
                "cosine_similarity_raw": [
                    0.06620956212282181,
                    0.053457170724868774,
                    0.008034192025661469
                ],
                "result_count_noun_chunks": [
                    23000.0,
                    86.0,
                    17700.0
                ],
                "question_answer_similarity": [
                    4.366573383100331,
                    3.27607574313879,
                    3.512851842213422
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    637000.0,
                    308000.0,
                    299000.0
                ],
                "word_count_appended": [
                    42.0,
                    32.0,
                    52.0
                ],
                "answer_relation_to_question": [
                    0.35135135135135137,
                    3.0472972972972974,
                    0.6013513513513513
                ],
                "result_count": [
                    88.0,
                    73.0,
                    11800.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these games is typically played using a deck with the fewest cards?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "euchre"
            ],
            "question": "which of these games is typically played using a deck with the fewest cards?",
            "lines": [
                [
                    0.2376848107483036,
                    0.24792149866190902,
                    0.558711427149015,
                    0.19481722673066162,
                    0.22325231771485843,
                    0.017753442840550856,
                    0.38003220611916266,
                    0.33462033462033464,
                    0.2864770837348409,
                    0.34403997144896503,
                    0.25217391304347825,
                    0.33526405365168327,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.2495778692200557,
                    0.188016651798989,
                    0.2803834321812879,
                    0.5690203108619913,
                    0.2931596091205212,
                    0.4911232785797246,
                    0.5893719806763285,
                    0.435006435006435,
                    0.21023951262339668,
                    0.3390435403283369,
                    0.36086956521739133,
                    0.318236388866134,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.5127373200316406,
                    0.564061849539102,
                    0.1609051406696971,
                    0.23616246240734706,
                    0.4835880731646204,
                    0.4911232785797246,
                    0.030595813204508857,
                    0.23037323037323038,
                    0.5032834036417625,
                    0.3169164882226981,
                    0.3869565217391304,
                    0.3464995574821828,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "euchre",
                "cribbage",
                "pinochle"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "pinochle": 0.5221351734360029,
                "cribbage": 0.003773387394301467,
                "euchre": 0.6273444293988752
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.0115843219100995,
                    1.909418333196804,
                    2.078997344893097
                ],
                "result_count_important_words": [
                    2360000.0,
                    3660000.0,
                    190000.0
                ],
                "wikipedia_search": [
                    1.4323854186742042,
                    1.0511975631169832,
                    2.516417018208812
                ],
                "answer_relation_to_question": [
                    1.188424053741518,
                    1.2478893461002785,
                    2.5636866001582033
                ],
                "word_count_appended_bing": [
                    58.0,
                    83.0,
                    89.0
                ],
                "answer_relation_to_question_bing": [
                    1.2396074933095451,
                    0.9400832589949449,
                    2.82030924769551
                ],
                "cosine_similarity_raw": [
                    0.13139665126800537,
                    0.0659400224685669,
                    0.03784135356545448
                ],
                "result_count_noun_chunks": [
                    130000.0,
                    169000.0,
                    89500.0
                ],
                "question_answer_similarity": [
                    0.30370713025331497,
                    0.8870649098535068,
                    0.36816160939633846
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    32100.0,
                    888000.0,
                    888000.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    89100.0,
                    117000.0,
                    193000.0
                ],
                "word_count_appended": [
                    482.0,
                    475.0,
                    444.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these foods is cultivated in a paddy?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "rice"
            ],
            "lines": [
                [
                    0.6470588235294118,
                    0.0,
                    0.9198372258277894,
                    0.5513478834851779,
                    0.40529184882736236,
                    0.7700857415052398,
                    0.8946813930006701,
                    0.4629239035266585,
                    1.0,
                    0.6532951289398281,
                    0.6945454545454546,
                    0.46776445740156253,
                    0.9299363057324841,
                    1.0,
                    -1.0
                ],
                [
                    0.11764705882352941,
                    0.0,
                    0.021196873105935102,
                    0.44865211651482206,
                    0.5946995972743206,
                    0.0219117180057161,
                    0.10529243604701764,
                    0.5370718880742186,
                    0.0,
                    0.2979942693409742,
                    0.1709090909090909,
                    0.44364632348148075,
                    0.07006369426751592,
                    0.0,
                    -1.0
                ],
                [
                    0.23529411764705882,
                    1.0,
                    0.05896590106627548,
                    0.0,
                    8.553898316959406e-06,
                    0.20800254048904415,
                    2.61709523122645e-05,
                    4.2083991229696226e-06,
                    0.0,
                    0.04871060171919771,
                    0.13454545454545455,
                    0.08858921911695676,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "cake": 0.20207750470318725,
                "rice": 0.6711977261658314,
                "dunkaroos": 0.12672476913098138
            },
            "question": "which of these foods is cultivated in a paddy?",
            "rate_limited": false,
            "answers": [
                "rice",
                "cake",
                "dunkaroos"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "cake": 0.23317989565633634,
                "rice": 0.8232899199413797,
                "dunkaroos": 0.12307128860687358
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.4032933722046876,
                    1.3309389704444423,
                    0.26576765735087027
                ],
                "result_count_important_words": [
                    1470000.0,
                    173000.0,
                    43.0
                ],
                "wikipedia_search": [
                    3.0,
                    0.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    191.0,
                    47.0,
                    37.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "cosine_similarity_raw": [
                    0.5456728339195251,
                    0.012574570253491402,
                    0.03498020023107529
                ],
                "result_count_noun_chunks": [
                    4620000.0,
                    5360000.0,
                    42.0
                ],
                "question_answer_similarity": [
                    2.9255062341690063,
                    2.380592368543148,
                    0.0
                ],
                "word_count_noun_chunks": [
                    146.0,
                    11.0,
                    0.0
                ],
                "word_count_raw": [
                    55.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    4850000.0,
                    138000.0,
                    1310000.0
                ],
                "word_count_appended": [
                    684.0,
                    312.0,
                    51.0
                ],
                "answer_relation_to_question": [
                    1.2941176470588236,
                    0.23529411764705882,
                    0.47058823529411764
                ],
                "result_count": [
                    1990000.0,
                    2920000.0,
                    42.0
                ]
            },
            "integer_answers": {
                "cake": 2,
                "rice": 11,
                "dunkaroos": 1
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "The government vehicle called Marine One is usually what kind of machine?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "helicopter"
            ],
            "lines": [
                [
                    0.31666666666666665,
                    0.3333333333333333,
                    0.2106461397030406,
                    0.3075435627728009,
                    0.07472843868603586,
                    0.3502262443438914,
                    0.2762589928057554,
                    0.24110218140068887,
                    0.3075547640765032,
                    0.24539877300613497,
                    0.2542372881355932,
                    0.328640037496878,
                    0,
                    0.04,
                    1.0
                ],
                [
                    0.2785714285714285,
                    0.26666666666666666,
                    0.6693239726641527,
                    0.4695321523393157,
                    0.7839288051302186,
                    0.3040723981900452,
                    0.42589928057553955,
                    0.505166475315729,
                    0.49782825869782393,
                    0.40245398773006136,
                    0.5,
                    0.3399283533596437,
                    0,
                    0.92,
                    1.0
                ],
                [
                    0.4047619047619048,
                    0.4,
                    0.12002988763280673,
                    0.22292428488788338,
                    0.1413427561837456,
                    0.34570135746606334,
                    0.29784172661870506,
                    0.2537313432835821,
                    0.1946169772256729,
                    0.3521472392638037,
                    0.2457627118644068,
                    0.3314316091434783,
                    0,
                    0.04,
                    1.0
                ]
            ],
            "fraction_answers": {
                "helicopter": 0.48949013686466347,
                "battleship": 0.25771475371785024,
                "limousine": 0.25279510941748634
            },
            "question": "the government vehicle called marine one is usually what kind of machine?",
            "rate_limited": false,
            "answers": [
                "limousine",
                "helicopter",
                "battleship"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "helicopter": 0.7117287636512574,
                "battleship": 0.16405797149988455,
                "limousine": 0.12102636375622179
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.3004802624781457,
                    2.379498473517506,
                    2.3200212640043483
                ],
                "result_count_important_words": [
                    576000.0,
                    888000.0,
                    621000.0
                ],
                "wikipedia_search": [
                    2.1528833485355223,
                    3.4847978108847677,
                    1.3623188405797102
                ],
                "answer_relation_to_question": [
                    1.9,
                    1.6714285714285713,
                    2.428571428571429
                ],
                "result_count": [
                    57100.0,
                    599000.0,
                    108000.0
                ],
                "answer_relation_to_question_bing": [
                    1.6666666666666665,
                    1.3333333333333333,
                    2.0
                ],
                "cosine_similarity_raw": [
                    0.07614126801490784,
                    0.24193738400936127,
                    0.043386638164520264
                ],
                "result_count_noun_chunks": [
                    630000.0,
                    1320000.0,
                    663000.0
                ],
                "question_answer_similarity": [
                    1.7929231487214565,
                    2.737287223339081,
                    1.2996081179007888
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    23.0,
                    1.0
                ],
                "result_count_bing": [
                    387000.0,
                    336000.0,
                    382000.0
                ],
                "word_count_appended": [
                    200.0,
                    328.0,
                    287.0
                ],
                "word_count_appended_bing": [
                    30.0,
                    59.0,
                    29.0
                ]
            },
            "integer_answers": {
                "helicopter": 10,
                "battleship": 2,
                "limousine": 1
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which former NFL star does NOT have a football video game named after him?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "brett favre"
            ],
            "lines": [
                [
                    0.32637875415405604,
                    0.3192033732665628,
                    0.3535789698954476,
                    0.3717581617080084,
                    0.462521055586749,
                    0.3381294964028777,
                    0.4610874200426439,
                    0.4733650221275201,
                    0.414966373785995,
                    0.3588082901554404,
                    0.4491525423728814,
                    0.334961989489444,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.36983581196118165,
                    0.3516931149235413,
                    0.35226734504023033,
                    0.32728874703125244,
                    0.08169567658618754,
                    0.3237410071942446,
                    0.08315565031982941,
                    0.05908867398787082,
                    0.2899205822012069,
                    0.3354922279792746,
                    0.2711864406779661,
                    0.3293562249588342,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.3037854338847623,
                    0.329103511809896,
                    0.29415368506432205,
                    0.3009530912607392,
                    0.45578326782706347,
                    0.3381294964028777,
                    0.4557569296375267,
                    0.4675463038846091,
                    0.2951130440127982,
                    0.30569948186528495,
                    0.2796610169491526,
                    0.3356817855517218,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "kurt warner": 0.30643882530820765,
                "brett favre": 0.4708797495230634,
                "emmitt smith": 0.22268142516872894
            },
            "question": "which former nfl star does not have a football video game named after him?",
            "rate_limited": false,
            "answers": [
                "emmitt smith",
                "brett favre",
                "kurt warner"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "kurt warner": 0.10641979500881776,
                "brett favre": 0.29751448084320553,
                "emmitt smith": 0.12401293458063398
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.980456126126672,
                    2.0477253004939895,
                    1.971818573379339
                ],
                "result_count_important_words": [
                    36500.0,
                    391000.0,
                    41500.0
                ],
                "wikipedia_search": [
                    1.020403514568061,
                    2.5209530135855176,
                    2.458643471846422
                ],
                "word_count_appended_bing": [
                    6.0,
                    27.0,
                    26.0
                ],
                "answer_relation_to_question_bing": [
                    2.1695595208012466,
                    1.779682620917505,
                    2.050757858281248
                ],
                "cosine_similarity_raw": [
                    0.031085189431905746,
                    0.03136364743113518,
                    0.0437011793255806
                ],
                "result_count_noun_chunks": [
                    32500.0,
                    538000.0,
                    39600.0
                ],
                "question_answer_similarity": [
                    1.8515626415610313,
                    2.4936144711682573,
                    2.8738501026527956
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    1800000.0,
                    1960000.0,
                    1800000.0
                ],
                "word_count_appended": [
                    109.0,
                    127.0,
                    150.0
                ],
                "answer_relation_to_question": [
                    2.083454950151328,
                    1.56197025646582,
                    2.354574793382852
                ],
                "result_count": [
                    26700.0,
                    298000.0,
                    31500.0
                ]
            },
            "integer_answers": {
                "kurt warner": 4,
                "brett favre": 7,
                "emmitt smith": 1
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these consists of frozen water?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "snowflake"
            ],
            "lines": [
                [
                    0.7222222222222222,
                    0.0,
                    0.7353107130701382,
                    0.1960033110082587,
                    0.11791383219954649,
                    0.36045845272206306,
                    0.26,
                    0.24110671936758893,
                    0.6818181818181819,
                    0.532967032967033,
                    0.6413043478260869,
                    0.38903858605944897,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.05555555555555555,
                    1.0,
                    0.14354983334431318,
                    0.7430981882193818,
                    0.7709750566893424,
                    0.2636103151862464,
                    0.516,
                    0.541501976284585,
                    0.0,
                    0.041758241758241756,
                    0.06521739130434782,
                    0.27362943012434965,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.2222222222222222,
                    0.0,
                    0.1211394535855486,
                    0.06089850077235948,
                    0.1111111111111111,
                    0.37593123209169055,
                    0.224,
                    0.21739130434782608,
                    0.3181818181818182,
                    0.42527472527472526,
                    0.29347826086956524,
                    0.3373319838162014,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "garden rake": 0.3679079990388636,
                "snowflake": 0.4065119499383807,
                "drake": 0.22558005102275572
            },
            "question": "which of these consists of frozen water?",
            "rate_limited": false,
            "answers": [
                "snowflake",
                "garden rake",
                "drake"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "garden rake": 0.08317886723886271,
                "snowflake": 0.7531195666983306,
                "drake": 0.35435141355327204
            },
            "integer_answers": {
                "garden rake": 5,
                "snowflake": 6,
                "drake": 1
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.167115758178347,
                    0.820888290373049,
                    1.011995951448604
                ],
                "result_count_important_words": [
                    650000.0,
                    1290000.0,
                    560000.0
                ],
                "wikipedia_search": [
                    1.3636363636363638,
                    0.0,
                    0.6363636363636364
                ],
                "word_count_appended_bing": [
                    59.0,
                    6.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    1.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.09841964393854141,
                    0.019213814288377762,
                    0.016214236617088318
                ],
                "result_count_noun_chunks": [
                    1220000.0,
                    2740000.0,
                    1100000.0
                ],
                "question_answer_similarity": [
                    0.7907843180000782,
                    2.9980636090040207,
                    0.24569778516888618
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    6290000.0,
                    4600000.0,
                    6560000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    1560000.0,
                    10200000.0,
                    1470000.0
                ],
                "answer_relation_to_question": [
                    1.4444444444444444,
                    0.1111111111111111,
                    0.4444444444444444
                ],
                "word_count_appended": [
                    485.0,
                    38.0,
                    387.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these is a national hamburger chain?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "fuddruckers"
            ],
            "question": "which of these is a national hamburger chain?",
            "lines": [
                [
                    0.15360983102918588,
                    0.0,
                    0.006052439492696768,
                    -0.0,
                    5.404821100421576e-05,
                    0.300844475721323,
                    5.194265530853937e-05,
                    4.772814051164567e-05,
                    0.0,
                    0.04722792607802875,
                    0.2076923076923077,
                    0.09056156817935641,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.15360983102918588,
                    0.0,
                    0.1856858490377482,
                    -0.0,
                    5.404821100421576e-05,
                    0.30154820548909217,
                    5.194265530853937e-05,
                    4.772814051164567e-05,
                    0.0,
                    0.04722792607802875,
                    0.2076923076923077,
                    0.08331664272500788,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.6927803379416283,
                    1.0,
                    0.808261711469555,
                    1.0,
                    0.9998919035779915,
                    0.3976073187895848,
                    0.9998961146893829,
                    0.9999045437189767,
                    1.0,
                    0.9055441478439425,
                    0.5846153846153846,
                    0.8261217890956357,
                    1.0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "puddchuckers",
                "huddsuckers",
                "fuddruckers"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "fuddruckers": 0.8540109676005189,
                "huddsuckers": 0.06340054139724746,
                "puddchuckers": 0.05034524493238396
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.27168470453806925,
                    0.24994992817502365,
                    2.4783653672869073
                ],
                "result_count_important_words": [
                    2.0,
                    2.0,
                    38500.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_count_appended_bing": [
                    27.0,
                    27.0,
                    76.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    2.0
                ],
                "cosine_similarity_raw": [
                    0.003297136165201664,
                    0.10115450620651245,
                    0.44030988216400146
                ],
                "result_count_noun_chunks": [
                    2.0,
                    2.0,
                    41900.0
                ],
                "question_answer_similarity": [
                    0.0,
                    0.0,
                    -0.8096805140376091
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    6.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    855000.0,
                    857000.0,
                    1130000.0
                ],
                "word_count_appended": [
                    23.0,
                    23.0,
                    441.0
                ],
                "answer_relation_to_question": [
                    0.4608294930875576,
                    0.4608294930875576,
                    2.078341013824885
                ],
                "result_count": [
                    2.0,
                    2.0,
                    37000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What word describes joining a cause just to feel good about it?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "slacktivism"
            ],
            "lines": [
                [
                    0.1752057142086846,
                    0.13846153846153847,
                    0.16635322114895737,
                    0.0,
                    0.0005052932549512577,
                    0.2769607843137255,
                    0.0004911591355599214,
                    0.0013767427430144848,
                    0.08739837398373984,
                    0.09349593495934959,
                    0.23728813559322035,
                    0.21209174367080944,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.1744757184832091,
                    0.05384615384615384,
                    0.1531088698477867,
                    1.415706857435261,
                    0.0,
                    0.3602941176470588,
                    0.0,
                    0.0,
                    0.2516759378120097,
                    0.04065040650406504,
                    0.23728813559322035,
                    0.0828787547782125,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.6503185673081062,
                    0.8076923076923077,
                    0.680537909003256,
                    -0.415706857435261,
                    0.9994947067450487,
                    0.3627450980392157,
                    0.99950884086444,
                    0.9986232572569855,
                    0.6609256882042505,
                    0.8658536585365854,
                    0.5254237288135594,
                    0.7050295015509781,
                    0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "slacktivism": 0.6800343389676518,
                "gung-faux": 0.21307115014976746,
                "joinerism": 0.10689451088258085
            },
            "question": "what word describes joining a cause just to feel good about it?",
            "rate_limited": false,
            "answers": [
                "joinerism",
                "gung-faux",
                "slacktivism"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "slacktivism": 0.7610576120966963,
                "gung-faux": 0.1489018555594606,
                "joinerism": 0.11159520436152207
            },
            "integer_answers": {
                "slacktivism": 12,
                "gung-faux": 1,
                "joinerism": 0
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2725504620248567,
                    0.49727252866927496,
                    4.230177009305868
                ],
                "result_count_important_words": [
                    40.0,
                    0,
                    81400.0
                ],
                "wikipedia_search": [
                    0.524390243902439,
                    1.5100556268720582,
                    3.9655541292255028
                ],
                "word_count_appended_bing": [
                    28.0,
                    28.0,
                    62.0
                ],
                "answer_relation_to_question_bing": [
                    0.6923076923076923,
                    0.2692307692307692,
                    4.038461538461538
                ],
                "cosine_similarity_raw": [
                    0.06262539327144623,
                    0.05763942003250122,
                    0.2561955451965332
                ],
                "result_count_noun_chunks": [
                    71.0,
                    0,
                    51500.0
                ],
                "question_answer_similarity": [
                    0.0,
                    4.151298344368115,
                    -1.2189834215678275
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    339000.0,
                    441000.0,
                    444000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    7.0
                ],
                "result_count": [
                    41.0,
                    0,
                    81100.0
                ],
                "answer_relation_to_question": [
                    1.0512342852521077,
                    1.0468543108992547,
                    3.9019114038486373
                ],
                "word_count_appended": [
                    46.0,
                    20.0,
                    426.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "In the musical \u201cHamilton,\u201d who is NOT in \u201cthe room where it happens\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "james monroe"
            ],
            "lines": [
                [
                    0.33585858585858586,
                    0.5,
                    0.31370265844843653,
                    0.3712344343241526,
                    0.2046850365908182,
                    0.24201114042802696,
                    0.4643020769700672,
                    0.3398752751283932,
                    0.23484848484848486,
                    0.3108108108108108,
                    0.38636363636363635,
                    0.3075543309695761,
                    0.0,
                    0,
                    0.0
                ],
                [
                    0.23232323232323232,
                    0.0,
                    0.2810204604732155,
                    0.33953331956766086,
                    0.4996431120628123,
                    0.4397537379067722,
                    0.06857055589492977,
                    0.3235509904622157,
                    0.5,
                    0.38175675675675674,
                    0.34090909090909094,
                    0.36959557520504815,
                    0.5,
                    0,
                    0.0
                ],
                [
                    0.4318181818181818,
                    0.5,
                    0.405276881078348,
                    0.28923224610818654,
                    0.2956718513463696,
                    0.31823512166520085,
                    0.467127367135003,
                    0.3365737344093911,
                    0.26515151515151514,
                    0.30743243243243246,
                    0.2727272727272727,
                    0.3228500938253757,
                    0.5,
                    0,
                    0.0
                ]
            ],
            "fraction_answers": {
                "james monroe": 0.3420527951443485,
                "james madison": 0.2750620465081112,
                "thomas jefferson": 0.3828851583475402
            },
            "question": "in the musical \u201chamilton,\u201d who is not in \u201cthe room where it happens\u201d?",
            "rate_limited": false,
            "answers": [
                "thomas jefferson",
                "james monroe",
                "james madison"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "james monroe": 0.4150240964445132,
                "james madison": 0.18283525257915206,
                "thomas jefferson": 0.12527385925854767
            },
            "integer_answers": {
                "james monroe": 5,
                "james madison": 3,
                "thomas jefferson": 5
            },
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.5395653522433912,
                    1.0432353983596148,
                    1.4171992493969943
                ],
                "result_count_important_words": [
                    935000.0,
                    11300000.0,
                    861000.0
                ],
                "wikipedia_search": [
                    1.0606060606060606,
                    0.0,
                    0.9393939393939393
                ],
                "word_count_appended_bing": [
                    5.0,
                    7.0,
                    10.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    1.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.05483996868133545,
                    0.06446056067943573,
                    0.02788345143198967
                ],
                "result_count_noun_chunks": [
                    873000.0,
                    962000.0,
                    891000.0
                ],
                "question_answer_similarity": [
                    2.805690234526992,
                    3.49643008899875,
                    4.5924469460733235
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    1760000.0,
                    411000.0,
                    1240000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    75300.0,
                    91.0,
                    52100.0
                ],
                "answer_relation_to_question": [
                    0.9848484848484849,
                    1.606060606060606,
                    0.4090909090909091
                ],
                "word_count_appended": [
                    56.0,
                    35.0,
                    57.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "How many of the three Baltic countries border Russia?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "three"
            ],
            "lines": [
                [
                    0.6777777777777778,
                    0.3405275779376499,
                    0.45030505841992374,
                    0.35343114781032764,
                    0.3030769230769231,
                    0.45617977528089887,
                    0.8457943925233645,
                    0.668693009118541,
                    0.02857142857142857,
                    0.47003154574132494,
                    0.35911602209944754,
                    0.35390489430746863,
                    0.6831168831168831,
                    0.6229508196721312,
                    5.0
                ],
                [
                    0.18055555555555555,
                    0.3333333333333333,
                    0.39565996603106396,
                    0.3613009743884807,
                    0.46,
                    0.10224719101123596,
                    0.11401869158878504,
                    0.225531914893617,
                    0.2357142857142857,
                    0.231335436382755,
                    0.20994475138121546,
                    0.33386390886245376,
                    0.015584415584415584,
                    0.00819672131147541,
                    5.0
                ],
                [
                    0.14166666666666666,
                    0.32613908872901676,
                    0.15403497554901227,
                    0.28526787780119167,
                    0.23692307692307693,
                    0.44157303370786516,
                    0.04018691588785047,
                    0.10577507598784194,
                    0.7357142857142858,
                    0.29863301787592006,
                    0.430939226519337,
                    0.31223119683007766,
                    0.3012987012987013,
                    0.36885245901639346,
                    5.0
                ]
            ],
            "fraction_answers": {
                "none": 0.29851682846480265,
                "three": 0.4723912325324351,
                "two": 0.2290919390027623
            },
            "question": "how many of the three baltic countries border russia?",
            "rate_limited": false,
            "answers": [
                "three",
                "two",
                "none"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "none": 0.17656135378582166,
                "three": 0.7381635946806018,
                "two": 0.1386403838727408
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 5
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.4156195772298745,
                    1.335455635449815,
                    1.2489247873203106
                ],
                "result_count_important_words": [
                    9050000.0,
                    1220000.0,
                    430000.0
                ],
                "wikipedia_search": [
                    0.05714285714285714,
                    0.4714285714285714,
                    1.4714285714285715
                ],
                "word_count_appended_bing": [
                    65.0,
                    38.0,
                    78.0
                ],
                "answer_relation_to_question_bing": [
                    0.6810551558752997,
                    0.6666666666666666,
                    0.6522781774580335
                ],
                "cosine_similarity_raw": [
                    0.10257619619369507,
                    0.09012844413518906,
                    0.03508803993463516
                ],
                "result_count_noun_chunks": [
                    11000000.0,
                    3710000.0,
                    1740000.0
                ],
                "question_answer_similarity": [
                    3.8972064778208733,
                    3.983985301107168,
                    3.1455852948129177
                ],
                "word_count_noun_chunks": [
                    263.0,
                    6.0,
                    116.0
                ],
                "word_count_raw": [
                    76.0,
                    1.0,
                    45.0
                ],
                "result_count_bing": [
                    812000.0,
                    182000.0,
                    786000.0
                ],
                "word_count_appended": [
                    447.0,
                    220.0,
                    284.0
                ],
                "answer_relation_to_question": [
                    1.3555555555555556,
                    0.3611111111111111,
                    0.2833333333333333
                ],
                "result_count": [
                    1970000.0,
                    2990000.0,
                    1540000.0
                ]
            },
            "integer_answers": {
                "none": 2,
                "three": 10,
                "two": 2
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "The components required for a fire to burn are typically illustrated with which shape?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "triangle",
                "top"
            ],
            "question": "the components required for a fire to burn are typically illustrated with which shape?",
            "lines": [
                [
                    0.3821428571428571,
                    0.23105413105413106,
                    0.646041900278379,
                    0.7039171977028298,
                    0.31620759943553023,
                    0.4076190476190476,
                    0.2556053811659193,
                    0.975284457966907,
                    0.5833333333333333,
                    0.4045112781954887,
                    0.44715447154471544,
                    0.3903543310085548,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.29166666666666663,
                    0.5364672364672365,
                    0.15892996101485296,
                    0.07387191362138677,
                    0.02001777034443109,
                    0.29333333333333333,
                    0.11659192825112108,
                    0.008835910473888218,
                    0.0625,
                    0.26015037593984963,
                    0.24390243902439024,
                    0.28637982941468654,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.32619047619047625,
                    0.23247863247863249,
                    0.195028138706768,
                    0.22221088867578342,
                    0.6637746302200387,
                    0.29904761904761906,
                    0.6278026905829597,
                    0.015879631559204768,
                    0.3541666666666667,
                    0.33533834586466166,
                    0.3089430894308943,
                    0.32326583957675864,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "triangle",
                "trapezoid",
                "pentagon"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "trapezoid": 0.09397045430929475,
                "triangle": 0.9403289375925957,
                "pentagon": 0.00797636380044416
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.732480317059884,
                    2.004658805902806,
                    2.2628608770373106
                ],
                "result_count_important_words": [
                    798000.0,
                    364000.0,
                    1960000.0
                ],
                "wikipedia_search": [
                    1.1666666666666665,
                    0.125,
                    0.7083333333333334
                ],
                "word_count_appended_bing": [
                    55.0,
                    30.0,
                    38.0
                ],
                "answer_relation_to_question_bing": [
                    0.4621082621082621,
                    1.072934472934473,
                    0.46495726495726497
                ],
                "cosine_similarity_raw": [
                    0.11793340742588043,
                    0.0290122851729393,
                    0.03560192137956619
                ],
                "result_count_noun_chunks": [
                    23400000.0,
                    212000.0,
                    381000.0
                ],
                "question_answer_similarity": [
                    3.005544900894165,
                    0.31541402032598853,
                    0.9487831886508502
                ],
                "word_count_noun_chunks": [
                    36.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    35.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    214000.0,
                    154000.0,
                    157000.0
                ],
                "word_count_appended": [
                    269.0,
                    173.0,
                    223.0
                ],
                "answer_relation_to_question": [
                    1.9107142857142856,
                    1.4583333333333333,
                    1.6309523809523812
                ],
                "result_count": [
                    605000.0,
                    38300.0,
                    1270000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which three-letter-titled movie grossed the most worldwide?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ray"
            ],
            "lines": [
                [
                    0.6172839506172839,
                    0.37777777777777777,
                    0.6907888717823303,
                    0.41395396639383125,
                    0.26153846153846155,
                    0.0818302252114542,
                    0.30458515283842796,
                    0.2509926262053318,
                    0.0,
                    0.3275217932752179,
                    0.4838709677419355,
                    0.34129522445331967,
                    0.2222222222222222,
                    0.0,
                    -1.0
                ],
                [
                    0.25925925925925924,
                    0.49444444444444446,
                    0.11756905893671712,
                    0.17180600473033586,
                    0.21153846153846154,
                    0.43717517578722104,
                    0.27237991266375544,
                    0.21298922291548497,
                    0.0,
                    0.38978829389788294,
                    0.24193548387096775,
                    0.31941230451240143,
                    0.4444444444444444,
                    0.25,
                    -1.0
                ],
                [
                    0.12345679012345678,
                    0.12777777777777777,
                    0.1916420692809526,
                    0.4142400288758329,
                    0.5269230769230769,
                    0.4809945990013248,
                    0.4230349344978166,
                    0.5360181508791833,
                    1.0,
                    0.28268991282689915,
                    0.27419354838709675,
                    0.3392924710342789,
                    0.3333333333333333,
                    0.75,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "big": 0.40000155265167764,
                "saw": 0.320163595537334,
                "ray": 0.2798348518109885
            },
            "question": "which three-letter-titled movie grossed the most worldwide?",
            "rate_limited": false,
            "answers": [
                "saw",
                "ray",
                "big"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "big": 0.10927937721448236,
                "saw": 0.2643585214260746,
                "ray": 0.5450125001213838
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7064761222665983,
                    1.5970615225620073,
                    1.6964623551713944
                ],
                "result_count_important_words": [
                    558000.0,
                    499000.0,
                    775000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_appended_bing": [
                    60.0,
                    30.0,
                    34.0
                ],
                "answer_relation_to_question_bing": [
                    1.1333333333333333,
                    1.4833333333333334,
                    0.3833333333333333
                ],
                "cosine_similarity_raw": [
                    0.160384863615036,
                    0.0272967591881752,
                    0.044494763016700745
                ],
                "result_count_noun_chunks": [
                    885000.0,
                    751000.0,
                    1890000.0
                ],
                "question_answer_similarity": [
                    3.4394874423742294,
                    1.4275128245353699,
                    3.4418642967939377
                ],
                "word_count_noun_chunks": [
                    2.0,
                    4.0,
                    3.0
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    3.0
                ],
                "result_count_bing": [
                    8030000.0,
                    42900000.0,
                    47200000.0
                ],
                "word_count_appended": [
                    263.0,
                    313.0,
                    227.0
                ],
                "answer_relation_to_question": [
                    1.8518518518518519,
                    0.7777777777777777,
                    0.37037037037037035
                ],
                "result_count": [
                    13600000.0,
                    11000000.0,
                    27400000.0
                ]
            },
            "integer_answers": {
                "big": 7,
                "saw": 4,
                "ray": 3
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these songs did NOT appear on the Backstreet Boys album \u201cMillennium\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "as long as you love me",
                "bottom"
            ],
            "question": "which of these songs did not appear on the backstreet boys album \u201cmillennium\u201d?",
            "lines": [
                [
                    0.31149241269090977,
                    0.3997873047865849,
                    0.24245477826349182,
                    0.40618827688658776,
                    0.3356643356643357,
                    0.4656122267638173,
                    0.31496062992125984,
                    0.4095367847411444,
                    0.48239791957770534,
                    0.33941605839416056,
                    0.45714285714285713,
                    0.32949892037160355,
                    0.29761904761904767,
                    0,
                    -1.0
                ],
                [
                    0.26922195274200245,
                    0.21585707658627323,
                    0.34648266964977115,
                    0.30722463869719596,
                    0.3251748251748252,
                    0.1463479651679403,
                    0.3464566929133858,
                    0.27029972752043596,
                    0.0959914997671169,
                    0.2883211678832117,
                    0.2928571428571428,
                    0.3276903521582479,
                    0.2261904761904762,
                    0,
                    -1.0
                ],
                [
                    0.4192856345670878,
                    0.3843556186271418,
                    0.41106255208673703,
                    0.2865870844162163,
                    0.33916083916083917,
                    0.3880398080682424,
                    0.33858267716535434,
                    0.3201634877384196,
                    0.42161058065517776,
                    0.3722627737226277,
                    0.25,
                    0.34281072747014857,
                    0.4761904761904762,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "larger than life",
                "i want it that way",
                "as long as you love me"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "i want it that way": -0.026085690371492894,
                "larger than life": 0.41438192439050614,
                "as long as you love me": 0.7563099423555738
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.0460129555407573,
                    2.0677157741010257,
                    1.8862712703582174
                ],
                "result_count_important_words": [
                    47.0,
                    39.0,
                    41.0
                ],
                "wikipedia_search": [
                    0.21122496506753607,
                    4.848102002794597,
                    0.9406730321378668
                ],
                "word_count_appended_bing": [
                    6.0,
                    29.0,
                    35.0
                ],
                "answer_relation_to_question_bing": [
                    1.2025523425609808,
                    3.409715080964721,
                    1.3877325764742983
                ],
                "cosine_similarity_raw": [
                    0.12056262791156769,
                    0.07186486572027206,
                    0.041633591055870056
                ],
                "result_count_noun_chunks": [
                    332000.0,
                    843000.0,
                    660000.0
                ],
                "question_answer_similarity": [
                    15.894886475871317,
                    32.662682035472244,
                    36.15938342362642
                ],
                "word_count_noun_chunks": [
                    17.0,
                    23.0,
                    2.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    38700.0,
                    398000.0,
                    126000.0
                ],
                "result_count": [
                    47.0,
                    50.0,
                    46.0
                ],
                "answer_relation_to_question": [
                    2.262091047709083,
                    2.7693365670959706,
                    0.9685723851949466
                ],
                "word_count_appended": [
                    44.0,
                    58.0,
                    35.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these is NOT an actual Jelly Belly flavor?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "orange mango smoothie"
            ],
            "question": "which of these is not an actual jelly belly flavor?",
            "lines": [
                [
                    0.41356382978723405,
                    0.4166666666666667,
                    0.414727000589955,
                    0.35666547565595313,
                    0.00012229423994131183,
                    0.3443465491923642,
                    0.11747599970385236,
                    5.530361685651686e-05,
                    0.5,
                    0.2142857142857143,
                    0.375,
                    0.2544610870811567,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.17553191489361702,
                    0.16666666666666666,
                    0.16858553921034314,
                    0.34781760687987606,
                    0.4998945212180506,
                    0.3193832599118943,
                    0.3825692450703762,
                    0.49996239354053756,
                    0.3088888888888889,
                    0.34210526315789475,
                    0.25,
                    0.3006339579970083,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.4109042553191489,
                    0.4166666666666667,
                    0.4166874601997019,
                    0.2955169174641708,
                    0.4999831845420081,
                    0.33627019089574156,
                    0.49995475522577143,
                    0.4999823028426059,
                    0.19111111111111112,
                    0.443609022556391,
                    0.375,
                    0.44490495492183496,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "blueberry pie",
                "sour peach",
                "orange mango smoothie"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sour peach": 0.007396199993642756,
                "orange mango smoothie": 0.582864715472837,
                "blueberry pie": 0.5822478290981445
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9643113033507462,
                    1.5949283360239335,
                    0.4407603606253204
                ],
                "result_count_important_words": [
                    186000.0,
                    57100.0,
                    22.0
                ],
                "wikipedia_search": [
                    0.0,
                    1.1466666666666665,
                    1.8533333333333333
                ],
                "word_count_appended_bing": [
                    2.0,
                    4.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.3333333333333333,
                    1.3333333333333333,
                    0.3333333333333333
                ],
                "cosine_similarity_raw": [
                    0.011730332858860493,
                    0.04559006914496422,
                    0.01146064791828394
                ],
                "result_count_noun_chunks": [
                    452000.0,
                    34.0,
                    16.0
                ],
                "question_answer_similarity": [
                    5.039269991219044,
                    5.350338101387024,
                    7.1890946459025145
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    212000.0,
                    246000.0,
                    223000.0
                ],
                "result_count": [
                    327000.0,
                    69.0,
                    11.0
                ],
                "answer_relation_to_question": [
                    0.6914893617021276,
                    2.595744680851064,
                    0.7127659574468085
                ],
                "word_count_appended": [
                    76.0,
                    42.0,
                    15.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Basketball is NOT a major theme of which of these 90s movies?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "point break"
            ],
            "lines": [
                [
                    0.13900226757369616,
                    0.22053872053872053,
                    0.16785187087982317,
                    0.1692651722464626,
                    0.49171923411274326,
                    0.3900138696255201,
                    0.4999412584983629,
                    0.49043414275202357,
                    0.28125,
                    0.3850129198966408,
                    0.40476190476190477,
                    0.32821200671079453,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.3734126984126984,
                    0.36868686868686873,
                    0.3665120805623735,
                    0.34904573858107607,
                    0.4808305284978713,
                    0.43869625520110955,
                    0.48595311917373696,
                    0.4841795437821928,
                    0.28125,
                    0.41860465116279066,
                    0.4523809523809524,
                    0.3529066304357538,
                    0.5,
                    0,
                    -1.0
                ],
                [
                    0.4875850340136054,
                    0.4107744107744108,
                    0.4656360485578033,
                    0.48168908917246134,
                    0.027450237389385457,
                    0.1712898751733703,
                    0.014105622327900191,
                    0.025386313465783683,
                    0.4375,
                    0.19638242894056845,
                    0.14285714285714285,
                    0.31888136285345164,
                    0.5,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "white men can't jump": 0.3895379434466627,
                "point break": 0.17654475894193475,
                "eddie": 0.4339172976114026
            },
            "question": "basketball is not a major theme of which of these 90s movies?",
            "rate_limited": false,
            "answers": [
                "white men can't jump",
                "point break",
                "eddie"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "white men can't jump": 0.2255162300549815,
                "point break": 0.4684720412989066,
                "eddie": 0.1812279979729257
            },
            "integer_answers": {
                "white men can't jump": 6,
                "point break": 0,
                "eddie": 7
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7178799328920549,
                    1.4709336956424617,
                    1.8111863714654837
                ],
                "result_count_important_words": [
                    92.0,
                    22000.0,
                    761000.0
                ],
                "wikipedia_search": [
                    1.75,
                    1.75,
                    0.5
                ],
                "word_count_appended_bing": [
                    8.0,
                    4.0,
                    30.0
                ],
                "answer_relation_to_question_bing": [
                    1.6767676767676767,
                    0.7878787878787878,
                    0.5353535353535354
                ],
                "cosine_similarity_raw": [
                    0.12475714832544327,
                    0.050138991326093674,
                    0.012907339259982109
                ],
                "result_count_noun_chunks": [
                    10400.0,
                    17200.0,
                    516000.0
                ],
                "question_answer_similarity": [
                    20.746155932545662,
                    9.468977510929108,
                    1.148596940562129
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    793000.0,
                    442000.0,
                    2370000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    7430.0,
                    17200.0,
                    424000.0
                ],
                "answer_relation_to_question": [
                    3.6099773242630384,
                    1.2658730158730158,
                    0.12414965986394558
                ],
                "word_count_appended": [
                    89.0,
                    63.0,
                    235.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "The creator of Wonder Woman also created an early version of what device?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "lie detector"
            ],
            "lines": [
                [
                    0.2328828828828829,
                    0.36458333333333337,
                    0.040550401711227285,
                    0.31936832344364147,
                    0.32867132867132864,
                    0.1341991341991342,
                    0.9945842842058653,
                    0.9950248756218906,
                    0.03469387755102041,
                    0.1484375,
                    0.09090909090909091,
                    0.2688571933596446,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.5900900900900902,
                    0.5208333333333334,
                    0.0038572961670072367,
                    0.3882621007823312,
                    0.15384615384615385,
                    0.5435305435305435,
                    0.0023161551823972205,
                    0.0021267707265200716,
                    0.3508967223252938,
                    0.265625,
                    0.2727272727272727,
                    0.20040937553488827,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.17702702702702702,
                    0.11458333333333334,
                    0.9555923021217655,
                    0.2923695757740273,
                    0.5174825174825175,
                    0.32227032227032226,
                    0.003099560611737457,
                    0.0028483536515893815,
                    0.6144094001236857,
                    0.5859375,
                    0.6363636363636364,
                    0.5307334311054671,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "lie detector": 0.48233692570465064,
                "hearing aid": 0.23532291530327368,
                "magic marker": 0.28234015899207576
            },
            "question": "the creator of wonder woman also created an early version of what device?",
            "rate_limited": false,
            "answers": [
                "magic marker",
                "hearing aid",
                "lie detector"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "lie detector": 0.7906221929744528,
                "hearing aid": 0.24447813546910271,
                "magic marker": 0.12958394167834492
            },
            "integer_answers": {
                "lie detector": 8,
                "hearing aid": 4,
                "magic marker": 2
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.8820003535175123,
                    1.4028656287442178,
                    3.71513401773827
                ],
                "result_count_important_words": [
                    29200.0,
                    68.0,
                    91.0
                ],
                "wikipedia_search": [
                    0.24285714285714285,
                    2.4562770562770564,
                    4.3008658008658
                ],
                "word_count_appended_bing": [
                    2.0,
                    6.0,
                    14.0
                ],
                "answer_relation_to_question_bing": [
                    1.4583333333333333,
                    2.083333333333333,
                    0.4583333333333333
                ],
                "cosine_similarity_raw": [
                    0.02869383990764618,
                    0.002729458501562476,
                    0.6761859655380249
                ],
                "result_count_noun_chunks": [
                    26200.0,
                    56.0,
                    75.0
                ],
                "question_answer_similarity": [
                    6.546491540968418,
                    7.958693370223045,
                    5.9930644780397415
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    16.0
                ],
                "result_count_bing": [
                    279000.0,
                    1130000.0,
                    670000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    21.0
                ],
                "result_count": [
                    47.0,
                    22.0,
                    74.0
                ],
                "answer_relation_to_question": [
                    1.1644144144144144,
                    2.9504504504504507,
                    0.8851351351351351
                ],
                "word_count_appended": [
                    19.0,
                    34.0,
                    75.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Anna Wintour became the editor-in-chief of Vogue in the same year as what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "world series cancelled"
            ],
            "lines": [
                [
                    0.5333333333333333,
                    0.6666666666666667,
                    0.5259247456964948,
                    0.4103360485672182,
                    0.5,
                    0.33406352683461116,
                    0.4666666666666667,
                    0.3333333333333333,
                    0.6111111111111112,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.34583333333333327,
                    0,
                    0,
                    1.0
                ],
                [
                    0.3212121212121212,
                    0.16666666666666669,
                    0.40620283094886256,
                    0.33205012281414287,
                    0.25,
                    0.3231106243154436,
                    0.13333333333333333,
                    0.16666666666666666,
                    0.05555555555555555,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.32708333333333334,
                    0,
                    0,
                    1.0
                ],
                [
                    0.14545454545454545,
                    0.16666666666666669,
                    0.06787242335464266,
                    0.25761382861863896,
                    0.25,
                    0.3428258488499452,
                    0.4,
                    0.5,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.32708333333333334,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "world series cancelled": 0.4494946193507863,
                "41st president elected": 0.26237899345939936,
                "the goonies premiered": 0.28812638718981437
            },
            "question": "anna wintour became the editor-in-chief of vogue in the same year as what?",
            "rate_limited": false,
            "answers": [
                "world series cancelled",
                "41st president elected",
                "the goonies premiered"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "world series cancelled": 0.3915954478367551,
                "41st president elected": 0.15230175741198854,
                "the goonies premiered": 0.11697536857172047
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.383333333333333,
                    1.3083333333333333,
                    1.3083333333333333
                ],
                "result_count_important_words": [
                    7.0,
                    2.0,
                    6.0
                ],
                "wikipedia_search": [
                    1.8333333333333335,
                    0.16666666666666666,
                    1.0
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.6666666666666666,
                    0.16666666666666666,
                    0.16666666666666666
                ],
                "cosine_similarity_raw": [
                    0.08038873970508575,
                    0.0620889849960804,
                    0.010374447330832481
                ],
                "result_count_noun_chunks": [
                    4.0,
                    2.0,
                    6.0
                ],
                "question_answer_similarity": [
                    11.707725159823895,
                    9.474067878676578,
                    7.350248444825411
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    30500.0,
                    29500.0,
                    31300.0
                ],
                "result_count": [
                    4.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question": [
                    1.6,
                    0.9636363636363636,
                    0.43636363636363634
                ],
                "word_count_appended": [
                    5.0,
                    5.0,
                    5.0
                ]
            },
            "integer_answers": {
                "world series cancelled": 10,
                "41st president elected": 0,
                "the goonies premiered": 2
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "As Ali Baba discovered, what password opened the den of the forty thieves?": {
        "raw_data": {
            "data": {
                "word_count_appended_relation_to_question": [
                    4.224973390341179,
                    2.1223713312366015,
                    0.6526552784222196
                ],
                "result_count_important_words": [
                    928.0,
                    1960.0,
                    0
                ],
                "wikipedia_search": [
                    0.9288343558282209,
                    0.35337423312883437,
                    0.7177914110429447
                ],
                "word_count_appended_bing": [
                    7.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    2.891477272727273,
                    0.6059659090909091,
                    0.5025568181818182
                ],
                "cosine_similarity_raw": [
                    0.20450961589813232,
                    0.011268619447946548,
                    0.028821729123592377
                ],
                "result_count_noun_chunks": [
                    9490.0,
                    39200.0,
                    3.0
                ],
                "question_answer_similarity": [
                    5.816380301490426,
                    10.439777279272676,
                    21.116004978772253
                ],
                "word_count_noun_chunks": [
                    35.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    14800.0,
                    82200.0,
                    15400.0
                ],
                "word_count_appended": [
                    64.0,
                    7.0,
                    3.0
                ],
                "answer_relation_to_question": [
                    5.334164770406675,
                    0.637901861252115,
                    1.02793336834121
                ],
                "result_count": [
                    58.0,
                    209.0,
                    0
                ]
            },
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "z-best_answer_by_ml": [
                "open sesame",
                "top"
            ],
            "lines": [
                [
                    0.762023538629525,
                    0.7228693181818182,
                    0.8360983058257563,
                    0.15563403087056304,
                    0.21722846441947566,
                    0.13167259786476868,
                    0.32132963988919666,
                    0.19489454336352247,
                    0.46441717791411047,
                    0.8648648648648649,
                    0.6363636363636364,
                    0.6035676271915971,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.09112883732173072,
                    0.15149147727272727,
                    0.04606958742769475,
                    0.2793463520512545,
                    0.7827715355805244,
                    0.7313167259786477,
                    0.6786703601108033,
                    0.8050438461380486,
                    0.17668711656441718,
                    0.0945945945945946,
                    0.18181818181818182,
                    0.30319590446237166,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.14684762404874427,
                    0.12563920454545455,
                    0.11783210674654893,
                    0.5650196170781825,
                    0.0,
                    0.13701067615658363,
                    0.0,
                    6.161049842893228e-05,
                    0.3588957055214724,
                    0.04054054054054054,
                    0.18181818181818182,
                    0.09323646834603137,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "question": "as ali baba discovered, what password opened the den of the forty thieves?",
            "rate_limited": false,
            "answers": [
                "open sesame",
                "oh, snap",
                "let's get in formation"
            ],
            "ml_answers": {
                "oh, snap": 0.03193397617655855,
                "open sesame": 1.0086570936254855,
                "let's get in formation": 0.0517941588650748
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these verbs has two meanings that are opposites of each other?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "cleave"
            ],
            "lines": [
                [
                    0.0,
                    0.0,
                    0.17216661284850598,
                    0.5529125994186738,
                    0.15710872162485065,
                    0.21979586992641822,
                    0.3143100511073254,
                    0.31761006289308175,
                    0,
                    0.3244005641748942,
                    0.3103448275862069,
                    0.332812558993568,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.16666666666666669,
                    0.0,
                    0.5365683158519028,
                    0.2563376711823779,
                    0.37873357228195936,
                    0.6812247804414906,
                    0.206984667802385,
                    0.029350104821802937,
                    0,
                    0.2651622002820874,
                    0.26436781609195403,
                    0.2966708070454468,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.8333333333333334,
                    1.0,
                    0.2912650712995912,
                    0.19074972939894838,
                    0.46415770609318996,
                    0.09897934963209115,
                    0.4787052810902896,
                    0.6530398322851153,
                    0,
                    0.4104372355430183,
                    0.42528735632183906,
                    0.37051663396098516,
                    1.0,
                    1.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "cleave": 0.5551131945352616,
                "jut": 0.23708204634369795,
                "branch": 0.20780475912104038
            },
            "question": "which of these verbs has two meanings that are opposites of each other?",
            "rate_limited": false,
            "answers": [
                "branch",
                "jut",
                "cleave"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "cleave": 0.5708356297307872,
                "jut": 0.18529520967241758,
                "branch": 0.04863383037840421
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.998437676980704,
                    0.8900124211363404,
                    1.1115499018829555
                ],
                "result_count_important_words": [
                    369000.0,
                    243000.0,
                    562000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    27.0,
                    23.0,
                    37.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "cosine_similarity_raw": [
                    0.03137444704771042,
                    0.09778048098087311,
                    0.053078122437000275
                ],
                "result_count_noun_chunks": [
                    3030000.0,
                    280000.0,
                    6230000.0
                ],
                "question_answer_similarity": [
                    3.058261123485863,
                    1.4178507328033447,
                    1.0550717823207378
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    2.0
                ],
                "result_count_bing": [
                    9260000.0,
                    28700000.0,
                    4170000.0
                ],
                "result_count": [
                    2630000.0,
                    6340000.0,
                    7770000.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    0.3333333333333333,
                    1.6666666666666665
                ],
                "word_count_appended": [
                    230.0,
                    188.0,
                    291.0
                ]
            },
            "integer_answers": {
                "cleave": 10,
                "jut": 2,
                "branch": 1
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these animals can turn into a moth?": {
        "raw_data": {
            "data": {
                "word_count_appended_relation_to_question": [
                    0.9810879192130344,
                    1.183947987308379,
                    0.8349640934785867
                ],
                "result_count_important_words": [
                    80100.0,
                    513000.0,
                    88700.0
                ],
                "wikipedia_search": [
                    0.34285714285714286,
                    1.9129464285714286,
                    0.7441964285714285
                ],
                "word_count_appended_bing": [
                    67.0,
                    66.0,
                    55.0
                ],
                "answer_relation_to_question_bing": [
                    0.3910761154855643,
                    0.28608923884514437,
                    0.3228346456692913
                ],
                "cosine_similarity_raw": [
                    0.049623314291238785,
                    0.10144096612930298,
                    0.03479299694299698
                ],
                "result_count_noun_chunks": [
                    121000.0,
                    3720000.0,
                    215000.0
                ],
                "question_answer_similarity": [
                    1.064222491811961,
                    1.4652829067781568,
                    0.7957098647020757
                ],
                "word_count_noun_chunks": [
                    1.0,
                    18.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    28.0,
                    0.0
                ],
                "result_count_bing": [
                    2700000.0,
                    2690000.0,
                    2700000.0
                ],
                "word_count_appended": [
                    337.0,
                    525.0,
                    452.0
                ],
                "answer_relation_to_question": [
                    0.5037581699346405,
                    1.2763071895424836,
                    1.219934640522876
                ],
                "result_count": [
                    64600.0,
                    1610000.0,
                    60800.0
                ]
            },
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "z-best_answer_by_ml": [
                "caterpillar",
                "middle"
            ],
            "lines": [
                [
                    0.1679193899782135,
                    0.3910761154855643,
                    0.2669968859716753,
                    0.3200461947712543,
                    0.03722484729745304,
                    0.3337453646477132,
                    0.11748313288354356,
                    0.029832347140039447,
                    0.11428571428571428,
                    0.256468797564688,
                    0.35638297872340424,
                    0.3270293064043448,
                    0.05263157894736842,
                    0.0,
                    -1.0
                ],
                [
                    0.4254357298474945,
                    0.28608923884514437,
                    0.5458003451265641,
                    0.4406580599318629,
                    0.9277400023049441,
                    0.33250927070457353,
                    0.7524200645350543,
                    0.9171597633136095,
                    0.6376488095238095,
                    0.3995433789954338,
                    0.35106382978723405,
                    0.394649329102793,
                    0.9473684210526315,
                    1.0,
                    -1.0
                ],
                [
                    0.40664488017429196,
                    0.3228346456692913,
                    0.18720276890176069,
                    0.23929574529688277,
                    0.03503515039760286,
                    0.3337453646477132,
                    0.13009680258140216,
                    0.053007889546351085,
                    0.24806547619047617,
                    0.3439878234398782,
                    0.2925531914893617,
                    0.2783213644928622,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "question": "which of these animals can turn into a moth?",
            "rate_limited": false,
            "answers": [
                "maggot",
                "caterpillar",
                "centipede"
            ],
            "ml_answers": {
                "caterpillar": 0.8445051393864376,
                "centipede": 0.16576212851324437,
                "maggot": 0.13074979174677057
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Not just a TV program, \u201cFrontline\u201d is also the name of a popular type of what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "hair styling product"
            ],
            "question": "not just a tv program, \u201cfrontline\u201d is also the name of a popular type of what?",
            "lines": [
                [
                    0.3861044657097289,
                    0.3359983766233766,
                    0.42637144478668937,
                    0.31939288255631637,
                    0.34567901234567905,
                    0.4087193460490463,
                    0.4987263055368248,
                    0.296875,
                    0.4583931133428981,
                    0.4051724137931034,
                    0.33333333333333337,
                    0.3821779674539715,
                    0,
                    0,
                    1.0
                ],
                [
                    0.3685340244550771,
                    0.4047754329004329,
                    0.33153694986118853,
                    0.31413383885703816,
                    0.43518518518518523,
                    0.2949591280653951,
                    0.4995129991758448,
                    0.4479166666666667,
                    0.09038737446197986,
                    0.3879310344827586,
                    0.33333333333333337,
                    0.39146350840039057,
                    0,
                    0,
                    1.0
                ],
                [
                    0.24536150983519406,
                    0.2592261904761905,
                    0.2420916053521221,
                    0.36647327858664547,
                    0.2191358024691358,
                    0.2963215258855586,
                    0.0017606952873304718,
                    0.25520833333333337,
                    0.45121951219512196,
                    0.20689655172413796,
                    0.33333333333333337,
                    0.22635852414563795,
                    0,
                    0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "hair styling product",
                "electric guitar brand",
                "pet medicine"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "hair styling product": 0.8159096896154568,
                "pet medicine": 0.20088832332092887,
                "electric guitar brand": 0.43329997473456944
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.178220325460285,
                    1.0853649159960943,
                    2.7364147585436207
                ],
                "result_count_important_words": [
                    34.0,
                    13.0,
                    13300.0
                ],
                "wikipedia_search": [
                    0.24964131994261118,
                    2.4576757532281204,
                    0.2926829268292683
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.312012987012987,
                    0.7617965367965368,
                    1.926190476190476
                ],
                "cosine_similarity_raw": [
                    0.014258721843361855,
                    0.03262413293123245,
                    0.0499458946287632
                ],
                "result_count_noun_chunks": [
                    39.0,
                    10.0,
                    47.0
                ],
                "question_answer_similarity": [
                    13.331680219620466,
                    13.719881359487772,
                    9.856397554278374
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    134000.0,
                    301000.0,
                    299000.0
                ],
                "word_count_appended": [
                    11.0,
                    13.0,
                    34.0
                ],
                "answer_relation_to_question": [
                    1.1389553429027113,
                    1.3146597554492292,
                    2.5463849016480595
                ],
                "result_count": [
                    50.0,
                    21.0,
                    91.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Siri made its debut on which iPhone?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "iphone 4s"
            ],
            "question": "siri made its debut on which iphone?",
            "lines": [
                [
                    0.2877172524848687,
                    0.3298097251585624,
                    0.21116790524024986,
                    0.32439948380321787,
                    0.34375,
                    0.32387706855791965,
                    0.26824034334763946,
                    0.29333333333333333,
                    0.10456273764258556,
                    0.2923076923076923,
                    0.20512820512820512,
                    0.3394149924463006,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.14517741129435283,
                    0.3572938689217759,
                    0.24023333758275192,
                    0.33540831680131483,
                    0.28125,
                    0.41134751773049644,
                    0.4291845493562232,
                    0.41555555555555557,
                    0.2467258132657372,
                    0.23692307692307693,
                    0.2564102564102564,
                    0.31565660317673555,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.5671053362207785,
                    0.3128964059196617,
                    0.5485987571769982,
                    0.3401921993954673,
                    0.375,
                    0.2647754137115839,
                    0.30257510729613735,
                    0.2911111111111111,
                    0.6487114490916772,
                    0.4707692307692308,
                    0.5384615384615384,
                    0.34492840437696387,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "iphone 4",
                "iphone 5",
                "iphone 4s"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "iphone 4": 0.2077174782187549,
                "iphone 5": 0.014161785840570946,
                "iphone 4s": 0.49279171950467177
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.0182449773389017,
                    0.9469698095302067,
                    1.0347852131308917
                ],
                "result_count_important_words": [
                    125000.0,
                    200000.0,
                    141000.0
                ],
                "wikipedia_search": [
                    0.31368821292775667,
                    0.7401774397972116,
                    1.9461343472750317
                ],
                "word_count_appended_bing": [
                    8.0,
                    10.0,
                    21.0
                ],
                "answer_relation_to_question_bing": [
                    0.3298097251585624,
                    0.3572938689217759,
                    0.3128964059196617
                ],
                "cosine_similarity_raw": [
                    0.07989874482154846,
                    0.0908961147069931,
                    0.2075710892677307
                ],
                "result_count_noun_chunks": [
                    132000.0,
                    187000.0,
                    131000.0
                ],
                "question_answer_similarity": [
                    3.6952184895053506,
                    3.820619562175125,
                    3.8751125324051827
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    1370000.0,
                    1740000.0,
                    1120000.0
                ],
                "word_count_appended": [
                    95.0,
                    77.0,
                    153.0
                ],
                "answer_relation_to_question": [
                    0.863151757454606,
                    0.4355322338830585,
                    1.7013160086623356
                ],
                "result_count": [
                    1650000.0,
                    1350000.0,
                    1800000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these can you travel to by train?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "chicago union station"
            ],
            "question": "which of these can you travel to by train?",
            "lines": [
                [
                    0.047619047619047616,
                    0.020833333333333336,
                    0.16037290733175621,
                    0.25539420755365466,
                    0.45248868778280543,
                    0.33271167474822827,
                    0.21052631578947367,
                    0.12507480550568523,
                    0.0,
                    0.0,
                    0.0,
                    0.19820448983036543,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.1056547619047619,
                    0.020833333333333336,
                    0.09773017038776698,
                    0.4123640294261679,
                    0.2420814479638009,
                    0.3342036553524804,
                    0.12424503882657463,
                    0.07899461400359066,
                    0.20673076923076922,
                    0.0,
                    0.0,
                    0.2736785101415028,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.8467261904761905,
                    0.9583333333333334,
                    0.7418969222804768,
                    0.33224176302017744,
                    0.3054298642533937,
                    0.3330846698992913,
                    0.6652286453839517,
                    0.7959305804907241,
                    0.7932692307692308,
                    1.0,
                    1.0,
                    0.5281170000281318,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "saturn\u2019s rings",
                "earth\u2019s core",
                "chicago union station"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "saturn\u2019s rings": 0.09125688021836516,
                "chicago union station": 0.734081695934292,
                "earth\u2019s core": 0.07403514028241698
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.39640897966073085,
                    0.5473570202830056,
                    1.0562340000562636
                ],
                "result_count_important_words": [
                    244000.0,
                    144000.0,
                    771000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.41346153846153844,
                    1.5865384615384617
                ],
                "word_count_appended_bing": [
                    0.0,
                    0.0,
                    23.0
                ],
                "answer_relation_to_question_bing": [
                    0.041666666666666664,
                    0.041666666666666664,
                    1.9166666666666665
                ],
                "cosine_similarity_raw": [
                    0.025373324751853943,
                    0.015462333336472511,
                    0.11737887561321259
                ],
                "result_count_noun_chunks": [
                    209000.0,
                    132000.0,
                    1330000.0
                ],
                "question_answer_similarity": [
                    5.12010210682638,
                    8.267007917165756,
                    6.660729572176933
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    89200000.0,
                    89600000.0,
                    89300000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    200000.0,
                    107000.0,
                    135000.0
                ],
                "answer_relation_to_question": [
                    0.09523809523809523,
                    0.2113095238095238,
                    1.693452380952381
                ],
                "word_count_appended": [
                    0.0,
                    0.0,
                    40.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Tic-tac-toe is typically played by writing what two symbols?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "x / o"
            ],
            "question": "tic-tac-toe is typically played by writing what two symbols?",
            "lines": [
                [
                    0.41275904104022754,
                    0.3042508710801394,
                    0.4807575871327284,
                    0.3752605231054516,
                    1.8988446440745773e-05,
                    0.004828367825371443,
                    0.00013573673259348223,
                    0.0375754864683516,
                    0.1212739921905076,
                    0.8333333333333334,
                    0.5,
                    0.17432150741886274,
                    0,
                    0,
                    1.0
                ],
                [
                    0.026737098740349453,
                    0.1006418485237484,
                    0.0,
                    0.335596582894098,
                    0.9988443059241612,
                    0.9780938038533791,
                    0.9913936038789778,
                    0.9468426153731454,
                    0.6467098388604741,
                    0.0,
                    0.0,
                    0.42249912308861765,
                    0,
                    0,
                    1.0
                ],
                [
                    0.560503860219423,
                    0.5951072803961123,
                    0.5192424128672716,
                    0.2891428940004504,
                    0.0011367056293980688,
                    0.017077828321249475,
                    0.0084706593884287,
                    0.015581898158502945,
                    0.2320161689490183,
                    0.16666666666666666,
                    0.5,
                    0.40317936949251953,
                    0,
                    0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "yin and yang",
                "+ / -",
                "x / o"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "yin and yang": 0.23158462683048783,
                "+ / -": 0.1495663046193192,
                "x / o": 0.3587753127000103
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2202505519320392,
                    2.9574938616203235,
                    2.822255586447637
                ],
                "result_count_important_words": [
                    79.0,
                    577000.0,
                    4930.0
                ],
                "wikipedia_search": [
                    0.4850959687620304,
                    2.5868393554418962,
                    0.9280646757960732
                ],
                "word_count_appended_bing": [
                    2.0,
                    0.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.5212543554006968,
                    0.503209242618742,
                    2.9755364019805612
                ],
                "cosine_similarity_raw": [
                    0.5305421352386475,
                    0.0,
                    0.5730122327804565
                ],
                "result_count_noun_chunks": [
                    504000.0,
                    12700000.0,
                    209000.0
                ],
                "question_answer_similarity": [
                    6.440497630741447,
                    5.759755860082805,
                    4.962483419105411
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    62200.0,
                    12600000.0,
                    220000.0
                ],
                "result_count": [
                    73.0,
                    3840000.0,
                    4370.0
                ],
                "answer_relation_to_question": [
                    2.0637952052011377,
                    0.13368549370174726,
                    2.802519301097115
                ],
                "word_count_appended": [
                    25.0,
                    0.0,
                    5.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "The youngest person ever to win an EGOT co-wrote which musical?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "avenue q"
            ],
            "question": "the youngest person ever to win an egot co-wrote which musical?",
            "lines": [
                [
                    0.4462706109679794,
                    0.2689463168493817,
                    0.3204296282031992,
                    0.5432897434381246,
                    0.13768115942028986,
                    0.4169381107491857,
                    0.26256983240223464,
                    0.5238095238095238,
                    0.313896748043538,
                    0.2247191011235955,
                    0.16666666666666666,
                    0.26769608164891656,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.3361036784852574,
                    0.44621000350722373,
                    0.37690406380636954,
                    0.11132773774341037,
                    0.7125603864734299,
                    0.2988599348534202,
                    0.45251396648044695,
                    0.049523809523809526,
                    0.1655184985042083,
                    0.6067415730337079,
                    0.4444444444444444,
                    0.45924538091316236,
                    0,
                    1.0,
                    -1.0
                ],
                [
                    0.21762571054676316,
                    0.2848436796433945,
                    0.30266630799043126,
                    0.34538251881846505,
                    0.1497584541062802,
                    0.28420195439739415,
                    0.2849162011173184,
                    0.4266666666666667,
                    0.5205847534522537,
                    0.16853932584269662,
                    0.3888888888888889,
                    0.27305853743792113,
                    0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "the sound of music",
                "avenue q",
                "the lion king"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "the sound of music": 0.1531814350863049,
                "avenue q": 0.4754020810742189,
                "the lion king": 0.14673920505419485
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.8738725715424158,
                    3.2147176663921364,
                    1.9114097620654478
                ],
                "result_count_important_words": [
                    47.0,
                    81.0,
                    51.0
                ],
                "wikipedia_search": [
                    1.8833804882612282,
                    0.9931109910252498,
                    3.1235085207135223
                ],
                "word_count_appended_bing": [
                    3.0,
                    8.0,
                    7.0
                ],
                "answer_relation_to_question_bing": [
                    1.3447315842469085,
                    2.2310500175361185,
                    1.4242183982169727
                ],
                "cosine_similarity_raw": [
                    0.09985045343637466,
                    0.11744869500398636,
                    0.09431514889001846
                ],
                "result_count_noun_chunks": [
                    1100000.0,
                    104000.0,
                    896000.0
                ],
                "question_answer_similarity": [
                    16.66175466775894,
                    3.4142287359572947,
                    10.592283150181174
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    512000.0,
                    367000.0,
                    349000.0
                ],
                "word_count_raw": [
                    0.0,
                    2.0,
                    0.0
                ],
                "result_count": [
                    57.0,
                    295.0,
                    62.0
                ],
                "answer_relation_to_question": [
                    1.7850824438719175,
                    1.3444147139410296,
                    0.8705028421870527
                ],
                "word_count_appended": [
                    20.0,
                    54.0,
                    15.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Occasionally found in print in the 1800s, what word was brought into widespread use by the telephone?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "hello"
            ],
            "question": "occasionally found in print in the 1800s, what word was brought into widespread use by the telephone?",
            "lines": [
                [
                    0.5228234315712875,
                    0.45314833250373315,
                    0.5056415291025421,
                    0.32684363107756037,
                    0.8924395946999221,
                    0.3333333333333333,
                    0.5310457516339869,
                    0.9052404285358359,
                    0.6659901118917513,
                    0.3657587548638132,
                    0.4479166666666667,
                    0.32536556461510846,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.23126461874317794,
                    0.10701841712294674,
                    0.1046217997178061,
                    0.3705126301314678,
                    0.05465705378020265,
                    0.3333333333333333,
                    0.04411764705882353,
                    0.047919608005979566,
                    0.16716107207910486,
                    0.32879377431906615,
                    0.28125,
                    0.3423647476532051,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.2459119496855346,
                    0.43983325037332005,
                    0.38973667117965183,
                    0.30264373879097184,
                    0.05290335151987529,
                    0.3333333333333333,
                    0.42483660130718953,
                    0.04683996345818454,
                    0.1668488160291439,
                    0.30544747081712065,
                    0.2708333333333333,
                    0.33226968773168636,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "hello",
                "chat",
                "goodbye"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "chat": 0.2520051827360793,
                "hello": 0.6810812866504973,
                "goodbye": 0.24386601953074666
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.9282900815359763,
                    3.081282728878846,
                    2.9904271895851773
                ],
                "result_count_important_words": [
                    1950000.0,
                    162000.0,
                    1560000.0
                ],
                "wikipedia_search": [
                    3.3299505594587564,
                    0.8358053603955242,
                    0.8342440801457195
                ],
                "word_count_appended_bing": [
                    43.0,
                    27.0,
                    26.0
                ],
                "answer_relation_to_question_bing": [
                    1.8125933300149326,
                    0.42807366849178696,
                    1.7593330014932802
                ],
                "cosine_similarity_raw": [
                    0.1212891936302185,
                    0.025095829740166664,
                    0.09348687529563904
                ],
                "result_count_noun_chunks": [
                    1090000.0,
                    57700.0,
                    56400.0
                ],
                "question_answer_similarity": [
                    3.479793258011341,
                    3.9447222761809826,
                    3.222145214676857
                ],
                "word_count_noun_chunks": [
                    12.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    1600000.0,
                    1600000.0,
                    1600000.0
                ],
                "word_count_raw": [
                    7.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    916000.0,
                    56100.0,
                    54300.0
                ],
                "answer_relation_to_question": [
                    2.6141171578564375,
                    1.1563230937158897,
                    1.229559748427673
                ],
                "word_count_appended": [
                    188.0,
                    169.0,
                    157.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these baseball legends was a first-ballot Hall of Famer?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "cy young"
            ],
            "question": "which of these baseball legends was a first-ballot hall of famer?",
            "lines": [
                [
                    0.17336835334476844,
                    0.06910569105691057,
                    0.34074415573454114,
                    0.1454772878483105,
                    0.23614578132800798,
                    0.7310344827586207,
                    0.663972286374134,
                    0.20449601160261058,
                    0.31088598901098907,
                    0.36046511627906974,
                    0.10714285714285714,
                    0.3268689212971677,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.5403530588907948,
                    0.30894308943089427,
                    0.38676344636643156,
                    0.449508943182101,
                    0.4493260109835247,
                    0.14758620689655172,
                    0.21747498075442648,
                    0.5192168237853517,
                    0.20633012820512822,
                    0.23255813953488372,
                    0.39285714285714285,
                    0.35305776298538455,
                    1.0,
                    0,
                    -1.0
                ],
                [
                    0.2862785877644368,
                    0.6219512195121951,
                    0.2724923978990273,
                    0.4050137689695885,
                    0.3145282076884673,
                    0.12137931034482759,
                    0.11855273287143957,
                    0.27628716461203773,
                    0.4827838827838828,
                    0.4069767441860465,
                    0.5,
                    0.3200733157174477,
                    0.0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "joe dimaggio",
                "cy young",
                "mickey mantle"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "cy young": 0.5949605237587383,
                "mickey mantle": 0.30935807156622125,
                "joe dimaggio": 0.1593279993172117
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6343446064858385,
                    1.7652888149269228,
                    1.6003665785872385
                ],
                "result_count_important_words": [
                    345000.0,
                    113000.0,
                    61600.0
                ],
                "wikipedia_search": [
                    1.243543956043956,
                    0.8253205128205128,
                    1.931135531135531
                ],
                "word_count_appended_bing": [
                    3.0,
                    11.0,
                    14.0
                ],
                "answer_relation_to_question_bing": [
                    0.2073170731707317,
                    0.9268292682926829,
                    1.8658536585365852
                ],
                "cosine_similarity_raw": [
                    0.04868236556649208,
                    0.055257175117731094,
                    0.038931187242269516
                ],
                "result_count_noun_chunks": [
                    28200.0,
                    71600.0,
                    38100.0
                ],
                "question_answer_similarity": [
                    1.2715598698705435,
                    3.928981229662895,
                    3.540066377259791
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count_bing": [
                    265000.0,
                    53500.0,
                    44000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    9460.0,
                    18000.0,
                    12600.0
                ],
                "answer_relation_to_question": [
                    0.6934734133790738,
                    2.161412235563179,
                    1.1451143510577473
                ],
                "word_count_appended": [
                    62.0,
                    40.0,
                    70.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these products is a depilatory?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "nair"
            ],
            "question": "which of these products is a depilatory?",
            "lines": [
                [
                    0.5806451612903226,
                    0.6481481481481483,
                    0.7413084382739592,
                    -0.353491728392961,
                    0.8693052732273108,
                    0.31542461005199307,
                    0.70435941366838,
                    0.5572616762635957,
                    0.0,
                    0.3963317384370016,
                    0.36649214659685864,
                    0.36383585045285327,
                    1.0,
                    0,
                    -1.0
                ],
                [
                    0.08602150537634409,
                    0.1851851851851852,
                    0.1646198934799061,
                    1.0632892786127845,
                    0.04053569293315796,
                    0.3460427498555748,
                    0.04054825813820674,
                    0.309660908509277,
                    0.0,
                    0.3157894736842105,
                    0.3089005235602094,
                    0.293986497155043,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.3333333333333333,
                    0.16666666666666669,
                    0.09407166824613468,
                    0.29020244978017634,
                    0.09015903383953126,
                    0.3385326400924321,
                    0.25509232819341326,
                    0.13307741522712732,
                    1.0,
                    0.2878787878787879,
                    0.32460732984293195,
                    0.34217765239210374,
                    0.0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "nair",
                "rogaine",
                "pomade"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "rogaine": 0.10429247200304952,
                "nair": 0.5310775673794066,
                "pomade": 0.3466155240675005
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.7276717009057065,
                    0.587972994310086,
                    0.6843553047842075
                ],
                "result_count_important_words": [
                    111000.0,
                    6390.0,
                    40200.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_appended_bing": [
                    70.0,
                    59.0,
                    62.0
                ],
                "answer_relation_to_question_bing": [
                    1.2962962962962963,
                    0.37037037037037035,
                    0.3333333333333333
                ],
                "cosine_similarity_raw": [
                    0.17466232180595398,
                    0.03878667950630188,
                    0.022164560854434967
                ],
                "result_count_noun_chunks": [
                    87100.0,
                    48400.0,
                    20800.0
                ],
                "question_answer_similarity": [
                    -0.16646214667707682,
                    0.5007116196502466,
                    0.13665870763361454
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    5460000.0,
                    5990000.0,
                    5860000.0
                ],
                "word_count_appended": [
                    497.0,
                    396.0,
                    361.0
                ],
                "answer_relation_to_question": [
                    1.1612903225806452,
                    0.17204301075268819,
                    0.6666666666666666
                ],
                "result_count": [
                    727000.0,
                    33900.0,
                    75400.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which game was almost pulled from the market before a late-night host saved it?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "twister",
                "bottom"
            ],
            "question": "which game was almost pulled from the market before a late-night host saved it?",
            "lines": [
                [
                    0.4622897617565864,
                    0.5135680928941058,
                    0.19706711888639047,
                    0.7899321984008764,
                    0.8610466615106986,
                    0.3353658536585366,
                    0.10860967447520535,
                    0.1276595744680851,
                    0.47466362020817465,
                    0.3515358361774744,
                    0.34285714285714286,
                    0.3382407166228281,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.34903158759793834,
                    0.28817998232992553,
                    0.29372577101939845,
                    0.6031703468925665,
                    0.03763856664088683,
                    0.3336236933797909,
                    0.2540310313355643,
                    0.3241551939924906,
                    0.07661843107387663,
                    0.2986348122866894,
                    0.2857142857142857,
                    0.3158650561743568,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.18867865064547532,
                    0.1982519247759687,
                    0.509207110094211,
                    -0.39310254529344285,
                    0.10131477184841454,
                    0.3310104529616725,
                    0.6373592941892303,
                    0.5481852315394243,
                    0.44871794871794873,
                    0.34982935153583616,
                    0.37142857142857144,
                    0.3458942272028151,
                    1.0,
                    1.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "jenga",
                "pictionary",
                "twister"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "jenga": 0.2404522662464996,
                "twister": 0.829399939123033,
                "pictionary": 0.12490295604683213
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.367685016359797,
                    2.2110553932204975,
                    2.4212595904197056
                ],
                "result_count_important_words": [
                    71400.0,
                    167000.0,
                    419000.0
                ],
                "wikipedia_search": [
                    2.847981721249048,
                    0.45971058644325974,
                    2.6923076923076925
                ],
                "word_count_appended_bing": [
                    36.0,
                    30.0,
                    39.0
                ],
                "answer_relation_to_question_bing": [
                    2.054272371576423,
                    1.1527199293197021,
                    0.7930076991038748
                ],
                "cosine_similarity_raw": [
                    0.03766108304262161,
                    0.05613331496715546,
                    0.09731350094079971
                ],
                "result_count_noun_chunks": [
                    102000.0,
                    259000.0,
                    438000.0
                ],
                "question_answer_similarity": [
                    -1.6880862042307854,
                    -1.2889758683741093,
                    0.8400606848299503
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    6.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    6.0
                ],
                "result_count_bing": [
                    385000.0,
                    383000.0,
                    380000.0
                ],
                "word_count_appended": [
                    206.0,
                    175.0,
                    205.0
                ],
                "answer_relation_to_question": [
                    2.7737385705395186,
                    2.09418952558763,
                    1.132071903872852
                ],
                "result_count": [
                    3340000.0,
                    146000.0,
                    393000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What was the name of the dog on the television show \u201c7th Heaven\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "happy"
            ],
            "lines": [
                [
                    0.5,
                    0.3333333333333333,
                    0.07470969559012156,
                    0.16834532708018576,
                    0.2257495590828924,
                    0.12229004420122079,
                    0.0609189826044771,
                    0.19791425260718423,
                    0.0,
                    0.3328550932568149,
                    0.24647887323943662,
                    0.2995759358810546,
                    0.016666666666666666,
                    0.0,
                    1.0
                ],
                [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.11318735665200726,
                    0.1812326207636048,
                    0.27336860670194,
                    0.21048200378867607,
                    0.7207706701781135,
                    0.0813441483198146,
                    1.0,
                    0.25538020086083213,
                    0.3591549295774648,
                    0.27483445352285435,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.16666666666666666,
                    0.3333333333333333,
                    0.8121029477578712,
                    0.6504220521562094,
                    0.5008818342151675,
                    0.6672279520101031,
                    0.21831034721740938,
                    0.7207415990730012,
                    0.0,
                    0.4117647058823529,
                    0.39436619718309857,
                    0.4255896105960912,
                    0.9833333333333333,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "comet": 0.29545868978799816,
                "eddie": 0.18420269739595627,
                "happy": 0.5203386128160457
            },
            "question": "what was the name of the dog on the television show \u201c7th heaven\u201d?",
            "rate_limited": false,
            "answers": [
                "eddie",
                "comet",
                "happy"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "comet": 0.1499273880156206,
                "eddie": 0.23541647334745136,
                "happy": 0.7745553692078216
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.198303743524218,
                    1.0993378140914172,
                    1.7023584423843645
                ],
                "result_count_important_words": [
                    87900.0,
                    1040000.0,
                    315000.0
                ],
                "wikipedia_search": [
                    0.0,
                    2.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    35.0,
                    51.0,
                    56.0
                ],
                "answer_relation_to_question_bing": [
                    1.0,
                    1.0,
                    1.0
                ],
                "cosine_similarity_raw": [
                    0.032556284219026566,
                    0.04932371526956558,
                    0.3538905382156372
                ],
                "result_count_noun_chunks": [
                    85400.0,
                    35100.0,
                    311000.0
                ],
                "question_answer_similarity": [
                    1.3901408510282636,
                    1.4965599225834012,
                    5.370973348617554
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    59.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    44.0
                ],
                "result_count_bing": [
                    581000.0,
                    1000000.0,
                    3170000.0
                ],
                "word_count_appended": [
                    232.0,
                    178.0,
                    287.0
                ],
                "answer_relation_to_question": [
                    1.5,
                    1.0,
                    0.5
                ],
                "result_count": [
                    128000.0,
                    155000.0,
                    284000.0
                ]
            },
            "integer_answers": {
                "comet": 2,
                "eddie": 2,
                "happy": 10
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What is the name of the gene that determines the color of a Palomino horse?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "cream",
                "middle"
            ],
            "question": "what is the name of the gene that determines the color of a palomino horse?",
            "lines": [
                [
                    0.5610169491525424,
                    0.2504845757087946,
                    0.17830741184898358,
                    0.17357219143029437,
                    0.2584053794428434,
                    0.13445378151260504,
                    0.16515513126491646,
                    0.023393141040199864,
                    0.07789408866995075,
                    0.1905564924114671,
                    0.25225225225225223,
                    0.20687711037608514,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.17796610169491525,
                    0.5184303032294096,
                    0.48340130929237757,
                    0.4034588741035224,
                    0.34774255523535064,
                    0.4631185807656396,
                    0.477326968973747,
                    0.6313876902112197,
                    0.08235837438423646,
                    0.5193929173693086,
                    0.4864864864864865,
                    0.45048617072302005,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.26101694915254237,
                    0.2310851210617957,
                    0.3382912788586389,
                    0.4229689344661833,
                    0.39385206532180594,
                    0.4024276377217554,
                    0.3575178997613365,
                    0.3452191687485805,
                    0.8397475369458128,
                    0.2900505902192243,
                    0.26126126126126126,
                    0.3426367189008948,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "lactose",
                "cream",
                "milk"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "lactose": 0.14799685403758703,
                "milk": 0.1048857195747323,
                "cream": 0.8187504735646736
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.0343855518804257,
                    2.2524308536151003,
                    1.713183594504474
                ],
                "result_count_important_words": [
                    34600.0,
                    100000.0,
                    74900.0
                ],
                "wikipedia_search": [
                    0.31157635467980294,
                    0.3294334975369458,
                    3.358990147783251
                ],
                "word_count_appended_bing": [
                    28.0,
                    54.0,
                    29.0
                ],
                "answer_relation_to_question_bing": [
                    0.5009691514175892,
                    1.0368606064588193,
                    0.4621702421235914
                ],
                "cosine_similarity_raw": [
                    0.020084287971258163,
                    0.054449621587991714,
                    0.03810463845729828
                ],
                "result_count_noun_chunks": [
                    10300.0,
                    278000.0,
                    152000.0
                ],
                "question_answer_similarity": [
                    1.498770635575056,
                    3.4838087148964405,
                    3.652275249361992
                ],
                "word_count_noun_chunks": [
                    0.0,
                    68.0,
                    0.0
                ],
                "result_count_bing": [
                    144000.0,
                    496000.0,
                    431000.0
                ],
                "word_count_raw": [
                    0.0,
                    3.0,
                    0.0
                ],
                "result_count": [
                    26900.0,
                    36200.0,
                    41000.0
                ],
                "answer_relation_to_question": [
                    1.1220338983050848,
                    0.3559322033898305,
                    0.5220338983050847
                ],
                "word_count_appended": [
                    113.0,
                    308.0,
                    172.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "In the original NES \u201cMike Tyson\u2019s Punch-Out!!\u201d, who does Little Mac face right before the final opponent?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "super macho man"
            ],
            "lines": [
                [
                    0.36523377012360814,
                    0.19088635276285318,
                    0.4252266697142876,
                    0.393206579596363,
                    0.42105263157894735,
                    0.2639821029082774,
                    0.4408893709327549,
                    0.11270491803278689,
                    0.0104775828460039,
                    0.3431372549019608,
                    0.17647058823529413,
                    0.323683314704996,
                    0.52,
                    0.2,
                    0.0
                ],
                [
                    0.4810620695910211,
                    0.4170044286942126,
                    0.45769247077795405,
                    0.3606642667217029,
                    0.5252416756176155,
                    0.2639821029082774,
                    0.05368763557483731,
                    0.44774590163934425,
                    0.91973858256753,
                    0.3431372549019608,
                    0.4411764705882353,
                    0.34637095390165856,
                    0.28,
                    0.6,
                    0.0
                ],
                [
                    0.1537041602853708,
                    0.3921092185429342,
                    0.11708085950775837,
                    0.2461291536819341,
                    0.05370569280343716,
                    0.4720357941834452,
                    0.5054229934924078,
                    0.4395491803278688,
                    0.06978383458646617,
                    0.3137254901960784,
                    0.38235294117647056,
                    0.3299457313933454,
                    0.2,
                    0.2,
                    0.0
                ]
            ],
            "fraction_answers": {
                "super macho man": 0.4241074152488821,
                "mr. dream": 0.29906793830986667,
                "mr. sandman": 0.2768246464412512
            },
            "question": "in the original nes \u201cmike tyson\u2019s punch-out!!\u201d, who does little mac face right before the final opponent?",
            "rate_limited": false,
            "answers": [
                "mr. dream",
                "super macho man",
                "mr. sandman"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "super macho man": 0.6374772501228256,
                "mr. dream": 0.26532228872964864,
                "mr. sandman": 0.09989428190398059
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.5605164617549567,
                    3.810080492918245,
                    3.6294030453267996
                ],
                "result_count_important_words": [
                    813.0,
                    99.0,
                    932.0
                ],
                "wikipedia_search": [
                    0.09429824561403508,
                    8.277647243107769,
                    0.6280545112781954
                ],
                "word_count_appended_bing": [
                    6.0,
                    15.0,
                    13.0
                ],
                "answer_relation_to_question_bing": [
                    1.9088635276285317,
                    4.170044286942126,
                    3.921092185429342
                ],
                "cosine_similarity_raw": [
                    0.1723729521036148,
                    0.1855335235595703,
                    0.04746074229478836
                ],
                "result_count_noun_chunks": [
                    2200.0,
                    8740.0,
                    8580.0
                ],
                "question_answer_similarity": [
                    20.357109935488552,
                    18.672327749431133,
                    12.74261037283577
                ],
                "word_count_noun_chunks": [
                    13.0,
                    7.0,
                    5.0
                ],
                "word_count_raw": [
                    1.0,
                    3.0,
                    1.0
                ],
                "result_count_bing": [
                    118000.0,
                    118000.0,
                    211000.0
                ],
                "word_count_appended": [
                    35.0,
                    35.0,
                    32.0
                ],
                "answer_relation_to_question": [
                    4.017571471359689,
                    5.291682765501232,
                    1.6907457631390788
                ],
                "result_count": [
                    784.0,
                    978.0,
                    100.0
                ]
            },
            "integer_answers": {
                "super macho man": 9,
                "mr. dream": 3,
                "mr. sandman": 2
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these do NOT have flippers?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "new yorkers"
            ],
            "lines": [
                [
                    0.5,
                    0,
                    0.3730384513498896,
                    0.299061207536534,
                    0.48055471811878203,
                    0.4080970909764684,
                    0.2815487571701721,
                    0.2815487571701721,
                    0.5,
                    0.44422572178477693,
                    0.4166666666666667,
                    0.3737151248164464,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.5,
                    0,
                    0.18828251621425135,
                    0.3917375172628791,
                    0.03572505275851673,
                    0.1507318880859737,
                    0.3212237093690249,
                    0.3212237093690249,
                    0.4,
                    0.11811023622047245,
                    0.1893939393939394,
                    0.3303964757709251,
                    0.5,
                    0.0,
                    -1.0
                ],
                [
                    0.0,
                    0,
                    0.43867903243585904,
                    0.30920127520058693,
                    0.48372022912270124,
                    0.4411710209375579,
                    0.3972275334608031,
                    0.3972275334608031,
                    0.09999999999999998,
                    0.4376640419947507,
                    0.3939393939393939,
                    0.29588839941262846,
                    0.0,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "new yorkers": 0.1756220776015526,
                "pinball machines": 0.3546586984669101,
                "dolphins": 0.46971922393153737
            },
            "question": "which of these do not have flippers?",
            "rate_limited": false,
            "answers": [
                "new yorkers",
                "dolphins",
                "pinball machines"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "new yorkers": 0.493594252616099,
                "pinball machines": 0.3092522856084977,
                "dolphins": 0.22859587671768583
            },
            "integer_answers": {
                "new yorkers": 3,
                "pinball machines": 4,
                "dolphins": 6
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.2525697503671072,
                    0.3392070484581498,
                    0.40822320117474303
                ],
                "result_count_important_words": [
                    457000.0,
                    374000.0,
                    215000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.2,
                    0.8
                ],
                "word_count_appended_bing": [
                    11.0,
                    41.0,
                    14.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.05516479164361954,
                    0.13544124364852905,
                    0.026643959805369377
                ],
                "result_count_noun_chunks": [
                    457000.0,
                    374000.0,
                    215000.0
                ],
                "question_answer_similarity": [
                    2.8958178497850895,
                    1.5602185428142548,
                    2.7496848478913307
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    4.0
                ],
                "result_count_bing": [
                    9920000.0,
                    37700000.0,
                    6350000.0
                ],
                "word_count_raw": [
                    0.0,
                    6.0,
                    0.0
                ],
                "result_count": [
                    129000.0,
                    3080000.0,
                    108000.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_appended": [
                    85.0,
                    582.0,
                    95.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "In \u201cPeanuts,\u201d what breed of dog is Snoopy?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "beagle"
            ],
            "lines": [
                [
                    0.31523330090105334,
                    0.2967171717171717,
                    0.14459802928745488,
                    0.5113000362162664,
                    0.11110211414141605,
                    0.5833053972510895,
                    0.154627539503386,
                    2.9027593393319743e-05,
                    0.3698524365133837,
                    0.09806157354618016,
                    0.07291666666666667,
                    0.32725024730659974,
                    0.0,
                    0.017543859649122806,
                    1.0
                ],
                [
                    0.0844748085790431,
                    0.2142255892255892,
                    0.16419063488624788,
                    0.21743359478926708,
                    8.09727272556083e-05,
                    0.19678176332551123,
                    0.2832957110609481,
                    0.6634878489901656,
                    0.31762611530542206,
                    0.1949828962371722,
                    0.3125,
                    0.26221871571512967,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.6002918905199035,
                    0.4890572390572391,
                    0.6912113358262972,
                    0.27126636899446643,
                    0.8888169131313284,
                    0.21991283942339926,
                    0.5620767494356659,
                    0.3364831234164411,
                    0.3125214481811942,
                    0.7069555302166477,
                    0.6145833333333334,
                    0.4105310369782706,
                    1.0,
                    0.9824561403508771,
                    1.0
                ]
            ],
            "fraction_answers": {
                "beagle": 0.5775831392046474,
                "pitbull": 0.20794990363155372,
                "border collie": 0.21446695716379893
            },
            "question": "in \u201cpeanuts,\u201d what breed of dog is snoopy?",
            "rate_limited": false,
            "answers": [
                "border collie",
                "pitbull",
                "beagle"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "beagle": 0.7906221929744528,
                "pitbull": 0.08503370439464925,
                "border collie": 0.12693445336246878
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.309000989226399,
                    1.0488748628605187,
                    1.6421241479130824
                ],
                "result_count_important_words": [
                    137000.0,
                    251000.0,
                    498000.0
                ],
                "wikipedia_search": [
                    0.7397048730267674,
                    0.6352522306108441,
                    0.6250428963623884
                ],
                "word_count_appended_bing": [
                    7.0,
                    30.0,
                    59.0
                ],
                "answer_relation_to_question_bing": [
                    0.5934343434343434,
                    0.4284511784511784,
                    0.9781144781144782
                ],
                "cosine_similarity_raw": [
                    0.06967154145240784,
                    0.07911182940006256,
                    0.33304575085639954
                ],
                "result_count_noun_chunks": [
                    49.0,
                    1120000.0,
                    568000.0
                ],
                "question_answer_similarity": [
                    3.995528713800013,
                    1.6991240167990327,
                    2.1197975545364898
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    56.0
                ],
                "result_count_bing": [
                    1740000.0,
                    587000.0,
                    656000.0
                ],
                "word_count_appended": [
                    86.0,
                    171.0,
                    620.0
                ],
                "answer_relation_to_question": [
                    0.9456999027031601,
                    0.2534244257371293,
                    1.8008756715597105
                ],
                "result_count": [
                    118000.0,
                    86.0,
                    944000.0
                ]
            },
            "integer_answers": {
                "beagle": 10,
                "pitbull": 1,
                "border collie": 3
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these two-letter combos is an acceptable word in Scrabble?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "fi"
            ],
            "question": "which of these two-letter combos is an acceptable word in scrabble?",
            "lines": [
                [
                    0.24705882352941178,
                    0.2587719298245614,
                    0.31779617514423236,
                    0.37227174369873567,
                    0.3192451986132151,
                    0.24166666666666667,
                    0.024371872814332597,
                    0.2053354890864996,
                    1.0,
                    0.3108581436077058,
                    0.1976470588235294,
                    0.3644542471166029,
                    0.23224852071005916,
                    0.2328767123287671,
                    -1.0
                ],
                [
                    0.41764705882352937,
                    0.3508771929824561,
                    0.25125135037533586,
                    0.2607979040957313,
                    0.0005795215388838618,
                    0.3958333333333333,
                    0.023349652983267875,
                    0.21827000808407437,
                    0.0,
                    0.35814360770577935,
                    0.6282352941176471,
                    0.20409011419470394,
                    0.5517751479289941,
                    0.547945205479452,
                    -1.0
                ],
                [
                    0.33529411764705885,
                    0.39035087719298245,
                    0.4309524744804318,
                    0.36693035220553305,
                    0.680175279847901,
                    0.3625,
                    0.9522784742023995,
                    0.5763945028294261,
                    0.0,
                    0.3309982486865149,
                    0.17411764705882352,
                    0.4314556386886932,
                    0.21597633136094674,
                    0.2191780821917808,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "fi",
                "ro",
                "da"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "fi": 0.3440273556491698,
                "ro": 0.23769470254137576,
                "da": 0.314527746866361
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.8222712355830146,
                    1.0204505709735197,
                    2.157278193443466
                ],
                "result_count_important_words": [
                    453000.0,
                    434000.0,
                    17700000.0
                ],
                "wikipedia_search": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    168.0,
                    534.0,
                    148.0
                ],
                "answer_relation_to_question_bing": [
                    0.5175438596491228,
                    0.7017543859649122,
                    0.7807017543859649
                ],
                "cosine_similarity_raw": [
                    0.046171754598617554,
                    0.036503635346889496,
                    0.06261193007230759
                ],
                "result_count_noun_chunks": [
                    2540000.0,
                    2700000.0,
                    7130000.0
                ],
                "question_answer_similarity": [
                    1.4547713492065668,
                    1.0191515344195068,
                    1.4338981471955776
                ],
                "word_count_noun_chunks": [
                    157.0,
                    373.0,
                    146.0
                ],
                "result_count_bing": [
                    116000.0,
                    190000.0,
                    174000.0
                ],
                "word_count_raw": [
                    51.0,
                    120.0,
                    48.0
                ],
                "result_count": [
                    31400.0,
                    57.0,
                    66900.0
                ],
                "answer_relation_to_question": [
                    0.49411764705882355,
                    0.8352941176470587,
                    0.6705882352941177
                ],
                "word_count_appended": [
                    355.0,
                    409.0,
                    378.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "If you capitalize it, what Disney character name doubles as a healthcare program?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "chip"
            ],
            "question": "if you capitalize it, what disney character name doubles as a healthcare program?",
            "lines": [
                [
                    0.35322398190045246,
                    0.4699453551912568,
                    0.5103806678795209,
                    0.699891797495377,
                    0.9937513831169471,
                    0.6801310043668122,
                    0.9905566928613881,
                    0.6735429352553747,
                    0.08333333333333333,
                    0.4166666666666667,
                    0.4423076923076923,
                    0.3690683270979431,
                    0.3333333333333333,
                    1.0,
                    1.0
                ],
                [
                    0.4590543597896539,
                    0.42477231329690346,
                    0.3208181125941807,
                    -0.29229345559689074,
                    0.006237950258152576,
                    0.15938864628820962,
                    0.009414750549268148,
                    0.3131601839194731,
                    0.6666666666666666,
                    0.3434959349593496,
                    0.28846153846153844,
                    0.3566115224877892,
                    0.3333333333333333,
                    0.0,
                    1.0
                ],
                [
                    0.18772165830989362,
                    0.1052823315118397,
                    0.16880121952629845,
                    0.5924016581015137,
                    1.0666624900304187e-05,
                    0.16048034934497818,
                    2.855658934375173e-05,
                    0.01329688082515223,
                    0.25,
                    0.23983739837398374,
                    0.2692307692307692,
                    0.27432015041426766,
                    0.3333333333333333,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "chip",
                "elsa",
                "scar"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "chip": 0.5428235076855523,
                "elsa": 0.3504248725421399,
                "scar": 0.07878779986685233
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.2144099625876588,
                    2.139669134926735,
                    1.645920902485606
                ],
                "result_count_important_words": [
                    2220000.0,
                    21100.0,
                    64.0
                ],
                "wikipedia_search": [
                    0.08333333333333333,
                    0.6666666666666666,
                    0.25
                ],
                "word_count_appended_bing": [
                    46.0,
                    30.0,
                    28.0
                ],
                "answer_relation_to_question_bing": [
                    2.349726775956284,
                    2.1238615664845173,
                    0.5264116575591985
                ],
                "cosine_similarity_raw": [
                    0.13875894248485565,
                    0.08722192049026489,
                    0.045892566442489624
                ],
                "result_count_noun_chunks": [
                    5420000.0,
                    2520000.0,
                    107000.0
                ],
                "question_answer_similarity": [
                    2.7400531247258186,
                    -1.1443191636353731,
                    2.319232801673934
                ],
                "word_count_noun_chunks": [
                    1.0,
                    1.0,
                    1.0
                ],
                "result_count_bing": [
                    623000.0,
                    146000.0,
                    147000.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    7360000.0,
                    46200.0,
                    79.0
                ],
                "answer_relation_to_question": [
                    1.4128959276018098,
                    1.8362174391586157,
                    0.7508866332395745
                ],
                "word_count_appended": [
                    205.0,
                    169.0,
                    118.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Featuring 20 scoops of ice cream, the Vermonster is found on what chain's menu?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ben & jerry's"
            ],
            "lines": [
                [
                    0.6422193351193357,
                    0.5105135090786799,
                    0.3198228465418106,
                    0.10819974874763372,
                    0.0219435736677116,
                    0.3333333333333333,
                    0.0461361014994233,
                    0.043115438108484005,
                    0.15921843066037847,
                    0.71,
                    0.7894736842105263,
                    0.2760707064998635,
                    0.03636363636363636,
                    0.08333333333333333,
                    1.0
                ],
                [
                    0.23272491558093172,
                    0.3584671682986541,
                    0.3070715402804091,
                    0.31262316818317987,
                    0.019704433497536946,
                    0.3333333333333333,
                    0.049596309111880045,
                    0.03894297635605007,
                    0.36529698691253404,
                    0.29,
                    0.10526315789473684,
                    0.23575192402134781,
                    0.03636363636363636,
                    0.08333333333333333,
                    1.0
                ],
                [
                    0.12505574929973262,
                    0.13101932262266608,
                    0.3731056131777803,
                    0.5791770830691864,
                    0.9583519928347515,
                    0.3333333333333333,
                    0.9042675893886967,
                    0.9179415855354659,
                    0.47548458242708747,
                    0.0,
                    0.10526315789473684,
                    0.4881773694787887,
                    0.9272727272727272,
                    0.8333333333333334,
                    1.0
                ]
            ],
            "fraction_answers": {
                "ben & jerry's": 0.5108416742620204,
                "dairy queen": 0.19774806308339737,
                "baskin-robbins": 0.2914102626545821
            },
            "question": "featuring 20 scoops of ice cream, the vermonster is found on what chain's menu?",
            "rate_limited": false,
            "answers": [
                "baskin-robbins",
                "dairy queen",
                "ben & jerry's"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "ben & jerry's": 0.6023840401923439,
                "dairy queen": 0.2975229937551846,
                "baskin-robbins": 0.41540902135596824
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.484636358498771,
                    2.1217673161921304,
                    4.393596325309098
                ],
                "result_count_important_words": [
                    40.0,
                    43.0,
                    784.0
                ],
                "wikipedia_search": [
                    1.2737474452830277,
                    2.9223758953002723,
                    3.8038766594166997
                ],
                "word_count_appended_bing": [
                    45.0,
                    6.0,
                    6.0
                ],
                "answer_relation_to_question_bing": [
                    3.573594563550759,
                    2.5092701780905786,
                    0.9171352583586626
                ],
                "cosine_similarity_raw": [
                    0.13548241555690765,
                    0.13008074462413788,
                    0.1580539047718048
                ],
                "result_count_noun_chunks": [
                    62.0,
                    56.0,
                    1320.0
                ],
                "question_answer_similarity": [
                    2.0412506726570427,
                    5.897816397249699,
                    10.926509757060558
                ],
                "word_count_noun_chunks": [
                    2.0,
                    2.0,
                    51.0
                ],
                "word_count_raw": [
                    2.0,
                    2.0,
                    20.0
                ],
                "result_count_bing": [
                    1710000.0,
                    1710000.0,
                    1710000.0
                ],
                "word_count_appended": [
                    71.0,
                    29.0,
                    0.0
                ],
                "answer_relation_to_question": [
                    5.137754680954686,
                    1.8617993246474538,
                    1.000445994397861
                ],
                "result_count": [
                    49.0,
                    44.0,
                    2140.0
                ]
            },
            "integer_answers": {
                "ben & jerry's": 9,
                "dairy queen": 0,
                "baskin-robbins": 5
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "In baseball, \u201cMendoza line\u201d is a nickname for what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                ".200 batting average"
            ],
            "question": "in baseball, \u201cmendoza line\u201d is a nickname for what?",
            "lines": [
                [
                    0.4578759065896673,
                    0.5129085254943379,
                    0.8484771084207497,
                    0.15867933312348592,
                    0.5151515151515151,
                    0.26499501353469157,
                    0.006161971830985915,
                    0.000874591223667199,
                    0.22804451737176284,
                    0.4,
                    0.5,
                    0.5542083062674252,
                    0,
                    0,
                    1.0
                ],
                [
                    0.16506251336794414,
                    0.2827260458839406,
                    0.04054714901544375,
                    0.37892184779848775,
                    0.0,
                    0.10813506197464026,
                    0.9881161971830986,
                    0.946840063883185,
                    0.6612684597274738,
                    0.1,
                    0.14285714285714285,
                    0.0890033335478495,
                    0,
                    0,
                    1.0
                ],
                [
                    0.37706158004238854,
                    0.20436542862172152,
                    0.11097574256380655,
                    0.4623988190780264,
                    0.48484848484848486,
                    0.6268699244906681,
                    0.005721830985915493,
                    0.05228534489314777,
                    0.11068702290076336,
                    0.5,
                    0.35714285714285715,
                    0.3567883601847252,
                    0,
                    0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                ".200 batting average",
                "rubber on pitching mound",
                "hard-hit ball"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                ".200 batting average": 0.8260449972074836,
                "hard-hit ball": 0.2320017834927576,
                "rubber on pitching mound": -0.03978636333009935
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.216833225069701,
                    0.356013334191398,
                    1.4271534407389008
                ],
                "result_count_important_words": [
                    28.0,
                    4490.0,
                    26.0
                ],
                "wikipedia_search": [
                    0.4560890347435257,
                    1.3225369194549477,
                    0.22137404580152673
                ],
                "word_count_appended_bing": [
                    7.0,
                    2.0,
                    5.0
                ],
                "answer_relation_to_question_bing": [
                    1.5387255764830134,
                    0.8481781376518218,
                    0.6130962858651645
                ],
                "cosine_similarity_raw": [
                    0.2416570782661438,
                    0.011548344045877457,
                    0.03160730376839638
                ],
                "result_count_noun_chunks": [
                    46.0,
                    49800.0,
                    2750.0
                ],
                "question_answer_similarity": [
                    4.00352166313678,
                    9.560298725962639,
                    11.666444852948189
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    186000.0,
                    75900.0,
                    440000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    34.0,
                    0,
                    32.0
                ],
                "answer_relation_to_question": [
                    1.8315036263586693,
                    0.6602500534717766,
                    1.5082463201695542
                ],
                "word_count_appended": [
                    12.0,
                    3.0,
                    15.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "The material that forms images in an Etch A Sketch is also the main component in which item?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "soda cans"
            ],
            "lines": [
                [
                    0.3732890499194847,
                    0.35155145929339476,
                    0.3547598550790928,
                    0.4638491537378296,
                    0.021739130434782608,
                    0.07805402820941908,
                    0.015503875968992248,
                    0.024691358024691357,
                    0.5069444444444444,
                    0.19230769230769232,
                    0.3333333333333333,
                    0.18723346258808535,
                    0,
                    0.3333333333333333,
                    -1.0
                ],
                [
                    0.4759863123993559,
                    0.32754224270353305,
                    0.20819305408269634,
                    0.28927240464062043,
                    0.30434782608695654,
                    0.04578054028209419,
                    0.3875968992248062,
                    0.43209876543209874,
                    0.23333333333333334,
                    0.15384615384615385,
                    0.3333333333333333,
                    0.2638081805215039,
                    0,
                    0.3333333333333333,
                    -1.0
                ],
                [
                    0.1507246376811594,
                    0.3209062980030722,
                    0.43704709083821086,
                    0.24687844162154998,
                    0.6739130434782609,
                    0.8761654315084867,
                    0.5968992248062015,
                    0.5432098765432098,
                    0.25972222222222224,
                    0.6538461538461539,
                    0.3333333333333333,
                    0.5489583568904108,
                    0,
                    0.3333333333333333,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "zinc supplement tablets": 0.24896847512881354,
                "soda cans": 0.45961057262350813,
                "u.s. nickels": 0.2914209522476784
            },
            "question": "the material that forms images in an etch a sketch is also the main component in which item?",
            "rate_limited": false,
            "answers": [
                "zinc supplement tablets",
                "u.s. nickels",
                "soda cans"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "zinc supplement tablets": 0.2759109726539752,
                "soda cans": 0.4908365697962334,
                "u.s. nickels": 0.22918877229780046
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.4978677007046826,
                    2.1104654441720307,
                    4.391666855123286
                ],
                "result_count_important_words": [
                    2.0,
                    50.0,
                    77.0
                ],
                "wikipedia_search": [
                    3.548611111111111,
                    1.6333333333333333,
                    1.8180555555555555
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.7577572964669739,
                    1.6377112135176652,
                    1.6045314900153609
                ],
                "cosine_similarity_raw": [
                    0.10283080488443375,
                    0.06034690514206886,
                    0.1266826093196869
                ],
                "result_count_noun_chunks": [
                    2.0,
                    35.0,
                    44.0
                ],
                "question_answer_similarity": [
                    10.300920786336064,
                    6.42401112918742,
                    5.4825480449944735
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    1.0
                ],
                "result_count_bing": [
                    6530.0,
                    3830.0,
                    73300.0
                ],
                "word_count_appended": [
                    5.0,
                    4.0,
                    17.0
                ],
                "answer_relation_to_question": [
                    2.239734299516908,
                    2.8559178743961353,
                    0.9043478260869564
                ],
                "result_count": [
                    2.0,
                    28.0,
                    62.0
                ]
            },
            "integer_answers": {
                "zinc supplement tablets": 5,
                "soda cans": 7,
                "u.s. nickels": 1
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "The time a baby spends lying on its stomach is commonly known as what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "tummy time",
                "top"
            ],
            "question": "the time a baby spends lying on its stomach is commonly known as what?",
            "lines": [
                [
                    0.5619082865064818,
                    0.4355010938639442,
                    0.933948280946433,
                    0.15097627143372305,
                    0.9990589509236462,
                    0.4826368452030606,
                    0.9990528460031399,
                    0.9239442501435506,
                    0.4687028657616893,
                    0.80078125,
                    0.8,
                    0.5037091602377688,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.22517421602787455,
                    0.3610271903323263,
                    0.01505458264622695,
                    0.426600967293242,
                    0.0004254057468449074,
                    0.24749852854620366,
                    0.000402216080858407,
                    0.060552278540481286,
                    0.013763197586726998,
                    0.15234375,
                    0.1,
                    0.22312276389001753,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.2129174974656435,
                    0.20347171580372958,
                    0.05099713640734003,
                    0.42242276127303496,
                    0.0005156433295089787,
                    0.2698646262507357,
                    0.0005449379160017126,
                    0.015503471315968053,
                    0.5175339366515836,
                    0.046875,
                    0.1,
                    0.27316807587221364,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "tummy time",
                "time to make the donuts",
                "vegas, baby, vegas!!!"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "vegas, baby, vegas!!!": 0.03920510680317075,
                "tummy time": 0.9847032991080854,
                "time to make the donuts": -0.005725013567749663
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.525964121664382,
                    1.5618593472301228,
                    1.9121765311054955
                ],
                "result_count_important_words": [
                    154000.0,
                    62.0,
                    84.0
                ],
                "wikipedia_search": [
                    1.8748114630467572,
                    0.05505279034690799,
                    2.0701357466063346
                ],
                "word_count_appended_bing": [
                    16.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.7420043754557768,
                    1.444108761329305,
                    0.8138868632149183
                ],
                "cosine_similarity_raw": [
                    0.6820166707038879,
                    0.010993624106049538,
                    0.037240710109472275
                ],
                "result_count_noun_chunks": [
                    3540000.0,
                    232000.0,
                    59400.0
                ],
                "question_answer_similarity": [
                    10.143081359565258,
                    28.660452919080853,
                    28.379747327417135
                ],
                "word_count_noun_chunks": [
                    93.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    37.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    164000.0,
                    84100.0,
                    91700.0
                ],
                "word_count_appended": [
                    205.0,
                    39.0,
                    12.0
                ],
                "answer_relation_to_question": [
                    3.3714497190388912,
                    1.3510452961672472,
                    1.277504984793861
                ],
                "result_count": [
                    155000.0,
                    66.0,
                    80.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Where in the home does the Maillard reaction typically occur?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "kitchen"
            ],
            "lines": [
                [
                    0.6632124352331606,
                    0.3428571428571429,
                    0.6096959892668528,
                    0.36107745697333116,
                    0.7786116322701688,
                    0.4020319303338171,
                    0.7777149321266968,
                    0.2511415525114155,
                    0.7032784793978824,
                    0.46153846153846156,
                    0.35398230088495575,
                    0.4265766208373214,
                    1.0,
                    0,
                    3.0
                ],
                [
                    0.17357512953367876,
                    0.4285714285714286,
                    0.21774296632798598,
                    0.29718541806908144,
                    0.04127579737335835,
                    0.22496371552975328,
                    0.041289592760180995,
                    0.319634703196347,
                    0.12590891695369308,
                    0.25040916530278234,
                    0.24778761061946902,
                    0.264138823729545,
                    0.0,
                    0,
                    3.0
                ],
                [
                    0.16321243523316062,
                    0.2285714285714286,
                    0.17256104440516126,
                    0.34173712495758735,
                    0.1801125703564728,
                    0.37300435413642963,
                    0.18099547511312217,
                    0.4292237442922374,
                    0.17081260364842454,
                    0.28805237315875615,
                    0.39823008849557523,
                    0.30928455543313366,
                    0.0,
                    0,
                    3.0
                ]
            ],
            "fraction_answers": {
                "bathroom": 0.24890752290780688,
                "kitchen": 0.5485937641716313,
                "bedroom": 0.20249871292056182
            },
            "question": "where in the home does the maillard reaction typically occur?",
            "rate_limited": false,
            "answers": [
                "kitchen",
                "bedroom",
                "bathroom"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "bathroom": 0.083052429444395,
                "kitchen": 0.6275505259603896,
                "bedroom": 0.13551494393744656
            },
            "integer_answers": {
                "bathroom": 2,
                "kitchen": 10,
                "bedroom": 1
            },
            "categorical_data": {
                "question_type": 3
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.132883104186607,
                    1.320694118647725,
                    1.5464227771656682
                ],
                "result_count_important_words": [
                    165000.0,
                    8760.0,
                    38400.0
                ],
                "wikipedia_search": [
                    2.109835438193647,
                    0.3777267508610792,
                    0.5124378109452736
                ],
                "word_count_appended_bing": [
                    40.0,
                    28.0,
                    45.0
                ],
                "answer_relation_to_question_bing": [
                    0.34285714285714286,
                    0.42857142857142855,
                    0.22857142857142856
                ],
                "cosine_similarity_raw": [
                    0.1090206429362297,
                    0.038934942334890366,
                    0.030855895951390266
                ],
                "result_count_noun_chunks": [
                    82500.0,
                    105000.0,
                    141000.0
                ],
                "question_answer_similarity": [
                    2.3942533154040575,
                    1.9705942831933498,
                    2.266010321676731
                ],
                "word_count_noun_chunks": [
                    5.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    277000.0,
                    155000.0,
                    257000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    166000.0,
                    8800.0,
                    38400.0
                ],
                "answer_relation_to_question": [
                    1.3264248704663213,
                    0.3471502590673575,
                    0.32642487046632124
                ],
                "word_count_appended": [
                    282.0,
                    153.0,
                    176.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "A town in West Texas is home to a permanent art installation depicting which of these?": {
        "raw_data": {
            "data": {
                "word_count_appended_relation_to_question": [
                    2.828006622343224,
                    2.1531482406583646,
                    3.0188451369984115
                ],
                "result_count_important_words": [
                    294000.0,
                    60100.0,
                    168000.0
                ],
                "wikipedia_search": [
                    2.261904761904762,
                    0.047619047619047616,
                    3.6904761904761907
                ],
                "word_count_appended_bing": [
                    44.0,
                    27.0,
                    28.0
                ],
                "result_count": [
                    118000.0,
                    17000.0,
                    183000.0
                ],
                "answer_relation_to_question_bing": [
                    2.6120689655172415,
                    0.8103448275862069,
                    0.5775862068965517
                ],
                "cosine_similarity_raw": [
                    0.08715148270130157,
                    0.011054197326302528,
                    0.03127097338438034
                ],
                "result_count_noun_chunks": [
                    291000.0,
                    21900.0,
                    767000.0
                ],
                "question_answer_similarity": [
                    0.8136250898241997,
                    -0.10423483559861779,
                    4.58960743714124
                ],
                "word_count_noun_chunks": [
                    72.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    334000.0,
                    21800.0,
                    180000.0
                ],
                "word_count_appended": [
                    180.0,
                    114.0,
                    189.0
                ],
                "answer_relation_to_question": [
                    0.45175730669626346,
                    0.799667036625971,
                    2.7485756566777653
                ]
            },
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "z-best_answer_by_ml": [
                "prada",
                "top"
            ],
            "lines": [
                [
                    0.11293932667406587,
                    0.6530172413793104,
                    0.673105771617316,
                    0.15354320518949566,
                    0.3710691823899371,
                    0.6233669279581934,
                    0.563110515226968,
                    0.26946939531438097,
                    0.376984126984127,
                    0.37267080745341613,
                    0.4444444444444444,
                    0.353500827792903,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.19991675915649276,
                    0.2025862068965517,
                    0.08537598891383877,
                    -0.019670670128510943,
                    0.05345911949685535,
                    0.04068682344158268,
                    0.11511204750047883,
                    0.0202796555236596,
                    0.007936507936507936,
                    0.2360248447204969,
                    0.2727272727272727,
                    0.2691435300822956,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.6871439141694413,
                    0.14439655172413793,
                    0.24151823946884526,
                    0.8661274649390153,
                    0.5754716981132075,
                    0.335946248600224,
                    0.32177743727255315,
                    0.7102509491619594,
                    0.6150793650793651,
                    0.391304347826087,
                    0.2828282828282828,
                    0.37735564212480144,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "question": "a town in west texas is home to a permanent art installation depicting which of these?",
            "rate_limited": false,
            "answers": [
                "prada",
                "kleenex",
                "coca-cola"
            ],
            "ml_answers": {
                "kleenex": 0.057609278916925855,
                "coca-cola": 0.2621849132644567,
                "prada": 0.9052147794395901
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What advertising mascot wears epaulettes?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "cap'n crunch"
            ],
            "lines": [
                [
                    0.2593201754385965,
                    0.1875,
                    0.31883078843428436,
                    0.5168333507362561,
                    0.06930693069306931,
                    0.03712770297837617,
                    0.07766990291262135,
                    0.06666666666666667,
                    0.08333333333333333,
                    0.15,
                    0.11224489795918367,
                    0.18644387195267403,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.34813596491228066,
                    0.6875,
                    0.29941114708592725,
                    0.3766530791713206,
                    0.5742574257425742,
                    0.7071943424452605,
                    0.5825242718446602,
                    0.6285714285714286,
                    0.21794871794871795,
                    0.3375,
                    0.2653061224489796,
                    0.4048589315641491,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.3925438596491228,
                    0.125,
                    0.3817580644797884,
                    0.10651357009242324,
                    0.3564356435643564,
                    0.2556779545763634,
                    0.33980582524271846,
                    0.3047619047619048,
                    0.6987179487179488,
                    0.5125,
                    0.6224489795918368,
                    0.4086971964831769,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "sun-maid raisin girl": 0.14751983007893293,
                "mr. peanut": 0.38784724512394997,
                "cap'n crunch": 0.46463292479711704
            },
            "question": "what advertising mascot wears epaulettes?",
            "rate_limited": false,
            "answers": [
                "sun-maid raisin girl",
                "mr. peanut",
                "cap'n crunch"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sun-maid raisin girl": 0.23951736155597145,
                "mr. peanut": 0.2429223168910891,
                "cap'n crunch": 0.6419846675078794
            },
            "integer_answers": {
                "sun-maid raisin girl": 1,
                "mr. peanut": 5,
                "cap'n crunch": 8
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.7457754878106961,
                    1.6194357262565964,
                    1.6347887859327077
                ],
                "result_count_important_words": [
                    8.0,
                    60.0,
                    35.0
                ],
                "wikipedia_search": [
                    0.25,
                    0.6538461538461539,
                    2.0961538461538463
                ],
                "word_count_appended_bing": [
                    11.0,
                    26.0,
                    61.0
                ],
                "answer_relation_to_question_bing": [
                    0.375,
                    1.375,
                    0.25
                ],
                "cosine_similarity_raw": [
                    0.06325826793909073,
                    0.05940527468919754,
                    0.07574348151683807
                ],
                "result_count_noun_chunks": [
                    7.0,
                    66.0,
                    32.0
                ],
                "question_answer_similarity": [
                    3.976561378221959,
                    2.898002006812021,
                    0.8195248013362288
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    10.0
                ],
                "result_count_bing": [
                    27300.0,
                    520000.0,
                    188000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count": [
                    7.0,
                    58.0,
                    36.0
                ],
                "answer_relation_to_question": [
                    0.7779605263157895,
                    1.044407894736842,
                    1.1776315789473684
                ],
                "word_count_appended": [
                    12.0,
                    27.0,
                    41.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What spice comes from the crocus flower?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "saffron"
            ],
            "question": "what spice comes from the crocus flower?",
            "lines": [
                [
                    0.2866986736667588,
                    0.21632996632996634,
                    0.036096921707444156,
                    0.3524370956702531,
                    0.13568904593639575,
                    0.6873156342182891,
                    0.6490469779635797,
                    0.057910547396528705,
                    0.12972103004291846,
                    0.1794095382286147,
                    0.1921182266009852,
                    0.30971356479917145,
                    0.0019455252918287938,
                    0.0,
                    1.0
                ],
                [
                    0.3549236667587731,
                    0.27946127946127947,
                    0.018041737790959585,
                    0.18123040536564214,
                    0.06395759717314488,
                    0.16755162241887905,
                    0.02359526677531354,
                    0.060914552736982645,
                    0.14019313304721032,
                    0.17713853141559424,
                    0.1477832512315271,
                    0.3279009916114701,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.3583776595744681,
                    0.5042087542087542,
                    0.9458613405015963,
                    0.4663324989641047,
                    0.8003533568904594,
                    0.14513274336283186,
                    0.3273577552611068,
                    0.8811748998664887,
                    0.7300858369098713,
                    0.6434519303557911,
                    0.6600985221674877,
                    0.36238544358935837,
                    0.9980544747081712,
                    1.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "cloves",
                "allspice",
                "saffron"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "cloves": 0.060317000258034235,
                "allspice": 0.1957251093736472,
                "saffron": 0.897113359330006
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2388542591966858,
                    1.3116039664458805,
                    1.4495417743574335
                ],
                "result_count_important_words": [
                    916000.0,
                    33300.0,
                    462000.0
                ],
                "wikipedia_search": [
                    0.5188841201716738,
                    0.5607725321888413,
                    2.920343347639485
                ],
                "word_count_appended_bing": [
                    39.0,
                    30.0,
                    134.0
                ],
                "answer_relation_to_question_bing": [
                    0.648989898989899,
                    0.8383838383838383,
                    1.5126262626262628
                ],
                "cosine_similarity_raw": [
                    0.0270970668643713,
                    0.013543486595153809,
                    0.7100347280502319
                ],
                "result_count_noun_chunks": [
                    34700.0,
                    36500.0,
                    528000.0
                ],
                "question_answer_similarity": [
                    1.2277398607693613,
                    0.6313290949910879,
                    1.6245026541873813
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    513.0
                ],
                "result_count_bing": [
                    2330000.0,
                    568000.0,
                    492000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    152.0
                ],
                "result_count": [
                    76800.0,
                    36200.0,
                    453000.0
                ],
                "answer_relation_to_question": [
                    1.146794694667035,
                    1.4196946670350925,
                    1.4335106382978724
                ],
                "word_count_appended": [
                    237.0,
                    234.0,
                    850.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these songs was written by the man nicknamed \u201cSlowhand\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "lay down sally"
            ],
            "lines": [
                [
                    0.3583851809954751,
                    0.5307528409090909,
                    0.8170045398442776,
                    0.2857582590948446,
                    0.9992678631497715,
                    0.9965955011750183,
                    0.9993088114054446,
                    0.6146953405017921,
                    0.2256383712905452,
                    0.717948717948718,
                    0.4782608695652174,
                    0.6421594351305486,
                    0,
                    1.0,
                    -1.0
                ],
                [
                    0.35859728506787336,
                    0.3730113636363636,
                    0.06971840804309642,
                    0.35340078968954997,
                    0.00042543087243010073,
                    0.0019321093426005998,
                    0.0003997235245621778,
                    0.2007168458781362,
                    0.6922015182884748,
                    0.16666666666666666,
                    0.43478260869565216,
                    0.20992457072288348,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.2830175339366516,
                    0.09623579545454546,
                    0.11327705211262598,
                    0.36084095121560544,
                    0.0003067059777984447,
                    0.001472389482381156,
                    0.00029146506999325466,
                    0.18458781362007168,
                    0.08216011042097998,
                    0.11538461538461539,
                    0.08695652173913043,
                    0.14791599414656798,
                    0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "lay down sally": 0.6665981331546726,
                "lover lay down": 0.11326514988930515,
                "lay lady lay": 0.22013671695602227
            },
            "question": "which of these songs was written by the man nicknamed \u201cslowhand\u201d?",
            "rate_limited": false,
            "answers": [
                "lay down sally",
                "lay lady lay",
                "lover lay down"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "lay down sally": 0.7610576120966963,
                "lover lay down": 0.09645008842074577,
                "lay lady lay": 0.14524095017430944
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.210797175652743,
                    1.0496228536144174,
                    0.7395799707328399
                ],
                "result_count_important_words": [
                    120000.0,
                    48.0,
                    35.0
                ],
                "wikipedia_search": [
                    0.6769151138716356,
                    2.0766045548654244,
                    0.24648033126293994
                ],
                "word_count_appended_bing": [
                    11.0,
                    10.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    2.1230113636363637,
                    1.4920454545454545,
                    0.3849431818181818
                ],
                "cosine_similarity_raw": [
                    0.36614540219306946,
                    0.031244715675711632,
                    0.05076577886939049
                ],
                "result_count_noun_chunks": [
                    343000.0,
                    112000.0,
                    103000.0
                ],
                "question_answer_similarity": [
                    8.875952580478042,
                    10.97700084373355,
                    11.208100099116564
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    31000000.0,
                    60100.0,
                    45800.0
                ],
                "word_count_appended": [
                    56.0,
                    13.0,
                    9.0
                ],
                "answer_relation_to_question": [
                    1.4335407239819005,
                    1.4343891402714934,
                    1.1320701357466063
                ],
                "result_count": [
                    101000.0,
                    43.0,
                    31.0
                ]
            },
            "integer_answers": {
                "lay down sally": 10,
                "lover lay down": 1,
                "lay lady lay": 2
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these quantities is the largest?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "two half-dozens"
            ],
            "lines": [
                [
                    0.6666666666666667,
                    0.0,
                    0.2150256011156894,
                    0.13127215168613302,
                    0.6051722612407617,
                    0.48461747890008167,
                    0.7308921535405081,
                    0.656397326543452,
                    0,
                    0.7505197505197505,
                    0.6888888888888889,
                    0.5245962802093774,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.16666666666666669,
                    0.0,
                    0.6155967652629101,
                    0.5353932794064021,
                    5.008322161992511e-06,
                    0.25619384699156006,
                    6.6444741230955274e-06,
                    1.1848327193925126e-06,
                    0,
                    0.008316008316008316,
                    0.044444444444444446,
                    0.0743345584140773,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.16666666666666669,
                    1.0,
                    0.16937763362140051,
                    0.3333345689074648,
                    0.3948227304370763,
                    0.2591886741083583,
                    0.26910120198536885,
                    0.34360148862382867,
                    0,
                    0.24116424116424118,
                    0.26666666666666666,
                    0.40106916137654525,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "two half-dozens": 0.15463258246646125,
                "baker's dozen": 0.3495448212325106,
                "dozen": 0.49582259630102815
            },
            "question": "which of these quantities is the largest?",
            "rate_limited": false,
            "answers": [
                "dozen",
                "two half-dozens",
                "baker's dozen"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "two half-dozens": 0.39979374258720957,
                "baker's dozen": 0.2799566335016134,
                "dozen": 0.3781749382740332
            },
            "integer_answers": {
                "two half-dozens": 2,
                "baker's dozen": 1,
                "dozen": 8
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.0491925604187549,
                    0.1486691168281546,
                    0.8021383227530905
                ],
                "result_count_important_words": [
                    1540000.0,
                    14.0,
                    567000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    31.0,
                    2.0,
                    12.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "cosine_similarity_raw": [
                    0.04077501595020294,
                    0.11673478782176971,
                    0.03211885318160057
                ],
                "result_count_noun_chunks": [
                    8310000.0,
                    15.0,
                    4350000.0
                ],
                "question_answer_similarity": [
                    2.4537939727306366,
                    10.007795142941177,
                    6.230829201638699
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    17800000.0,
                    9410000.0,
                    9520000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    361.0,
                    4.0,
                    116.0
                ],
                "answer_relation_to_question": [
                    1.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "result_count": [
                    1450000.0,
                    12.0,
                    946000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these substances expands when it freezes?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "carbon dioxide"
            ],
            "lines": [
                [
                    0.3333333333333333,
                    0.0,
                    0.36573226741823556,
                    0.36134323360892967,
                    0.22020665547667811,
                    0.26605504587155965,
                    0.4199177188253653,
                    0.40579304554677115,
                    0.0,
                    0.27472527472527475,
                    0.3157894736842105,
                    0.3279938615105052,
                    0.0,
                    0,
                    2.0
                ],
                [
                    0.0,
                    0.0,
                    0.2757820595324963,
                    0.4964247601308732,
                    0.7776528742358213,
                    0.5504587155963303,
                    0.5518513264292807,
                    0.5499195410340726,
                    1.0,
                    0.38095238095238093,
                    0.3684210526315789,
                    0.3609837153086255,
                    1.0,
                    0,
                    2.0
                ],
                [
                    0.6666666666666666,
                    1.0,
                    0.3584856730492682,
                    0.1422320062601971,
                    0.0021404702875005774,
                    0.1834862385321101,
                    0.02823095474535395,
                    0.04428741341915623,
                    0.0,
                    0.3443223443223443,
                    0.3157894736842105,
                    0.3110224231808693,
                    0.0,
                    0,
                    2.0
                ]
            ],
            "fraction_answers": {
                "sodium chloride": 0.25314537769237405,
                "carbon dioxide": 0.4855728019885739,
                "dihydrogen monoxide": 0.26128182031905206
            },
            "question": "which of these substances expands when it freezes?",
            "rate_limited": false,
            "answers": [
                "sodium chloride",
                "carbon dioxide",
                "dihydrogen monoxide"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sodium chloride": 0.0596709997427442,
                "carbon dioxide": 0.3906118941771382,
                "dihydrogen monoxide": 0.128900673471955
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 2
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.9839815845315155,
                    1.0829511459258765,
                    0.9330672695426079
                ],
                "result_count_important_words": [
                    296000.0,
                    389000.0,
                    19900.0
                ],
                "wikipedia_search": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    6.0,
                    7.0,
                    6.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "cosine_similarity_raw": [
                    0.028181370347738266,
                    0.02125028893351555,
                    0.02762298658490181
                ],
                "result_count_noun_chunks": [
                    5800000.0,
                    7860000.0,
                    633000.0
                ],
                "question_answer_similarity": [
                    2.973916858434677,
                    4.085661016404629,
                    1.170593834016472
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    2030000.0,
                    4200000.0,
                    1400000.0
                ],
                "word_count_appended": [
                    75.0,
                    104.0,
                    94.0
                ],
                "answer_relation_to_question": [
                    0.3333333333333333,
                    0.0,
                    0.6666666666666666
                ],
                "result_count": [
                    286000.0,
                    1010000.0,
                    2780.0
                ]
            },
            "integer_answers": {
                "sodium chloride": 1,
                "carbon dioxide": 10,
                "dihydrogen monoxide": 2
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these is NOT a skin care brand?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "kristin ess"
            ],
            "lines": [
                [
                    0.2249806164943976,
                    0.21261582323592304,
                    0.30846336459861035,
                    0.15931768978302185,
                    0.3918032786885246,
                    0.3328173374613003,
                    0.21455756422454803,
                    0.21793635486981677,
                    0.5,
                    0.339323467230444,
                    0.3854166666666667,
                    0.2944018896934909,
                    0.5,
                    0,
                    -1.0
                ],
                [
                    0.4554379303618448,
                    0.4641304347826087,
                    0.40008827932463287,
                    0.524452446262377,
                    0.2169398907103825,
                    0.33436532507739936,
                    0.35965746907706947,
                    0.35776277724204436,
                    0.3232456140350878,
                    0.3583509513742072,
                    0.30208333333333337,
                    0.3987892849375494,
                    0.5,
                    0,
                    -1.0
                ],
                [
                    0.31958145314375763,
                    0.3232537419814683,
                    0.2914483560767568,
                    0.3162298639546012,
                    0.3912568306010929,
                    0.3328173374613003,
                    0.4257849666983825,
                    0.42430086788813887,
                    0.17675438596491228,
                    0.3023255813953488,
                    0.3125,
                    0.3068088253689597,
                    0.0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "kristin ess": 0.23149173284330204,
                "dr. dennis gross": 0.37205629954665476,
                "drunk elephant": 0.39645196761004325
            },
            "question": "which of these is not a skin care brand?",
            "rate_limited": false,
            "answers": [
                "dr. dennis gross",
                "kristin ess",
                "drunk elephant"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "kristin ess": 0.42267916692500057,
                "dr. dennis gross": 0.19464335480584577,
                "drunk elephant": 0.13445441139038655
            },
            "integer_answers": {
                "kristin ess": 2,
                "dr. dennis gross": 7,
                "drunk elephant": 4
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2335886618390544,
                    0.6072642903747035,
                    1.159147047786242
                ],
                "result_count_important_words": [
                    1200000.0,
                    590000.0,
                    312000.0
                ],
                "wikipedia_search": [
                    0.0,
                    1.0605263157894735,
                    1.9394736842105262
                ],
                "word_count_appended_bing": [
                    11.0,
                    19.0,
                    18.0
                ],
                "answer_relation_to_question_bing": [
                    1.7243050605844616,
                    0.2152173913043478,
                    1.0604775481111903
                ],
                "cosine_similarity_raw": [
                    0.054375119507312775,
                    0.028363825753331184,
                    0.059205491095781326
                ],
                "result_count_noun_chunks": [
                    1170000.0,
                    590000.0,
                    314000.0
                ],
                "question_answer_similarity": [
                    7.580420333892107,
                    -0.5440840786322951,
                    4.089014410972595
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    2.0
                ],
                "result_count_bing": [
                    10800000.0,
                    10700000.0,
                    10800000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    198000.0,
                    518000.0,
                    199000.0
                ],
                "answer_relation_to_question": [
                    1.6501163010336144,
                    0.2673724178289312,
                    1.0825112811374544
                ],
                "word_count_appended": [
                    152.0,
                    134.0,
                    187.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which classic toy was introduced before Vin Diesel was born?": {
        "raw_data": {
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1455128116208346,
                    3.283287354968386,
                    1.5711998334107795
                ],
                "result_count_important_words": [
                    11.0,
                    10200.0,
                    18.0
                ],
                "wikipedia_search": [
                    1.6199608610567515,
                    0.6477495107632094,
                    1.732289628180039
                ],
                "word_count_appended_bing": [
                    3.0,
                    2.0,
                    0.0
                ],
                "answer_relation_to_question_bing": [
                    0.9204545454545454,
                    0.7954545454545454,
                    1.2840909090909092
                ],
                "cosine_similarity_raw": [
                    0.04148854687809944,
                    0.038663167506456375,
                    0.02616088092327118
                ],
                "result_count_noun_chunks": [
                    38.0,
                    20000.0,
                    74.0
                ],
                "question_answer_similarity": [
                    3.039310361724347,
                    10.99026960413903,
                    11.397492479067296
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    249000.0,
                    367000.0,
                    260000.0
                ],
                "word_count_appended": [
                    3.0,
                    12.0,
                    0.0
                ],
                "answer_relation_to_question": [
                    2.06624634502924,
                    1.5021016081871346,
                    1.4316520467836258
                ],
                "result_count": [
                    10.0,
                    48.0,
                    18.0
                ]
            },
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "z-best_answer_by_ml": [
                "guess who?",
                "middle"
            ],
            "lines": [
                [
                    0.41324926900584796,
                    0.3068181818181818,
                    0.3902505320086467,
                    0.1195304873695865,
                    0.13157894736842105,
                    0.2842465753424658,
                    0.0010753739368462216,
                    0.0018894192521877486,
                    0.4049902152641879,
                    0.2,
                    0.6,
                    0.19091880193680577,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.30042032163742693,
                    0.26515151515151514,
                    0.3636743830259019,
                    0.43222709291215045,
                    0.631578947368421,
                    0.4189497716894977,
                    0.9971649232574055,
                    0.994431185361973,
                    0.16193737769080235,
                    0.8,
                    0.4,
                    0.5472145591613976,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.28633040935672516,
                    0.42803030303030304,
                    0.2460750849654514,
                    0.44824241971826306,
                    0.23684210526315788,
                    0.2968036529680365,
                    0.0017597028057483625,
                    0.0036793953858393,
                    0.43307240704500977,
                    0.0,
                    0.0,
                    0.26186663890179657,
                    0,
                    0,
                    -1.0
                ]
            ],
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "question": "which classic toy was introduced before vin diesel was born?",
            "rate_limited": false,
            "answers": [
                "nerf ball",
                "guess who?",
                "rock \u2018em sock \u2018em robots"
            ],
            "ml_answers": {
                "guess who?": 0.33123249836848717,
                "rock \u2018em sock \u2018em robots": 0.21412937667982088,
                "nerf ball": 0.12288443800544364
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What soda is named for a medical condition?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "pepsi"
            ],
            "lines": [
                [
                    0.28270874424720577,
                    0.37142857142857144,
                    0.17036371173248932,
                    4.749959552994105,
                    0.07273802483737433,
                    0.012075817792723938,
                    0.10181958962446767,
                    0.014821915787921245,
                    0.024390243902439025,
                    0.2558613659531091,
                    0.5024390243902439,
                    0.2948559141601118,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.5950032873109796,
                    0.5,
                    0.6093064097383513,
                    -6.197112683480115,
                    0.41395623891188643,
                    0.02491592785081015,
                    0.4039230868499161,
                    0.8166801858270039,
                    0.8048780487804879,
                    0.46992864424057085,
                    0.24878048780487805,
                    0.39136588779980946,
                    0,
                    1.0,
                    1.0
                ],
                [
                    0.1222879684418146,
                    0.12857142857142856,
                    0.22032987852915947,
                    2.4471531304860106,
                    0.5133057362507392,
                    0.9630082543564659,
                    0.4942573235256162,
                    0.16849789838507484,
                    0.17073170731707318,
                    0.2742099898063201,
                    0.24878048780487805,
                    0.31377819804007867,
                    0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "fanta": 0.4665316924242045,
                "pepsi": 0.006278886279583035,
                "faygo": 0.5271894212962124
            },
            "question": "what soda is named for a medical condition?",
            "rate_limited": false,
            "answers": [
                "faygo",
                "pepsi",
                "fanta"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "fanta": 0.07844826540933159,
                "pepsi": 0.7468675511902355,
                "faygo": 0.16288566345960515
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1794236566404472,
                    1.5654635511992379,
                    1.2551127921603147
                ],
                "result_count_important_words": [
                    78900.0,
                    313000.0,
                    383000.0
                ],
                "wikipedia_search": [
                    0.04878048780487805,
                    1.6097560975609757,
                    0.34146341463414637
                ],
                "word_count_appended_bing": [
                    103.0,
                    51.0,
                    51.0
                ],
                "answer_relation_to_question_bing": [
                    0.7428571428571429,
                    1.0,
                    0.2571428571428571
                ],
                "cosine_similarity_raw": [
                    0.040354788303375244,
                    0.1443290412425995,
                    0.05219049006700516
                ],
                "result_count_noun_chunks": [
                    8040.0,
                    443000.0,
                    91400.0
                ],
                "question_answer_similarity": [
                    -0.7330479547381401,
                    0.9563830443657935,
                    -0.3776622889563441
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    12.0,
                    0.0
                ],
                "result_count_bing": [
                    158000.0,
                    326000.0,
                    12600000.0
                ],
                "word_count_appended": [
                    251.0,
                    461.0,
                    269.0
                ],
                "answer_relation_to_question": [
                    0.8481262327416172,
                    1.7850098619329389,
                    0.3668639053254438
                ],
                "result_count": [
                    61500.0,
                    350000.0,
                    434000.0
                ]
            },
            "integer_answers": {
                "fanta": 3,
                "pepsi": 9,
                "faygo": 1
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these is NOT a step in the famous Korean 10-step skin care regime?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "spelunking"
            ],
            "lines": [
                [
                    0.31541468076655743,
                    0.43366666666666664,
                    0.29150090393827166,
                    0.15906619240928,
                    0.2187250672760957,
                    0.14471403812824957,
                    0.20857666323142104,
                    0.16343564889606893,
                    0.4184626436781609,
                    0.29101899827288424,
                    0.31521739130434784,
                    0.29717944872943014,
                    0.11363636363636365,
                    0.07142857142857145,
                    -1.0
                ],
                [
                    0.20074272664084997,
                    0.14966666666666667,
                    0.26285103966736934,
                    0.2844611204772735,
                    0.28498695046586164,
                    0.48994800693240903,
                    0.2984655902897057,
                    0.34203913121522167,
                    0.3315373563218391,
                    0.33506044905008636,
                    0.3315217391304348,
                    0.2961734585174761,
                    0.38636363636363635,
                    0.4285714285714286,
                    -1.0
                ],
                [
                    0.4838425925925926,
                    0.4166666666666667,
                    0.445648056394359,
                    0.5564726871134464,
                    0.4962879822580427,
                    0.3653379549393414,
                    0.49295774647887325,
                    0.4945252198887094,
                    0.25,
                    0.3739205526770294,
                    0.3532608695652174,
                    0.4066470927530937,
                    0.5,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "moisturizing": 0.36823009995567735,
                "spelunking": 0.12349036838180404,
                "cleansing": 0.5082795316625186
            },
            "question": "which of these is not a step in the famous korean 10-step skin care regime?",
            "rate_limited": false,
            "answers": [
                "cleansing",
                "moisturizing",
                "spelunking"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "moisturizing": 0.2063267973494002,
                "spelunking": 0.44465527131842314,
                "cleansing": 0.1615122192269276
            },
            "integer_answers": {
                "moisturizing": 4,
                "spelunking": 1,
                "cleansing": 9
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.839487717787977,
                    2.853571580755334,
                    1.3069407014566883
                ],
                "result_count_important_words": [
                    509000.0,
                    352000.0,
                    12300.0
                ],
                "wikipedia_search": [
                    0.6522988505747127,
                    1.3477011494252873,
                    2.0
                ],
                "answer_relation_to_question": [
                    1.4766825538675405,
                    2.3940581868732003,
                    0.12925925925925924
                ],
                "word_count_appended_bing": [
                    34.0,
                    31.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    0.6633333333333332,
                    3.5033333333333334,
                    0.8333333333333333
                ],
                "cosine_similarity_raw": [
                    0.16280262172222137,
                    0.18517333269119263,
                    0.04243969917297363
                ],
                "result_count_noun_chunks": [
                    750000.0,
                    352000.0,
                    12200.0
                ],
                "question_answer_similarity": [
                    3.367438416928053,
                    2.1288997661322355,
                    -0.5577865610830486
                ],
                "word_count_noun_chunks": [
                    17.0,
                    5.0,
                    0.0
                ],
                "result_count_bing": [
                    2050000.0,
                    58000.0,
                    777000.0
                ],
                "word_count_raw": [
                    6.0,
                    1.0,
                    0.0
                ],
                "result_count": [
                    416000.0,
                    318000.0,
                    5490.0
                ],
                "word_count_appended": [
                    242.0,
                    191.0,
                    146.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "The Olympic flame is said to be a reference to what mythical figure?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "prometheus"
            ],
            "question": "the olympic flame is said to be a reference to what mythical figure?",
            "lines": [
                [
                    0.24722222222222223,
                    0.4444444444444444,
                    0.19327807279956638,
                    0.09092321584408959,
                    0.24498269896193772,
                    0.3673281033687125,
                    0.45045045045045046,
                    0.4407894736842105,
                    0.3995144818349238,
                    0.4111888111888112,
                    0.3898305084745763,
                    0.36564322070815763,
                    0.8,
                    0.5,
                    1.0
                ],
                [
                    0.45972222222222225,
                    0.4444444444444444,
                    0.5988437114551165,
                    0.7849807946189914,
                    0.0006920415224913495,
                    0.20996769727734194,
                    0.31981981981981983,
                    0.3201754385964912,
                    0.05434455047714716,
                    0.23636363636363636,
                    0.2457627118644068,
                    0.28656354184896293,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.2930555555555555,
                    0.1111111111111111,
                    0.20787821574531712,
                    0.124095989536919,
                    0.754325259515571,
                    0.42270419935394554,
                    0.22972972972972974,
                    0.23903508771929824,
                    0.546140967687929,
                    0.35244755244755244,
                    0.3644067796610169,
                    0.34779323744287943,
                    0.2,
                    0.5,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "prometheus",
                "icarus",
                "odysseus"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "odysseus": 0.263279815854872,
                "prometheus": 0.5842425265289843,
                "icarus": 0.25863168210554954
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.1938593242489457,
                    1.7193812510937776,
                    2.0867594246572767
                ],
                "result_count_important_words": [
                    200000.0,
                    142000.0,
                    102000.0
                ],
                "wikipedia_search": [
                    1.997572409174619,
                    0.2717227523857358,
                    2.730704838439645
                ],
                "word_count_appended_bing": [
                    46.0,
                    29.0,
                    43.0
                ],
                "answer_relation_to_question_bing": [
                    1.3333333333333333,
                    1.3333333333333333,
                    0.3333333333333333
                ],
                "cosine_similarity_raw": [
                    0.025699248537421227,
                    0.07962534576654434,
                    0.027640558779239655
                ],
                "result_count_noun_chunks": [
                    201000.0,
                    146000.0,
                    109000.0
                ],
                "question_answer_similarity": [
                    -0.08075769222341478,
                    -0.6972172819077969,
                    -0.11022163741290569
                ],
                "word_count_noun_chunks": [
                    4.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    79600.0,
                    45500.0,
                    91600.0
                ],
                "word_count_appended": [
                    294.0,
                    169.0,
                    252.0
                ],
                "answer_relation_to_question": [
                    1.2361111111111112,
                    2.298611111111111,
                    1.4652777777777777
                ],
                "result_count": [
                    35400.0,
                    100.0,
                    109000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "The director of \u201cLady Bird\u201d starred in which indie movie?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "frances ha"
            ],
            "question": "the director of \u201clady bird\u201d starred in which indie movie?",
            "lines": [
                [
                    0.26977124183006534,
                    0.3047297297297297,
                    0.11522702037664947,
                    0.46967509650502687,
                    0.7760532150776053,
                    0.8477508650519031,
                    0.22424469881778947,
                    0.39575184474801706,
                    0.0,
                    0.3886363636363636,
                    0.4803921568627451,
                    0.3315855952952758,
                    0.0,
                    0.06666666666666667,
                    -1.0
                ],
                [
                    0.7010270774976656,
                    0.675,
                    0.7809352144802486,
                    0.3507798725442844,
                    0.032520325203252036,
                    0.07785467128027682,
                    0.03171326702946144,
                    0.0011977251597664854,
                    1.0,
                    0.20681818181818182,
                    0.11764705882352941,
                    0.3316901884507498,
                    1.0,
                    0.5333333333333333,
                    -1.0
                ],
                [
                    0.029201680672268904,
                    0.02027027027027027,
                    0.10383776514310193,
                    0.17954503095068877,
                    0.19142645971914266,
                    0.07439446366782007,
                    0.744042034152749,
                    0.6030504300922165,
                    0.0,
                    0.40454545454545454,
                    0.4019607843137255,
                    0.33672421625397436,
                    0.0,
                    0.4,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "room",
                "frances ha",
                "brooklyn"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "brooklyn": 0.035434444478283623,
                "frances ha": 0.7650164909859677,
                "room": 0.14000548538894023
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.989513571771655,
                    1.990141130704499,
                    2.0203452975238463
                ],
                "result_count_important_words": [
                    239000.0,
                    33800.0,
                    793000.0
                ],
                "wikipedia_search": [
                    0.0,
                    4.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    49.0,
                    12.0,
                    41.0
                ],
                "answer_relation_to_question_bing": [
                    1.218918918918919,
                    2.7,
                    0.08108108108108109
                ],
                "cosine_similarity_raw": [
                    0.04257582128047943,
                    0.2885517477989197,
                    0.03836754709482193
                ],
                "result_count_noun_chunks": [
                    1890000.0,
                    5720.0,
                    2880000.0
                ],
                "question_answer_similarity": [
                    2.997769206762314,
                    2.2389032505452633,
                    1.145972117781639
                ],
                "word_count_noun_chunks": [
                    0.0,
                    2.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    8.0,
                    6.0
                ],
                "result_count_bing": [
                    147000000.0,
                    13500000.0,
                    12900000.0
                ],
                "word_count_appended": [
                    171.0,
                    91.0,
                    178.0
                ],
                "answer_relation_to_question": [
                    1.6186274509803922,
                    4.206162464985994,
                    0.17521008403361343
                ],
                "result_count": [
                    315000.0,
                    13200.0,
                    77700.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "The inventor of the Erector Set made another toy that contained what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "uranium ore"
            ],
            "lines": [
                [
                    0.3690378289473684,
                    0.3675889328063241,
                    0.25497711972187104,
                    0.23705153961247064,
                    0.6049382716049383,
                    0.1618682021753039,
                    0.34615384615384615,
                    0.39473684210526316,
                    0.2516666666666667,
                    0.6153846153846154,
                    0.3333333333333333,
                    0.5573524593451482,
                    0,
                    0,
                    1.0
                ],
                [
                    0.21504934210526316,
                    0.25691699604743085,
                    0.3517550321990939,
                    0.44099157939983646,
                    0.2345679012345679,
                    0.37747920665387075,
                    0.5384615384615384,
                    0.4824561403508772,
                    0.12666666666666668,
                    0.25,
                    0.3333333333333333,
                    0.2781703580839518,
                    0,
                    0,
                    1.0
                ],
                [
                    0.4159128289473684,
                    0.37549407114624506,
                    0.3932678480790351,
                    0.3219568809876929,
                    0.16049382716049382,
                    0.46065259117082535,
                    0.11538461538461539,
                    0.12280701754385964,
                    0.6216666666666667,
                    0.1346153846153846,
                    0.3333333333333333,
                    0.16447718257089994,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "uranium ore": 0.3745074714880958,
                "asbestos powder": 0.3016718539672017,
                "live ants": 0.32382067454470254
            },
            "question": "the inventor of the erector set made another toy that contained what?",
            "rate_limited": false,
            "answers": [
                "uranium ore",
                "live ants",
                "asbestos powder"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "uranium ore": 0.45842188051008687,
                "asbestos powder": 0.15964102633967603,
                "live ants": 0.08871006430833575
            },
            "integer_answers": {
                "uranium ore": 4,
                "asbestos powder": 5,
                "live ants": 3
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.7867622967257413,
                    1.390851790419759,
                    0.8223859128544997
                ],
                "result_count_important_words": [
                    36.0,
                    56.0,
                    12.0
                ],
                "wikipedia_search": [
                    0.5033333333333334,
                    0.25333333333333335,
                    1.2433333333333334
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.3675889328063241,
                    0.25691699604743085,
                    0.37549407114624506
                ],
                "cosine_similarity_raw": [
                    0.013845871202647686,
                    0.01910114474594593,
                    0.021355390548706055
                ],
                "result_count_noun_chunks": [
                    45.0,
                    55.0,
                    14.0
                ],
                "question_answer_similarity": [
                    2.8342179199680686,
                    5.272550597786903,
                    3.8493568236008286
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    7590.0,
                    17700.0,
                    21600.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    49.0,
                    19.0,
                    13.0
                ],
                "answer_relation_to_question": [
                    1.4761513157894737,
                    0.8601973684210527,
                    1.6636513157894737
                ],
                "word_count_appended": [
                    32.0,
                    13.0,
                    7.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these is a French territory?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "french guiana"
            ],
            "lines": [
                [
                    0.7015376166941242,
                    0.71712158808933,
                    0.7572529712598172,
                    0.22107387686146088,
                    0.9998595287788639,
                    0.5757561278608629,
                    0.9981115148908668,
                    0.9982419187232177,
                    0.48484848484848486,
                    0.6218181818181818,
                    0.8235294117647058,
                    0.48912059909511674,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.18753432180120816,
                    0.184863523573201,
                    0.09038061833842549,
                    0.3876407651729475,
                    0.00013822347490870598,
                    0.008552725882746429,
                    0.0018839855903562545,
                    0.0017539203805604197,
                    0.18181818181818182,
                    0.3018181818181818,
                    0.11764705882352941,
                    0.32189921472775496,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.11092806150466777,
                    0.09801488833746898,
                    0.15236641040175739,
                    0.3912853579655916,
                    2.247746227487368e-06,
                    0.41569114625639064,
                    4.499518776967863e-06,
                    4.160896221967805e-06,
                    0.3333333333333333,
                    0.07636363636363637,
                    0.058823529411764705,
                    0.1889801861771283,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "french guiana": 0.7420194157632166,
                "french stewart": 0.12756648010014301,
                "french cyprus": 0.1304141041366404
            },
            "question": "which of these is a french territory?",
            "rate_limited": false,
            "answers": [
                "french guiana",
                "french stewart",
                "french cyprus"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "french guiana": 0.8130286131296881,
                "french stewart": 0.17421635870487695,
                "french cyprus": 0.07792305784352663
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.9782411981902335,
                    0.6437984294555099,
                    0.3779603723542566
                ],
                "result_count_important_words": [
                    99600000.0,
                    188000.0,
                    449.0
                ],
                "wikipedia_search": [
                    0.9696969696969697,
                    0.36363636363636365,
                    0.6666666666666666
                ],
                "answer_relation_to_question": [
                    1.4030752333882481,
                    0.37506864360241626,
                    0.22185612300933552
                ],
                "answer_relation_to_question_bing": [
                    1.43424317617866,
                    0.369727047146402,
                    0.19602977667493796
                ],
                "word_count_appended": [
                    171.0,
                    83.0,
                    21.0
                ],
                "cosine_similarity_raw": [
                    0.13738316297531128,
                    0.016397129744291306,
                    0.027642782777547836
                ],
                "result_count_noun_chunks": [
                    107000000.0,
                    188000.0,
                    446.0
                ],
                "question_answer_similarity": [
                    1.5891291573643684,
                    2.786449721083045,
                    2.8126478805206716
                ],
                "word_count_noun_chunks": [
                    4.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    24100000.0,
                    358000.0,
                    17400000.0
                ],
                "result_count": [
                    38700000.0,
                    5350.0,
                    87.0
                ],
                "word_count_appended_bing": [
                    28.0,
                    4.0,
                    2.0
                ]
            },
            "integer_answers": {
                "french guiana": 12,
                "french stewart": 0,
                "french cyprus": 1
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Where does Queen Elizabeth II typically spend her summer vacation?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "scotland"
            ],
            "question": "where does queen elizabeth ii typically spend her summer vacation?",
            "lines": [
                [
                    0.4418016194331984,
                    0.35576923076923084,
                    0.16895829086213465,
                    0.20452004737342733,
                    0.06067305330385655,
                    0.21094190637337845,
                    0.015422112030928269,
                    0.2181571815718157,
                    0.17916666666666664,
                    0.30996309963099633,
                    0.3191489361702128,
                    0.2863194362376804,
                    0.0,
                    0.0,
                    3.0
                ],
                [
                    0.10989010989010989,
                    0.3511904761904762,
                    0.6649187410816951,
                    0.3595165959631331,
                    0.09432571849668386,
                    0.4066553863508178,
                    0.11892255326301635,
                    0.42005420054200543,
                    0.5333333333333333,
                    0.4022140221402214,
                    0.375886524822695,
                    0.3577461731932607,
                    0.9428571428571428,
                    0.9230769230769231,
                    3.0
                ],
                [
                    0.4483082706766917,
                    0.29304029304029305,
                    0.16612296805617027,
                    0.43596335666343955,
                    0.8450012281994596,
                    0.3824027072758037,
                    0.8656553347060554,
                    0.3617886178861789,
                    0.2875,
                    0.2878228782287823,
                    0.3049645390070922,
                    0.35593439056905884,
                    0.05714285714285714,
                    0.07692307692307693,
                    3.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "monaco",
                "scotland",
                "france"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "monaco": 0.07375860162363788,
                "scotland": 0.8367414738488148,
                "france": 0.04948552969503649
            },
            "categorical_data": {
                "question_type": 3
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.004236053663763,
                    2.5042232123528247,
                    2.491540733983412
                ],
                "result_count_important_words": [
                    36700.0,
                    283000.0,
                    2060000.0
                ],
                "wikipedia_search": [
                    0.8958333333333333,
                    2.6666666666666665,
                    1.4374999999999998
                ],
                "word_count_appended_bing": [
                    45.0,
                    53.0,
                    43.0
                ],
                "answer_relation_to_question_bing": [
                    0.7115384615384616,
                    0.7023809523809523,
                    0.586080586080586
                ],
                "cosine_similarity_raw": [
                    0.06082022935152054,
                    0.2393520325422287,
                    0.059799592941999435
                ],
                "result_count_noun_chunks": [
                    1610000.0,
                    3100000.0,
                    2670000.0
                ],
                "question_answer_similarity": [
                    1.0881454527843744,
                    1.912801967933774,
                    2.3195356652140617
                ],
                "word_count_noun_chunks": [
                    0.0,
                    33.0,
                    2.0
                ],
                "result_count_bing": [
                    37400.0,
                    72100.0,
                    67800.0
                ],
                "word_count_raw": [
                    0.0,
                    12.0,
                    1.0
                ],
                "result_count": [
                    494000.0,
                    768000.0,
                    6880000.0
                ],
                "answer_relation_to_question": [
                    1.7672064777327936,
                    0.43956043956043955,
                    1.7932330827067668
                ],
                "word_count_appended": [
                    252.0,
                    327.0,
                    234.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Catherine de\u2019 Medici was present at the performance of the first-ever formal what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ballet"
            ],
            "question": "catherine de\u2019 medici was present at the performance of the first-ever formal what?",
            "lines": [
                [
                    0.08222811671087533,
                    0.13507625272331156,
                    0.07816444551380787,
                    0.6003400338647699,
                    0.9831492272042116,
                    0.4196505299341163,
                    0.5323741007194245,
                    0.5549738219895288,
                    0.31058582222529885,
                    0.30802292263610315,
                    0.3225806451612903,
                    0.34251158999112846,
                    0.0,
                    0.05714285714285714,
                    1.0
                ],
                [
                    0.11352785145888596,
                    0.15468409586056645,
                    0.1153983734532698,
                    -0.10404605610236911,
                    0.002654727890391235,
                    0.12489258092237181,
                    0.06806862202545656,
                    0.07657068062827226,
                    0.504052631444006,
                    0.0,
                    0.0,
                    0.31350112422706666,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.8042440318302386,
                    0.710239651416122,
                    0.8064371810329224,
                    0.5037060222375992,
                    0.014196044905397196,
                    0.4554568891435119,
                    0.39955727725511897,
                    0.36845549738219896,
                    0.1853615463306951,
                    0.6919770773638968,
                    0.6774193548387096,
                    0.34398728578180493,
                    1.0,
                    0.9428571428571428,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "opera",
                "commedia dell\u2019arte",
                "ballet"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "opera": 0.07722765325456858,
                "ballet": 0.8906167103308177,
                "commedia dell\u2019arte": 0.1645709992650553
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7125579499556423,
                    1.5675056211353333,
                    1.7199364289090247
                ],
                "result_count_important_words": [
                    96200.0,
                    12300.0,
                    72200.0
                ],
                "wikipedia_search": [
                    1.5529291111264942,
                    2.5202631572200302,
                    0.9268077316534755
                ],
                "word_count_appended_bing": [
                    30.0,
                    0.0,
                    63.0
                ],
                "answer_relation_to_question_bing": [
                    0.40522875816993464,
                    0.46405228758169936,
                    2.130718954248366
                ],
                "cosine_similarity_raw": [
                    0.03004775382578373,
                    0.044361114501953125,
                    0.31000828742980957
                ],
                "result_count_noun_chunks": [
                    84800.0,
                    11700.0,
                    56300.0
                ],
                "question_answer_similarity": [
                    2.802265089121647,
                    -0.48566581308841705,
                    2.3511971910484135
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    38.0
                ],
                "result_count_bing": [
                    29300.0,
                    8720.0,
                    31800.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    33.0
                ],
                "word_count_appended": [
                    215.0,
                    0.0,
                    483.0
                ],
                "answer_relation_to_question": [
                    0.41114058355437666,
                    0.5676392572944298,
                    4.021220159151193
                ],
                "result_count": [
                    4370000.0,
                    11800.0,
                    63100.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "For which presidential candidate did Olympic hero Jesse Owens campaign in 1936?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "alf landon"
            ],
            "question": "for which presidential candidate did olympic hero jesse owens campaign in 1936?",
            "lines": [
                [
                    0.16344852236165958,
                    0.03488372093023256,
                    0.30385434980621534,
                    0.9256643978248802,
                    0.8524438267315265,
                    0.675739089629282,
                    0.8938459104572062,
                    0.7158218125960062,
                    0.5481935356238762,
                    0.224,
                    0.2857142857142857,
                    0.32680782438381567,
                    0.0,
                    0.3333333333333333,
                    -1.0
                ],
                [
                    0.6746241543566615,
                    0.5642764857881137,
                    0.6013344113690662,
                    0.08222221361286315,
                    0.05304609682649988,
                    0.22477709995307368,
                    0.03591203923429837,
                    0.08064516129032258,
                    0.1379371253674659,
                    0.584,
                    0.5714285714285714,
                    0.40375698163191415,
                    1.0,
                    0.6666666666666666,
                    -1.0
                ],
                [
                    0.161927323281679,
                    0.40083979328165376,
                    0.09481123882471844,
                    -0.007886611437743317,
                    0.0945100764419736,
                    0.0994838104176443,
                    0.0702420503084955,
                    0.20353302611367127,
                    0.3138693390086579,
                    0.192,
                    0.14285714285714285,
                    0.2694351939842701,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "franklin roosevelt",
                "alf landon",
                "wendell willkie"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "wendell willkie": 0.11043211933365504,
                "franklin roosevelt": 0.2689482827157812,
                "alf landon": 0.7097847526785325
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.6144625950705254,
                    3.230055853055313,
                    2.155481551874161
                ],
                "result_count_important_words": [
                    56500.0,
                    2270.0,
                    4440.0
                ],
                "wikipedia_search": [
                    2.740967678119381,
                    0.6896856268373296,
                    1.5693466950432895
                ],
                "word_count_appended_bing": [
                    4.0,
                    8.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.13953488372093023,
                    2.257105943152455,
                    1.603359173126615
                ],
                "cosine_similarity_raw": [
                    0.04888065159320831,
                    0.096735879778862,
                    0.015252159908413887
                ],
                "result_count_noun_chunks": [
                    93200.0,
                    10500.0,
                    26500.0
                ],
                "question_answer_similarity": [
                    3.9653243941720575,
                    0.3522202540661965,
                    -0.033784352941438556
                ],
                "word_count_noun_chunks": [
                    0.0,
                    2.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    2.0,
                    0.0
                ],
                "result_count_bing": [
                    28800.0,
                    9580.0,
                    4240.0
                ],
                "result_count": [
                    36800.0,
                    2290.0,
                    4080.0
                ],
                "answer_relation_to_question": [
                    1.3075881788932766,
                    5.396993234853292,
                    1.295418586253432
                ],
                "word_count_appended": [
                    28.0,
                    73.0,
                    24.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these is the most common color for lab coats?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "white",
                "top"
            ],
            "question": "which of these is the most common color for lab coats?",
            "lines": [
                [
                    0.26119778124458315,
                    0.2157997392213278,
                    0.3833978862009905,
                    0.27282956962425486,
                    0.9861258217658507,
                    0.4078656650463986,
                    0.9999933621130274,
                    0.9358620369200846,
                    0.6564361459163439,
                    0.8197802197802198,
                    0.8947368421052632,
                    0.5881002490197001,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.5925463685214075,
                    0.36740518037191494,
                    0.40150436872893247,
                    0.3416966719350711,
                    4.0805206417897274e-07,
                    0.29606716747680073,
                    6.896505945607086e-07,
                    1.963346930601576e-06,
                    0.3131041701586256,
                    0.026373626373626374,
                    0.05263157894736842,
                    0.06790410743325757,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.14625585023400936,
                    0.4167950804067573,
                    0.21509774507007703,
                    0.38547375844067405,
                    0.013873770182085073,
                    0.29606716747680073,
                    5.948236378086111e-06,
                    0.06413599973298481,
                    0.03045968392503046,
                    0.15384615384615385,
                    0.05263157894736842,
                    0.3439956435470423,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "white",
                "pantone midnight navy",
                "rose gold"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "rose gold": 0.2676534664689986,
                "white": 0.848800561869315,
                "pantone midnight navy": 0.13578764363667073
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.3524009960788006,
                    0.2716164297330303,
                    1.3759825741881693
                ],
                "result_count_important_words": [
                    11600000.0,
                    8.0,
                    69.0
                ],
                "wikipedia_search": [
                    2.6257445836653757,
                    1.2524166806345025,
                    0.12183873570012184
                ],
                "word_count_appended_bing": [
                    34.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.6473992176639835,
                    1.1022155411157448,
                    1.2503852412202718
                ],
                "cosine_similarity_raw": [
                    0.08746455609798431,
                    0.09159518778324127,
                    0.049070246517658234
                ],
                "result_count_noun_chunks": [
                    2860000.0,
                    6.0,
                    196000.0
                ],
                "question_answer_similarity": [
                    3.9828855395317078,
                    4.9882376585155725,
                    5.627314738929272
                ],
                "word_count_noun_chunks": [
                    122.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    27.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    9230000.0,
                    6700000.0,
                    6700000.0
                ],
                "word_count_appended": [
                    373.0,
                    12.0,
                    70.0
                ],
                "answer_relation_to_question": [
                    0.7835933437337494,
                    1.7776391055642227,
                    0.4387675507020281
                ],
                "result_count": [
                    14500000.0,
                    6.0,
                    204000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which creature is known for being poisonous, as opposed to venomous?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "box jellyfish",
                "middle"
            ],
            "question": "which creature is known for being poisonous, as opposed to venomous?",
            "lines": [
                [
                    0.19523809523809524,
                    0.35714285714285715,
                    0.22153586739018982,
                    0.24213104121248902,
                    0.048402087652820475,
                    0.162532981530343,
                    0.012502264903062148,
                    0.08637400228050171,
                    0.12527131782945738,
                    0.23121387283236994,
                    0.07407407407407407,
                    0.24435425648292602,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.6560776942355889,
                    0.4642857142857143,
                    0.5193780685501262,
                    0.45103286946626847,
                    0.8364910273825695,
                    0.4179419525065963,
                    0.6323609349519841,
                    0.34635119726339797,
                    0.6084496124031007,
                    0.3786127167630058,
                    0.8148148148148148,
                    0.3862217410050541,
                    0,
                    1.0,
                    -1.0
                ],
                [
                    0.1486842105263158,
                    0.17857142857142858,
                    0.2590860640596839,
                    0.3068360893212425,
                    0.11510688496461,
                    0.41952506596306066,
                    0.3551368001449538,
                    0.5672748004561003,
                    0.26627906976744187,
                    0.3901734104046243,
                    0.1111111111111111,
                    0.3694240025120198,
                    0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "pickerel frog",
                "box jellyfish",
                "black mamba"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "box jellyfish": 0.4533510519384467,
                "pickerel frog": 0.06633407681513018,
                "black mamba": 0.10584650460361038
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.22177128241463,
                    1.9311087050252704,
                    1.847120012560099
                ],
                "result_count_important_words": [
                    1380.0,
                    69800.0,
                    39200.0
                ],
                "wikipedia_search": [
                    0.6263565891472869,
                    3.0422480620155037,
                    1.3313953488372094
                ],
                "word_count_appended_bing": [
                    2.0,
                    22.0,
                    3.0
                ],
                "answer_relation_to_question_bing": [
                    1.4285714285714286,
                    1.8571428571428572,
                    0.7142857142857143
                ],
                "cosine_similarity_raw": [
                    0.05355997383594513,
                    0.12556827068328857,
                    0.06263835728168488
                ],
                "result_count_noun_chunks": [
                    60600.0,
                    243000.0,
                    398000.0
                ],
                "question_answer_similarity": [
                    2.6681510331109166,
                    4.970134397502989,
                    3.3811651105061173
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count_bing": [
                    30800.0,
                    79200.0,
                    79500.0
                ],
                "word_count_appended": [
                    80.0,
                    131.0,
                    135.0
                ],
                "answer_relation_to_question": [
                    0.780952380952381,
                    2.6243107769423557,
                    0.5947368421052632
                ],
                "result_count": [
                    67700.0,
                    1170000.0,
                    161000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "In the movie industry, what does a Foley artist work on?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "sound effects"
            ],
            "question": "in the movie industry, what does a foley artist work on?",
            "lines": [
                [
                    0.32937080825279585,
                    0.19141184902054467,
                    0.789869268938773,
                    0.33925362837151657,
                    0.19055374592833876,
                    0.09868841970569418,
                    0.25,
                    0.22151898734177214,
                    0.44942528735632176,
                    0.5217391304347826,
                    0.627906976744186,
                    0.4012402735076236,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.3701247831682614,
                    0.6227902532250359,
                    0.10977157883966016,
                    0.3222952234743572,
                    0.5944625407166124,
                    0.06158029430582214,
                    0.4563106796116505,
                    0.5775316455696202,
                    0.3689655172413793,
                    0.27053140096618356,
                    0.11627906976744186,
                    0.3214185216046289,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.30050440857894267,
                    0.1857978977544195,
                    0.10035915222156683,
                    0.33845114815412625,
                    0.21498371335504887,
                    0.8397312859884837,
                    0.2936893203883495,
                    0.20094936708860758,
                    0.18160919540229886,
                    0.20772946859903382,
                    0.2558139534883721,
                    0.2773412048877475,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "sound effects",
                "visual effects",
                "set design"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "visual effects": 0.03870600032097025,
                "set design": 0.07335658221233708,
                "sound effects": 1.1172898107471818
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.006201367538118,
                    1.6070926080231445,
                    1.3867060244387375
                ],
                "result_count_important_words": [
                    103000.0,
                    188000.0,
                    121000.0
                ],
                "wikipedia_search": [
                    2.247126436781609,
                    1.8448275862068966,
                    0.9080459770114943
                ],
                "word_count_appended_bing": [
                    27.0,
                    5.0,
                    11.0
                ],
                "answer_relation_to_question_bing": [
                    0.7656473960821787,
                    2.4911610129001436,
                    0.743191591017678
                ],
                "cosine_similarity_raw": [
                    0.4404125511646271,
                    0.06120605394244194,
                    0.05595790594816208
                ],
                "result_count_noun_chunks": [
                    140000.0,
                    365000.0,
                    127000.0
                ],
                "question_answer_similarity": [
                    7.5355284512043,
                    7.158847019076347,
                    7.5177036970853806
                ],
                "word_count_noun_chunks": [
                    49.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    14.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    6170000.0,
                    3850000.0,
                    52500000.0
                ],
                "result_count": [
                    117000.0,
                    365000.0,
                    132000.0
                ],
                "answer_relation_to_question": [
                    1.6468540412639792,
                    1.8506239158413071,
                    1.5025220428947135
                ],
                "word_count_appended": [
                    108.0,
                    56.0,
                    43.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What animal term also means an indentation in a brick for holding mortar?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "frog"
            ],
            "question": "what animal term also means an indentation in a brick for holding mortar?",
            "lines": [
                [
                    0.5011130599028603,
                    0.2527472527472528,
                    0.06572387824001256,
                    0.5350378103954142,
                    0.5327960599955227,
                    0.31943250214961305,
                    0.5405996935872182,
                    0.2766117748358627,
                    0.3952759197324415,
                    0.38247863247863245,
                    0.3277310924369748,
                    0.3711513482174355,
                    0.11475409836065574,
                    0.02564102564102564,
                    1.0
                ],
                [
                    0.2990420939017809,
                    0.6043956043956045,
                    0.8584593149357809,
                    0.4978261618449155,
                    0.3581822252070741,
                    0.5631986242476354,
                    0.35237469905887503,
                    0.6726940049510278,
                    0.5902313266443702,
                    0.41452991452991456,
                    0.44537815126050423,
                    0.37881736819352163,
                    0.8852459016393442,
                    0.9743589743589743,
                    1.0
                ],
                [
                    0.19984484619535886,
                    0.14285714285714288,
                    0.07581680682420659,
                    -0.032863972240329734,
                    0.10902171479740318,
                    0.1173688736027515,
                    0.10702560735390676,
                    0.05069422021310946,
                    0.014492753623188408,
                    0.202991452991453,
                    0.226890756302521,
                    0.2500312835890428,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "bat",
                "frog",
                "coot"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "bat": 0.15330880456889054,
                "frog": 0.962252563626604,
                "coot": -0.03313965481801407
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.598059437522049,
                    2.651721577354652,
                    1.7502189851232999
                ],
                "result_count_important_words": [
                    49400.0,
                    32200.0,
                    9780.0
                ],
                "wikipedia_search": [
                    1.5811036789297659,
                    2.3609253065774802,
                    0.057971014492753624
                ],
                "word_count_appended_bing": [
                    39.0,
                    53.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    0.7582417582417582,
                    1.8131868131868132,
                    0.42857142857142855
                ],
                "cosine_similarity_raw": [
                    0.029861804097890854,
                    0.39004307985305786,
                    0.034447550773620605
                ],
                "result_count_noun_chunks": [
                    51400.0,
                    125000.0,
                    9420.0
                ],
                "question_answer_similarity": [
                    2.5953746140003204,
                    2.4148674309253693,
                    -0.1594173675402999
                ],
                "word_count_noun_chunks": [
                    7.0,
                    54.0,
                    0.0
                ],
                "result_count_bing": [
                    74300.0,
                    131000.0,
                    27300.0
                ],
                "word_count_raw": [
                    1.0,
                    38.0,
                    0.0
                ],
                "result_count": [
                    47600.0,
                    32000.0,
                    9740.0
                ],
                "answer_relation_to_question": [
                    2.004452239611441,
                    1.1961683756071235,
                    0.7993793847814354
                ],
                "word_count_appended": [
                    179.0,
                    194.0,
                    95.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Knucklebones is another name for what classic game?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "jacks"
            ],
            "question": "knucklebones is another name for what classic game?",
            "lines": [
                [
                    0.30713025951121187,
                    0.5875,
                    0.13970549793194673,
                    0.06777975708628381,
                    0.3261610359263947,
                    0.09277607638263376,
                    0.8540995544365041,
                    0.9920898942855031,
                    0.08210526315789474,
                    0.25,
                    0.448,
                    0.3416394707069345,
                    0.017391304347826087,
                    0.023809523809523808,
                    1.0
                ],
                [
                    0.49634668682287736,
                    0.025,
                    0.7688010629962263,
                    0.06252091391351378,
                    0.6717943725051114,
                    0.05062150963790308,
                    0.1394700139470014,
                    0.0014045982109854365,
                    0.3719298245614035,
                    0.7309782608695652,
                    0.496,
                    0.43579219879218556,
                    0.9826086956521739,
                    0.9761904761904762,
                    1.0
                ],
                [
                    0.19652305366591083,
                    0.3875,
                    0.09149343907182696,
                    0.8696993290002024,
                    0.0020445915684938177,
                    0.8566024139794631,
                    0.0064304316164944895,
                    0.006505507503511495,
                    0.5459649122807018,
                    0.019021739130434784,
                    0.056,
                    0.2225683305008799,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "dominoes",
                "jacks",
                "rock, paper, scissors"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "dominoes": 0.27906417157171287,
                "jacks": 0.9231855239603043,
                "rock, paper, scissors": 0.25664688399222324
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.0249184121208035,
                    1.3073765963765567,
                    0.6677049915026397
                ],
                "result_count_important_words": [
                    69200.0,
                    11300.0,
                    521.0
                ],
                "wikipedia_search": [
                    0.2463157894736842,
                    1.1157894736842104,
                    1.6378947368421053
                ],
                "word_count_appended_bing": [
                    56.0,
                    62.0,
                    7.0
                ],
                "answer_relation_to_question_bing": [
                    1.175,
                    0.05,
                    0.775
                ],
                "cosine_similarity_raw": [
                    0.04649834334850311,
                    0.2558809518814087,
                    0.03045186772942543
                ],
                "result_count_noun_chunks": [
                    67100.0,
                    95.0,
                    440.0
                ],
                "question_answer_similarity": [
                    0.668140327790752,
                    0.6163011747412384,
                    8.573078744113445
                ],
                "word_count_noun_chunks": [
                    2.0,
                    113.0,
                    0.0
                ],
                "result_count_bing": [
                    1030000.0,
                    562000.0,
                    9510000.0
                ],
                "word_count_raw": [
                    1.0,
                    41.0,
                    0.0
                ],
                "result_count": [
                    67000.0,
                    138000.0,
                    420.0
                ],
                "answer_relation_to_question": [
                    0.9213907785336356,
                    1.489040060468632,
                    0.5895691609977325
                ],
                "word_count_appended": [
                    184.0,
                    538.0,
                    14.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Who defeated Napoleon at the Battle of Waterloo?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "the duke of wellington"
            ],
            "lines": [
                [
                    0.0,
                    0.0,
                    0.024504883285301784,
                    0.05217046836658575,
                    0.007798713496897592,
                    0.33696729435084244,
                    0.0003361344537815126,
                    0.00018295285914662654,
                    0.0375,
                    0.14096916299559473,
                    0.16666666666666666,
                    0.14229139358668327,
                    0.0,
                    0.0,
                    0.0
                ],
                [
                    0.967548076923077,
                    1.0,
                    0.9295390261704148,
                    0.7759516233692595,
                    0.9791085558148802,
                    0.3419226957383548,
                    0.983957219251337,
                    0.9838798202996362,
                    0.9625,
                    0.42731277533039647,
                    0.6666666666666666,
                    0.5494388978647906,
                    1.0,
                    1.0,
                    0.0
                ],
                [
                    0.03245192307692308,
                    0.0,
                    0.045956090544283366,
                    0.17187790826415475,
                    0.013092730688222235,
                    0.3211100099108028,
                    0.01570664629488159,
                    0.015937226841217247,
                    0.0,
                    0.43171806167400884,
                    0.16666666666666666,
                    0.3082697085485262,
                    0.0,
                    0.0,
                    0.0
                ]
            ],
            "fraction_answers": {
                "jack skellington": 0.06495626214725002,
                "beef wellington": 0.10877049803640618,
                "the duke of wellington": 0.8262732398163438
            },
            "question": "who defeated napoleon at the battle of waterloo?",
            "rate_limited": false,
            "answers": [
                "jack skellington",
                "the duke of wellington",
                "beef wellington"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "jack skellington": 0.10170845874711071,
                "beef wellington": 0.3137481915293897,
                "the duke of wellington": 0.776432132067992
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.5691655743467331,
                    2.1977555914591624,
                    1.2330788341941048
                ],
                "result_count_important_words": [
                    55.0,
                    161000.0,
                    2570.0
                ],
                "wikipedia_search": [
                    0.15,
                    3.85,
                    0.0
                ],
                "word_count_appended_bing": [
                    2.0,
                    8.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    4.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.008136066608130932,
                    0.3086238503456116,
                    0.015258257277309895
                ],
                "result_count_noun_chunks": [
                    45.0,
                    242000.0,
                    3920.0
                ],
                "question_answer_similarity": [
                    0.6204546950757504,
                    9.228263478260487,
                    2.0441153491847217
                ],
                "word_count_noun_chunks": [
                    0.0,
                    13.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    5.0,
                    0.0
                ],
                "result_count_bing": [
                    340000.0,
                    345000.0,
                    324000.0
                ],
                "result_count": [
                    1370.0,
                    172000.0,
                    2300.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    3.870192307692308,
                    0.12980769230769232
                ],
                "word_count_appended": [
                    32.0,
                    97.0,
                    98.0
                ]
            },
            "integer_answers": {
                "jack skellington": 0,
                "beef wellington": 1,
                "the duke of wellington": 13
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which company once put a giant, melting kiwi-strawberry popsicle in New York\u2019s Union Square?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "snapple"
            ],
            "question": "which company once put a giant, melting kiwi-strawberry popsicle in new york\u2019s union square?",
            "lines": [
                [
                    0.4908728085452223,
                    0.4190642829973374,
                    0.7551425872440007,
                    -0.029795745514932297,
                    0.9967221887751689,
                    0.12098501070663811,
                    0.03725490196078431,
                    0.9969561956857382,
                    0.422192513368984,
                    0.8389830508474576,
                    0.5571428571428572,
                    0.7270901396361904,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.13367148496458842,
                    0.40293647774819324,
                    0.0944453759482946,
                    0.5779617466922505,
                    0.0012040939193257074,
                    0.25267665952890794,
                    0.0024635495223730517,
                    0.0011028276500948432,
                    0.47829768270944745,
                    0.038135593220338986,
                    0.07142857142857142,
                    0.11608337804466214,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.3754557064901893,
                    0.1779992392544694,
                    0.1504120368077047,
                    0.45183399882268177,
                    0.002073717305505385,
                    0.6263383297644539,
                    0.9602815485168427,
                    0.001940976664166924,
                    0.09950980392156865,
                    0.1228813559322034,
                    0.37142857142857144,
                    0.15682648231914742,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "snapple",
                "edy's",
                "baskin-robbins"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "snapple": 0.9663399842481154,
                "edy's": 0.19807632193362626,
                "baskin-robbins": 0.050634924042181645
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    7.2709013963619045,
                    1.1608337804466213,
                    1.5682648231914742
                ],
                "result_count_important_words": [
                    741.0,
                    49.0,
                    19100.0
                ],
                "wikipedia_search": [
                    1.6887700534759358,
                    1.9131907308377896,
                    0.39803921568627454
                ],
                "word_count_appended_bing": [
                    39.0,
                    5.0,
                    26.0
                ],
                "answer_relation_to_question_bing": [
                    2.095321414986687,
                    2.014682388740966,
                    0.8899961962723469
                ],
                "cosine_similarity_raw": [
                    0.35061997175216675,
                    0.04385189712047577,
                    0.06983775645494461
                ],
                "result_count_noun_chunks": [
                    45200.0,
                    50.0,
                    88.0
                ],
                "question_answer_similarity": [
                    -0.22626015357673168,
                    4.388872012030333,
                    3.4310948827769607
                ],
                "word_count_noun_chunks": [
                    38.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    22600.0,
                    47200.0,
                    117000.0
                ],
                "word_count_raw": [
                    25.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    14900.0,
                    18.0,
                    31.0
                ],
                "answer_relation_to_question": [
                    4.417855276907001,
                    1.2030433646812957,
                    3.3791013584117033
                ],
                "word_count_appended": [
                    198.0,
                    9.0,
                    29.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Cheese is traditionally considered part of which food group?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "dairy",
                "middle"
            ],
            "question": "cheese is traditionally considered part of which food group?",
            "lines": [
                [
                    0.15869565217391304,
                    0.318452380952381,
                    0.04635144103706577,
                    0.3495791444903876,
                    0.03938464803801785,
                    0.300161377084454,
                    0.04654707977333875,
                    0.4444438702275578,
                    0.11481481481481481,
                    0.06998444790046657,
                    0.04838709677419355,
                    0.3810255144251986,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.6823578595317726,
                    0.5684523809523809,
                    0.9400319646129127,
                    0.1945686574858369,
                    0.960601171658972,
                    0.4513179128563744,
                    0.9534384579579169,
                    0.5555548377844473,
                    0.6962962962962962,
                    0.911353032659409,
                    0.9193548387096774,
                    0.4361347995171446,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.15894648829431438,
                    0.11309523809523808,
                    0.01361659435002149,
                    0.45585219802377547,
                    1.418030301020387e-05,
                    0.2485207100591716,
                    1.4462268744305482e-05,
                    1.291987994847552e-06,
                    0.18888888888888888,
                    0.01866251944012442,
                    0.03225806451612903,
                    0.18283968605765677,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "tropical fruits",
                "dairy",
                "sushi delivery orders"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sushi delivery orders": 0.030784990747970645,
                "dairy": 0.9663217663182815,
                "tropical fruits": 0.06458597950013299
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.905127572125993,
                    2.180673997585723,
                    0.9141984302882838
                ],
                "result_count_important_words": [
                    86900.0,
                    1780000.0,
                    27.0
                ],
                "wikipedia_search": [
                    0.5740740740740741,
                    3.481481481481481,
                    0.9444444444444444
                ],
                "word_count_appended_bing": [
                    3.0,
                    57.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.273809523809524,
                    2.2738095238095237,
                    0.45238095238095233
                ],
                "cosine_similarity_raw": [
                    0.02189529873430729,
                    0.44404834508895874,
                    0.006432149559259415
                ],
                "result_count_noun_chunks": [
                    1720000.0,
                    2150000.0,
                    5.0
                ],
                "question_answer_similarity": [
                    4.942765273153782,
                    2.7510428428649902,
                    6.445379965007305
                ],
                "word_count_noun_chunks": [
                    0.0,
                    7.0,
                    0.0
                ],
                "result_count_bing": [
                    558000.0,
                    839000.0,
                    462000.0
                ],
                "word_count_raw": [
                    0.0,
                    18.0,
                    0.0
                ],
                "word_count_appended": [
                    45.0,
                    586.0,
                    12.0
                ],
                "answer_relation_to_question": [
                    0.6347826086956522,
                    2.7294314381270905,
                    0.6357859531772575
                ],
                "result_count": [
                    86100.0,
                    2100000.0,
                    31.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "The '90s band The Lightning Seeds took their name from which song?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "raspberry beret"
            ],
            "lines": [
                [
                    0.20424836601307192,
                    0.4202898550724638,
                    0.29905176822715146,
                    0.0985450346115554,
                    0.011519135667613355,
                    0.1963919639196392,
                    0.010167884077002344,
                    0.011617169580888266,
                    0.241156116068292,
                    0.38181818181818183,
                    0.2222222222222222,
                    0.3500363276630772,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.4950980392156863,
                    0.463768115942029,
                    0.44631082175009895,
                    0.3294429145588686,
                    0.988063792178904,
                    0.3956539565395654,
                    0.9894308720670442,
                    0.9879558746140021,
                    0.4220824843673154,
                    0.3090909090909091,
                    0.4444444444444444,
                    0.36409636357081027,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.3006535947712418,
                    0.11594202898550725,
                    0.2546374100227496,
                    0.5720120508295761,
                    0.0004170721534825525,
                    0.40795407954079543,
                    0.0004012438559534557,
                    0.00042695580510956877,
                    0.33676139956439266,
                    0.3090909090909091,
                    0.3333333333333333,
                    0.28586730876611255,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "when doves cry": 0.20839267047994026,
                "raspberry beret": 0.3176474303529399,
                "purple rain": 0.4739598991671198
            },
            "question": "the '90s band the lightning seeds took their name from which song?",
            "rate_limited": false,
            "answers": [
                "raspberry beret",
                "purple rain",
                "when doves cry"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "when doves cry": 0.1583616692212927,
                "raspberry beret": 0.6348021194555852,
                "purple rain": 0.34817434464907643
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.100217965978463,
                    2.1845781814248615,
                    1.7152038525966753
                ],
                "result_count_important_words": [
                    2230.0,
                    217000.0,
                    88.0
                ],
                "wikipedia_search": [
                    0.964624464273168,
                    1.6883299374692615,
                    1.3470455982575706
                ],
                "word_count_appended_bing": [
                    2.0,
                    4.0,
                    3.0
                ],
                "answer_relation_to_question_bing": [
                    1.2608695652173914,
                    1.391304347826087,
                    0.34782608695652173
                ],
                "cosine_similarity_raw": [
                    0.08908778429031372,
                    0.1329563856124878,
                    0.07585670799016953
                ],
                "result_count_noun_chunks": [
                    2340.0,
                    199000.0,
                    86.0
                ],
                "question_answer_similarity": [
                    2.0106428859289736,
                    6.721719212830067,
                    11.670927563216537
                ],
                "word_count_noun_chunks": [
                    3.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    95800.0,
                    193000.0,
                    199000.0
                ],
                "result_count": [
                    2320.0,
                    199000.0,
                    84.0
                ],
                "answer_relation_to_question": [
                    0.6127450980392157,
                    1.4852941176470589,
                    0.9019607843137255
                ],
                "word_count_appended": [
                    21.0,
                    17.0,
                    17.0
                ]
            },
            "integer_answers": {
                "when doves cry": 2,
                "raspberry beret": 3,
                "purple rain": 9
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Marsha Bell was the model for what iconic character?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "carmen sandiego"
            ],
            "lines": [
                [
                    0.38751585623678647,
                    0.11111111111111112,
                    0.2702503698143433,
                    0.49175856099886156,
                    0.000995787054768288,
                    0.1974000962927299,
                    0.0010647977852206068,
                    0.010445205479452055,
                    0.7828525641025642,
                    0.34057971014492755,
                    0.5319148936170213,
                    0.2787969412162335,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.08096899224806202,
                    0.11111111111111112,
                    0.24618569588883193,
                    0.45629530303514226,
                    0.8655687476062811,
                    0.5825710158883004,
                    0.9970379261611135,
                    0.9726027397260274,
                    0.0,
                    0.30434782608695654,
                    0.06382978723404255,
                    0.36473298827113426,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.5315151515151515,
                    0.7777777777777778,
                    0.48356393429682476,
                    0.05194613596599621,
                    0.1334354653389506,
                    0.22002888781896968,
                    0.0018972760536658084,
                    0.016952054794520548,
                    0.2171474358974359,
                    0.35507246376811596,
                    0.40425531914893614,
                    0.35647007051263224,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "little debbie": 0.36037515237550016,
                "rosie the riveter": 0.24319184956100143,
                "carmen sandiego": 0.39643299806349835
            },
            "question": "marsha bell was the model for what iconic character?",
            "rate_limited": false,
            "answers": [
                "rosie the riveter",
                "little debbie",
                "carmen sandiego"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "little debbie": 0.17863036840751265,
                "rosie the riveter": 0.27324276508577616,
                "carmen sandiego": 0.6554243524524308
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3939847060811674,
                    1.8236649413556714,
                    1.7823503525631612
                ],
                "result_count_important_words": [
                    55.0,
                    51500.0,
                    98.0
                ],
                "wikipedia_search": [
                    3.131410256410257,
                    0.0,
                    0.8685897435897436
                ],
                "word_count_appended_bing": [
                    25.0,
                    3.0,
                    19.0
                ],
                "answer_relation_to_question_bing": [
                    0.3333333333333333,
                    0.3333333333333333,
                    2.333333333333333
                ],
                "cosine_similarity_raw": [
                    0.09575701504945755,
                    0.08723025023937225,
                    0.17133978009223938
                ],
                "result_count_noun_chunks": [
                    61.0,
                    5680.0,
                    99.0
                ],
                "question_answer_similarity": [
                    4.68056751228869,
                    4.343027535825968,
                    0.49442432867363095
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    2.0
                ],
                "result_count_bing": [
                    410000.0,
                    1210000.0,
                    457000.0
                ],
                "word_count_appended": [
                    47.0,
                    42.0,
                    49.0
                ],
                "answer_relation_to_question": [
                    1.9375792811839323,
                    0.4048449612403101,
                    2.6575757575757573
                ],
                "result_count": [
                    65.0,
                    56500.0,
                    8710.0
                ]
            },
            "integer_answers": {
                "little debbie": 5,
                "rosie the riveter": 3,
                "carmen sandiego": 6
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "The voice of which \u201cSimpsons\u201d character is also in the band Spinal Tap?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "kent brockman"
            ],
            "question": "the voice of which \u201csimpsons\u201d character is also in the band spinal tap?",
            "lines": [
                [
                    0.3098664423174588,
                    0.3025462962962963,
                    0.18796408495662426,
                    1.21429813061392,
                    0.33555555555555555,
                    0.3415892672858617,
                    0.010752688172043012,
                    0.005244604935087267,
                    0.6690615835777126,
                    0.28431372549019607,
                    0.2222222222222222,
                    0.33061627188846254,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.32324591851337703,
                    0.28773148148148153,
                    0.2035508085372007,
                    -0.03852630612491001,
                    0.24166666666666667,
                    0.30959752321981426,
                    0.011730205278592375,
                    0.006018399105837847,
                    0.2423264907135875,
                    0.22549019607843138,
                    0.3333333333333333,
                    0.29290291381860745,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.3668876391691641,
                    0.40972222222222227,
                    0.608485106506175,
                    -0.17577182448901008,
                    0.42277777777777775,
                    0.34881320949432404,
                    0.9775171065493646,
                    0.9887369959590749,
                    0.0886119257086999,
                    0.49019607843137253,
                    0.4444444444444444,
                    0.37648081429293007,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "comic book guy",
                "groundskeeper willie",
                "kent brockman"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "kent brockman": 0.601392443102576,
                "groundskeeper willie": 0.10918201925164114,
                "comic book guy": 0.31246912302715313
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9836976313307753,
                    1.7574174829116447,
                    2.2588848857575803
                ],
                "result_count_important_words": [
                    88.0,
                    96.0,
                    8000.0
                ],
                "wikipedia_search": [
                    3.345307917888563,
                    1.2116324535679375,
                    0.4430596285434995
                ],
                "answer_relation_to_question": [
                    1.549332211587294,
                    1.616229592566885,
                    1.8344381958458207
                ],
                "answer_relation_to_question_bing": [
                    1.210185185185185,
                    1.150925925925926,
                    1.6388888888888888
                ],
                "word_count_appended": [
                    29.0,
                    23.0,
                    50.0
                ],
                "cosine_similarity_raw": [
                    0.019615087658166885,
                    0.021241648122668266,
                    0.06349877268075943
                ],
                "result_count_noun_chunks": [
                    61.0,
                    70.0,
                    11500.0
                ],
                "question_answer_similarity": [
                    12.730281638447195,
                    -0.40389646915718913,
                    -1.8427310175611638
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    33100.0,
                    30000.0,
                    33800.0
                ],
                "result_count": [
                    6040.0,
                    4350.0,
                    7610.0
                ],
                "word_count_appended_bing": [
                    2.0,
                    3.0,
                    4.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What actor famously yelled \"Not the bees! Not the bees!\" in a 2006 film?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "macauley culkin"
            ],
            "lines": [
                [
                    0.18523206751054855,
                    0.2540322580645161,
                    0.0284090683113184,
                    1.4907865878808582,
                    0.08823529411764708,
                    0.33859223300970875,
                    0.31202566309365465,
                    0.2906045156591406,
                    0.3038935430478885,
                    0.25,
                    0.3731343283582089,
                    0.23952909847911785,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.4222837552742616,
                    0.3089717741935484,
                    0.48478512925171247,
                    -1.1773369883279967,
                    0.5,
                    0.22572815533980584,
                    0.49898286519051716,
                    0.4861616897305171,
                    0.3826627183370782,
                    0.4423076923076923,
                    0.4626865671641791,
                    0.40966464333376384,
                    0.5,
                    0.5,
                    1.0
                ],
                [
                    0.39248417721518986,
                    0.4369959677419355,
                    0.4868058024369692,
                    0.6865504004471386,
                    0.4117647058823529,
                    0.4356796116504854,
                    0.1889914717158282,
                    0.22323379461034232,
                    0.3134437386150333,
                    0.3076923076923077,
                    0.16417910447761191,
                    0.3508062581871183,
                    0.5,
                    0.5,
                    1.0
                ]
            ],
            "fraction_answers": {
                "macauley culkin": 0.3647288568864172,
                "nicolas cage": 0.4065036203524847,
                "oprah winfrey": 0.22876752276109813
            },
            "question": "what actor famously yelled \"not the bees! not the bees!\" in a 2006 film?",
            "rate_limited": false,
            "answers": [
                "nicolas cage",
                "macauley culkin",
                "oprah winfrey"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "macauley culkin": 0.5241321790420751,
                "nicolas cage": 0.1016053595147207,
                "oprah winfrey": 0.460608549387696
            },
            "integer_answers": {
                "macauley culkin": 1,
                "nicolas cage": 10,
                "oprah winfrey": 3
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.64659262129235,
                    1.2646949933273064,
                    2.0887123853803438
                ],
                "result_count_important_words": [
                    9610.0,
                    52.0,
                    15900.0
                ],
                "wikipedia_search": [
                    1.176638741712669,
                    0.7040236899775306,
                    1.1193375683098006
                ],
                "word_count_appended_bing": [
                    17.0,
                    5.0,
                    45.0
                ],
                "answer_relation_to_question_bing": [
                    0.9838709677419355,
                    0.7641129032258065,
                    0.25201612903225806
                ],
                "cosine_similarity_raw": [
                    0.5552423000335693,
                    0.017913702875375748,
                    0.015534600242972374
                ],
                "result_count_noun_chunks": [
                    230000.0,
                    15200.0,
                    304000.0
                ],
                "question_answer_similarity": [
                    2.9484580010175705,
                    -4.991546841803938,
                    0.5551508544012904
                ],
                "word_count_noun_chunks": [
                    10.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    266000.0,
                    452000.0,
                    106000.0
                ],
                "word_count_raw": [
                    4.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    14.0,
                    0,
                    3.0
                ],
                "answer_relation_to_question": [
                    3.1476793248945145,
                    0.7771624472573839,
                    1.0751582278481013
                ],
                "word_count_appended": [
                    13.0,
                    3.0,
                    10.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Actually taking place in Bethel, NY, which of these things is misnamed?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "the woodstock festival"
            ],
            "question": "actually taking place in bethel, ny, which of these things is misnamed?",
            "lines": [
                [
                    0.5173198146882357,
                    0.3157894736842105,
                    0.6094573567203921,
                    0.2559929752011973,
                    0.34375,
                    0.09925064994647499,
                    0.34146341463414637,
                    0.25256322624743677,
                    0.49596771203345963,
                    0.30434782608695654,
                    0.3333333333333333,
                    0.26527270502449757,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.16974823422191843,
                    0.5371710526315789,
                    0.26700248417600725,
                    0.29474721527131714,
                    0.0,
                    0.5780700412907173,
                    0.6585365853658537,
                    0.5194805194805194,
                    0.23003940209666615,
                    0.13043478260869565,
                    0.5,
                    0.22509618485621496,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.3129319510898458,
                    0.14703947368421053,
                    0.12354015910360058,
                    0.4492598095274855,
                    0.65625,
                    0.32267930876280776,
                    0.0,
                    0.22795625427204375,
                    0.2739928858698742,
                    0.5652173913043478,
                    0.16666666666666666,
                    0.5096311101192874,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "the woodstock festival",
                "the manhattan project",
                "the battle of brooklyn"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "the woodstock festival": 0.3314132448251912,
                "the battle of brooklyn": 0.13563340087707879,
                "the manhattan project": 0.13191081138608415
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.8569089351714831,
                    1.5756732939935048,
                    3.567417770835012
                ],
                "result_count_important_words": [
                    14.0,
                    27.0,
                    0
                ],
                "wikipedia_search": [
                    2.975806272200758,
                    1.3802364125799969,
                    1.6439573152192453
                ],
                "word_count_appended_bing": [
                    4.0,
                    6.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.5789473684210527,
                    2.6858552631578947,
                    0.7351973684210527
                ],
                "cosine_similarity_raw": [
                    0.10905491560697556,
                    0.04777681827545166,
                    0.022105995565652847
                ],
                "result_count_noun_chunks": [
                    73900.0,
                    152000.0,
                    66700.0
                ],
                "question_answer_similarity": [
                    9.58661448827479,
                    11.037911966443062,
                    16.824214006774127
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    649000.0,
                    3780000.0,
                    2110000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    7.0,
                    3.0,
                    13.0
                ],
                "answer_relation_to_question": [
                    3.1039188881294146,
                    1.0184894053315106,
                    1.8775917065390748
                ],
                "result_count": [
                    11.0,
                    0,
                    21.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "In baking, yeast helps bread rise, but scientifically yeast is what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "fungus"
            ],
            "lines": [
                [
                    0.7098039215686274,
                    0.5,
                    0.44314645141350506,
                    0.3043206998380932,
                    0.6001805054151624,
                    0.2630350194552529,
                    0.6012658227848101,
                    0.12354113293481354,
                    0.8807241145950824,
                    0.35097493036211697,
                    0.29464285714285715,
                    0.3334024161636499,
                    0.4117647058823529,
                    0.7142857142857143,
                    1.0
                ],
                [
                    0.2,
                    0.0,
                    0.213702139932093,
                    0.32993410956901953,
                    0.24819494584837545,
                    0.40077821011673154,
                    0.24773960216998192,
                    0.71733561058924,
                    0.06468531468531469,
                    0.318941504178273,
                    0.32142857142857145,
                    0.3232375110821559,
                    0.47058823529411764,
                    0.14285714285714285,
                    1.0
                ],
                [
                    0.09019607843137255,
                    0.5,
                    0.34315140865440197,
                    0.3657451905928873,
                    0.15162454873646208,
                    0.3361867704280156,
                    0.15099457504520797,
                    0.1591232564759465,
                    0.05459057071960298,
                    0.33008356545961004,
                    0.38392857142857145,
                    0.34336007275419417,
                    0.11764705882352941,
                    0.14285714285714285,
                    1.0
                ]
            ],
            "fraction_answers": {
                "fungus": 0.4665063065601456,
                "plant": 0.28567306412507265,
                "bacteria": 0.24782062931478174
            },
            "question": "in baking, yeast helps bread rise, but scientifically yeast is what?",
            "rate_limited": false,
            "answers": [
                "fungus",
                "plant",
                "bacteria"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "fungus": 0.5520455203070409,
                "plant": 0.15447812185319845,
                "bacteria": 0.36688537791827897
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.3338169131455495,
                    2.2626625775750915,
                    2.403520509279359
                ],
                "result_count_important_words": [
                    1330000.0,
                    548000.0,
                    334000.0
                ],
                "wikipedia_search": [
                    3.5228964583803295,
                    0.25874125874125875,
                    0.21836228287841192
                ],
                "answer_relation_to_question": [
                    3.549019607843137,
                    1.0,
                    0.45098039215686275
                ],
                "result_count": [
                    1330000.0,
                    550000.0,
                    336000.0
                ],
                "answer_relation_to_question_bing": [
                    2.0,
                    0.0,
                    2.0
                ],
                "cosine_similarity_raw": [
                    0.10530021041631699,
                    0.050779782235622406,
                    0.08153944462537766
                ],
                "result_count_noun_chunks": [
                    434000.0,
                    2520000.0,
                    559000.0
                ],
                "question_answer_similarity": [
                    3.1372757628560066,
                    3.4013272374868393,
                    3.7705076336860657
                ],
                "word_count_noun_chunks": [
                    7.0,
                    8.0,
                    2.0
                ],
                "word_count_raw": [
                    5.0,
                    1.0,
                    1.0
                ],
                "result_count_bing": [
                    676000.0,
                    1030000.0,
                    864000.0
                ],
                "word_count_appended": [
                    252.0,
                    229.0,
                    237.0
                ],
                "word_count_appended_bing": [
                    33.0,
                    36.0,
                    43.0
                ]
            },
            "integer_answers": {
                "fungus": 8,
                "plant": 3,
                "bacteria": 3
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these knots is typically used to add another line to a rope?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "rolling hitch"
            ],
            "lines": [
                [
                    0.6117108255036325,
                    0.3578168743620065,
                    0.4428781242296326,
                    0.009768658477227432,
                    0.7838374917200265,
                    0.6419400855920114,
                    0.7796340493237868,
                    0.7500699692135461,
                    0.5455273736757995,
                    0.6012024048096193,
                    0.46875,
                    0.4470153920819559,
                    0.75,
                    0.0,
                    -1.0
                ],
                [
                    0.2954169573599923,
                    0.43842810170165314,
                    0.5349761125612279,
                    0.9902313415227726,
                    0.193751379995584,
                    0.19400855920114124,
                    0.1994166003712543,
                    0.21130702490904002,
                    0.39576091075341446,
                    0.14829659318637275,
                    0.109375,
                    0.3584511463430306,
                    0.25,
                    1.0,
                    -1.0
                ],
                [
                    0.09287221713637513,
                    0.20375502393634048,
                    0.022145763209139496,
                    0.0,
                    0.02241112828438949,
                    0.16405135520684735,
                    0.020949350304958897,
                    0.038623005877413935,
                    0.058711715570786034,
                    0.250501002004008,
                    0.421875,
                    0.19453346157501353,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "grantchester": 0.10645921593609088,
                "rolling hitch": 0.37995855199324885,
                "bowline": 0.5135822320706602
            },
            "question": "which of these knots is typically used to add another line to a rope?",
            "rate_limited": false,
            "answers": [
                "bowline",
                "rolling hitch",
                "grantchester"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "grantchester": 0.15871385009139574,
                "rolling hitch": 0.6348021194555852,
                "bowline": 0.5731660372494805
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.2350769604097795,
                    1.792255731715153,
                    0.9726673078750676
                ],
                "result_count_important_words": [
                    147000.0,
                    37600.0,
                    3950.0
                ],
                "wikipedia_search": [
                    2.182109494703198,
                    1.5830436430136579,
                    0.23484686228314414
                ],
                "word_count_appended_bing": [
                    30.0,
                    7.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    1.431267497448026,
                    1.7537124068066126,
                    0.8150200957453619
                ],
                "cosine_similarity_raw": [
                    0.1980799287557602,
                    0.23927131295204163,
                    0.009904826991260052
                ],
                "result_count_noun_chunks": [
                    268000.0,
                    75500.0,
                    13800.0
                ],
                "question_answer_similarity": [
                    0.07064885040745139,
                    7.161546908318996,
                    0.0
                ],
                "word_count_noun_chunks": [
                    24.0,
                    8.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    2.0,
                    0.0
                ],
                "result_count_bing": [
                    450000.0,
                    136000.0,
                    115000.0
                ],
                "result_count": [
                    142000.0,
                    35100.0,
                    4060.0
                ],
                "answer_relation_to_question": [
                    3.0585541275181627,
                    1.4770847867999615,
                    0.4643610856818756
                ],
                "word_count_appended": [
                    300.0,
                    74.0,
                    125.0
                ]
            },
            "integer_answers": {
                "grantchester": 0,
                "rolling hitch": 4,
                "bowline": 10
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these celebrities is known for having aviophobia?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "angelina jolie"
            ],
            "lines": [
                [
                    0.19583333333333333,
                    0.0,
                    0.2558089911117708,
                    0.26241121544127877,
                    0.4304635761589404,
                    0.33112582781456956,
                    0.4378698224852071,
                    0.5035460992907801,
                    0.8344907407407408,
                    0.417910447761194,
                    0.3225806451612903,
                    0.3764582854268468,
                    0.3333333333333333,
                    0.16666666666666666,
                    -1.0
                ],
                [
                    0.4583333333333333,
                    0.4375,
                    0.46393681148048377,
                    0.4589561755030482,
                    0.3443708609271523,
                    0.3355408388520971,
                    0.1893491124260355,
                    0.22695035460992907,
                    0.07423941798941798,
                    0.3880597014925373,
                    0.25806451612903225,
                    0.38809388473699336,
                    0.3333333333333333,
                    0.3333333333333333,
                    -1.0
                ],
                [
                    0.3458333333333334,
                    0.5625,
                    0.2802541974077455,
                    0.27863260905567305,
                    0.2251655629139073,
                    0.3333333333333333,
                    0.3727810650887574,
                    0.2695035460992908,
                    0.09126984126984126,
                    0.19402985074626866,
                    0.41935483870967744,
                    0.23544782983615983,
                    0.3333333333333333,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "angelina jolie": 0.3477499274804251,
                "john travolta": 0.31724566722338005,
                "john madden": 0.3350044052961948
            },
            "question": "which of these celebrities is known for having aviophobia?",
            "rate_limited": false,
            "answers": [
                "angelina jolie",
                "john madden",
                "john travolta"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "angelina jolie": 0.46224809399276434,
                "john travolta": 0.21834245662727622,
                "john madden": 0.4535024340847416
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.5058331417073871,
                    1.5523755389479734,
                    0.9417913193446393
                ],
                "result_count_important_words": [
                    74.0,
                    32.0,
                    63.0
                ],
                "wikipedia_search": [
                    2.5034722222222223,
                    0.22271825396825395,
                    0.2738095238095238
                ],
                "word_count_appended_bing": [
                    10.0,
                    8.0,
                    13.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.875,
                    1.125
                ],
                "cosine_similarity_raw": [
                    0.11690175533294678,
                    0.21201376616954803,
                    0.12807293236255646
                ],
                "result_count_noun_chunks": [
                    71.0,
                    32.0,
                    38.0
                ],
                "question_answer_similarity": [
                    1.0088321383518633,
                    1.7644434105604887,
                    1.0711948052048683
                ],
                "word_count_noun_chunks": [
                    4.0,
                    4.0,
                    4.0
                ],
                "word_count_raw": [
                    1.0,
                    2.0,
                    3.0
                ],
                "result_count_bing": [
                    4500000.0,
                    4560000.0,
                    4530000.0
                ],
                "word_count_appended": [
                    28.0,
                    26.0,
                    13.0
                ],
                "answer_relation_to_question": [
                    0.5875,
                    1.375,
                    1.0375
                ],
                "result_count": [
                    65.0,
                    52.0,
                    34.0
                ]
            },
            "integer_answers": {
                "angelina jolie": 6,
                "john travolta": 3,
                "john madden": 5
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What is Telluride, Colorado named after?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "an element"
            ],
            "lines": [
                [
                    0.3157894736842105,
                    0,
                    0.7544189322250512,
                    0.27549462008946013,
                    0.9783109414105724,
                    0.7690322580645161,
                    0.9974174214449254,
                    0.8642440218414611,
                    0,
                    0.5121951219512195,
                    0.3333333333333333,
                    0.4696113008738994,
                    0,
                    0,
                    1.0
                ],
                [
                    0.21052631578947367,
                    0,
                    0.10785356850828243,
                    0.2947672410525409,
                    0.02156083336895145,
                    0.11638709677419355,
                    0.0025639826370476896,
                    0.08981359442666165,
                    0,
                    0.3902439024390244,
                    0.3333333333333333,
                    0.4274734694379188,
                    0,
                    0,
                    1.0
                ],
                [
                    0.47368421052631576,
                    0,
                    0.13772749926666636,
                    0.429738138857999,
                    0.000128225220476143,
                    0.11458064516129032,
                    1.8595918026939285e-05,
                    0.045942383731877236,
                    0,
                    0.0975609756097561,
                    0.3333333333333333,
                    0.10291522968818187,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "a european city": 0.17356292373139232,
                "a governor": 0.19945233377674276,
                "an element": 0.626984742491865
            },
            "question": "what is telluride, colorado named after?",
            "rate_limited": false,
            "answers": [
                "an element",
                "a governor",
                "a european city"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "a european city": 0.2219388861570518,
                "a governor": 0.3143178110234338,
                "an element": 0.5319921901361423
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.408833902621698,
                    1.2824204083137563,
                    0.3087456890645456
                ],
                "result_count_important_words": [
                    1770000.0,
                    4550.0,
                    33.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    0.0
                ],
                "answer_relation_to_question": [
                    0.3157894736842105,
                    0.21052631578947367,
                    0.47368421052631576
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    63.0,
                    48.0,
                    12.0
                ],
                "cosine_similarity_raw": [
                    0.3329974412918091,
                    0.047606125473976135,
                    0.060792356729507446
                ],
                "result_count_noun_chunks": [
                    45900000.0,
                    4770000.0,
                    2440000.0
                ],
                "question_answer_similarity": [
                    3.8979606702923775,
                    4.170648095197976,
                    6.08034510165453
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    2980000.0,
                    451000.0,
                    444000.0
                ],
                "result_count": [
                    206000.0,
                    4540.0,
                    27.0
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ]
            },
            "integer_answers": {
                "a european city": 2,
                "a governor": 0,
                "an element": 8
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which French culinary term refers to a type of knife cut?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "brunoise"
            ],
            "question": "which french culinary term refers to a type of knife cut?",
            "lines": [
                [
                    0.16625662773918587,
                    0.14785297193372174,
                    0.12482892983691338,
                    0.509143760411231,
                    0.41622945085694296,
                    0.3284671532846715,
                    0.11482398239823982,
                    0.061040339702760085,
                    0.11904761904761904,
                    0.28,
                    0.2647058823529412,
                    0.26840591055976065,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.732942800093963,
                    0.6229501852248788,
                    0.8620023473142493,
                    0.490856239588769,
                    0.09059111577474642,
                    0.34549878345498786,
                    0.8388338833883389,
                    0.8651804670912951,
                    0.7976190476190477,
                    0.48869565217391303,
                    0.47058823529411764,
                    0.4639546437770195,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.10080057216685123,
                    0.22919684284139943,
                    0.013168722848837279,
                    -0.0,
                    0.4931794333683106,
                    0.3260340632603406,
                    0.04634213421342134,
                    0.0737791932059448,
                    0.08333333333333333,
                    0.23130434782608697,
                    0.2647058823529412,
                    0.2676394456632198,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "vichyssoise",
                "brunoise",
                "bavaroise"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "bavaroise": 0.019021492654867335,
                "brunoise": 0.9217317602807424,
                "vichyssoise": 0.06579568007768169
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.8788413739183247,
                    3.2476825064391366,
                    1.8734761196425387
                ],
                "result_count_important_words": [
                    16700.0,
                    122000.0,
                    6740.0
                ],
                "wikipedia_search": [
                    0.47619047619047616,
                    3.1904761904761907,
                    0.3333333333333333
                ],
                "word_count_appended_bing": [
                    27.0,
                    48.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    0.8871178316023305,
                    3.737701111349273,
                    1.3751810570483967
                ],
                "cosine_similarity_raw": [
                    0.0641571432352066,
                    0.44303518533706665,
                    0.006768203806132078
                ],
                "result_count_noun_chunks": [
                    115000.0,
                    1630000.0,
                    139000.0
                ],
                "question_answer_similarity": [
                    -0.6397246289998293,
                    -0.6167468801140785,
                    0.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    23.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    20.0,
                    0.0
                ],
                "result_count_bing": [
                    135000.0,
                    142000.0,
                    134000.0
                ],
                "word_count_appended": [
                    161.0,
                    281.0,
                    133.0
                ],
                "answer_relation_to_question": [
                    0.9975397664351152,
                    4.3976568005637775,
                    0.6048034330011074
                ],
                "result_count": [
                    119000.0,
                    25900.0,
                    141000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these rappers is NOT a founding member of the Wu-Tang Clan?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "redman"
            ],
            "question": "which of these rappers is not a founding member of the wu-tang clan?",
            "lines": [
                [
                    0.38187494331744637,
                    0.3261331294337539,
                    0.24362487934053834,
                    0.5751777517825865,
                    0.3959421454399357,
                    0.3054129724685021,
                    0.4426086956521739,
                    0.4769326683291771,
                    0.26666666666666666,
                    0.24751655629139074,
                    0.30434782608695654,
                    0.32297591000319503,
                    0.20370370370370372,
                    0.25,
                    -1.0
                ],
                [
                    0.2011674075590006,
                    0.25398666156917715,
                    0.3399884552368968,
                    -0.18703140123935358,
                    0.1946564885496183,
                    0.3331777881474568,
                    0.33999999999999997,
                    0.43231207695048096,
                    0.24047619047619045,
                    0.4503311258278146,
                    0.42934782608695654,
                    0.32984755585356984,
                    0.40740740740740744,
                    0.25,
                    -1.0
                ],
                [
                    0.416957649123553,
                    0.41988020899706896,
                    0.41638666542256486,
                    0.6118536494567671,
                    0.40940136601044597,
                    0.36140923938404107,
                    0.2173913043478261,
                    0.09075525472034202,
                    0.4928571428571429,
                    0.30215231788079466,
                    0.2663043478260869,
                    0.3471765341432351,
                    0.3888888888888889,
                    0.5,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "raekwon",
                "inspectah deck",
                "redman"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "inspectah deck": 0.10069821503806574,
                "redman": 0.6288970855911641,
                "raekwon": 0.12859219695863644
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.12428907996166,
                    2.0418293297571615,
                    1.8338815902811783
                ],
                "result_count_important_words": [
                    66000.0,
                    184000.0,
                    325000.0
                ],
                "wikipedia_search": [
                    2.333333333333333,
                    2.595238095238095,
                    0.07142857142857142
                ],
                "word_count_appended_bing": [
                    36.0,
                    13.0,
                    43.0
                ],
                "answer_relation_to_question_bing": [
                    1.7386687056624612,
                    2.4601333843082283,
                    0.8011979100293105
                ],
                "cosine_similarity_raw": [
                    0.13003471493721008,
                    0.08115863800048828,
                    0.04240909218788147
                ],
                "result_count_noun_chunks": [
                    51800.0,
                    152000.0,
                    919000.0
                ],
                "question_answer_similarity": [
                    -0.15664296224713326,
                    1.4315223759040236,
                    -0.23306213039904833
                ],
                "word_count_noun_chunks": [
                    16.0,
                    5.0,
                    6.0
                ],
                "result_count_bing": [
                    83400.0,
                    71500.0,
                    59400.0
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    0.0
                ],
                "result_count": [
                    51800.0,
                    152000.0,
                    45100.0
                ],
                "answer_relation_to_question": [
                    1.4175006801906433,
                    3.5859911092919927,
                    0.9965082105173644
                ],
                "word_count_appended": [
                    305.0,
                    60.0,
                    239.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "By definition, a hypoglycemic person would be most helped by which of these?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "long walk",
                "bottom"
            ],
            "question": "by definition, a hypoglycemic person would be most helped by which of these?",
            "lines": [
                [
                    0.575,
                    0.41666666666666663,
                    0.42904455509486666,
                    0.2789871897906493,
                    0.029877711460669962,
                    0.4760031471282455,
                    0.051722099621689784,
                    0.603000981629505,
                    0.9230769230769231,
                    0.2631578947368421,
                    0.2222222222222222,
                    0.31980093697565903,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.275,
                    0.125,
                    0.14598498486520678,
                    0.2635336823475806,
                    0.9686914262639089,
                    0.3288749016522423,
                    0.9457755359394704,
                    0.3683447856775581,
                    0.0,
                    0.40789473684210525,
                    0.5555555555555556,
                    0.3764567476226135,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.15,
                    0.4583333333333333,
                    0.4249704600399265,
                    0.4574791278617701,
                    0.0014308622754211475,
                    0.1951219512195122,
                    0.0025023644388398486,
                    0.028654232692936942,
                    0.07692307692307693,
                    0.32894736842105265,
                    0.2222222222222222,
                    0.30374231540172747,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "hot bath",
                "orange juice",
                "long walk"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "long walk": 0.22454153666075538,
                "hot bath": 0.13873097427305794,
                "orange juice": 0.12419704358049462
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2792037479026361,
                    1.505826990490454,
                    1.2149692616069099
                ],
                "result_count_important_words": [
                    105000.0,
                    1920000.0,
                    5080.0
                ],
                "wikipedia_search": [
                    0.9230769230769231,
                    0.0,
                    0.07692307692307693
                ],
                "word_count_appended_bing": [
                    2.0,
                    5.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.8333333333333333,
                    0.25,
                    0.9166666666666666
                ],
                "cosine_similarity_raw": [
                    0.05263451114296913,
                    0.017909208312630653,
                    0.05213470757007599
                ],
                "result_count_noun_chunks": [
                    129000.0,
                    78800.0,
                    6130.0
                ],
                "question_answer_similarity": [
                    5.856987493578345,
                    5.532560411840677,
                    9.604202732283738
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    6050000.0,
                    4180000.0,
                    2480000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    128000.0,
                    4150000.0,
                    6130.0
                ],
                "answer_relation_to_question": [
                    1.15,
                    0.55,
                    0.3
                ],
                "word_count_appended": [
                    20.0,
                    31.0,
                    25.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "The Benihana company has a restaurant in which of these places?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "japan"
            ],
            "question": "the benihana company has a restaurant in which of these places?",
            "lines": [
                [
                    0.14285714285714285,
                    0.22077922077922077,
                    0.21617977566742586,
                    0.5035127232280738,
                    0.4543269230769231,
                    0.4144789180588703,
                    0.6940397350993377,
                    0.6476331726609813,
                    0.3333333333333333,
                    0.18245264207377868,
                    0.24603174603174602,
                    0.35690676611531696,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.5676937441643324,
                    0.39610389610389607,
                    0.2132373280638798,
                    0.05543601476327453,
                    0.26778846153846153,
                    0.19888623707239458,
                    0.11019867549668874,
                    0.008775182301322458,
                    0.13333333333333333,
                    0.39780658025922233,
                    0.4365079365079365,
                    0.28435384415953385,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.28944911297852477,
                    0.38311688311688313,
                    0.5705828962686944,
                    0.4410512620086516,
                    0.2778846153846154,
                    0.38663484486873506,
                    0.1957615894039735,
                    0.3435916450376962,
                    0.5333333333333333,
                    0.419740777666999,
                    0.31746031746031744,
                    0.35873938972514924,
                    1.0,
                    1.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "china",
                "aruba",
                "japan"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "china": 0.1244958255248295,
                "aruba": 0.11072714634721449,
                "japan": 0.9117124847260214
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.4276270644612676,
                    1.1374153766381352,
                    1.4349575589005967
                ],
                "result_count_important_words": [
                    262000.0,
                    41600.0,
                    73900.0
                ],
                "wikipedia_search": [
                    1.0,
                    0.4,
                    1.6
                ],
                "word_count_appended_bing": [
                    31.0,
                    55.0,
                    40.0
                ],
                "answer_relation_to_question_bing": [
                    0.6623376623376623,
                    1.1883116883116882,
                    1.1493506493506493
                ],
                "cosine_similarity_raw": [
                    0.06742141395807266,
                    0.06650373339653015,
                    0.17795145511627197
                ],
                "result_count_noun_chunks": [
                    2620000.0,
                    35500.0,
                    1390000.0
                ],
                "question_answer_similarity": [
                    2.037246011197567,
                    0.22429780766833574,
                    1.7845227792859077
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    58.0
                ],
                "result_count_bing": [
                    52100000.0,
                    25000000.0,
                    48600000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    75.0
                ],
                "result_count": [
                    94500.0,
                    55700.0,
                    57800.0
                ],
                "answer_relation_to_question": [
                    0.42857142857142855,
                    1.7030812324929971,
                    0.8683473389355743
                ],
                "word_count_appended": [
                    183.0,
                    399.0,
                    421.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What nickname was given to New York City\u2019s 28th Street between Fifth and Sixth Avenues?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "tin pan alley",
                "bottom"
            ],
            "question": "what nickname was given to new york city\u2019s 28th street between fifth and sixth avenues?",
            "lines": [
                [
                    0.09859361098997979,
                    0.18195924445924447,
                    0.329006145899004,
                    0.38320847994766066,
                    0.7040641099026903,
                    0.21565362198168192,
                    0.5882352941176471,
                    0.5710059171597633,
                    0.17213779128672746,
                    0.11875,
                    0.1111111111111111,
                    0.3715238775898524,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.40890382394593267,
                    0.17013734722068052,
                    0.128124348768079,
                    0.299305463694864,
                    0.11848883800801374,
                    0.3855120732722731,
                    0.24369747899159663,
                    0.2588757396449704,
                    0.29868287740628163,
                    0.26875,
                    0.18518518518518517,
                    0.3032419554444773,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.4925025650640875,
                    0.647903408320075,
                    0.542869505332917,
                    0.31748605635747534,
                    0.17744705208929593,
                    0.398834304746045,
                    0.16806722689075632,
                    0.17011834319526628,
                    0.5291793313069908,
                    0.6125,
                    0.7037037037037037,
                    0.32523416696567026,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "five points",
                "diamond district",
                "tin pan alley"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "diamond district": -0.0014426766050626987,
                "five points": 0.22704014590132562,
                "tin pan alley": 0.7165333773402597
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.715238775898524,
                    3.0324195544447727,
                    3.2523416696567025
                ],
                "result_count_important_words": [
                    42000.0,
                    17400.0,
                    12000.0
                ],
                "wikipedia_search": [
                    1.7213779128672746,
                    2.9868287740628165,
                    5.291793313069908
                ],
                "word_count_appended_bing": [
                    3.0,
                    5.0,
                    19.0
                ],
                "answer_relation_to_question_bing": [
                    1.6376332001332001,
                    1.5312361249861248,
                    5.831130674880675
                ],
                "cosine_similarity_raw": [
                    0.0942077785730362,
                    0.03668718785047531,
                    0.15544551610946655
                ],
                "result_count_noun_chunks": [
                    38600.0,
                    17500.0,
                    11500.0
                ],
                "question_answer_similarity": [
                    9.562085453420877,
                    7.468478831462562,
                    7.922133668791503
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    5.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    5.0
                ],
                "result_count_bing": [
                    25900.0,
                    46300.0,
                    47900.0
                ],
                "result_count": [
                    36900.0,
                    6210.0,
                    9300.0
                ],
                "answer_relation_to_question": [
                    0.9859361098997979,
                    4.0890382394593265,
                    4.925025650640875
                ],
                "word_count_appended": [
                    19.0,
                    43.0,
                    98.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What did Yankee Doodle stick in his cap?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "feather"
            ],
            "question": "what did yankee doodle stick in his cap?",
            "lines": [
                [
                    0.32,
                    0.16428571428571428,
                    0.9076932769663477,
                    0.257263365662809,
                    0.41334873259984195,
                    0.30090819564503773,
                    0.43812170860152133,
                    0.38424657534246576,
                    0.2567567567567568,
                    0.5780730897009967,
                    0.647887323943662,
                    0.47220876589901845,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.023809523809523808,
                    0.16428571428571428,
                    0.03136735602537429,
                    0.44615722471499186,
                    0.003100115494498815,
                    0.06116642958748222,
                    0.05280866003510825,
                    0.2232876712328767,
                    0.6216216216216217,
                    0.044850498338870434,
                    0.08450704225352113,
                    0.20556515483722265,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.6561904761904762,
                    0.6714285714285715,
                    0.060939367008278005,
                    0.29657940962219914,
                    0.5835511519056592,
                    0.63792537476748,
                    0.5090696313633704,
                    0.39246575342465756,
                    0.12162162162162164,
                    0.3770764119601329,
                    0.2676056338028169,
                    0.3222260792637589,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "feather",
                "noodle soup",
                "duck"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "feather": 0.7783667384251826,
                "noodle soup": 0.12028618117953958,
                "duck": 0.18127460351028826
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.8888350635960738,
                    0.8222606193488906,
                    1.2889043170550356
                ],
                "result_count_important_words": [
                    59900.0,
                    7220.0,
                    69600.0
                ],
                "wikipedia_search": [
                    0.5135135135135135,
                    1.2432432432432432,
                    0.24324324324324326
                ],
                "word_count_appended_bing": [
                    92.0,
                    12.0,
                    38.0
                ],
                "answer_relation_to_question_bing": [
                    0.32857142857142857,
                    0.32857142857142857,
                    1.342857142857143
                ],
                "cosine_similarity_raw": [
                    0.6004437804222107,
                    0.020749667659401894,
                    0.04031170532107353
                ],
                "result_count_noun_chunks": [
                    561000.0,
                    326000.0,
                    573000.0
                ],
                "question_answer_similarity": [
                    1.9004967287182808,
                    3.2959233969449997,
                    2.1909384429454803
                ],
                "word_count_noun_chunks": [
                    153.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    275000.0,
                    55900.0,
                    583000.0
                ],
                "word_count_raw": [
                    56.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    1020000.0,
                    7650.0,
                    1440000.0
                ],
                "answer_relation_to_question": [
                    0.64,
                    0.047619047619047616,
                    1.3123809523809524
                ],
                "word_count_appended": [
                    348.0,
                    27.0,
                    227.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "In classic literature, which of these comes right after the Telemachiad and the Odyssey?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "the iliad",
                "top"
            ],
            "question": "in classic literature, which of these comes right after the telemachiad and the odyssey?",
            "lines": [
                [
                    0.36259922154231095,
                    0.511040646569524,
                    0.672281773747637,
                    0.3184274835498429,
                    0.47368421052631576,
                    0.3333333333333333,
                    0.46226415094339623,
                    0.484375,
                    0.810171568627451,
                    0.4857142857142857,
                    0.7241379310344828,
                    0.4288934846793951,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.2619501615436575,
                    0.14379387646746505,
                    0.13717343366313217,
                    0.3407862582250785,
                    0.4824561403508772,
                    0.3333333333333333,
                    0.49056603773584906,
                    0.4921875,
                    0.09607843137254903,
                    0.34285714285714286,
                    0.06896551724137931,
                    0.4116782513882844,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.37545061691403153,
                    0.34516547696301103,
                    0.19054479258923085,
                    0.3407862582250785,
                    0.043859649122807015,
                    0.3333333333333333,
                    0.04716981132075472,
                    0.0234375,
                    0.09375,
                    0.17142857142857143,
                    0.20689655172413793,
                    0.1594282639323205,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "the iliad",
                "the nostos",
                "the telegony"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "the nostos": 0.31103628872369626,
                "the telegony": -0.02019159037579181,
                "the iliad": 0.8839515012101651
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.5733609080763706,
                    2.4700695083297064,
                    0.9565695835939231
                ],
                "result_count_important_words": [
                    49.0,
                    52.0,
                    5.0
                ],
                "wikipedia_search": [
                    3.240686274509804,
                    0.3843137254901961,
                    0.375
                ],
                "word_count_appended_bing": [
                    21.0,
                    2.0,
                    6.0
                ],
                "answer_relation_to_question_bing": [
                    2.55520323284762,
                    0.7189693823373252,
                    1.7258273848150552
                ],
                "cosine_similarity_raw": [
                    0.2327987551689148,
                    0.047500625252723694,
                    0.0659821406006813
                ],
                "result_count_noun_chunks": [
                    62.0,
                    63.0,
                    3.0
                ],
                "question_answer_similarity": [
                    7.283338570967317,
                    7.794747084379196,
                    7.794747084379196
                ],
                "word_count_noun_chunks": [
                    5.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    140000.0,
                    140000.0,
                    140000.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    34.0,
                    24.0,
                    12.0
                ],
                "answer_relation_to_question": [
                    1.8129961077115548,
                    1.3097508077182876,
                    1.8772530845701576
                ],
                "result_count": [
                    54.0,
                    55.0,
                    5.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which scientific plant name is one of the 11 herbs and spices in Colonel Sanders' \u201coriginal recipe\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "zingiber officinale"
            ],
            "question": "which scientific plant name is one of the 11 herbs and spices in colonel sanders' \u201coriginal recipe\u201d?",
            "lines": [
                [
                    0.33903870665939745,
                    0.4805068226120858,
                    0.4586243544942367,
                    0.1860688788809157,
                    0.5675675675675675,
                    0.13251961639058413,
                    0.7736804049168474,
                    0.20610687022900764,
                    0.45453839288527637,
                    0.4166666666666667,
                    0.2222222222222222,
                    0.385605745306912,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.29662918995084064,
                    0.21393762183235865,
                    0.3520517295530894,
                    0.8139311211190843,
                    0.13513513513513514,
                    0.5841325196163906,
                    0.028199566160520606,
                    0.7022900763358778,
                    0.34032917637253685,
                    0.25,
                    0.5555555555555556,
                    0.270270061562166,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.36433210338976196,
                    0.3055555555555555,
                    0.1893239159526739,
                    -0.0,
                    0.2972972972972973,
                    0.2833478639930253,
                    0.19812002892263195,
                    0.0916030534351145,
                    0.20513243074218682,
                    0.3333333333333333,
                    0.2222222222222222,
                    0.3441241931309219,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "zingiber officinale",
                "pimenta dioica",
                "petroselinum crispum"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "pimenta dioica": 0.13505240119505785,
                "petroselinum crispum": 0.255793758027924,
                "zingiber officinale": 0.6646840021837105
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.470451707762208,
                    2.432430554059494,
                    3.097117738178297
                ],
                "result_count_important_words": [
                    2140.0,
                    78.0,
                    548.0
                ],
                "wikipedia_search": [
                    2.727230357311658,
                    2.041975058235221,
                    1.230794584453121
                ],
                "word_count_appended_bing": [
                    2.0,
                    5.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    2.883040935672515,
                    1.283625730994152,
                    1.833333333333333
                ],
                "cosine_similarity_raw": [
                    0.027250373736023903,
                    0.02091808058321476,
                    0.011249179020524025
                ],
                "result_count_noun_chunks": [
                    27.0,
                    92.0,
                    12.0
                ],
                "question_answer_similarity": [
                    -0.8044166648760438,
                    -3.518803154118359,
                    0.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    3040.0,
                    13400.0,
                    6500.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    21.0,
                    5.0,
                    11.0
                ],
                "answer_relation_to_question": [
                    2.034232239956385,
                    1.7797751397050439,
                    2.185992620338572
                ],
                "word_count_appended": [
                    5.0,
                    3.0,
                    4.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Tom from MySpace shares his name with a key character in what film franchise?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "harry potter"
            ],
            "lines": [
                [
                    0.2873179181709988,
                    0.418085051827605,
                    0.16164225277470645,
                    0.2408913770840809,
                    0.5312053358742258,
                    0.336,
                    0.22588099364529174,
                    0.1931407942238267,
                    0.2508146878989451,
                    0.36403508771929827,
                    0.34065934065934067,
                    0.34167784027889414,
                    1.0,
                    0,
                    1.0
                ],
                [
                    0.3120748288046866,
                    0.24866664848154207,
                    0.39441402399603226,
                    0.37857666352058394,
                    0.1924726060028585,
                    0.456,
                    0.3437319468515309,
                    0.38086642599277976,
                    0.2851105287912383,
                    0.18421052631578946,
                    0.37362637362637363,
                    0.32686977602922634,
                    0.0,
                    0,
                    1.0
                ],
                [
                    0.4006072530243146,
                    0.33324829969085284,
                    0.4439437232292613,
                    0.38053195939533513,
                    0.27632205812291566,
                    0.208,
                    0.43038705950317735,
                    0.4259927797833935,
                    0.46407478330981655,
                    0.4517543859649123,
                    0.2857142857142857,
                    0.3314523836918795,
                    0.0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "the godfather": 0.29820156526251096,
                "the matrix": 0.3409253054946265,
                "harry potter": 0.3608731292428626
            },
            "question": "tom from myspace shares his name with a key character in what film franchise?",
            "rate_limited": false,
            "answers": [
                "harry potter",
                "the godfather",
                "the matrix"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "the godfather": 0.10813029274791792,
                "the matrix": 0.29867154909798527,
                "harry potter": 0.5119906729449544
            },
            "integer_answers": {
                "the godfather": 2,
                "the matrix": 7,
                "harry potter": 4
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.391744881952259,
                    2.2880884322045842,
                    2.3201666858431564
                ],
                "result_count_important_words": [
                    782000.0,
                    1190000.0,
                    1490000.0
                ],
                "wikipedia_search": [
                    1.2540734394947255,
                    1.4255526439561916,
                    2.320373916549083
                ],
                "word_count_appended_bing": [
                    31.0,
                    34.0,
                    26.0
                ],
                "answer_relation_to_question_bing": [
                    2.5085103109656304,
                    1.4919998908892527,
                    1.9994897981451174
                ],
                "cosine_similarity_raw": [
                    0.017140304669737816,
                    0.04182307794690132,
                    0.04707513377070427
                ],
                "result_count_noun_chunks": [
                    1070000.0,
                    2110000.0,
                    2360000.0
                ],
                "question_answer_similarity": [
                    5.276467658113688,
                    8.292316418141127,
                    8.335145080462098
                ],
                "word_count_noun_chunks": [
                    3.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    168000.0,
                    228000.0,
                    104000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    2230000.0,
                    808000.0,
                    1160000.0
                ],
                "answer_relation_to_question": [
                    1.7239075090259925,
                    1.8724489728281197,
                    2.4036435181458877
                ],
                "word_count_appended": [
                    83.0,
                    42.0,
                    103.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What did the first-ever State of the Union \u201cHero in the Balcony\u201d do to earn his invitation?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "rescue a drowning woman"
            ],
            "lines": [
                [
                    0.3883228840125392,
                    0.14492753623188406,
                    0.14997854762669421,
                    0.3212687211876556,
                    0.4166666666666667,
                    0.33519553072625696,
                    0.08333333333333333,
                    0.09806295399515738,
                    0.0,
                    0.3,
                    0.3333333333333333,
                    0.35855319193783125,
                    0,
                    0,
                    1.0
                ],
                [
                    0.40287729511867443,
                    0.4492753623188406,
                    0.5707637904043806,
                    0.2893956254529456,
                    0.0,
                    0.28212290502793297,
                    0.8333333333333334,
                    0.2602905569007264,
                    0.75,
                    0.15,
                    0.3333333333333333,
                    0.18126523968156794,
                    0,
                    0,
                    1.0
                ],
                [
                    0.2087998208687864,
                    0.4057971014492754,
                    0.2792576619689252,
                    0.38933565335939874,
                    0.5833333333333334,
                    0.38268156424581007,
                    0.08333333333333333,
                    0.6416464891041163,
                    0.25,
                    0.55,
                    0.3333333333333333,
                    0.4601815683806009,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "serve as pow": 0.3752214534643113,
                "build a hospital": 0.24413689158761268,
                "rescue a drowning woman": 0.38064165494807606
            },
            "question": "what did the first-ever state of the union \u201chero in the balcony\u201d do to earn his invitation?",
            "rate_limited": false,
            "answers": [
                "build a hospital",
                "serve as pow",
                "rescue a drowning woman"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "serve as pow": 0.31932680869065144,
                "build a hospital": 0.14888710827133908,
                "rescue a drowning woman": 0.3727603250328644
            },
            "integer_answers": {
                "serve as pow": 5,
                "build a hospital": 1,
                "rescue a drowning woman": 6
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.1513191516269874,
                    1.0875914380894076,
                    2.7610894102836054
                ],
                "result_count_important_words": [
                    10.0,
                    100.0,
                    10.0
                ],
                "wikipedia_search": [
                    0.0,
                    3.0,
                    1.0
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.43478260869565216,
                    1.3478260869565217,
                    1.2173913043478262
                ],
                "cosine_similarity_raw": [
                    0.01194948423653841,
                    0.04547538980841637,
                    0.022249748930335045
                ],
                "result_count_noun_chunks": [
                    1620000.0,
                    4300000.0,
                    10600000.0
                ],
                "question_answer_similarity": [
                    19.938595544546843,
                    17.96048587281257,
                    24.162968914955854
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    120000.0,
                    101000.0,
                    137000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    10.0,
                    0,
                    14.0
                ],
                "answer_relation_to_question": [
                    1.553291536050157,
                    1.6115091804746977,
                    0.8351992834751456
                ],
                "word_count_appended": [
                    6.0,
                    3.0,
                    11.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these restaurant brands has its original location in Europe?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "benihana"
            ],
            "lines": [
                [
                    0.1669364881693649,
                    0.3786375661375661,
                    0.29348677355022645,
                    -0.09341174832817872,
                    0.23267008985879334,
                    0.3333333333333333,
                    0.5505026328386788,
                    0.06634382566585957,
                    0.6653508771929825,
                    0.8195652173913044,
                    0.8809523809523809,
                    0.3713908657467212,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.5424934274249342,
                    0.3568121693121693,
                    0.3659790038192885,
                    0.6188005737291824,
                    0.6370346598202824,
                    0.3333333333333333,
                    0.15509813307802778,
                    0.8958837772397095,
                    0.2655701754385965,
                    0.11304347826086956,
                    0.07142857142857142,
                    0.3153106754225738,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.2905700844057008,
                    0.2645502645502645,
                    0.34053422263048505,
                    0.4746111745989963,
                    0.13029525032092426,
                    0.3333333333333333,
                    0.29439923408329344,
                    0.03777239709443099,
                    0.06907894736842105,
                    0.06739130434782609,
                    0.047619047619047616,
                    0.31329845883070506,
                    0.5,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "mr. chow": 0.2616752656559592,
                "benihana": 0.40469702160778814,
                "p.f. chang's": 0.33362771273625275
            },
            "question": "which of these restaurant brands has its original location in europe?",
            "rate_limited": false,
            "answers": [
                "benihana",
                "p.f. chang's",
                "mr. chow"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "mr. chow": 0.09501769441273002,
                "benihana": 0.45154518088222373,
                "p.f. chang's": 0.2154564648448365
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.8569543287336059,
                    1.576553377112869,
                    1.5664922941535253
                ],
                "result_count_important_words": [
                    115000.0,
                    32400.0,
                    61500.0
                ],
                "wikipedia_search": [
                    2.66140350877193,
                    1.062280701754386,
                    0.2763157894736842
                ],
                "word_count_appended_bing": [
                    37.0,
                    3.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.1359126984126984,
                    1.070436507936508,
                    0.7936507936507935
                ],
                "cosine_similarity_raw": [
                    0.06169611215591431,
                    0.07693526148796082,
                    0.07158631831407547
                ],
                "result_count_noun_chunks": [
                    137000.0,
                    1850000.0,
                    78000.0
                ],
                "question_answer_similarity": [
                    -1.2132769133895636,
                    8.037280786782503,
                    6.164479214698076
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    409000.0,
                    409000.0,
                    409000.0
                ],
                "result_count": [
                    145000.0,
                    397000.0,
                    81200.0
                ],
                "answer_relation_to_question": [
                    0.6677459526774596,
                    2.169973709699737,
                    1.1622803376228033
                ],
                "word_count_appended": [
                    377.0,
                    52.0,
                    31.0
                ]
            },
            "integer_answers": {
                "mr. chow": 0,
                "benihana": 9,
                "p.f. chang's": 5
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which country did NOT have a native player selected in the first round of the 2016 NBA draft?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "haiti"
            ],
            "lines": [
                [
                    0.36319103313840156,
                    0.3084598018311802,
                    0.27469689704673467,
                    0.4334543957104662,
                    0.3183856502242153,
                    0.33333333333333337,
                    0.43118383060635224,
                    0.39890236857307915,
                    0.3753654432396138,
                    0.31894484412470026,
                    0.31868131868131866,
                    0.33801605371525345,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.2663255360623782,
                    0.35951649316442996,
                    0.39708848637703376,
                    0.38675200249967023,
                    0.33856502242152464,
                    0.33333333333333337,
                    0.3787295476419634,
                    0.4400154053533603,
                    0.2638855120163557,
                    0.34772182254196643,
                    0.3351648351648352,
                    0.33383562402814826,
                    0.5,
                    0,
                    -1.0
                ],
                [
                    0.3704834307992203,
                    0.3320237050043898,
                    0.32821461657623163,
                    0.1797936017898636,
                    0.3430493273542601,
                    0.33333333333333337,
                    0.19008662175168434,
                    0.16108222607356054,
                    0.36074904474403063,
                    0.33333333333333337,
                    0.34615384615384615,
                    0.3281483222565983,
                    0.5,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "bahamas": 0.2798563660607693,
                "brazil": 0.3682382447430228,
                "haiti": 0.35190538919620784
            },
            "question": "which country did not have a native player selected in the first round of the 2016 nba draft?",
            "rate_limited": false,
            "answers": [
                "haiti",
                "bahamas",
                "brazil"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "bahamas": 0.2177624825396658,
                "brazil": 0.205004757039626,
                "haiti": 0.30290258797200675
            },
            "integer_answers": {
                "bahamas": 2,
                "brazil": 4,
                "haiti": 7
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.5917431405559452,
                    2.6586300155496274,
                    2.7496268438944274
                ],
                "result_count_important_words": [
                    71500.0,
                    126000.0,
                    322000.0
                ],
                "wikipedia_search": [
                    1.9941529081661797,
                    3.77783180773831,
                    2.2280152840955103
                ],
                "word_count_appended_bing": [
                    33.0,
                    30.0,
                    28.0
                ],
                "answer_relation_to_question_bing": [
                    2.681562774363477,
                    1.9667690956979804,
                    2.3516681299385427
                ],
                "cosine_similarity_raw": [
                    0.045612871646881104,
                    0.020834553986787796,
                    0.03477814793586731
                ],
                "result_count_noun_chunks": [
                    105000.0,
                    62300.0,
                    352000.0
                ],
                "question_answer_similarity": [
                    0.46972580114379525,
                    0.7993842256255448,
                    2.260242559015751
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    505000.0,
                    505000.0,
                    505000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    151.0,
                    127.0,
                    139.0
                ],
                "answer_relation_to_question": [
                    1.3680896686159842,
                    2.336744639376218,
                    1.2951656920077972
                ],
                "result_count": [
                    81.0,
                    72.0,
                    70.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which player won Rookie of the Year in their sport most recently?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "mike trout"
            ],
            "lines": [
                [
                    0.5322469861287864,
                    0.37511050353155617,
                    0.4826854771140381,
                    0.5754058205595037,
                    0.3434343434343434,
                    0.40166975881261596,
                    0.40298507462686567,
                    0.2423270728355474,
                    0.39216705261113155,
                    0.24864864864864866,
                    0.35802469135802467,
                    0.3532286779305869,
                    0.4,
                    0,
                    -1.0
                ],
                [
                    0.2186241001546023,
                    0.1723490544543176,
                    0.14312604916084481,
                    0.300034466776248,
                    0.37373737373737376,
                    0.2782931354359926,
                    0.26865671641791045,
                    0.0842876775080165,
                    0.16188290419375945,
                    0.44324324324324327,
                    0.2716049382716049,
                    0.3327628155953313,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.24912891371661128,
                    0.45254044201412624,
                    0.3741884737251171,
                    0.12455971266424831,
                    0.2828282828282828,
                    0.3200371057513915,
                    0.3283582089552239,
                    0.6733852496564361,
                    0.445950043195109,
                    0.3081081081081081,
                    0.37037037037037035,
                    0.3140085064740819,
                    0.6,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "mike trout": 0.3929180082762806,
                "von miller": 0.23450788268840345,
                "blake griffin": 0.37257410903531585
            },
            "question": "which player won rookie of the year in their sport most recently?",
            "rate_limited": false,
            "answers": [
                "mike trout",
                "von miller",
                "blake griffin"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "mike trout": 0.4757194791850875,
                "von miller": 0.3008174886492512,
                "blake griffin": 0.18283525257915206
            },
            "integer_answers": {
                "mike trout": 6,
                "von miller": 2,
                "blake griffin": 5
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.1193720675835213,
                    1.9965768935719876,
                    1.8840510388444913
                ],
                "result_count_important_words": [
                    27.0,
                    18.0,
                    22.0
                ],
                "wikipedia_search": [
                    1.9608352630556578,
                    0.8094145209687973,
                    2.229750215975545
                ],
                "word_count_appended_bing": [
                    29.0,
                    22.0,
                    30.0
                ],
                "answer_relation_to_question_bing": [
                    1.875552517657781,
                    0.861745272271588,
                    2.262702210070631
                ],
                "cosine_similarity_raw": [
                    0.09483902901411057,
                    0.028121698647737503,
                    0.07352131605148315
                ],
                "result_count_noun_chunks": [
                    529000.0,
                    184000.0,
                    1470000.0
                ],
                "question_answer_similarity": [
                    3.5639198571443558,
                    1.858338507823646,
                    0.7714917324483395
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    3.0
                ],
                "result_count_bing": [
                    433000.0,
                    300000.0,
                    345000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    34.0,
                    37.0,
                    28.0
                ],
                "answer_relation_to_question": [
                    3.1934819167727184,
                    1.3117446009276137,
                    1.4947734822996677
                ],
                "word_count_appended": [
                    46.0,
                    82.0,
                    57.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these has NEVER been named Pantone\u2019s Color of the Year?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "cucumber"
            ],
            "lines": [
                [
                    0.43311072242539883,
                    0.3464646464646465,
                    0.31457762529392175,
                    0.4497752310993193,
                    0.35684730379071006,
                    0.2836474648261216,
                    0.22712146422628954,
                    0.24744245524296676,
                    0.3277777777777778,
                    0.17931034482758623,
                    0.17857142857142855,
                    0.3757656669292377,
                    0,
                    0.33333333333333337,
                    -1.0
                ],
                [
                    0.2985600961113761,
                    0.3242424242424242,
                    0.37173026454525193,
                    0.22207050061374806,
                    0.14628937533368924,
                    0.4393416511813114,
                    0.3826955074875208,
                    0.43371696504688834,
                    0.40416666666666673,
                    0.4,
                    0.39285714285714285,
                    0.31300362577055485,
                    0,
                    0.33333333333333337,
                    -1.0
                ],
                [
                    0.26832918146322504,
                    0.3292929292929293,
                    0.31369211016082627,
                    0.32815426828693267,
                    0.49686332087560064,
                    0.277010883992567,
                    0.39018302828618967,
                    0.3188405797101449,
                    0.26805555555555555,
                    0.4206896551724138,
                    0.4285714285714286,
                    0.31123070730020747,
                    0,
                    0.33333333333333337,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "chili pepper": 0.3101158489228686,
                "cucumber": 0.3763468515678865,
                "sand dollar": 0.3135372995092449
            },
            "question": "which of these has never been named pantone\u2019s color of the year?",
            "rate_limited": false,
            "answers": [
                "cucumber",
                "sand dollar",
                "chili pepper"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "chili pepper": 0.2587560772791508,
                "cucumber": 0.28543370597102813,
                "sand dollar": 0.22568283237120018
            },
            "integer_answers": {
                "chili pepper": 5,
                "cucumber": 5,
                "sand dollar": 3
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.9938746645660984,
                    1.4959709938355612,
                    1.5101543415983403
                ],
                "result_count_important_words": [
                    328000.0,
                    141000.0,
                    132000.0
                ],
                "wikipedia_search": [
                    1.0333333333333334,
                    0.575,
                    1.3916666666666668
                ],
                "word_count_appended_bing": [
                    27.0,
                    9.0,
                    6.0
                ],
                "answer_relation_to_question_bing": [
                    0.30707070707070705,
                    0.3515151515151515,
                    0.3414141414141414
                ],
                "cosine_similarity_raw": [
                    0.06203324347734451,
                    0.04291277006268501,
                    0.0623294934630394
                ],
                "result_count_noun_chunks": [
                    23700000.0,
                    6220000.0,
                    17000000.0
                ],
                "question_answer_similarity": [
                    1.1156974867917597,
                    6.173950637457892,
                    3.817396380007267
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    16300000.0,
                    4570000.0,
                    16800000.0
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    1.0
                ],
                "word_count_appended": [
                    186.0,
                    58.0,
                    46.0
                ],
                "answer_relation_to_question": [
                    0.4013356654476069,
                    1.2086394233317432,
                    1.39002491122065
                ],
                "result_count": [
                    4290000.0,
                    10600000.0,
                    94000.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these Supreme Court cases dealt with the First Amendment?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "miranda v. arizona"
            ],
            "lines": [
                [
                    0.26881199507036646,
                    0.18949701619778347,
                    0.484741766196998,
                    0.698490878534633,
                    0.07011289364230541,
                    0.34737678855325915,
                    0.043017456359102244,
                    0.07066508313539192,
                    0.4826877637130802,
                    0.04081632653061224,
                    0.06666666666666667,
                    0.2832305367363727,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.3531315729505095,
                    0.33958104981122883,
                    0.22482815824586386,
                    0.2839919389250533,
                    0.44860368389780153,
                    0.29411764705882354,
                    0.47381546134663344,
                    0.4483372921615202,
                    0.2826107594936709,
                    0.47619047619047616,
                    0.43333333333333335,
                    0.3569367332533441,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.378056431979124,
                    0.47092193399098764,
                    0.2904300755571382,
                    0.017517182540313685,
                    0.48128342245989303,
                    0.3585055643879173,
                    0.48316708229426436,
                    0.4809976247030879,
                    0.23470147679324893,
                    0.48299319727891155,
                    0.5,
                    0.35983273001028326,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "miranda v. arizona": 0.36795650888902154,
                "plessy v. ferguson": 0.37820056016626413,
                "ny times co. v. sullivan": 0.2538429309447143
            },
            "question": "which of these supreme court cases dealt with the first amendment?",
            "rate_limited": false,
            "answers": [
                "ny times co. v. sullivan",
                "miranda v. arizona",
                "plessy v. ferguson"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "miranda v. arizona": 0.4415111113278891,
                "plessy v. ferguson": 0.39423120353260965,
                "ny times co. v. sullivan": 0.26200279281204586
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.4161526836818636,
                    1.7846836662667203,
                    1.7991636500514163
                ],
                "result_count_important_words": [
                    13800.0,
                    152000.0,
                    155000.0
                ],
                "wikipedia_search": [
                    2.413438818565401,
                    1.4130537974683546,
                    1.1735073839662447
                ],
                "word_count_appended_bing": [
                    2.0,
                    13.0,
                    15.0
                ],
                "answer_relation_to_question_bing": [
                    0.9474850809889174,
                    1.6979052490561442,
                    2.354609669954938
                ],
                "cosine_similarity_raw": [
                    0.08185433596372604,
                    0.037964873015880585,
                    0.04904252663254738
                ],
                "result_count_noun_chunks": [
                    23800.0,
                    151000.0,
                    162000.0
                ],
                "question_answer_similarity": [
                    7.861860077828169,
                    3.1964696400100365,
                    0.1971645476296544
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    4370000.0,
                    3700000.0,
                    4510000.0
                ],
                "word_count_appended": [
                    6.0,
                    70.0,
                    71.0
                ],
                "answer_relation_to_question": [
                    1.3440599753518323,
                    1.7656578647525476,
                    1.8902821598956199
                ],
                "result_count": [
                    23600.0,
                    151000.0,
                    162000.0
                ]
            },
            "integer_answers": {
                "miranda v. arizona": 0,
                "plessy v. ferguson": 9,
                "ny times co. v. sullivan": 3
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Blackbeard and Long John Silver are both names of what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "pirates"
            ],
            "question": "blackbeard and long john silver are both names of what?",
            "lines": [
                [
                    0.469828722002635,
                    1.0,
                    0.024877707310209555,
                    0.4728242290230917,
                    0.03962973676598207,
                    0.20116618075801748,
                    0.04097908411496213,
                    0.05045246064212223,
                    0.2206896551724138,
                    0.04103671706263499,
                    0.03773584905660377,
                    0.24092938576646877,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.09354413702239789,
                    0.0,
                    0.10511737900304696,
                    0.33184181016635317,
                    0.10702921608330923,
                    0.24489795918367346,
                    0.11135075487786528,
                    0.1375976199330606,
                    0.0,
                    0.019438444924406047,
                    0.03773584905660377,
                    0.18043914902387787,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.4366271409749671,
                    0.0,
                    0.8700049136867435,
                    0.19533396081055515,
                    0.8533410471507087,
                    0.5539358600583091,
                    0.8476701610071726,
                    0.8119499194248172,
                    0.7793103448275862,
                    0.9395248380129589,
                    0.9245283018867925,
                    0.5786314652096534,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "supreme court justices",
                "killer robots",
                "pirates"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "supreme court justices": 0.17017565307439306,
                "killer robots": 0.02520791314712899,
                "pirates": 0.9838225200813946
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2046469288323438,
                    0.9021957451193894,
                    2.893157326048267
                ],
                "result_count_important_words": [
                    8170.0,
                    22200.0,
                    169000.0
                ],
                "wikipedia_search": [
                    1.103448275862069,
                    0.0,
                    3.896551724137931
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    49.0
                ],
                "answer_relation_to_question_bing": [
                    2.0,
                    0.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.007400084286928177,
                    0.031268052756786346,
                    0.2587903141975403
                ],
                "result_count_noun_chunks": [
                    8140.0,
                    22200.0,
                    131000.0
                ],
                "question_answer_similarity": [
                    5.714865938061848,
                    4.010859303176403,
                    2.3609352707862854
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    46.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    18.0
                ],
                "result_count_bing": [
                    207000.0,
                    252000.0,
                    570000.0
                ],
                "word_count_appended": [
                    19.0,
                    9.0,
                    435.0
                ],
                "answer_relation_to_question": [
                    2.349143610013175,
                    0.46772068511198944,
                    2.1831357048748354
                ],
                "result_count": [
                    8220.0,
                    22200.0,
                    177000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which statement is NOT true about photosynthesis and chemosynthesis?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "both involve carbon"
            ],
            "question": "which statement is not true about photosynthesis and chemosynthesis?",
            "lines": [
                [
                    0.44390507011866237,
                    0.3162162162162162,
                    0.46020232643153,
                    0.36655614210623755,
                    0.3072124487467242,
                    0.4907621247113164,
                    0.40186278964107225,
                    0.313392439972194,
                    0.06666666666666665,
                    0.3157894736842105,
                    0.35714285714285715,
                    0.2667778970620627,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.11438272587405168,
                    0.21756756756756757,
                    0.07229947153598944,
                    0.3704553139415504,
                    0.49999373048613815,
                    0.4531691044393123,
                    0.3298500681508405,
                    0.4999943020592358,
                    0.43333333333333335,
                    0.34210526315789475,
                    0.2857142857142857,
                    0.4719265969760528,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.44171220400728595,
                    0.46621621621621623,
                    0.46749820203248066,
                    0.262988543952212,
                    0.19279382076713772,
                    0.056068770849371286,
                    0.26828714220808725,
                    0.18661325796857015,
                    0.5,
                    0.34210526315789475,
                    0.35714285714285715,
                    0.26129550596188456,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "both powered by sunlight",
                "both involve carbon",
                "both can occur in water"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "both powered by sunlight": 0.16546200299143754,
                "both involve carbon": 0.23867239362342407,
                "both can occur in water": 0.2185218852826646
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.8657768235034984,
                    0.22458722419157753,
                    1.9096359523049238
                ],
                "result_count_important_words": [
                    43200.0,
                    74900.0,
                    102000.0
                ],
                "wikipedia_search": [
                    1.7333333333333334,
                    0.26666666666666666,
                    0.0
                ],
                "answer_relation_to_question": [
                    0.3365695792880259,
                    2.31370364475569,
                    0.34972677595628415
                ],
                "answer_relation_to_question_bing": [
                    1.4702702702702704,
                    2.2594594594594595,
                    0.2702702702702703
                ],
                "word_count_appended": [
                    7.0,
                    6.0,
                    6.0
                ],
                "cosine_similarity_raw": [
                    0.025776971131563187,
                    0.2770218253135681,
                    0.021051429212093353
                ],
                "result_count_noun_chunks": [
                    131000.0,
                    4.0,
                    220000.0
                ],
                "question_answer_similarity": [
                    9.364760307595134,
                    9.09112606011331,
                    16.6328785084188
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    72000.0,
                    365000.0,
                    3460000.0
                ],
                "result_count": [
                    123000.0,
                    4.0,
                    196000.0
                ],
                "word_count_appended_bing": [
                    2.0,
                    3.0,
                    2.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which game is an example of combinatorics?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "sudoku"
            ],
            "lines": [
                [
                    0.6029106029106029,
                    0.5730337078651686,
                    0.37484961160174485,
                    0.42868967912442885,
                    0.11132306465848589,
                    0.010336969803718668,
                    0.28583928831850663,
                    0.11006953241561805,
                    0.14516129032258066,
                    0.3173173173173173,
                    0.3630573248407643,
                    0.37087723869177963,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.13721413721413722,
                    0.2921348314606742,
                    0.1950946896881642,
                    0.4580737238810853,
                    0.8712239842838027,
                    0.49648278822972514,
                    0.6693889456030334,
                    0.8726740421698618,
                    0.3544142614601019,
                    0.06106106106106106,
                    0.07643312101910828,
                    0.29729976120530205,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.2598752598752599,
                    0.13483146067415733,
                    0.4300556987100909,
                    0.11323659699448582,
                    0.017452951057711472,
                    0.4931802419665562,
                    0.044771766078459965,
                    0.01725642541452017,
                    0.5004244482173175,
                    0.6216216216216216,
                    0.5605095541401274,
                    0.33182300010291826,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "sudoku": 0.2937532520711022,
                "risk": 0.3077888023225597,
                "crossword puzzles": 0.3984579456063381
            },
            "question": "which game is an example of combinatorics?",
            "rate_limited": false,
            "answers": [
                "risk",
                "crossword puzzles",
                "sudoku"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sudoku": 0.5609330330327317,
                "risk": 0.3012202217787225,
                "crossword puzzles": 0.047967999689000895
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.112631716075339,
                    0.8918992836159062,
                    0.9954690003087548
                ],
                "result_count_important_words": [
                    392000.0,
                    918000.0,
                    61400.0
                ],
                "wikipedia_search": [
                    0.2903225806451613,
                    0.7088285229202038,
                    1.000848896434635
                ],
                "answer_relation_to_question": [
                    1.2058212058212059,
                    0.27442827442827444,
                    0.5197505197505198
                ],
                "answer_relation_to_question_bing": [
                    0.5730337078651685,
                    0.29213483146067415,
                    0.1348314606741573
                ],
                "word_count_appended": [
                    317.0,
                    61.0,
                    621.0
                ],
                "cosine_similarity_raw": [
                    0.04157327115535736,
                    0.02163727581501007,
                    0.047695986926555634
                ],
                "result_count_noun_chunks": [
                    391000.0,
                    3100000.0,
                    61300.0
                ],
                "question_answer_similarity": [
                    2.276065156329423,
                    2.4320754446089268,
                    0.6012131511233747
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    939000.0,
                    45100000.0,
                    44800000.0
                ],
                "result_count": [
                    391000.0,
                    3060000.0,
                    61300.0
                ],
                "word_count_appended_bing": [
                    57.0,
                    12.0,
                    88.0
                ]
            },
            "integer_answers": {
                "sudoku": 4,
                "risk": 3,
                "crossword puzzles": 5
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What is the name for anger experienced while driving a car?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "road rage"
            ],
            "question": "what is the name for anger experienced while driving a car?",
            "lines": [
                [
                    0.18541383989145183,
                    0.0925925925925926,
                    0.14045261749934335,
                    0.2690379668809628,
                    0.3136561042303362,
                    0.30753968253968256,
                    0.0,
                    0.0,
                    0.10526315789473684,
                    0.02666666666666667,
                    0.08333333333333333,
                    0.4116122782607475,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.3373134328358209,
                    0.0925925925925926,
                    0.08664854535182598,
                    0.3442987908485699,
                    5.361642807356174e-05,
                    0.30753968253968256,
                    7.910208974734202e-06,
                    7.520801854219511e-05,
                    0.12280701754385964,
                    0.04888888888888889,
                    0.08333333333333333,
                    0.0746474666225914,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.4772727272727273,
                    0.8148148148148149,
                    0.7728988371488307,
                    0.3866632422704673,
                    0.6862902793415903,
                    0.38492063492063494,
                    0.9999920897910253,
                    0.9999247919814578,
                    0.7719298245614036,
                    0.9244444444444444,
                    0.8333333333333334,
                    0.5137402551166612,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "highway displeasure",
                "street sadness",
                "road rage"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "road rage": 1.0103981235894897,
                "highway displeasure": 0.14148071799619674,
                "street sadness": 0.06175589236071524
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6464491130429897,
                    0.29858986649036556,
                    2.0549610204666444
                ],
                "result_count_important_words": [
                    0,
                    67.0,
                    8470000.0
                ],
                "wikipedia_search": [
                    0.3157894736842105,
                    0.3684210526315789,
                    2.3157894736842106
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    20.0
                ],
                "answer_relation_to_question_bing": [
                    0.2777777777777778,
                    0.2777777777777778,
                    2.4444444444444446
                ],
                "cosine_similarity_raw": [
                    0.06009414419531822,
                    0.03707350045442581,
                    0.33069297671318054
                ],
                "result_count_noun_chunks": [
                    0,
                    88.0,
                    1170000.0
                ],
                "question_answer_similarity": [
                    5.11338834092021,
                    6.543810315430164,
                    7.348997384309769
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    9.0
                ],
                "result_count_bing": [
                    1550000.0,
                    1550000.0,
                    1940000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    8.0
                ],
                "result_count": [
                    117000.0,
                    20.0,
                    256000.0
                ],
                "answer_relation_to_question": [
                    0.7416553595658073,
                    1.3492537313432835,
                    1.9090909090909092
                ],
                "word_count_appended": [
                    6.0,
                    11.0,
                    208.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these holidays does NOT involve fasting?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ramadan"
            ],
            "lines": [
                [
                    0.46616541353383456,
                    0.389957264957265,
                    0.4029195054409304,
                    0.08287465216945383,
                    0.21240576931456706,
                    0.20087609511889865,
                    0.25895022108115817,
                    0.15726227795193315,
                    0.38084816418149753,
                    0.29777070063694266,
                    0.32552083333333337,
                    0.3330353491089687,
                    0.0,
                    0.33333333333333337,
                    -1.0
                ],
                [
                    0.1797827903091061,
                    0.1955128205128205,
                    0.35727088161045173,
                    0.3758552752161449,
                    0.3071811407904484,
                    0.39549436795994997,
                    0.31172443303380404,
                    0.4101358411703239,
                    0.29062049062049067,
                    0.2814490445859873,
                    0.28385416666666663,
                    0.33912072565072715,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.3540517961570593,
                    0.41452991452991456,
                    0.23980961294861786,
                    0.5412700726144013,
                    0.48041308989498455,
                    0.40362953692115144,
                    0.4293253458850378,
                    0.43260188087774293,
                    0.32853134519801186,
                    0.42078025477707004,
                    0.390625,
                    0.32784392524030415,
                    0.5,
                    0.16666666666666669,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "yom kippur": 0.22427450832700538,
                "easter": 0.4511543456911262,
                "ramadan": 0.3245711459818685
            },
            "question": "which of these holidays does not involve fasting?",
            "rate_limited": false,
            "answers": [
                "easter",
                "ramadan",
                "yom kippur"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "yom kippur": 0.30254693022365486,
                "easter": 0.18923307663490713,
                "ramadan": 0.4276415149469218
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.0017879053461876,
                    0.9652756460956371,
                    1.0329364485581753
                ],
                "result_count_important_words": [
                    338000.0,
                    264000.0,
                    99100.0
                ],
                "wikipedia_search": [
                    0.7149110149110149,
                    1.2562770562770562,
                    1.0288119288119286
                ],
                "answer_relation_to_question": [
                    0.20300751879699247,
                    1.9213032581453633,
                    0.8756892230576441
                ],
                "result_count": [
                    1320000.0,
                    885000.0,
                    89900.0
                ],
                "answer_relation_to_question_bing": [
                    0.44017094017094016,
                    1.217948717948718,
                    0.3418803418803419
                ],
                "cosine_similarity_raw": [
                    0.07834909856319427,
                    0.11518995463848114,
                    0.20998741686344147
                ],
                "result_count_noun_chunks": [
                    6560000.0,
                    1720000.0,
                    1290000.0
                ],
                "question_answer_similarity": [
                    1.67487733066082,
                    0.49847650434821844,
                    -0.16571112116798759
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    4.0
                ],
                "result_count_bing": [
                    47800000.0,
                    16700000.0,
                    15400000.0
                ],
                "word_count_appended": [
                    508.0,
                    549.0,
                    199.0
                ],
                "word_count_appended_bing": [
                    67.0,
                    83.0,
                    42.0
                ]
            },
            "integer_answers": {
                "yom kippur": 3,
                "easter": 6,
                "ramadan": 5
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What country recently added two red stripes to its flag?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "venezuela"
            ],
            "lines": [
                [
                    0.17427248677248677,
                    0.2911290322580645,
                    0.17852270575154056,
                    0.6540067805717774,
                    0.9999825853372893,
                    0.29927007299270075,
                    0.9998991252209954,
                    0.4858339048609736,
                    0.3776198275335099,
                    0.494910941475827,
                    0.24786324786324787,
                    0.4361395138007124,
                    0,
                    0.07142857142857142,
                    1.0
                ],
                [
                    0.13315696649029982,
                    0.3887096774193548,
                    0.22936722435581622,
                    0.34599321942822264,
                    1.360520524268421e-05,
                    0.3445255474452555,
                    6.901958563472357e-05,
                    0.5141624124330421,
                    0.6223801724664901,
                    0.4618320610687023,
                    0.5384615384615384,
                    0.47354203666632194,
                    0,
                    0.9285714285714286,
                    1.0
                ],
                [
                    0.6925705467372134,
                    0.32016129032258067,
                    0.5921100698926433,
                    0.0,
                    3.8094574679515785e-06,
                    0.3562043795620438,
                    3.185519336987242e-05,
                    3.682705984368896e-06,
                    0.0,
                    0.043256997455470736,
                    0.21367521367521367,
                    0.09031844953296568,
                    0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "jamaica": 0.4392983689128998,
                "venezuela": 0.38313730073825764,
                "muaritania": 0.17756433034884256
            },
            "question": "what country recently added two red stripes to its flag?",
            "rate_limited": false,
            "answers": [
                "jamaica",
                "venezuela",
                "muaritania"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "jamaica": 0.2722751603793054,
                "venezuela": 0.7318992725222034,
                "muaritania": 0.19439748459447748
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.6168370828042744,
                    2.8412522199979318,
                    0.5419106971977941
                ],
                "result_count_important_words": [
                    1130000.0,
                    78.0,
                    36.0
                ],
                "wikipedia_search": [
                    1.5104793101340397,
                    2.4895206898659605,
                    0.0
                ],
                "word_count_appended_bing": [
                    29.0,
                    63.0,
                    25.0
                ],
                "answer_relation_to_question_bing": [
                    0.582258064516129,
                    0.7774193548387096,
                    0.6403225806451613
                ],
                "cosine_similarity_raw": [
                    0.04639171063899994,
                    0.05960439518094063,
                    0.15386837720870972
                ],
                "result_count_noun_chunks": [
                    3430000.0,
                    3630000.0,
                    26.0
                ],
                "question_answer_similarity": [
                    1.0611465505789965,
                    0.5613848697394133,
                    0.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    13.0,
                    0.0
                ],
                "result_count_bing": [
                    205000.0,
                    236000.0,
                    244000.0
                ],
                "word_count_appended": [
                    389.0,
                    363.0,
                    34.0
                ],
                "answer_relation_to_question": [
                    0.6970899470899471,
                    0.5326278659611993,
                    2.7702821869488536
                ],
                "result_count": [
                    7350000.0,
                    100.0,
                    28.0
                ]
            },
            "integer_answers": {
                "jamaica": 4,
                "venezuela": 6,
                "muaritania": 3
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these astronomical objects orbits the Earth?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "sun"
            ],
            "lines": [
                [
                    0.29500000000000004,
                    0.6861702127659575,
                    0.42211034075305975,
                    0.28026798016523896,
                    0.32674772036474165,
                    0.08620689655172414,
                    0.27139507620164127,
                    0.26063829787234044,
                    0.40514200711569137,
                    0.4921875,
                    0.3448275862068966,
                    0.3389424976476997,
                    0.4918032786885246,
                    0.3333333333333333,
                    -1.0
                ],
                [
                    0.545,
                    0.30319148936170215,
                    0.37529086926307464,
                    0.4728802127287828,
                    0.2066869300911854,
                    0.2150635208711434,
                    0.16588511137162953,
                    0.20638297872340425,
                    0.3276821862348178,
                    0.09040178571428571,
                    0.2413793103448276,
                    0.3254996673373903,
                    0.06557377049180328,
                    0.0,
                    -1.0
                ],
                [
                    0.16000000000000003,
                    0.010638297872340425,
                    0.2025987899838656,
                    0.2468518071059783,
                    0.46656534954407297,
                    0.6987295825771325,
                    0.5627198124267292,
                    0.5329787234042553,
                    0.26717580664949087,
                    0.4174107142857143,
                    0.41379310344827586,
                    0.33555783501491,
                    0.4426229508196721,
                    0.6666666666666666,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "sun": 0.3874506742713646,
                "milky way": 0.25292270232386044,
                "moon": 0.3596266234047749
            },
            "question": "which of these astronomical objects orbits the earth?",
            "rate_limited": false,
            "answers": [
                "moon",
                "milky way",
                "sun"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sun": 0.6351742441216272,
                "milky way": 0.18299687276582882,
                "moon": 0.6189866885092548
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3557699905907987,
                    1.301998669349561,
                    1.34223134005964
                ],
                "result_count_important_words": [
                    926000.0,
                    566000.0,
                    1920000.0
                ],
                "wikipedia_search": [
                    1.6205680284627655,
                    1.3107287449392713,
                    1.0687032265979635
                ],
                "answer_relation_to_question": [
                    1.18,
                    2.1799999999999997,
                    0.64
                ],
                "result_count": [
                    2150000.0,
                    1360000.0,
                    3070000.0
                ],
                "answer_relation_to_question_bing": [
                    2.74468085106383,
                    1.2127659574468086,
                    0.0425531914893617
                ],
                "cosine_similarity_raw": [
                    0.08714041858911514,
                    0.07747501134872437,
                    0.041824474930763245
                ],
                "result_count_noun_chunks": [
                    2450000.0,
                    1940000.0,
                    5010000.0
                ],
                "question_answer_similarity": [
                    3.1645723432302475,
                    5.3394028171896935,
                    2.7872623950242996
                ],
                "word_count_noun_chunks": [
                    60.0,
                    8.0,
                    54.0
                ],
                "word_count_raw": [
                    13.0,
                    0.0,
                    26.0
                ],
                "result_count_bing": [
                    190000.0,
                    474000.0,
                    1540000.0
                ],
                "word_count_appended": [
                    441.0,
                    81.0,
                    374.0
                ],
                "word_count_appended_bing": [
                    50.0,
                    35.0,
                    60.0
                ]
            },
            "integer_answers": {
                "sun": 6,
                "milky way": 2,
                "moon": 6
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "In which sport can a player encounter the \u201cunplayable lie\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "golf"
            ],
            "question": "in which sport can a player encounter the \u201cunplayable lie\u201d?",
            "lines": [
                [
                    0.4257539682539683,
                    0.5748720975993703,
                    0.6069276253840512,
                    0.5848245196076196,
                    0.4867370589750193,
                    0.49092256391256023,
                    0.8581151057848794,
                    0.9487966096334483,
                    0.6846047566313347,
                    0.7758346581875993,
                    0.46153846153846156,
                    0.5039637988802842,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.2545899470899471,
                    0.19185360094451004,
                    0.16516936266753815,
                    0.2213478156123599,
                    0.49961370074684525,
                    0.48907002593553167,
                    0.08951028258618139,
                    0.01704671241974762,
                    0.21686652955755945,
                    0.12559618441971382,
                    0.23846153846153847,
                    0.30078215895449756,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.31965608465608464,
                    0.23327430145611966,
                    0.22790301194841067,
                    0.19382766478002053,
                    0.013649240278135463,
                    0.020007410151908114,
                    0.052374611628939194,
                    0.03415667794680414,
                    0.09852871381110584,
                    0.0985691573926868,
                    0.3,
                    0.19525404216521816,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "golf",
                "billiards",
                "croquet"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "croquet": 0.010246757127234385,
                "billiards": -0.022243762272295176,
                "golf": 0.9650568595592626
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.5198189944014215,
                    1.5039107947724881,
                    0.976270210826091
                ],
                "result_count_important_words": [
                    116000.0,
                    12100.0,
                    7080.0
                ],
                "wikipedia_search": [
                    3.4230237831566734,
                    1.0843326477877973,
                    0.4926435690555292
                ],
                "answer_relation_to_question": [
                    1.703015873015873,
                    1.0183597883597884,
                    1.2786243386243386
                ],
                "word_count_appended_bing": [
                    60.0,
                    31.0,
                    39.0
                ],
                "answer_relation_to_question_bing": [
                    2.299488390397481,
                    0.7674144037780402,
                    0.9330972058244786
                ],
                "cosine_similarity_raw": [
                    0.15838229656219482,
                    0.04310217872262001,
                    0.05947299301624298
                ],
                "result_count_noun_chunks": [
                    300000.0,
                    5390.0,
                    10800.0
                ],
                "question_answer_similarity": [
                    3.0191935524344444,
                    1.1427220907062292,
                    1.0006475723348558
                ],
                "word_count_noun_chunks": [
                    373.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    2650000.0,
                    2640000.0,
                    108000.0
                ],
                "word_count_raw": [
                    49.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    1890.0,
                    1940.0,
                    53.0
                ],
                "word_count_appended": [
                    488.0,
                    79.0,
                    62.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Appropriately, science-themed \u201cMole Day\u201d occurs in what month?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "october"
            ],
            "question": "appropriately, science-themed \u201cmole day\u201d occurs in what month?",
            "lines": [
                [
                    0.3839638565361659,
                    0.22222828459188126,
                    0.29559937998771996,
                    0.342164539306822,
                    0.3153153153153153,
                    0.321011673151751,
                    0.34134419551934825,
                    0.3473389355742297,
                    0.1267010064478419,
                    0.2799043062200957,
                    0.25663716814159293,
                    0.3199997047050357,
                    0.0,
                    0.3333333333333333,
                    1.0
                ],
                [
                    0.29822707993893055,
                    0.4919794849410738,
                    0.3565282442850053,
                    0.3149441061282392,
                    0.30180180180180183,
                    0.33949416342412453,
                    0.32260692464358454,
                    0.3137254901960784,
                    0.4225437010247137,
                    0.2942583732057416,
                    0.3185840707964602,
                    0.31311125087694125,
                    0.011235955056179775,
                    0.0,
                    1.0
                ],
                [
                    0.31780906352490357,
                    0.28579223046704494,
                    0.34787237572727475,
                    0.3428913545649388,
                    0.38288288288288286,
                    0.33949416342412453,
                    0.3360488798370672,
                    0.3389355742296919,
                    0.45075529252744445,
                    0.4258373205741627,
                    0.4247787610619469,
                    0.36688904441802306,
                    0.9887640449438202,
                    0.6666666666666666,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "july",
                "february",
                "october"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "july": 0.015706595592299845,
                "october": 0.6975681348231264,
                "february": 0.11100097711990284
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.2399979329352497,
                    2.191778756138589,
                    2.5682233109261614
                ],
                "result_count_important_words": [
                    838000.0,
                    792000.0,
                    825000.0
                ],
                "wikipedia_search": [
                    0.5068040257913675,
                    1.6901748040988547,
                    1.8030211701097776
                ],
                "word_count_appended_bing": [
                    29.0,
                    36.0,
                    48.0
                ],
                "answer_relation_to_question_bing": [
                    0.44445656918376253,
                    0.9839589698821476,
                    0.5715844609340899
                ],
                "cosine_similarity_raw": [
                    0.08442221581935883,
                    0.10182330012321472,
                    0.09935121238231659
                ],
                "result_count_noun_chunks": [
                    124000.0,
                    112000.0,
                    121000.0
                ],
                "question_answer_similarity": [
                    2.0873295377241448,
                    1.921274883672595,
                    2.091763378120959
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    88.0
                ],
                "result_count_bing": [
                    330000.0,
                    349000.0,
                    349000.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    2.0
                ],
                "result_count": [
                    70.0,
                    67.0,
                    85.0
                ],
                "answer_relation_to_question": [
                    1.1518915696084977,
                    0.8946812398167916,
                    0.9534271905747107
                ],
                "word_count_appended": [
                    117.0,
                    123.0,
                    178.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "U.S. air traffic controllers refer to the plane the Pope travels on by what official call sign?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "shepherd one"
            ],
            "question": "u.s. air traffic controllers refer to the plane the pope travels on by what official call sign?",
            "lines": [
                [
                    0.4120315066387058,
                    0.4719298245614035,
                    0.5505747516120139,
                    0.4673147262088004,
                    0.5714285714285714,
                    0.2956721562619686,
                    0.4268209666439755,
                    0.5882352941176471,
                    0.82539183823607,
                    0.6153846153846154,
                    0.5,
                    0.4672432301569411,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.15998812357481004,
                    0.254093567251462,
                    0.023358993192037464,
                    0.18758266992472503,
                    0.3673469387755102,
                    0.34354653389505935,
                    0.5677331518039482,
                    0.35294117647058826,
                    0.002012072434607646,
                    0.2692307692307692,
                    0.25,
                    0.32782184294295236,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.42798036978648424,
                    0.2739766081871345,
                    0.4260662551959487,
                    0.3451026038664746,
                    0.061224489795918366,
                    0.360781309842972,
                    0.005445881552076242,
                    0.058823529411764705,
                    0.17259608932932233,
                    0.11538461538461539,
                    0.25,
                    0.20493492690010665,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "shepherd one",
                "il papa",
                "air vatican"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "il papa": 0.0814004469390863,
                "shepherd one": 0.941131769795345,
                "air vatican": 0.27821186597535835
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    4.205189071412469,
                    2.9503965864865704,
                    1.8444143421009596
                ],
                "result_count_important_words": [
                    627.0,
                    834.0,
                    8.0
                ],
                "wikipedia_search": [
                    5.77774286765249,
                    0.014084507042253521,
                    1.2081726253052563
                ],
                "word_count_appended_bing": [
                    4.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    2.831578947368421,
                    1.5245614035087718,
                    1.643859649122807
                ],
                "cosine_similarity_raw": [
                    0.16107751429080963,
                    0.006833964958786964,
                    0.1246509999036789
                ],
                "result_count_noun_chunks": [
                    30.0,
                    18.0,
                    3.0
                ],
                "question_answer_similarity": [
                    10.149813587311655,
                    4.074190315790474,
                    7.495434877811931
                ],
                "word_count_noun_chunks": [
                    9.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    77200.0,
                    89700.0,
                    94200.0
                ],
                "word_count_raw": [
                    8.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    28.0,
                    18.0,
                    3.0
                ],
                "answer_relation_to_question": [
                    3.2962520531096464,
                    1.2799049885984803,
                    3.423842958291874
                ],
                "word_count_appended": [
                    16.0,
                    7.0,
                    3.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which human sense is most closely associated with the bony labyrinth?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "hearing"
            ],
            "lines": [
                [
                    0.4443371943371943,
                    0.2303921568627451,
                    0.13707029914483904,
                    0.3330204235621606,
                    0.4420459877991553,
                    0.18975332068311196,
                    0.34513274336283184,
                    0.3004694835680751,
                    0.3904761904761905,
                    0.30383480825958703,
                    0.35135135135135137,
                    0.33482126809050206,
                    0.4,
                    0.0,
                    -1.0
                ],
                [
                    0.3844916344916345,
                    0.6813725490196079,
                    0.6765664652398113,
                    0.3099602330094307,
                    0.44533083059596434,
                    0.6204933586337761,
                    0.3303834808259587,
                    0.37715179968701096,
                    0.5,
                    0.40707964601769914,
                    0.3783783783783784,
                    0.33063610415274153,
                    0.28888888888888886,
                    1.0,
                    -1.0
                ],
                [
                    0.17117117117117117,
                    0.08823529411764706,
                    0.1863632356153496,
                    0.3570193434284087,
                    0.11262318160488034,
                    0.18975332068311196,
                    0.32448377581120946,
                    0.3223787167449139,
                    0.10952380952380951,
                    0.2890855457227139,
                    0.2702702702702703,
                    0.3345426277567564,
                    0.3111111111111111,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "touch": 0.30019323053555313,
                "sight": 0.21904010025438236,
                "hearing": 0.48076666921006445
            },
            "question": "which human sense is most closely associated with the bony labyrinth?",
            "rate_limited": false,
            "answers": [
                "touch",
                "hearing",
                "sight"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "touch": 0.11523992049450761,
                "sight": 0.05328575333416943,
                "hearing": 0.8638491898393257
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.0089276085430123,
                    1.9838166249164493,
                    2.0072557665405384
                ],
                "result_count_important_words": [
                    234000.0,
                    224000.0,
                    220000.0
                ],
                "wikipedia_search": [
                    1.9523809523809523,
                    2.5,
                    0.5476190476190476
                ],
                "word_count_appended_bing": [
                    39.0,
                    42.0,
                    30.0
                ],
                "answer_relation_to_question_bing": [
                    0.4607843137254902,
                    1.3627450980392157,
                    0.17647058823529413
                ],
                "cosine_similarity_raw": [
                    0.03414810448884964,
                    0.16855192184448242,
                    0.0464283749461174
                ],
                "result_count_noun_chunks": [
                    192000000.0,
                    241000000.0,
                    206000000.0
                ],
                "question_answer_similarity": [
                    3.739047773182392,
                    3.4801352620124817,
                    4.008500039577484
                ],
                "word_count_noun_chunks": [
                    36.0,
                    26.0,
                    28.0
                ],
                "word_count_raw": [
                    0.0,
                    6.0,
                    0.0
                ],
                "result_count_bing": [
                    30000.0,
                    98100.0,
                    30000.0
                ],
                "word_count_appended": [
                    206.0,
                    276.0,
                    196.0
                ],
                "answer_relation_to_question": [
                    1.333011583011583,
                    1.1534749034749034,
                    0.5135135135135135
                ],
                "result_count": [
                    94200.0,
                    94900.0,
                    24000.0
                ]
            },
            "integer_answers": {
                "touch": 4,
                "sight": 1,
                "hearing": 9
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which TV comedy centers on a vice president?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "veep"
            ],
            "lines": [
                [
                    0.14932600732600732,
                    0.2682685334969599,
                    0.0560686828040383,
                    0.6525233818279421,
                    0.055091054861986025,
                    0.09076227390180878,
                    0.06997863247863248,
                    0.06993795826283136,
                    0.31792355371900827,
                    0.0791974656810982,
                    0.08396946564885496,
                    0.32383877163397756,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.16888278388278388,
                    0.19097590227031852,
                    0.04037540835477659,
                    0.17565075979398295,
                    0.4346581147634864,
                    0.8107235142118863,
                    0.390491452991453,
                    0.39593908629441626,
                    0.02706611570247934,
                    0.2576557550158395,
                    0.3053435114503817,
                    0.29775496033904914,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.6817912087912088,
                    0.5407555642327215,
                    0.9035559088411851,
                    0.17182585837807487,
                    0.5102508303745276,
                    0.09851421188630491,
                    0.5395299145299145,
                    0.5341229554427523,
                    0.6550103305785124,
                    0.6631467793030623,
                    0.6106870229007634,
                    0.3784062680269733,
                    1.0,
                    1.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "young sheldon": 0.15834898440308182,
                "veep": 0.5919712038061429,
                "superstore": 0.24967981179077525
            },
            "question": "which tv comedy centers on a vice president?",
            "rate_limited": false,
            "answers": [
                "young sheldon",
                "superstore",
                "veep"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "young sheldon": 0.14270027711526276,
                "veep": 0.8616135232371673,
                "superstore": 0.14364367220201885
            },
            "integer_answers": {
                "young sheldon": 1,
                "veep": 12,
                "superstore": 1
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6191938581698877,
                    1.4887748016952458,
                    1.8920313401348665
                ],
                "result_count_important_words": [
                    13100.0,
                    73100.0,
                    101000.0
                ],
                "wikipedia_search": [
                    1.271694214876033,
                    0.10826446280991736,
                    2.6200413223140497
                ],
                "word_count_appended_bing": [
                    11.0,
                    40.0,
                    80.0
                ],
                "answer_relation_to_question_bing": [
                    1.0730741339878396,
                    0.7639036090812741,
                    2.163022256930886
                ],
                "cosine_similarity_raw": [
                    0.023849716410040855,
                    0.017174329608678818,
                    0.3843420445919037
                ],
                "result_count_noun_chunks": [
                    12400.0,
                    70200.0,
                    94700.0
                ],
                "question_answer_similarity": [
                    2.3016564340214245,
                    0.6195758078247309,
                    0.6060841702856123
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    22.0
                ],
                "result_count_bing": [
                    2810000.0,
                    25100000.0,
                    3050000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    9.0
                ],
                "word_count_appended": [
                    75.0,
                    244.0,
                    628.0
                ],
                "answer_relation_to_question": [
                    0.7466300366300366,
                    0.8444139194139194,
                    3.408956043956044
                ],
                "result_count": [
                    9620.0,
                    75900.0,
                    89100.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Set to dance at the Apollo Theater Amateur Night, who decided to sing at the last minute?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ella fitzgerald",
                "top"
            ],
            "question": "set to dance at the apollo theater amateur night, who decided to sing at the last minute?",
            "lines": [
                [
                    0.47126393332238287,
                    0.15828625235404897,
                    0.6790176562980585,
                    0.04666289770151539,
                    0.43831475533723785,
                    0.3226837060702875,
                    0.2229420731707317,
                    0.481919283765825,
                    0.5656122545979639,
                    0.7096774193548387,
                    0.7058823529411765,
                    0.3469288301796813,
                    1.0,
                    0.8333333333333334,
                    0.0
                ],
                [
                    0.21799124600147998,
                    0.1415859564164649,
                    0.07426265479069935,
                    0.240471863071024,
                    0.03174003400717929,
                    0.3635782747603834,
                    0.026295731707317072,
                    0.036161432468350005,
                    0.09394456188635344,
                    0.0967741935483871,
                    0.11764705882352941,
                    0.2973936338566733,
                    0.0,
                    0.0,
                    0.0
                ],
                [
                    0.3107448206761371,
                    0.7001277912294861,
                    0.2467196889112421,
                    0.7128652392274606,
                    0.5299452106555829,
                    0.3137380191693291,
                    0.7507621951219512,
                    0.481919283765825,
                    0.34044318351568265,
                    0.1935483870967742,
                    0.17647058823529413,
                    0.3556775359636454,
                    0.0,
                    0.16666666666666666,
                    0.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "ella fitzgerald",
                "etta james",
                "billie holiday"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "etta james": 0.01957625569092834,
                "billie holiday": 0.22232681479836328,
                "ella fitzgerald": 0.714816823840876
            },
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.1223594716171315,
                    2.6765427047100596,
                    3.2010978236728085
                ],
                "result_count_important_words": [
                    585.0,
                    69.0,
                    1970.0
                ],
                "wikipedia_search": [
                    4.5248980367837115,
                    0.7515564950908276,
                    2.723545468125461
                ],
                "word_count_appended_bing": [
                    12.0,
                    2.0,
                    3.0
                ],
                "answer_relation_to_question_bing": [
                    0.6331450094161959,
                    0.5663438256658596,
                    2.8005111649179444
                ],
                "cosine_similarity_raw": [
                    0.259463369846344,
                    0.028376933187246323,
                    0.09427548944950104
                ],
                "result_count_noun_chunks": [
                    6890.0,
                    517.0,
                    6890.0
                ],
                "question_answer_similarity": [
                    0.37092661487986334,
                    1.9115275419317186,
                    5.666615299880505
                ],
                "word_count_noun_chunks": [
                    17.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    5.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    505000.0,
                    569000.0,
                    491000.0
                ],
                "word_count_appended": [
                    88.0,
                    12.0,
                    24.0
                ],
                "answer_relation_to_question": [
                    3.29884753325668,
                    1.52593872201036,
                    2.17521374473296
                ],
                "result_count": [
                    4640.0,
                    336.0,
                    5610.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What is the total cost of all the vowels on \u201cWheel of Fortune\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "$2,500"
            ],
            "lines": [
                [
                    0.26452991452991453,
                    0.2777777777777778,
                    0.2011574129089494,
                    0.29829337871079203,
                    0.30714285714285716,
                    0.3333333333333333,
                    0.26151560178306094,
                    0.08942675159235669,
                    0.0,
                    0.325,
                    0.35294117647058826,
                    0.347989934430612,
                    0,
                    0,
                    1.0
                ],
                [
                    0.5256410256410257,
                    0.2777777777777778,
                    0.5589035217223928,
                    0.4713793088949262,
                    0.40714285714285714,
                    0.3333333333333333,
                    0.6322436849925706,
                    0.8738853503184714,
                    1.0,
                    0.35,
                    0.35294117647058826,
                    0.4273002506625877,
                    0,
                    0,
                    1.0
                ],
                [
                    0.20982905982905986,
                    0.4444444444444444,
                    0.23993906536865783,
                    0.2303273123942818,
                    0.2857142857142857,
                    0.3333333333333333,
                    0.1062407132243685,
                    0.03668789808917197,
                    0.0,
                    0.325,
                    0.29411764705882354,
                    0.22470981490680025,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "$2,500": 0.5175456905797109,
                "$1,250": 0.25492567822335344,
                "$2,900": 0.22752863119693556
            },
            "question": "what is the total cost of all the vowels on \u201cwheel of fortune\u201d?",
            "rate_limited": false,
            "answers": [
                "$1,250",
                "$2,500",
                "$2,900"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "$2,500": 0.7879067029384634,
                "$1,250": 0.15575988041489416,
                "$2,900": 0.26352356426179974
            },
            "integer_answers": {
                "$2,500": 9,
                "$1,250": 2,
                "$2,900": 1
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.73994967215306,
                    2.1365012533129386,
                    1.1235490745340013
                ],
                "result_count_important_words": [
                    3520.0,
                    8510.0,
                    1430.0
                ],
                "wikipedia_search": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    6.0,
                    6.0,
                    5.0
                ],
                "answer_relation_to_question_bing": [
                    0.5555555555555556,
                    0.5555555555555556,
                    0.8888888888888888
                ],
                "cosine_similarity_raw": [
                    0.044646892696619034,
                    0.12404865026473999,
                    0.05325448140501976
                ],
                "result_count_noun_chunks": [
                    3510.0,
                    34300.0,
                    1440.0
                ],
                "question_answer_similarity": [
                    2.751216939795995,
                    4.34762161099934,
                    2.1243528981285635
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    363000.0,
                    363000.0,
                    363000.0
                ],
                "result_count": [
                    43.0,
                    57.0,
                    40.0
                ],
                "answer_relation_to_question": [
                    1.0581196581196581,
                    2.1025641025641026,
                    0.8393162393162394
                ],
                "word_count_appended": [
                    13.0,
                    14.0,
                    13.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What iconic painting once hung in Napoleon's bedroom?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "mona lisa"
            ],
            "lines": [
                [
                    0.358128078817734,
                    0.21428571428571427,
                    0.12707721482301051,
                    0.4054201490299891,
                    0.7073030477285797,
                    0.5577689243027888,
                    0.7334785766158315,
                    0.5553218342448465,
                    0.7308965102286402,
                    0.13569321533923304,
                    0.056338028169014086,
                    0.27612363787901995,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.1022167487684729,
                    0.1326530612244898,
                    0.8152481720274101,
                    0.10499486390772106,
                    0.05462909718228867,
                    0.26145418326693226,
                    0.10675381263616558,
                    0.12536811106436685,
                    0.20532490974729242,
                    0.6430678466076696,
                    0.7605633802816901,
                    0.4057631836310163,
                    0.9565217391304348,
                    1.0,
                    1.0
                ],
                [
                    0.539655172413793,
                    0.653061224489796,
                    0.05767461314957937,
                    0.48958498706228987,
                    0.2380678550891317,
                    0.1807768924302789,
                    0.15976761074800291,
                    0.3193100546907867,
                    0.06377858002406739,
                    0.22123893805309736,
                    0.18309859154929578,
                    0.31811317848996373,
                    0.043478260869565216,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "the birth of venus": 0.247686139932832,
                "mona lisa": 0.40532565067685355,
                "the starry night": 0.3469882093903144
            },
            "question": "what iconic painting once hung in napoleon's bedroom?",
            "rate_limited": false,
            "answers": [
                "the starry night",
                "mona lisa",
                "the birth of venus"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "the birth of venus": 0.20888592458796984,
                "mona lisa": 0.7906221929744528,
                "the starry night": 0.096086251899523
            },
            "integer_answers": {
                "the birth of venus": 3,
                "mona lisa": 6,
                "the starry night": 5
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3806181893950997,
                    2.0288159181550816,
                    1.5905658924498187
                ],
                "result_count_important_words": [
                    1010000.0,
                    147000.0,
                    220000.0
                ],
                "wikipedia_search": [
                    2.1926895306859207,
                    0.6159747292418772,
                    0.19133574007220217
                ],
                "word_count_appended_bing": [
                    4.0,
                    54.0,
                    13.0
                ],
                "answer_relation_to_question_bing": [
                    0.42857142857142855,
                    0.2653061224489796,
                    1.306122448979592
                ],
                "cosine_similarity_raw": [
                    0.06755483895540237,
                    0.43338972330093384,
                    0.03066009283065796
                ],
                "result_count_noun_chunks": [
                    1320000.0,
                    298000.0,
                    759000.0
                ],
                "question_answer_similarity": [
                    8.063914388883859,
                    2.0883757155388594,
                    9.737975360127166
                ],
                "word_count_noun_chunks": [
                    0.0,
                    22.0,
                    1.0
                ],
                "result_count_bing": [
                    112000.0,
                    52500.0,
                    36300.0
                ],
                "word_count_raw": [
                    0.0,
                    15.0,
                    0.0
                ],
                "result_count": [
                    2460000.0,
                    190000.0,
                    828000.0
                ],
                "answer_relation_to_question": [
                    1.79064039408867,
                    0.5110837438423645,
                    2.6982758620689653
                ],
                "word_count_appended": [
                    46.0,
                    218.0,
                    75.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these is NOT among the four \u201cC\u2019s\u201d of diamond buying?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "core"
            ],
            "lines": [
                [
                    0.3316157525838369,
                    0.33982247327012305,
                    0.17434280047174477,
                    0.35298536436153694,
                    0.17524841915085815,
                    0.33156911581569115,
                    0.4631233203941475,
                    0.3084763948497854,
                    0.15657439199123635,
                    0.27001127395715896,
                    0.1942307692307692,
                    0.3268599271732113,
                    0.2053941908713693,
                    0.06837606837606836,
                    -1.0
                ],
                [
                    0.2693957754450962,
                    0.3099657050635465,
                    0.36882906581024966,
                    0.3104077788750854,
                    0.4083107497741644,
                    0.3334371108343711,
                    0.15362794864138551,
                    0.30686695278969955,
                    0.45210846887589246,
                    0.31454340473506204,
                    0.4096153846153846,
                    0.32140668087293695,
                    0.2946058091286307,
                    0.43162393162393164,
                    -1.0
                ],
                [
                    0.3989884719710669,
                    0.35021182166633047,
                    0.45682813371800557,
                    0.3366068567633777,
                    0.4164408310749774,
                    0.33499377334993774,
                    0.383248730964467,
                    0.38465665236051505,
                    0.39131713913287125,
                    0.415445321307779,
                    0.39615384615384613,
                    0.35173339195385184,
                    0.5,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "color": 0.4716242482146375,
                "core": 0.19762500422613916,
                "cut": 0.3307507475592233
            },
            "question": "which of these is not among the four \u201cc\u2019s\u201d of diamond buying?",
            "rate_limited": false,
            "answers": [
                "color",
                "cut",
                "core"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "color": 0.15739648008548568,
                "core": 0.587405225712829,
                "cut": 0.18696114317489043
            },
            "integer_answers": {
                "color": 8,
                "core": 0,
                "cut": 6
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.0388404369607325,
                    1.0715599147623784,
                    0.8895996482768893
                ],
                "result_count_important_words": [
                    2470000.0,
                    23200000.0,
                    7820000.0
                ],
                "wikipedia_search": [
                    1.3737024320350546,
                    0.19156612449643015,
                    0.4347314434685151
                ],
                "word_count_appended_bing": [
                    159.0,
                    47.0,
                    54.0
                ],
                "answer_relation_to_question_bing": [
                    0.3203550534597539,
                    0.380068589872907,
                    0.2995763566673391
                ],
                "cosine_similarity_raw": [
                    0.21935856342315674,
                    0.08835507929325104,
                    0.029080022126436234
                ],
                "result_count_noun_chunks": [
                    3570000.0,
                    3600000.0,
                    2150000.0
                ],
                "question_answer_similarity": [
                    3.8351904675364494,
                    4.945917636156082,
                    4.26245878636837
                ],
                "word_count_noun_chunks": [
                    142.0,
                    99.0,
                    0.0
                ],
                "result_count_bing": [
                    54100000.0,
                    53500000.0,
                    53000000.0
                ],
                "word_count_raw": [
                    101.0,
                    16.0,
                    0.0
                ],
                "word_count_appended": [
                    408.0,
                    329.0,
                    150.0
                ],
                "answer_relation_to_question": [
                    1.0103054844969788,
                    1.3836253473294227,
                    0.6060691681735986
                ],
                "result_count": [
                    719000.0,
                    203000.0,
                    185000.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What is the correct pronunciation of the performer who sings \u201cSmooth Operator\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "shah-day"
            ],
            "lines": [
                [
                    0.8888888888888888,
                    0.3333333333333333,
                    0.7603270936853664,
                    0.44677802128757055,
                    0.8085106382978723,
                    0.24945837202104612,
                    0.639344262295082,
                    0.45259461568474446,
                    0.0,
                    0.7608695652173914,
                    0.34146341463414637,
                    0.7063092510146416,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.0,
                    0.3333333333333333,
                    0.14261963323269905,
                    -0.02854204455048535,
                    0.13829787234042554,
                    0.5478180129990715,
                    0.22950819672131148,
                    0.0031213421771361686,
                    0.0,
                    0.125,
                    0.32926829268292684,
                    0.1632074105768559,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.1111111111111111,
                    0.3333333333333333,
                    0.09705327308193447,
                    0.5817640232629148,
                    0.05319148936170213,
                    0.2027236149798824,
                    0.13114754098360656,
                    0.5442840421381194,
                    1.0,
                    0.11413043478260869,
                    0.32926829268292684,
                    0.1304833384085024,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "sayd": 0.14168800353666244,
                "shah-day": 0.5991341040257202,
                "say-dee": 0.2591778924376173
            },
            "question": "what is the correct pronunciation of the performer who sings \u201csmooth operator\u201d?",
            "rate_limited": false,
            "answers": [
                "shah-day",
                "sayd",
                "say-dee"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sayd": 0.09523895959228616,
                "shah-day": 0.7906221929744528,
                "say-dee": 0.08195424085485645
            },
            "integer_answers": {
                "sayd": 1,
                "shah-day": 10,
                "say-dee": 3
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    4.23785550608785,
                    0.9792444634611356,
                    0.7829000304510145
                ],
                "result_count_important_words": [
                    39.0,
                    14.0,
                    8.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_appended_bing": [
                    28.0,
                    27.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "cosine_similarity_raw": [
                    0.13098712265491486,
                    0.024570129811763763,
                    0.016720078885555267
                ],
                "result_count_noun_chunks": [
                    2320.0,
                    16.0,
                    2790.0
                ],
                "question_answer_similarity": [
                    6.507756527513266,
                    -0.41574264597147703,
                    8.473958967253566
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    80600.0,
                    177000.0,
                    65500.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    140.0,
                    23.0,
                    21.0
                ],
                "answer_relation_to_question": [
                    2.6666666666666665,
                    0.0,
                    0.3333333333333333
                ],
                "result_count": [
                    76.0,
                    13.0,
                    5.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these companies went public first?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "facebook"
            ],
            "lines": [
                [
                    0.5333333333333333,
                    0.0,
                    0.5020786388215926,
                    1.1639659299124,
                    0.9722467739084321,
                    0.4289013526888816,
                    0.985604205244609,
                    0.9875756610385473,
                    0.5581804281345565,
                    0.2953281423804227,
                    0.22815533980582525,
                    0.35743442882295096,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.23333333333333334,
                    0.0,
                    0.1486442693495346,
                    0.3346810499462215,
                    0.016174650875022095,
                    0.17518970636753547,
                    0.007645899289170301,
                    0.007327174259318254,
                    0.12904106596767148,
                    0.3364849833147942,
                    0.3932038834951456,
                    0.30708240727986746,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.23333333333333334,
                    1.0,
                    0.34927709182887273,
                    -0.4986469798586215,
                    0.011578575216545872,
                    0.395908940943583,
                    0.0067498954662206555,
                    0.005097164702134438,
                    0.31277850589777195,
                    0.3681868743047831,
                    0.3786407766990291,
                    0.3354831638971815,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "ferrari": 0.14920060167697247,
                "facebook": 0.6437717310065395,
                "alibaba": 0.20702766731648814
            },
            "question": "which of these companies went public first?",
            "rate_limited": false,
            "answers": [
                "facebook",
                "ferrari",
                "alibaba"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "ferrari": 0.04982586533813474,
                "facebook": 0.816051231526463,
                "alibaba": 0.14102925790457016
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.0723032864688529,
                    0.9212472218396024,
                    1.0064494916915445
                ],
                "result_count_important_words": [
                    330000000.0,
                    2560000.0,
                    2260000.0
                ],
                "wikipedia_search": [
                    1.6745412844036696,
                    0.3871231979030144,
                    0.9383355176933159
                ],
                "word_count_appended_bing": [
                    47.0,
                    81.0,
                    78.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "cosine_similarity_raw": [
                    0.0997619479894638,
                    0.02953529730439186,
                    0.0694006085395813
                ],
                "result_count_noun_chunks": [
                    403000000.0,
                    2990000.0,
                    2080000.0
                ],
                "question_answer_similarity": [
                    1.5726340487599373,
                    0.4521874748170376,
                    -0.6737217977643013
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    130000000.0,
                    53100000.0,
                    120000000.0
                ],
                "word_count_appended": [
                    531.0,
                    605.0,
                    662.0
                ],
                "answer_relation_to_question": [
                    1.0666666666666667,
                    0.4666666666666667,
                    0.4666666666666667
                ],
                "result_count": [
                    330000000.0,
                    5490000.0,
                    3930000.0
                ]
            },
            "integer_answers": {
                "ferrari": 1,
                "facebook": 11,
                "alibaba": 2
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What pop star first recorded under the same name as the so-called \"sailor's devil\"?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "david bowie"
            ],
            "question": "what pop star first recorded under the same name as the so-called \"sailor's devil\"?",
            "lines": [
                [
                    0.4969799968745116,
                    0.20177320177320177,
                    0.41603447320170817,
                    0.34383292456690917,
                    0.7804878048780488,
                    0.312823461759632,
                    0.48892015476609213,
                    0.518918918918919,
                    0.3343506573192969,
                    0.25925925925925924,
                    0.43243243243243246,
                    0.34761747888913624,
                    0.3333333333333333,
                    0,
                    1.0
                ],
                [
                    0.3037701203313018,
                    0.2604747604747605,
                    0.2790379203994883,
                    0.35829794056607317,
                    0.13414634146341464,
                    0.1144335825186889,
                    0.13119943721421035,
                    0.12072072072072072,
                    0.5229788705920422,
                    0.4567901234567901,
                    0.3153153153153153,
                    0.3402923557407232,
                    0.6666666666666666,
                    0,
                    1.0
                ],
                [
                    0.1992498827941866,
                    0.5377520377520377,
                    0.3049276063988035,
                    0.2978691348670176,
                    0.08536585365853659,
                    0.5727429557216791,
                    0.3798804080196975,
                    0.36036036036036034,
                    0.1426704720886609,
                    0.2839506172839506,
                    0.25225225225225223,
                    0.3120901653701405,
                    0.0,
                    0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "elton john",
                "david bowie",
                "freddie mercury"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "david bowie": 0.37473115734230533,
                "elton john": 0.37118977006639925,
                "freddie mercury": 0.04159454941235125
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.0857048733348176,
                    2.0417541344443393,
                    1.8725409922208431
                ],
                "result_count_important_words": [
                    139000.0,
                    37300.0,
                    108000.0
                ],
                "wikipedia_search": [
                    2.0061039439157815,
                    3.137873223552253,
                    0.8560228325319654
                ],
                "word_count_appended_bing": [
                    48.0,
                    35.0,
                    28.0
                ],
                "answer_relation_to_question_bing": [
                    0.6053196053196053,
                    0.7814242814242814,
                    1.6132561132561132
                ],
                "cosine_similarity_raw": [
                    0.024059949442744255,
                    0.016137216240167618,
                    0.01763445883989334
                ],
                "result_count_noun_chunks": [
                    288000.0,
                    67000.0,
                    200000.0
                ],
                "question_answer_similarity": [
                    3.7604308486916125,
                    3.9186317902058363,
                    3.257734217972029
                ],
                "word_count_noun_chunks": [
                    1.0,
                    2.0,
                    0.0
                ],
                "result_count_bing": [
                    272000.0,
                    99500.0,
                    498000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    128000.0,
                    22000.0,
                    14000.0
                ],
                "answer_relation_to_question": [
                    2.484899984372558,
                    1.5188506016565089,
                    0.996249413970933
                ],
                "word_count_appended": [
                    42.0,
                    74.0,
                    46.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which Jim Carrey movie was released the same year Netflix was founded?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "the mask",
                "bottom"
            ],
            "question": "which jim carrey movie was released the same year netflix was founded?",
            "lines": [
                [
                    0.4103143898229072,
                    0.2777430793827796,
                    0.17666804098622263,
                    0.27289680006829004,
                    0.10416666666666667,
                    0.21284403669724772,
                    0.1378968253968254,
                    0.4685328568592416,
                    0.5916666666666667,
                    0.3,
                    0.08108108108108109,
                    0.3149969578820065,
                    0.3333333333333333,
                    0.0,
                    -1.0
                ],
                [
                    0.24309322034105188,
                    0.41433889579878386,
                    0.2532247332318861,
                    0.2524803031817099,
                    0.10714285714285714,
                    0.2,
                    0.14781746031746032,
                    0.5082390311693469,
                    0.125,
                    0.23333333333333334,
                    0.3918918918918919,
                    0.33225348334950217,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.34659238983604085,
                    0.3079180248184365,
                    0.5701072257818912,
                    0.4746228967500001,
                    0.7886904761904762,
                    0.5871559633027523,
                    0.7142857142857143,
                    0.023228111971411555,
                    0.2833333333333333,
                    0.4666666666666667,
                    0.527027027027027,
                    0.35274955876849134,
                    0.6666666666666666,
                    1.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "bruce almighty",
                "liar liar",
                "the mask"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "liar liar": 0.14189665056179518,
                "bruce almighty": 0.17908722526404994,
                "the mask": 0.6220664105979448
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.2049787051740455,
                    2.325774383446515,
                    2.4692469113794395
                ],
                "result_count_important_words": [
                    139000.0,
                    149000.0,
                    720000.0
                ],
                "wikipedia_search": [
                    2.9583333333333335,
                    0.625,
                    1.4166666666666665
                ],
                "word_count_appended_bing": [
                    6.0,
                    29.0,
                    39.0
                ],
                "answer_relation_to_question_bing": [
                    1.6664584762966774,
                    2.486033374792703,
                    1.847508148910619
                ],
                "cosine_similarity_raw": [
                    0.02987757697701454,
                    0.04282461851835251,
                    0.09641484916210175
                ],
                "result_count_noun_chunks": [
                    2360000.0,
                    2560000.0,
                    117000.0
                ],
                "question_answer_similarity": [
                    3.993300465750508,
                    3.69454574782867,
                    6.945159614086151
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    2.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    116000.0,
                    109000.0,
                    320000.0
                ],
                "result_count": [
                    140000.0,
                    144000.0,
                    1060000.0
                ],
                "answer_relation_to_question": [
                    2.4618863389374432,
                    1.4585593220463113,
                    2.079554339016245
                ],
                "word_count_appended": [
                    54.0,
                    42.0,
                    84.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "The boys who inspired \u201cPeter Pan\u201d were cousins of the author of what novel?": {
        "raw_data": {
            "data": {
                "word_count_appended_relation_to_question": [
                    3.476682675401592,
                    1.865612506415157,
                    1.6577048181832514
                ],
                "result_count_important_words": [
                    276000.0,
                    76700.0,
                    79.0
                ],
                "wikipedia_search": [
                    1.6812698412698412,
                    0.9915873015873016,
                    2.3271428571428574
                ],
                "answer_relation_to_question": [
                    1.945223986606976,
                    1.8054222752985796,
                    1.2493537380944444
                ],
                "answer_relation_to_question_bing": [
                    1.05595646039105,
                    0.9174561580326547,
                    1.0265873815762951
                ],
                "word_count_appended": [
                    225.0,
                    22.0,
                    16.0
                ],
                "cosine_similarity_raw": [
                    0.02534085139632225,
                    0.029216133058071136,
                    0.04024072736501694
                ],
                "result_count_noun_chunks": [
                    315000.0,
                    268000.0,
                    2080000.0
                ],
                "question_answer_similarity": [
                    1.3894918970763683,
                    15.520058428257471,
                    28.3397333920002
                ],
                "word_count_noun_chunks": [
                    7.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    145000.0,
                    25500.0,
                    29800.0
                ],
                "result_count": [
                    227000.0,
                    53.0,
                    47.0
                ],
                "word_count_appended_bing": [
                    37.0,
                    2.0,
                    4.0
                ]
            },
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "z-best_answer_by_ml": [
                "rebecca",
                "top"
            ],
            "lines": [
                [
                    0.3890447973213952,
                    0.3519854867970167,
                    0.2673150111955928,
                    0.030707489333009784,
                    0.9995596653456627,
                    0.7239141288067898,
                    0.7823594941875792,
                    0.1182876455125798,
                    0.33625396825396825,
                    0.8555133079847909,
                    0.8604651162790697,
                    0.4966689536287988,
                    1.0,
                    0,
                    1.0
                ],
                [
                    0.3610844550597159,
                    0.3058187193442182,
                    0.30819449644236013,
                    0.3429901459923457,
                    0.00023337736679876706,
                    0.127309036445332,
                    0.21741656958038885,
                    0.10063837776943298,
                    0.1983174603174603,
                    0.08365019011406843,
                    0.046511627906976744,
                    0.26651607234502245,
                    0.0,
                    0,
                    1.0
                ],
                [
                    0.2498707476188889,
                    0.34219579385876503,
                    0.42449049236204706,
                    0.6263023646746445,
                    0.00020695728753852927,
                    0.14877683474787817,
                    0.00022393623203195202,
                    0.7810739767179873,
                    0.46542857142857147,
                    0.060836501901140684,
                    0.09302325581395349,
                    0.23681497402617877,
                    0.0,
                    0,
                    1.0
                ]
            ],
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "question": "the boys who inspired \u201cpeter pan\u201d were cousins of the author of what novel?",
            "rate_limited": false,
            "answers": [
                "rebecca",
                "to the lighthouse",
                "the portrait of a lady"
            ],
            "ml_answers": {
                "to the lighthouse": 0.10369322931410824,
                "rebecca": 0.7108528107453532,
                "the portrait of a lady": 0.15731824591387528
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Lonnie Johnson accidentally created the Supersoaker while trying to build what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "heat pump"
            ],
            "question": "lonnie johnson accidentally created the supersoaker while trying to build what?",
            "lines": [
                [
                    0.2125,
                    0.2916666666666667,
                    0.6049672397694191,
                    0.3596845519671524,
                    1.0,
                    0.9616969830191789,
                    0.07061340941512126,
                    0.15594059405940594,
                    0.8125,
                    0.9076923076923077,
                    0.6923076923076923,
                    0.7447199153990359,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.2125,
                    0.16666666666666669,
                    0.28781095274011737,
                    0.37873800767123034,
                    0.0,
                    0.002253118645930648,
                    0.05206847360912981,
                    0.3935643564356436,
                    0.03125,
                    0.046153846153846156,
                    0.15384615384615385,
                    0.12764004230048204,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.575,
                    0.5416666666666667,
                    0.10722180749046349,
                    0.2615774403616173,
                    0.0,
                    0.03604989833489037,
                    0.8773181169757489,
                    0.4504950495049505,
                    0.15625,
                    0.046153846153846156,
                    0.15384615384615385,
                    0.12764004230048204,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "heat pump",
                "nerf blaster rival",
                "efficient sprinklers"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "efficient sprinklers": -0.027344093175014284,
                "heat pump": 0.8790527978385551,
                "nerf blaster rival": 0.04540248223981307
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    5.213039407793252,
                    0.8934802961033743,
                    0.8934802961033743
                ],
                "result_count_important_words": [
                    99.0,
                    73.0,
                    1230.0
                ],
                "wikipedia_search": [
                    0.8125,
                    0.03125,
                    0.15625
                ],
                "word_count_appended_bing": [
                    9.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.5833333333333333,
                    0.3333333333333333,
                    1.0833333333333333
                ],
                "cosine_similarity_raw": [
                    0.13120998442173004,
                    0.06242267042398453,
                    0.02325509674847126
                ],
                "result_count_noun_chunks": [
                    1260.0,
                    3180.0,
                    3640.0
                ],
                "question_answer_similarity": [
                    4.473501366563141,
                    4.710474735766184,
                    3.2533146906644106
                ],
                "word_count_noun_chunks": [
                    4.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    17500.0,
                    41.0,
                    656.0
                ],
                "word_count_raw": [
                    4.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    332.0,
                    0,
                    0
                ],
                "answer_relation_to_question": [
                    0.85,
                    0.85,
                    2.3
                ],
                "word_count_appended": [
                    59.0,
                    3.0,
                    3.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which artist painted the ceiling of one of France's most iconic opera houses?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "marc chagall"
            ],
            "lines": [
                [
                    0.5969455544455544,
                    0.4246575342465753,
                    0.8725147596527358,
                    -0.25652443449254586,
                    0.5778175313059034,
                    0.2639022822243652,
                    0.009991020584457654,
                    0.17382342837512882,
                    0.8431401931401932,
                    0.6036036036036037,
                    0.7,
                    0.38608760999360736,
                    0.9629629629629629,
                    1.0,
                    -1.0
                ],
                [
                    0.12130369630369632,
                    0.3647260273972603,
                    0.0738361845052557,
                    0.2935615048420961,
                    0.24865831842576028,
                    0.388942462230794,
                    0.631968992600507,
                    0.4878048780487805,
                    0.07062937062937064,
                    0.2747747747747748,
                    0.2,
                    0.3094331229554043,
                    0.037037037037037035,
                    0.0,
                    -1.0
                ],
                [
                    0.2817507492507492,
                    0.2106164383561644,
                    0.053649055842008464,
                    0.9629629296504498,
                    0.1735241502683363,
                    0.3471552555448409,
                    0.3580399868150354,
                    0.3383716935760907,
                    0.08623043623043623,
                    0.12162162162162163,
                    0.1,
                    0.3044792670509883,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "marc chagall": 0.5113515747173245,
                "edgar degas": 0.2501911692679098,
                "auguste renoir": 0.2384572560147658
            },
            "question": "which artist painted the ceiling of one of france's most iconic opera houses?",
            "rate_limited": false,
            "answers": [
                "marc chagall",
                "edgar degas",
                "auguste renoir"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "marc chagall": 0.7906221929744528,
                "edgar degas": 0.09628375534246894,
                "auguste renoir": 0.08734073138702975
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.7026132699552514,
                    2.16603186068783,
                    2.1313548693569184
                ],
                "result_count_important_words": [
                    87900.0,
                    5560000.0,
                    3150000.0
                ],
                "wikipedia_search": [
                    5.058841158841159,
                    0.4237762237762238,
                    0.5173826173826174
                ],
                "word_count_appended_bing": [
                    35.0,
                    10.0,
                    5.0
                ],
                "answer_relation_to_question_bing": [
                    1.273972602739726,
                    1.0941780821917808,
                    0.6318493150684932
                ],
                "cosine_similarity_raw": [
                    0.22796885669231415,
                    0.019291765987873077,
                    0.014017314650118351
                ],
                "result_count_noun_chunks": [
                    50600.0,
                    142000.0,
                    98500.0
                ],
                "question_answer_similarity": [
                    0.3011175722349435,
                    -0.344593012414407,
                    -1.1303603888736689
                ],
                "word_count_noun_chunks": [
                    26.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    7.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    82100.0,
                    121000.0,
                    108000.0
                ],
                "word_count_appended": [
                    134.0,
                    61.0,
                    27.0
                ],
                "answer_relation_to_question": [
                    4.178618881118881,
                    0.8491258741258743,
                    1.9722552447552446
                ],
                "result_count": [
                    323000.0,
                    139000.0,
                    97000.0
                ]
            },
            "integer_answers": {
                "marc chagall": 11,
                "edgar degas": 3,
                "auguste renoir": 0
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "The mother of your father is what relation to you?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "grandmother",
                "top"
            ],
            "question": "the mother of your father is what relation to you?",
            "lines": [
                [
                    0.32983107595416955,
                    0.48495423340961097,
                    0.5213811747371143,
                    0.2816048091833457,
                    0.48590373032342965,
                    0.42178155492603087,
                    0.6737983805066489,
                    0.536599352803834,
                    0.49794238683127573,
                    0.44808743169398907,
                    0.48863636363636365,
                    0.41511567936868304,
                    0.8571428571428571,
                    0,
                    1.0
                ],
                [
                    0.6692631269443813,
                    0.515045766590389,
                    0.4107554884935948,
                    0.27943641127031554,
                    0.4965328744242547,
                    0.4091910607491344,
                    0.31795557536587177,
                    0.4546757874902716,
                    0.49794238683127573,
                    0.4633879781420765,
                    0.48863636363636365,
                    0.41757099684135174,
                    0.14285714285714285,
                    0,
                    1.0
                ],
                [
                    0.0009057971014492755,
                    0.0,
                    0.06786333676929089,
                    0.4389587795463388,
                    0.017563395252315636,
                    0.16902738432483475,
                    0.008246044127479386,
                    0.0087248597058944,
                    0.004115226337448559,
                    0.08852459016393442,
                    0.022727272727272728,
                    0.1673133237899652,
                    0.0,
                    0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "grandmother",
                "grandfather",
                "granny smith apple"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "grandmother": 0.9756772199369513,
                "granny smith apple": 0.02020899378073524,
                "grandfather": 0.5020793864692241
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2453470381060492,
                    1.2527129905240553,
                    0.5019399713698955
                ],
                "result_count_important_words": [
                    90700000.0,
                    42800000.0,
                    1110000.0
                ],
                "wikipedia_search": [
                    1.4938271604938271,
                    1.4938271604938271,
                    0.012345679012345678
                ],
                "word_count_appended_bing": [
                    43.0,
                    43.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.9699084668192219,
                    1.030091533180778,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.0913020521402359,
                    0.07192975282669067,
                    0.011883938685059547
                ],
                "result_count_noun_chunks": [
                    13100000.0,
                    11100000.0,
                    213000.0
                ],
                "question_answer_similarity": [
                    3.6769508868455887,
                    3.64863783121109,
                    5.73154229298234
                ],
                "word_count_noun_chunks": [
                    6.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    134000000.0,
                    130000000.0,
                    53700000.0
                ],
                "result_count": [
                    9600000.0,
                    9810000.0,
                    347000.0
                ],
                "answer_relation_to_question": [
                    0.9894932278625085,
                    2.0077893808331435,
                    0.002717391304347826
                ],
                "word_count_appended": [
                    410.0,
                    424.0,
                    81.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which popular root beer brand typically contains caffeine?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "mug"
            ],
            "question": "which popular root beer brand typically contains caffeine?",
            "lines": [
                [
                    0.6864419804614048,
                    0.62004662004662,
                    0.8969331213355272,
                    0.5386628643075225,
                    0.020925751669131156,
                    0.2086858432036097,
                    0.01851925288495856,
                    0.07486865148861646,
                    0.4980549966465459,
                    0.0,
                    0.0,
                    0.3186015074054608,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.08221145721849828,
                    0.06876456876456877,
                    0.03643356108819035,
                    0.0,
                    0.9141013577093695,
                    0.6204173716864072,
                    0.9239798548598168,
                    0.8231173380035026,
                    0.22575452716297786,
                    0.0425531914893617,
                    0.1323529411764706,
                    0.34573102911984954,
                    0.3333333333333333,
                    0.29411764705882354,
                    -1.0
                ],
                [
                    0.2313465623200969,
                    0.3111888111888112,
                    0.06663331757628249,
                    0.4613371356924775,
                    0.0649728906214993,
                    0.17089678510998307,
                    0.05750089225522465,
                    0.1020140105078809,
                    0.2761904761904762,
                    0.9574468085106383,
                    0.8676470588235294,
                    0.3356674634746897,
                    0.6666666666666666,
                    0.7058823529411765,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "barq\u2019s",
                "a&w",
                "mug"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "mug": 0.5192261856426379,
                "a&w": 0.13826803872988083,
                "barq\u2019s": 0.5079997985097009
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.2302105518382254,
                    2.420117203838947,
                    2.3496722443228277
                ],
                "result_count_important_words": [
                    46700.0,
                    2330000.0,
                    145000.0
                ],
                "wikipedia_search": [
                    3.4863849765258212,
                    1.580281690140845,
                    1.9333333333333333
                ],
                "word_count_appended_bing": [
                    0.0,
                    9.0,
                    59.0
                ],
                "answer_relation_to_question_bing": [
                    3.72027972027972,
                    0.4125874125874126,
                    1.867132867132867
                ],
                "cosine_similarity_raw": [
                    0.6453279852867126,
                    0.026213321834802628,
                    0.047941528260707855
                ],
                "result_count_noun_chunks": [
                    171000.0,
                    1880000.0,
                    233000.0
                ],
                "question_answer_similarity": [
                    2.144518733024597,
                    0.0,
                    1.836670383810997
                ],
                "word_count_noun_chunks": [
                    0.0,
                    21.0,
                    42.0
                ],
                "result_count_bing": [
                    370000.0,
                    1100000.0,
                    303000.0
                ],
                "word_count_raw": [
                    0.0,
                    5.0,
                    12.0
                ],
                "result_count": [
                    46700.0,
                    2040000.0,
                    145000.0
                ],
                "answer_relation_to_question": [
                    4.805093863229834,
                    0.5754802005294879,
                    1.6194259362406782
                ],
                "word_count_appended": [
                    0.0,
                    14.0,
                    315.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which Denzel Washington film was based on a Pulitzer Prize-winning play?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "fences",
                "top"
            ],
            "question": "which denzel washington film was based on a pulitzer prize-winning play?",
            "lines": [
                [
                    0.5769158969390773,
                    0.5298787450053661,
                    0.9148535072089171,
                    0.06591891181864042,
                    0.2702702702702703,
                    0.19376263355472134,
                    0.36919592298980747,
                    0.3171684670190761,
                    0.0243120863810519,
                    0.7361268403171007,
                    0.7770700636942676,
                    0.3867390066499312,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.36559423377329486,
                    0.42363990809540913,
                    0.07450966450859232,
                    0.824085982775008,
                    0.1966966966966967,
                    0.09009529309846953,
                    0.15855039637599094,
                    0.069179498965755,
                    0.16440264716126787,
                    0.05436013590033975,
                    0.025477707006369428,
                    0.2670891493781775,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.05748986928762777,
                    0.04648134689922481,
                    0.010636828282490525,
                    0.10999510540635161,
                    0.5330330330330331,
                    0.7161420733468091,
                    0.4722536806342016,
                    0.6136520340151689,
                    0.8112852664576803,
                    0.20951302378255945,
                    0.19745222929936307,
                    0.3461718439718912,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "fences",
                "devil in a blue dress",
                "philadelphia"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "fences": 1.0269993899448815,
                "philadelphia": 0.0394537157014926,
                "devil in a blue dress": 0.05381290679163668
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.0939120531994497,
                    2.13671319502542,
                    2.7693747517751297
                ],
                "result_count_important_words": [
                    32600.0,
                    14000.0,
                    41700.0
                ],
                "wikipedia_search": [
                    0.12156043190525949,
                    0.8220132358063393,
                    4.056426332288401
                ],
                "word_count_appended_bing": [
                    122.0,
                    4.0,
                    31.0
                ],
                "answer_relation_to_question_bing": [
                    4.239029960042928,
                    3.3891192647632726,
                    0.37185077519379844
                ],
                "cosine_similarity_raw": [
                    0.4864761531352997,
                    0.039620742201805115,
                    0.00565616600215435
                ],
                "result_count_noun_chunks": [
                    138000.0,
                    30100.0,
                    267000.0
                ],
                "question_answer_similarity": [
                    1.2564476961269975,
                    15.707494342699647,
                    2.0965621694922447
                ],
                "word_count_noun_chunks": [
                    378.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    87.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    67100.0,
                    31200.0,
                    248000.0
                ],
                "word_count_appended": [
                    650.0,
                    48.0,
                    185.0
                ],
                "answer_relation_to_question": [
                    4.615327175512618,
                    2.924753870186359,
                    0.45991895430102214
                ],
                "result_count": [
                    18000.0,
                    13100.0,
                    35500.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Around the world, which of these is NOT a common term for ziplining?": {
        "raw_data": {
            "data": {
                "word_count_appended_relation_to_question": [
                    2.3676525049482224,
                    1.1609051650335052,
                    0.4714423300182722
                ],
                "result_count_important_words": [
                    18200.0,
                    42.0,
                    6.0
                ],
                "wikipedia_search": [
                    2.407017543859649,
                    0.5929824561403509,
                    0.0
                ],
                "word_count_appended_bing": [
                    15.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    2.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    92.0,
                    12.0,
                    2.0
                ],
                "cosine_similarity_raw": [
                    0.09879983961582184,
                    0.15814489126205444,
                    0.022442778572440147
                ],
                "result_count_noun_chunks": [
                    17800.0,
                    43.0,
                    5.0
                ],
                "question_answer_similarity": [
                    6.01498831063509,
                    1.301072958856821,
                    2.480501203564927
                ],
                "word_count_noun_chunks": [
                    3.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    128000.0,
                    68600.0,
                    181000.0
                ],
                "result_count": [
                    23000.0,
                    18.0,
                    3.0
                ],
                "answer_relation_to_question": [
                    2.370262729597768,
                    0.6626365961404325,
                    0.9671006742617995
                ]
            },
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "z-best_answer_by_ml": [
                "flid streak",
                "bottom"
            ],
            "lines": [
                [
                    0.20371715880027902,
                    0.0,
                    0.32318493799131087,
                    0.19300515731013235,
                    0.00045610529516526555,
                    0.3305084745762712,
                    0.001315212626041229,
                    0.0013446884805020098,
                    0.09883040935672516,
                    0.06603773584905659,
                    0.10526315789473684,
                    0.2040434368814722,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.41717042548244593,
                    0.5,
                    0.21697931025406597,
                    0.4335954339884465,
                    0.49960905260414407,
                    0.4091631355932204,
                    0.4988491889522139,
                    0.4987953832362169,
                    0.40116959064327484,
                    0.44339622641509435,
                    0.4473684210526316,
                    0.35488685437081186,
                    0.5,
                    0,
                    -1.0
                ],
                [
                    0.37911241571727505,
                    0.5,
                    0.45983575175462316,
                    0.3733994087014212,
                    0.49993484210069067,
                    0.26032838983050843,
                    0.49983559842174485,
                    0.499859928283281,
                    0.5,
                    0.49056603773584906,
                    0.4473684210526316,
                    0.441069708747716,
                    0.5,
                    0,
                    -1.0
                ]
            ],
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "question": "around the world, which of these is not a common term for ziplining?",
            "rate_limited": false,
            "answers": [
                "flying fox",
                "foofy slide",
                "flid streak"
            ],
            "ml_answers": {
                "flying fox": -0.00469106357227189,
                "flid streak": 0.7972961545278494,
                "foofy slide": 0.486912522329567
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which NBA franchise has NOT retired any jersey numbers?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "brooklyn nets"
            ],
            "lines": [
                [
                    0.3988002256356852,
                    0.2786220871327254,
                    0.3922072191899955,
                    0.3569612677629822,
                    0.32857142857142857,
                    0.40325077399380804,
                    0.33033749082905356,
                    0.2982791586998088,
                    0.36394707455711417,
                    0.2844036697247706,
                    0.1875,
                    0.32712740673249674,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.40045452573114637,
                    0.28115501519756836,
                    0.3854272766024958,
                    0.3592748944801212,
                    0.30952380952380953,
                    0.4117647058823529,
                    0.3314380044020543,
                    0.3173996175908222,
                    0.3047349146498657,
                    0.33256880733944955,
                    0.359375,
                    0.3376838640189753,
                    0.125,
                    0.5,
                    -1.0
                ],
                [
                    0.20074524863316845,
                    0.4402228976697062,
                    0.22236550420750867,
                    0.2837638377568966,
                    0.3619047619047619,
                    0.18498452012383904,
                    0.33822450476889215,
                    0.38432122370936905,
                    0.33131801079302015,
                    0.3830275229357798,
                    0.453125,
                    0.3351887292485279,
                    0.375,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "dallas mavericks": 0.3205999377973341,
                "brooklyn nets": 0.29285602816716155,
                "los angeles clippers": 0.3865440340355043
            },
            "question": "which nba franchise has not retired any jersey numbers?",
            "rate_limited": false,
            "answers": [
                "brooklyn nets",
                "dallas mavericks",
                "los angeles clippers"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "dallas mavericks": 0.1590588820824112,
                "brooklyn nets": 0.36730754930158915,
                "los angeles clippers": 0.24298712584071994
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7287259326750326,
                    1.6231613598102466,
                    1.648112707514721
                ],
                "result_count_important_words": [
                    925000.0,
                    919000.0,
                    882000.0
                ],
                "wikipedia_search": [
                    1.360529254428858,
                    1.9526508535013432,
                    1.6868198920697985
                ],
                "answer_relation_to_question": [
                    0.8095981949145188,
                    0.7963637941508288,
                    2.3940380109346524
                ],
                "answer_relation_to_question_bing": [
                    1.3282674772036474,
                    1.3130699088145896,
                    0.3586626139817629
                ],
                "word_count_appended": [
                    94.0,
                    73.0,
                    51.0
                ],
                "cosine_similarity_raw": [
                    0.0395403727889061,
                    0.04202738031744957,
                    0.10184143483638763
                ],
                "result_count_noun_chunks": [
                    2110000.0,
                    1910000.0,
                    1210000.0
                ],
                "question_answer_similarity": [
                    2.967781642335467,
                    2.9197782883420587,
                    4.486489097587764
                ],
                "word_count_noun_chunks": [
                    0.0,
                    3.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    2.0
                ],
                "result_count_bing": [
                    1250000.0,
                    1140000.0,
                    4070000.0
                ],
                "result_count": [
                    144000.0,
                    160000.0,
                    116000.0
                ],
                "word_count_appended_bing": [
                    20.0,
                    9.0,
                    3.0
                ]
            },
            "integer_answers": {
                "dallas mavericks": 3,
                "brooklyn nets": 6,
                "los angeles clippers": 5
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "The \u201cCC:\u201d feature in email stands for what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "carbon copy"
            ],
            "lines": [
                [
                    0.14391459604225562,
                    0.1906204906204906,
                    0.028534451165466528,
                    0.34693977848371177,
                    0.0032126187894421358,
                    0.29902912621359223,
                    0.00015876223190832204,
                    0.33761160714285715,
                    0.2376543209876543,
                    0.1072961373390558,
                    0.08695652173913043,
                    0.17287084100894295,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.34229777314883697,
                    0.4658189033189033,
                    0.013388619966767231,
                    0.33172066386817906,
                    0.22225664581046223,
                    0.34951456310679613,
                    0.007577288341079006,
                    0.5245535714285714,
                    0.024691358024691357,
                    0.18454935622317598,
                    0.08695652173913043,
                    0.2839827141751924,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.5137876308089074,
                    0.34356060606060607,
                    0.9580769288677662,
                    0.3213395576481092,
                    0.7745307354000956,
                    0.35145631067961164,
                    0.9922639494270127,
                    0.13783482142857142,
                    0.7376543209876543,
                    0.7081545064377682,
                    0.8260869565217391,
                    0.5431464448158647,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "carbon copy": 0.6577066263631219,
                "copy chain": 0.13962851798317913,
                "copy contacts": 0.20266485565369893
            },
            "question": "the \u201ccc:\u201d feature in email stands for what?",
            "rate_limited": false,
            "answers": [
                "copy chain",
                "copy contacts",
                "carbon copy"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "carbon copy": 0.7906221929744528,
                "copy chain": 0.09449297153440057,
                "copy contacts": 0.12010909070783748
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.6914833640357718,
                    1.1359308567007695,
                    2.172585779263459
                ],
                "result_count_important_words": [
                    264.0,
                    12600.0,
                    1650000.0
                ],
                "wikipedia_search": [
                    0.7129629629629629,
                    0.07407407407407407,
                    2.212962962962963
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    19.0
                ],
                "answer_relation_to_question_bing": [
                    0.5718614718614718,
                    1.39745670995671,
                    1.0306818181818183
                ],
                "cosine_similarity_raw": [
                    0.018460551276803017,
                    0.008661855943500996,
                    0.6198341846466064
                ],
                "result_count_noun_chunks": [
                    1210000.0,
                    1880000.0,
                    494000.0
                ],
                "question_answer_similarity": [
                    5.126299988478422,
                    4.901425955817103,
                    4.748037189245224
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    27.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    33.0
                ],
                "result_count_bing": [
                    15400000.0,
                    18000000.0,
                    18100000.0
                ],
                "word_count_appended": [
                    25.0,
                    43.0,
                    165.0
                ],
                "answer_relation_to_question": [
                    0.43174378812676684,
                    1.0268933194465109,
                    1.5413628924267222
                ],
                "result_count": [
                    477.0,
                    33000.0,
                    115000.0
                ]
            },
            "integer_answers": {
                "carbon copy": 11,
                "copy chain": 1,
                "copy contacts": 2
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these mountain ranges is found in Mexico?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "sierra madre"
            ],
            "question": "which of these mountain ranges is found in mexico?",
            "lines": [
                [
                    0.2189917483238783,
                    0.09418103448275862,
                    0.04523466573820833,
                    0.031195195463956896,
                    0.4861910241657077,
                    0.18507157464212678,
                    0.3865979381443299,
                    0.4748247291268324,
                    0.22105757196495618,
                    0.5582010582010583,
                    0.5625,
                    0.2844789388585083,
                    0.05434782608695652,
                    0.1,
                    -1.0
                ],
                [
                    0.32876784726965236,
                    0.2084530651340996,
                    0.9247335757587569,
                    0.17585925287804688,
                    0.08227848101265822,
                    0.11247443762781185,
                    0.19845360824742267,
                    0.07265774378585087,
                    0.4530378882694277,
                    0.27380952380952384,
                    0.29375,
                    0.35226228403034127,
                    0.8260869565217391,
                    0.8666666666666667,
                    -1.0
                ],
                [
                    0.4522404044064694,
                    0.6973659003831418,
                    0.03003175850303475,
                    0.7929455516579962,
                    0.4315304948216341,
                    0.7024539877300614,
                    0.41494845360824745,
                    0.45251752708731674,
                    0.3259045397656161,
                    0.167989417989418,
                    0.14375,
                    0.3632587771111505,
                    0.11956521739130435,
                    0.03333333333333333,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "andes",
                "sierra madre",
                "rocky mountains"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "andes": 0.06370675800104161,
                "sierra madre": 0.9303553719010506,
                "rocky mountains": 0.5532209854290354
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1379157554340331,
                    1.409049136121365,
                    1.453035108444602
                ],
                "result_count_important_words": [
                    600000.0,
                    308000.0,
                    644000.0
                ],
                "wikipedia_search": [
                    0.8842302878598247,
                    1.8121515530777108,
                    1.3036181590624645
                ],
                "word_count_appended_bing": [
                    90.0,
                    47.0,
                    23.0
                ],
                "answer_relation_to_question_bing": [
                    0.3767241379310345,
                    0.8338122605363985,
                    2.7894636015325673
                ],
                "cosine_similarity_raw": [
                    0.040949560701847076,
                    0.8371330499649048,
                    0.027186833322048187
                ],
                "result_count_noun_chunks": [
                    1490000.0,
                    228000.0,
                    1420000.0
                ],
                "question_answer_similarity": [
                    0.23089898098260164,
                    1.3016659034183249,
                    5.869183287024498
                ],
                "word_count_noun_chunks": [
                    5.0,
                    76.0,
                    11.0
                ],
                "word_count_raw": [
                    3.0,
                    26.0,
                    1.0
                ],
                "result_count_bing": [
                    1810000.0,
                    1100000.0,
                    6870000.0
                ],
                "word_count_appended": [
                    422.0,
                    207.0,
                    127.0
                ],
                "answer_relation_to_question": [
                    0.8759669932955132,
                    1.3150713890786094,
                    1.8089616176258776
                ],
                "result_count": [
                    1690000.0,
                    286000.0,
                    1500000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "How do you spell the last name of Duke University\u2019s men's basketball coach?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "krzyzewski"
            ],
            "lines": [
                [
                    0.34173705036432284,
                    0.4572553308280655,
                    0,
                    0,
                    0.0,
                    0.4026079869600652,
                    0.0,
                    0.0,
                    0.0,
                    0.04819277108433735,
                    0.27835051546391754,
                    0.06253498789080215,
                    0.0,
                    0.0,
                    5.0
                ],
                [
                    0.3124648081511719,
                    0.43057286991155325,
                    0,
                    0,
                    1.0,
                    0.32599837000814996,
                    1.0,
                    1.0,
                    1.0,
                    0.9036144578313253,
                    0.44329896907216493,
                    0.8749300242183956,
                    1.0,
                    1.0,
                    5.0
                ],
                [
                    0.3457981414845052,
                    0.11217179926038112,
                    0,
                    0,
                    0.0,
                    0.2713936430317848,
                    0.0,
                    0.0,
                    0.0,
                    0.04819277108433735,
                    0.27835051546391754,
                    0.06253498789080215,
                    0.0,
                    0.0,
                    5.0
                ]
            ],
            "fraction_answers": {
                "khzyrweski": 0.09320348818464402,
                "crzyzewski": 0.13255655354929255,
                "krzyzewski": 0.7742399582660634
            },
            "question": "how do you spell the last name of duke university\u2019s men's basketball coach?",
            "rate_limited": false,
            "answers": [
                "crzyzewski",
                "krzyzewski",
                "khzyrweski"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "khzyrweski": 0.15104784383726919,
                "crzyzewski": 0.15713699989156787,
                "krzyzewski": 0.6580514912745106
            },
            "integer_answers": {
                "khzyrweski": 1,
                "crzyzewski": 2,
                "krzyzewski": 9
            },
            "categorical_data": {
                "question_type": 5
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.37520992734481295,
                    5.249580145310374,
                    0.37520992734481295
                ],
                "result_count_important_words": [
                    0,
                    53100.0,
                    0
                ],
                "wikipedia_search": [
                    0.0,
                    5.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    27.0,
                    43.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    2.743531984968393,
                    2.5834372194693196,
                    0.6730307955622867
                ],
                "cosine_similarity_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    0,
                    16700.0,
                    0
                ],
                "question_answer_similarity": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    133.0,
                    0.0
                ],
                "result_count_bing": [
                    494000.0,
                    400000.0,
                    333000.0
                ],
                "word_count_raw": [
                    0.0,
                    32.0,
                    0.0
                ],
                "result_count": [
                    0,
                    55600.0,
                    0
                ],
                "answer_relation_to_question": [
                    2.050422302185937,
                    1.8747888489070315,
                    2.074788848907031
                ],
                "word_count_appended": [
                    16.0,
                    300.0,
                    16.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What beauty product entrepreneur became the first U.S. self-made female millionaire?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "c.j. walker"
            ],
            "question": "what beauty product entrepreneur became the first u.s. self-made female millionaire?",
            "lines": [
                [
                    0.21189674213253937,
                    0.1952380952380952,
                    0.03109522098360259,
                    -0.11396757688883784,
                    0.577713111947963,
                    0.3219417475728155,
                    0.5697607498150128,
                    0.5410216718266254,
                    0.24989177489177491,
                    0.48872180451127817,
                    0.2631578947368421,
                    0.2900933180495736,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.18527565678590255,
                    0.195578231292517,
                    0.02579723597022433,
                    0.24540546109379532,
                    0.07052379322149949,
                    0.32543689320388347,
                    0.06684206199128505,
                    0.07585139318885449,
                    0.08775252525252525,
                    0.14661654135338345,
                    0.10526315789473684,
                    0.31366186596549916,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.6028276010815581,
                    0.6091836734693877,
                    0.9431075430461731,
                    0.8685621157950425,
                    0.35176309483053747,
                    0.35262135922330096,
                    0.3633971881937022,
                    0.3831269349845201,
                    0.6623556998557,
                    0.36466165413533835,
                    0.631578947368421,
                    0.39624481598492717,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "helena rubinstein",
                "elizabeth arden",
                "c.j. walker"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "c.j. walker": 0.9003600009258962,
                "helena rubinstein": 0.060516007933951295,
                "elizabeth arden": -0.046525268630869615
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.0306532263470154,
                    2.1956330617584943,
                    2.7737137118944903
                ],
                "result_count_important_words": [
                    69300.0,
                    8130.0,
                    44200.0
                ],
                "wikipedia_search": [
                    1.4993506493506494,
                    0.5265151515151515,
                    3.9741341991341996
                ],
                "word_count_appended_bing": [
                    5.0,
                    2.0,
                    12.0
                ],
                "answer_relation_to_question_bing": [
                    1.3666666666666665,
                    1.369047619047619,
                    4.264285714285714
                ],
                "cosine_similarity_raw": [
                    0.02502378262579441,
                    0.020760245621204376,
                    0.758962869644165
                ],
                "result_count_noun_chunks": [
                    69900.0,
                    9800.0,
                    49500.0
                ],
                "question_answer_similarity": [
                    -0.8834412302821875,
                    1.9023068523965776,
                    6.732823537196964
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    73.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    14.0
                ],
                "result_count_bing": [
                    82900.0,
                    83800.0,
                    90800.0
                ],
                "word_count_appended": [
                    130.0,
                    39.0,
                    97.0
                ],
                "answer_relation_to_question": [
                    1.4832771949277757,
                    1.2969295975013178,
                    4.219793207570906
                ],
                "result_count": [
                    67500.0,
                    8240.0,
                    41100.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "One of Apple\u2019s biggest flops was a product named after a man who did what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "discovered saturn"
            ],
            "lines": [
                [
                    0.32602843166223444,
                    0.5249433106575964,
                    0.14752903191592426,
                    0.2589584497031569,
                    0.5434782608695652,
                    0.333843797856049,
                    0.5633802816901409,
                    0.5208333333333334,
                    0.3467086834733893,
                    0.35294117647058826,
                    0.42857142857142855,
                    0.3563749329172821,
                    0,
                    0,
                    1.0
                ],
                [
                    0.5307257296693916,
                    0.3038548752834467,
                    0.3120795819795179,
                    0.500655128221995,
                    0.13043478260869565,
                    0.3315467075038285,
                    0.14788732394366197,
                    0.1597222222222222,
                    0.2574229691876751,
                    0.35294117647058826,
                    0.14285714285714285,
                    0.3055139458285154,
                    0,
                    0,
                    1.0
                ],
                [
                    0.14324583866837387,
                    0.17120181405895693,
                    0.5403913861045578,
                    0.2403864220748481,
                    0.32608695652173914,
                    0.3346094946401225,
                    0.2887323943661972,
                    0.3194444444444444,
                    0.39586834733893556,
                    0.29411764705882354,
                    0.42857142857142855,
                    0.33811112125420245,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "invented the transistor": 0.28963679881472343,
                "developed calculus": 0.3183972745918858,
                "discovered saturn": 0.3919659265933908
            },
            "question": "one of apple\u2019s biggest flops was a product named after a man who did what?",
            "rate_limited": false,
            "answers": [
                "discovered saturn",
                "invented the transistor",
                "developed calculus"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "invented the transistor": 0.23959110245507648,
                "developed calculus": 0.3569632692530953,
                "discovered saturn": 0.36868467009045297
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.138249597503693,
                    1.8330836749710921,
                    2.0286667275252146
                ],
                "result_count_important_words": [
                    80.0,
                    21.0,
                    41.0
                ],
                "wikipedia_search": [
                    1.040126050420168,
                    0.7722689075630252,
                    1.1876050420168067
                ],
                "word_count_appended_bing": [
                    6.0,
                    2.0,
                    6.0
                ],
                "answer_relation_to_question_bing": [
                    1.5748299319727892,
                    0.91156462585034,
                    0.5136054421768708
                ],
                "cosine_similarity_raw": [
                    0.02318807877600193,
                    0.04905153810977936,
                    0.08493676036596298
                ],
                "result_count_noun_chunks": [
                    75.0,
                    23.0,
                    46.0
                ],
                "question_answer_similarity": [
                    6.830728069879115,
                    13.2061303332448,
                    6.340840713120997
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    436000.0,
                    433000.0,
                    437000.0
                ],
                "word_count_appended": [
                    6.0,
                    6.0,
                    5.0
                ],
                "answer_relation_to_question": [
                    1.3041137266489378,
                    2.1229029186775663,
                    0.5729833546734955
                ],
                "result_count": [
                    75.0,
                    18.0,
                    45.0
                ]
            },
            "integer_answers": {
                "invented the transistor": 2,
                "developed calculus": 3,
                "discovered saturn": 7
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Whose poem inspired a Rush song, \u201cTwilight Zone\u201d episode, and Ray Bradbury story?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "walt whitman",
                "bottom"
            ],
            "question": "whose poem inspired a rush song, \u201ctwilight zone\u201d episode, and ray bradbury story?",
            "lines": [
                [
                    0.2373978511909546,
                    0.24168210381025027,
                    0.2066134563636397,
                    0.43505564060447993,
                    0.13823272090988625,
                    0.3523447401774398,
                    0.23644578313253012,
                    0.2803867403314917,
                    0.36904761904761907,
                    0.07692307692307693,
                    0.2857142857142857,
                    0.28098977441475237,
                    0.3333333333333333,
                    0.0,
                    -1.0
                ],
                [
                    0.28469442434959674,
                    0.38381733318804256,
                    0.1972539021164279,
                    0.31795535484105897,
                    0.3394575678040245,
                    0.3637515842839037,
                    0.5647590361445783,
                    0.2610497237569061,
                    0.3288690476190476,
                    0.3333333333333333,
                    0.42857142857142855,
                    0.3730210803589549,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.4779077244594486,
                    0.3745005630017072,
                    0.5961326415199324,
                    0.24698900455446107,
                    0.5223097112860893,
                    0.28390367553865653,
                    0.19879518072289157,
                    0.4585635359116022,
                    0.3020833333333333,
                    0.5897435897435898,
                    0.2857142857142857,
                    0.34598914522629276,
                    0.6666666666666666,
                    1.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "e.e. cummings",
                "t.s. eliot",
                "walt whitman"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "t.s. eliot": 0.33450551873390755,
                "e.e. cummings": 0.10253207825117229,
                "walt whitman": 0.8216611157144127
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.809897744147524,
                    3.730210803589549,
                    3.4598914522629274
                ],
                "result_count_important_words": [
                    1570.0,
                    3750.0,
                    1320.0
                ],
                "wikipedia_search": [
                    2.2142857142857144,
                    1.9732142857142858,
                    1.8125
                ],
                "word_count_appended_bing": [
                    2.0,
                    3.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.4500926228615014,
                    2.302903999128255,
                    2.247003378010243
                ],
                "cosine_similarity_raw": [
                    0.055276937782764435,
                    0.05277290195226669,
                    0.15948809683322906
                ],
                "result_count_noun_chunks": [
                    203000.0,
                    189000.0,
                    332000.0
                ],
                "question_answer_similarity": [
                    4.528503133566119,
                    3.3096038445364684,
                    2.570913641131483
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    2.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    27800.0,
                    28700.0,
                    22400.0
                ],
                "result_count": [
                    1580.0,
                    3880.0,
                    5970.0
                ],
                "answer_relation_to_question": [
                    1.186989255954773,
                    1.4234721217479838,
                    2.389538622297243
                ],
                "word_count_appended": [
                    3.0,
                    13.0,
                    23.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "George Washington Carver is famous for his scientific work on what subject?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "soil depletion"
            ],
            "question": "george washington carver is famous for his scientific work on what subject?",
            "lines": [
                [
                    0.3709677419354839,
                    0.4625,
                    0.3054060713376658,
                    0.24313216910422933,
                    0.00941801497222893,
                    0.07773664727657324,
                    0.08471690080885483,
                    0.1962071480671043,
                    0.36000000000000004,
                    0.4,
                    0.5,
                    0.3366591436634122,
                    0,
                    0,
                    1.0
                ],
                [
                    0.343778801843318,
                    0.1625,
                    0.47975292123629476,
                    0.3422945275273178,
                    0.975609756097561,
                    0.39344262295081966,
                    0.34653043848446147,
                    0.2975929978118162,
                    0.38000000000000006,
                    0.3333333333333333,
                    0.25,
                    0.3579125024082633,
                    0,
                    0,
                    1.0
                ],
                [
                    0.28525345622119813,
                    0.375,
                    0.21484100742603945,
                    0.41457330336845283,
                    0.014972228930210094,
                    0.5288207297726071,
                    0.5687526607066837,
                    0.5061998541210795,
                    0.26,
                    0.26666666666666666,
                    0.25,
                    0.3054283539283245,
                    0,
                    0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "soil depletion",
                "irrigation technology",
                "food packaging"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "irrigation technology": 0.40715164372716167,
                "food packaging": 0.00927475500653514,
                "soil depletion": 0.4147768827017337
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.3566140056438853,
                    2.505387516857843,
                    2.1379984774982717
                ],
                "result_count_important_words": [
                    995.0,
                    4070.0,
                    6680.0
                ],
                "wikipedia_search": [
                    1.08,
                    1.1400000000000001,
                    0.78
                ],
                "word_count_appended_bing": [
                    4.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.925,
                    0.325,
                    0.75
                ],
                "cosine_similarity_raw": [
                    0.025156088173389435,
                    0.03951691836118698,
                    0.017696306109428406
                ],
                "result_count_noun_chunks": [
                    2690.0,
                    4080.0,
                    6940.0
                ],
                "question_answer_similarity": [
                    3.170943743083626,
                    4.4642249289900064,
                    5.406888883560896
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    73500.0,
                    372000.0,
                    500000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    39.0,
                    4040.0,
                    62.0
                ],
                "answer_relation_to_question": [
                    1.8548387096774195,
                    1.7188940092165899,
                    1.4262672811059907
                ],
                "word_count_appended": [
                    18.0,
                    15.0,
                    12.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Though perhaps more famous as butter, which of these is a location in Florida?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "tillamook"
            ],
            "lines": [
                [
                    0.1946473617826361,
                    0.12241521918941274,
                    0.073705403597849,
                    -0.2536779518234636,
                    0.9468125235760091,
                    0.4972265806280524,
                    0.5302518788163457,
                    0.08858131487889273,
                    0.19230769230769232,
                    0.6111111111111112,
                    0.5,
                    0.38382895540917816,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.3702263331231168,
                    0.27832919768403636,
                    0.7634720499246703,
                    0.0,
                    0.027536778574122973,
                    0.4972265806280524,
                    0.4697040864255519,
                    0.6899653979238755,
                    0.5,
                    0.3888888888888889,
                    0.5,
                    0.31298282932712884,
                    1.0,
                    0,
                    -1.0
                ],
                [
                    0.4351263050942471,
                    0.5992555831265508,
                    0.16282254647748068,
                    1.2536779518234635,
                    0.025650697849867975,
                    0.005546838743895163,
                    4.403475810239549e-05,
                    0.22145328719723184,
                    0.3076923076923077,
                    0.0,
                    0.0,
                    0.303188215263693,
                    0.0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "tillamook": 0.29901616072874737,
                "kerrygold": 0.44602554942303413,
                "land o\u2019 lakes": 0.2549582898482185
            },
            "question": "though perhaps more famous as butter, which of these is a location in florida?",
            "rate_limited": false,
            "answers": [
                "tillamook",
                "kerrygold",
                "land o\u2019 lakes"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "tillamook": 0.40764433403719735,
                "kerrygold": 0.3599392076674976,
                "land o\u2019 lakes": 0.17470412104409289
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.5353158216367127,
                    1.2519313173085154,
                    1.212752861054772
                ],
                "result_count_important_words": [
                    867000.0,
                    768000.0,
                    72.0
                ],
                "wikipedia_search": [
                    0.38461538461538464,
                    1.0,
                    0.6153846153846154
                ],
                "word_count_appended_bing": [
                    39.0,
                    39.0,
                    0.0
                ],
                "answer_relation_to_question_bing": [
                    0.36724565756823824,
                    0.8349875930521091,
                    1.7977667493796525
                ],
                "cosine_similarity_raw": [
                    0.013325626030564308,
                    0.13803252577781677,
                    0.029437629505991936
                ],
                "result_count_noun_chunks": [
                    128000.0,
                    997000.0,
                    320000.0
                ],
                "question_answer_similarity": [
                    -1.7059812098741531,
                    0.0,
                    8.430969320237637
                ],
                "word_count_noun_chunks": [
                    0.0,
                    7.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    2250000.0,
                    2250000.0,
                    25100.0
                ],
                "word_count_appended": [
                    209.0,
                    133.0,
                    0.0
                ],
                "answer_relation_to_question": [
                    0.7785894471305445,
                    1.4809053324924673,
                    1.7405052203769884
                ],
                "result_count": [
                    2510.0,
                    73.0,
                    68.0
                ]
            },
            "integer_answers": {
                "tillamook": 6,
                "kerrygold": 4,
                "land o\u2019 lakes": 3
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which verb describes the sound minerals make when they are heated?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "decrepitate"
            ],
            "lines": [
                [
                    0.8714574898785425,
                    1.0,
                    0.7203094327513677,
                    0,
                    1.0,
                    0.3355119825708061,
                    1.0,
                    1.0,
                    1.0,
                    0.7217391304347827,
                    0.5137614678899083,
                    0.7354746488654065,
                    1.0,
                    0,
                    2.0
                ],
                [
                    0.025809716599190284,
                    0.0,
                    0.07812464411946665,
                    0,
                    0.0,
                    0.3311546840958606,
                    0.0,
                    0.0,
                    0.0,
                    0.1391304347826087,
                    0.24770642201834864,
                    0.13226267556729676,
                    0.0,
                    0,
                    2.0
                ],
                [
                    0.10273279352226722,
                    0.0,
                    0.20156592312916558,
                    0,
                    0.0,
                    0.3333333333333333,
                    0.0,
                    0.0,
                    0.0,
                    0.1391304347826087,
                    0.23853211009174313,
                    0.13226267556729676,
                    0.0,
                    0,
                    2.0
                ]
            ],
            "fraction_answers": {
                "frangelle": 0.07951571476523096,
                "decrepitate": 0.8248545126992345,
                "recleft": 0.09562977253553456
            },
            "question": "which verb describes the sound minerals make when they are heated?",
            "rate_limited": false,
            "answers": [
                "decrepitate",
                "frangelle",
                "recleft"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "frangelle": 0.1308211994649323,
                "decrepitate": 0.6775951740413777,
                "recleft": 0.1308211994649323
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 2
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.677373244327032,
                    0.6613133778364838,
                    0.6613133778364838
                ],
                "result_count_important_words": [
                    4240.0,
                    0,
                    0
                ],
                "wikipedia_search": [
                    2.0,
                    0.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    56.0,
                    27.0,
                    26.0
                ],
                "answer_relation_to_question_bing": [
                    4.0,
                    0.0,
                    0.0
                ],
                "cosine_similarity_raw": [
                    0.15951946377754211,
                    0.01730145514011383,
                    0.04463871568441391
                ],
                "result_count_noun_chunks": [
                    4440.0,
                    0,
                    0
                ],
                "question_answer_similarity": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_noun_chunks": [
                    27.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    154000.0,
                    152000.0,
                    153000.0
                ],
                "word_count_appended": [
                    83.0,
                    16.0,
                    16.0
                ],
                "answer_relation_to_question": [
                    3.48582995951417,
                    0.10323886639676114,
                    0.4109311740890689
                ],
                "result_count": [
                    4220.0,
                    0,
                    0
                ]
            },
            "integer_answers": {
                "frangelle": 0,
                "decrepitate": 12,
                "recleft": 0
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "One symptom of argyria is turning roughly the same skin color as which cartoon character?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "garfield"
            ],
            "lines": [
                [
                    0.4483686735662153,
                    0.40217948717948715,
                    0.7601374835528155,
                    0.22209221161234527,
                    0.027777777777777776,
                    0.17083587553386212,
                    0.032809295967190705,
                    0.02903225806451613,
                    0.0347008547008547,
                    0.18309859154929578,
                    0.16666666666666666,
                    0.29962590534408073,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.13021456886689547,
                    0.19833333333333333,
                    0.10904072651314722,
                    0.7749534690347367,
                    0.011904761904761904,
                    0.13361805979255645,
                    0.017088174982911826,
                    0.01129032258064516,
                    0.06256410256410257,
                    0.08450704225352113,
                    0.05555555555555555,
                    0.20969485564807608,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.42141675756688923,
                    0.3994871794871795,
                    0.13082178993403726,
                    0.00295431935291795,
                    0.9603174603174603,
                    0.6955460646735815,
                    0.9501025290498974,
                    0.9596774193548387,
                    0.9027350427350427,
                    0.7323943661971831,
                    0.7777777777777778,
                    0.4906792390078431,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "garfield": 0.6186591621212207,
                "the grinch": 0.14989708108585362,
                "papa smurf": 0.23144375679292564
            },
            "question": "one symptom of argyria is turning roughly the same skin color as which cartoon character?",
            "rate_limited": false,
            "answers": [
                "papa smurf",
                "the grinch",
                "garfield"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "garfield": 0.4111419727148074,
                "the grinch": 0.14426135435173482,
                "papa smurf": 0.38158045794732526
            },
            "integer_answers": {
                "garfield": 8,
                "the grinch": 1,
                "papa smurf": 3
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.397007242752646,
                    1.6775588451846086,
                    3.925433912062745
                ],
                "result_count_important_words": [
                    48.0,
                    25.0,
                    1390.0
                ],
                "wikipedia_search": [
                    0.1735042735042735,
                    0.3128205128205128,
                    4.513675213675214
                ],
                "word_count_appended_bing": [
                    6.0,
                    2.0,
                    28.0
                ],
                "answer_relation_to_question_bing": [
                    2.010897435897436,
                    0.9916666666666667,
                    1.9974358974358974
                ],
                "cosine_similarity_raw": [
                    0.08494608104228973,
                    0.01218540407717228,
                    0.014619458466768265
                ],
                "result_count_noun_chunks": [
                    36.0,
                    14.0,
                    1190.0
                ],
                "question_answer_similarity": [
                    2.1387014188803732,
                    7.4626393773942254,
                    0.02844947576522827
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    5600.0,
                    4380.0,
                    22800.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    35.0,
                    15.0,
                    1210.0
                ],
                "answer_relation_to_question": [
                    2.6902120413972916,
                    0.7812874132013728,
                    2.5285005454013354
                ],
                "word_count_appended": [
                    13.0,
                    6.0,
                    52.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Talking is discouraged on what Amtrak car?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "quiet car"
            ],
            "lines": [
                [
                    0.13563760146635245,
                    0.4236874236874237,
                    0.18355840100394022,
                    0.3747164863362907,
                    0.9894422843657126,
                    0.20611265471078555,
                    0.9909605065983468,
                    0.7996187248464308,
                    0.23684210526315788,
                    0.09401709401709402,
                    0.13333333333333333,
                    0.31346059063812154,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.02641180064589334,
                    0.047619047619047616,
                    0.0533048761957842,
                    0.36944963713959095,
                    0.010187590927913633,
                    0.7476635514018691,
                    0.00873132885386958,
                    0.1678669773353103,
                    0.15789473684210525,
                    0.0,
                    0.0,
                    0.3383513228520838,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.8379505978877542,
                    0.5286935286935287,
                    0.7631367228002756,
                    0.2558338765241184,
                    0.0003701247063738406,
                    0.04622379388734529,
                    0.00030816454778363224,
                    0.03251429781825884,
                    0.6052631578947368,
                    0.905982905982906,
                    0.8666666666666667,
                    0.3481880865097946,
                    0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "sports argument car": 0.3754913235589991,
                "quiet car": 0.47624091722458023,
                "meet & greet car": 0.1482677592164206
            },
            "question": "talking is discouraged on what amtrak car?",
            "rate_limited": false,
            "answers": [
                "sports argument car",
                "meet & greet car",
                "quiet car"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sports argument car": 0.09573648083777203,
                "quiet car": 0.7610576120966963,
                "meet & greet car": 0.2712017493599086
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2538423625524862,
                    1.3534052914083352,
                    1.3927523460391784
                ],
                "result_count_important_words": [
                    3280000.0,
                    28900.0,
                    1020.0
                ],
                "wikipedia_search": [
                    0.47368421052631576,
                    0.3157894736842105,
                    1.2105263157894737
                ],
                "word_count_appended_bing": [
                    2.0,
                    0.0,
                    13.0
                ],
                "answer_relation_to_question_bing": [
                    1.271062271062271,
                    0.14285714285714285,
                    1.5860805860805862
                ],
                "cosine_similarity_raw": [
                    0.11452482640743256,
                    0.033257707953453064,
                    0.4761323928833008
                ],
                "result_count_noun_chunks": [
                    1510000.0,
                    317000.0,
                    61400.0
                ],
                "question_answer_similarity": [
                    7.02377974241972,
                    6.925056600943208,
                    4.7954143062233925
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    2.0
                ],
                "result_count_bing": [
                    4080000.0,
                    14800000.0,
                    915000.0
                ],
                "word_count_appended": [
                    11.0,
                    0.0,
                    106.0
                ],
                "answer_relation_to_question": [
                    0.40691280439905736,
                    0.07923540193768003,
                    2.5138517936632625
                ],
                "result_count": [
                    2700000.0,
                    27800.0,
                    1010.0
                ]
            },
            "integer_answers": {
                "sports argument car": 4,
                "quiet car": 8,
                "meet & greet car": 1
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these is NOT a name of one of the Florida Keys?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "turtle key"
            ],
            "lines": [
                [
                    0.24224341871556304,
                    0.3744789693065555,
                    0.38610875008731615,
                    0.2792305500054321,
                    0.4297752808988764,
                    0.3723331039229181,
                    0.476364522417154,
                    0.4999927356804696,
                    0.33333333333333337,
                    0.40625,
                    0.46511627906976744,
                    0.3818392025212387,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.47206119101786,
                    0.35297461159530125,
                    0.3987978594784563,
                    0.3551920914320528,
                    0.2359550561797753,
                    0.3719889883000688,
                    0.364766081871345,
                    0.48429523301515615,
                    0.5,
                    0.3506944444444444,
                    0.4534883720930233,
                    0.35663309582534025,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.285695390266577,
                    0.27254641909814326,
                    0.21509339043422754,
                    0.36557735856251505,
                    0.3342696629213483,
                    0.25567790777701305,
                    0.158869395711501,
                    0.01571203130437432,
                    0.16666666666666669,
                    0.24305555555555558,
                    0.08139534883720928,
                    0.26152770165342104,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "pigeon key": 0.5573188618685746,
                "turtle key": 0.21719216245786274,
                "fat deer key": 0.2254889756735626
            },
            "question": "which of these is not a name of one of the florida keys?",
            "rate_limited": false,
            "answers": [
                "fat deer key",
                "turtle key",
                "pigeon key"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "pigeon key": 0.13851532755811163,
                "turtle key": 0.3934851968464327,
                "fat deer key": 0.3796236363014837
            },
            "integer_answers": {
                "pigeon key": 9,
                "turtle key": 1,
                "fat deer key": 2
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.47264318991504517,
                    0.573467616698639,
                    0.9538891933863158
                ],
                "result_count_important_words": [
                    7760.0,
                    44400.0,
                    112000.0
                ],
                "wikipedia_search": [
                    0.3333333333333333,
                    0.0,
                    0.6666666666666666
                ],
                "word_count_appended_bing": [
                    3.0,
                    4.0,
                    36.0
                ],
                "answer_relation_to_question_bing": [
                    0.502084122773778,
                    0.588101553618795,
                    0.9098143236074271
                ],
                "cosine_similarity_raw": [
                    0.03496728837490082,
                    0.031071433797478676,
                    0.08747301995754242
                ],
                "result_count_noun_chunks": [
                    21.0,
                    45400.0,
                    1400000.0
                ],
                "question_answer_similarity": [
                    11.634681515395641,
                    7.631463035941124,
                    7.084153272211552
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    3710000.0,
                    3720000.0,
                    7100000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    25.0,
                    94.0,
                    59.0
                ],
                "answer_relation_to_question": [
                    1.0310263251377478,
                    0.1117552359285601,
                    0.8572184389336921
                ],
                "word_count_appended": [
                    27.0,
                    43.0,
                    74.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "In the old saying, what keeps the doctor away?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "apple a day"
            ],
            "question": "in the old saying, what keeps the doctor away?",
            "lines": [
                [
                    0.8203015734265735,
                    0.7095959595959597,
                    0.986905644702088,
                    0.3846568860807157,
                    0.15375088086442162,
                    0.304654442877292,
                    0.5205992509363296,
                    0.29968066814050603,
                    0.18817204301075266,
                    0.5663265306122449,
                    0.75,
                    0.5003943549548486,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.10437791375291375,
                    0.10101010101010101,
                    0.005131195502980962,
                    0.24471596589998465,
                    0.00702556108394371,
                    0.3004231311706629,
                    0.0449438202247191,
                    0.024809629083763204,
                    0.10752688172043011,
                    0.2193877551020408,
                    0.10714285714285714,
                    0.23292115081015244,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.07532051282051282,
                    0.18939393939393936,
                    0.007963159794931103,
                    0.3706271480192997,
                    0.8392235580516346,
                    0.39492242595204513,
                    0.4344569288389513,
                    0.6755097027757307,
                    0.7043010752688171,
                    0.21428571428571427,
                    0.14285714285714285,
                    0.26668449423499896,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "apple a day",
                "axe body spray",
                "50 shades of grey"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "apple a day": 1.0262772064601384,
                "axe body spray": 0.0008681112810324458,
                "50 shades of grey": 0.022892494432016932
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.501971774774243,
                    1.1646057540507622,
                    1.3334224711749947
                ],
                "result_count_important_words": [
                    1390000.0,
                    120000.0,
                    1160000.0
                ],
                "wikipedia_search": [
                    0.564516129032258,
                    0.3225806451612903,
                    2.1129032258064515
                ],
                "word_count_appended_bing": [
                    21.0,
                    3.0,
                    4.0
                ],
                "answer_relation_to_question_bing": [
                    2.128787878787879,
                    0.30303030303030304,
                    0.5681818181818181
                ],
                "cosine_similarity_raw": [
                    0.7548282146453857,
                    0.003924560733139515,
                    0.0060905697755515575
                ],
                "result_count_noun_chunks": [
                    1220000.0,
                    101000.0,
                    2750000.0
                ],
                "question_answer_similarity": [
                    11.533798962831497,
                    7.337720591574907,
                    11.113122291862965
                ],
                "word_count_noun_chunks": [
                    9.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    43200000.0,
                    42600000.0,
                    56000000.0
                ],
                "word_count_raw": [
                    61.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    1440000.0,
                    65800.0,
                    7860000.0
                ],
                "answer_relation_to_question": [
                    4.1015078671328675,
                    0.5218895687645687,
                    0.3766025641025641
                ],
                "word_count_appended": [
                    222.0,
                    86.0,
                    84.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "The scientist who famously separated visible light into seven colors is also famous for his work on what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "gravity"
            ],
            "lines": [
                [
                    0.5511363636363636,
                    0.29440993788819875,
                    0.24383712893126194,
                    0.23885856220271673,
                    0.9198045415349239,
                    0.690155713908683,
                    0.9199500802282047,
                    0.9424133358590642,
                    0.7547619047619049,
                    0.915929203539823,
                    0.84375,
                    0.421840127164112,
                    1.0,
                    0.5,
                    1.0
                ],
                [
                    0.38838383838383833,
                    0.5063664596273292,
                    0.39902901501101684,
                    0.27211160095612996,
                    0.0635958608795631,
                    0.032726313011348644,
                    0.0631128543412373,
                    0.04271642356506914,
                    0.1785714285714286,
                    0.05309734513274336,
                    0.09375,
                    0.3317361992956773,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.060479797979797974,
                    0.19922360248447202,
                    0.3571338560577212,
                    0.48902983684115325,
                    0.01659959758551308,
                    0.2771179730799683,
                    0.01693706543055803,
                    0.01487024057586664,
                    0.06666666666666668,
                    0.030973451327433628,
                    0.0625,
                    0.24642367354021066,
                    0.0,
                    0.5,
                    1.0
                ]
            ],
            "fraction_answers": {
                "atomic nuclei": 0.17322838134109872,
                "gravity": 0.659774778546804,
                "steel production": 0.16699684011209726
            },
            "question": "the scientist who famously separated visible light into seven colors is also famous for his work on what?",
            "rate_limited": false,
            "answers": [
                "gravity",
                "atomic nuclei",
                "steel production"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "atomic nuclei": 0.27038105749792696,
                "gravity": 0.4783149177593956,
                "steel production": 0.2705163003427973
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.796561144477008,
                    2.985625793661096,
                    2.217813061861896
                ],
                "result_count_important_words": [
                    25800.0,
                    1770.0,
                    475.0
                ],
                "wikipedia_search": [
                    3.7738095238095237,
                    0.8928571428571428,
                    0.3333333333333333
                ],
                "word_count_appended_bing": [
                    27.0,
                    3.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.177639751552795,
                    2.0254658385093167,
                    0.7968944099378881
                ],
                "cosine_similarity_raw": [
                    0.05208047106862068,
                    0.08522745966911316,
                    0.07627919316291809
                ],
                "result_count_noun_chunks": [
                    1990000.0,
                    90200.0,
                    31400.0
                ],
                "question_answer_similarity": [
                    3.8784078508615494,
                    4.418345985701308,
                    7.940503120422363
                ],
                "word_count_noun_chunks": [
                    3.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    523000.0,
                    24800.0,
                    210000.0
                ],
                "word_count_appended": [
                    207.0,
                    12.0,
                    7.0
                ],
                "answer_relation_to_question": [
                    3.3068181818181817,
                    2.33030303030303,
                    0.36287878787878786
                ],
                "result_count": [
                    25600.0,
                    1770.0,
                    462.0
                ]
            },
            "integer_answers": {
                "atomic nuclei": 2,
                "gravity": 11,
                "steel production": 1
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which Hawaiian island has active volcanoes?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "oahu"
            ],
            "lines": [
                [
                    0.6810360047564584,
                    0.6415289941613588,
                    0.7765701742964894,
                    0.6555825533334825,
                    0.3683431952662722,
                    0.2801807617817947,
                    0.38033261026753434,
                    0.36787564766839376,
                    0.0,
                    0.17365269461077845,
                    0.1574074074074074,
                    0.3631099860108257,
                    0.0392156862745098,
                    0.046511627906976744,
                    -1.0
                ],
                [
                    0.15686021549298332,
                    0.1659617170912951,
                    0.12652543822207332,
                    0.17846040819448183,
                    0.34245562130177515,
                    0.3615235635894125,
                    0.33550253073029646,
                    0.3427091043671355,
                    0.3795045045045045,
                    0.45748502994011975,
                    0.39814814814814814,
                    0.3271644206519448,
                    0.45098039215686275,
                    0.5581395348837209,
                    -1.0
                ],
                [
                    0.16210377975055823,
                    0.19250928874734607,
                    0.09690438748143729,
                    0.1659570384720357,
                    0.28920118343195267,
                    0.35829567462879275,
                    0.2841648590021692,
                    0.28941524796447077,
                    0.6204954954954954,
                    0.3688622754491018,
                    0.4444444444444444,
                    0.3097255933372295,
                    0.5098039215686274,
                    0.3953488372093023,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "maui": 0.32724433066248243,
                "oahu": 0.320516573355926,
                "big island": 0.35223909598159153
            },
            "question": "which hawaiian island has active volcanoes?",
            "rate_limited": false,
            "answers": [
                "big island",
                "maui",
                "oahu"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "maui": 0.3527811042295974,
                "oahu": 0.47104156438740113,
                "big island": 0.24307718127639405
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.452439944043303,
                    1.3086576826077791,
                    1.238902373348918
                ],
                "result_count_important_words": [
                    526000.0,
                    464000.0,
                    393000.0
                ],
                "wikipedia_search": [
                    0.0,
                    1.1385135135135136,
                    1.8614864864864864
                ],
                "word_count_appended_bing": [
                    17.0,
                    43.0,
                    48.0
                ],
                "answer_relation_to_question_bing": [
                    2.566115976645435,
                    0.6638468683651804,
                    0.7700371549893843
                ],
                "cosine_similarity_raw": [
                    0.342020720243454,
                    0.05572493374347687,
                    0.042679090052843094
                ],
                "result_count_noun_chunks": [
                    497000.0,
                    463000.0,
                    391000.0
                ],
                "question_answer_similarity": [
                    4.776122942566872,
                    1.3001396171748638,
                    1.2090486772358418
                ],
                "word_count_noun_chunks": [
                    2.0,
                    23.0,
                    26.0
                ],
                "word_count_raw": [
                    2.0,
                    24.0,
                    17.0
                ],
                "result_count_bing": [
                    868000.0,
                    1120000.0,
                    1110000.0
                ],
                "word_count_appended": [
                    145.0,
                    382.0,
                    308.0
                ],
                "answer_relation_to_question": [
                    2.7241440190258337,
                    0.6274408619719333,
                    0.6484151190022329
                ],
                "result_count": [
                    498000.0,
                    463000.0,
                    391000.0
                ]
            },
            "integer_answers": {
                "maui": 3,
                "oahu": 3,
                "big island": 8
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "The westernmost region of Russia touches which of these countries?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ukraine"
            ],
            "question": "the westernmost region of russia touches which of these countries?",
            "lines": [
                [
                    0.24029761904761904,
                    0.0,
                    0.17769234848328183,
                    0.36487477398228985,
                    0.33978234582829503,
                    0.4265898979324784,
                    0.34026465028355385,
                    0.34050179211469533,
                    0.3450179628064243,
                    0.25802879291251385,
                    0.3177570093457944,
                    0.3414709547248695,
                    0.2608695652173913,
                    0.0,
                    -1.0
                ],
                [
                    0.37017857142857147,
                    0.3333333333333333,
                    0.4027162371973995,
                    0.29955570238502743,
                    0.39903264812575573,
                    0.3768646951059932,
                    0.3996759384283014,
                    0.3978494623655914,
                    0.18828073635765943,
                    0.3289036544850498,
                    0.3177570093457944,
                    0.32214844956942384,
                    0.13043478260869565,
                    1.0,
                    -1.0
                ],
                [
                    0.38952380952380955,
                    0.6666666666666666,
                    0.4195914143193187,
                    0.3355695236326827,
                    0.26118500604594924,
                    0.1965454069615284,
                    0.26005941128814475,
                    0.2616487455197133,
                    0.4667013008359162,
                    0.4130675526024363,
                    0.3644859813084112,
                    0.33638059570570666,
                    0.6086956521739131,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "poland",
                "norway",
                "ukraine"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "ukraine": 0.6620527873974127,
                "poland": 0.5060780224177844,
                "norway": 0.34139306278300413
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7073547736243475,
                    1.6107422478471192,
                    1.6819029785285333
                ],
                "result_count_important_words": [
                    1260000.0,
                    1480000.0,
                    963000.0
                ],
                "wikipedia_search": [
                    1.035053888419273,
                    0.5648422090729783,
                    1.4001039025077486
                ],
                "word_count_appended_bing": [
                    34.0,
                    34.0,
                    39.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    1.0,
                    2.0
                ],
                "cosine_similarity_raw": [
                    0.04583783447742462,
                    0.10388539731502533,
                    0.10823854804039001
                ],
                "result_count_noun_chunks": [
                    1900000.0,
                    2220000.0,
                    1460000.0
                ],
                "question_answer_similarity": [
                    1.413699469063431,
                    1.1606221301481128,
                    1.3001569064799696
                ],
                "word_count_noun_chunks": [
                    6.0,
                    3.0,
                    14.0
                ],
                "result_count_bing": [
                    1630000.0,
                    1440000.0,
                    751000.0
                ],
                "word_count_raw": [
                    0.0,
                    2.0,
                    0.0
                ],
                "result_count": [
                    2810000.0,
                    3300000.0,
                    2160000.0
                ],
                "answer_relation_to_question": [
                    0.9611904761904762,
                    1.4807142857142859,
                    1.5580952380952382
                ],
                "word_count_appended": [
                    233.0,
                    297.0,
                    373.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these states does NOT touch the Mason-Dixon Line?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "west virginia"
            ],
            "lines": [
                [
                    0.417973674453096,
                    0.3055419405681774,
                    0.325086984399501,
                    0.1962000328379232,
                    0.3821039903264813,
                    0.2534562211981567,
                    0.36495271867612294,
                    0.3509719222462203,
                    0.3337639783124365,
                    0.4400998336106489,
                    0.45108695652173914,
                    0.340623102847155,
                    0.4107142857142857,
                    0.4166666666666667,
                    -1.0
                ],
                [
                    0.16905821282906935,
                    0.3687879617883366,
                    0.24179677976234842,
                    0.42186295554480047,
                    0.3778718258766627,
                    0.37327188940092165,
                    0.33008274231678486,
                    0.37365010799136067,
                    0.3209575849994352,
                    0.2512479201331115,
                    0.23369565217391303,
                    0.3214928209828667,
                    0.125,
                    0.08333333333333331,
                    -1.0
                ],
                [
                    0.4129681127178346,
                    0.325670097643486,
                    0.43311623583815057,
                    0.3819370116172764,
                    0.2400241837968561,
                    0.37327188940092165,
                    0.3049645390070922,
                    0.275377969762419,
                    0.3452784366881283,
                    0.3086522462562396,
                    0.31521739130434784,
                    0.3378840761699783,
                    0.4642857142857143,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "tennessee": 0.2830502993587936,
                "delaware": 0.42969860183815084,
                "west virginia": 0.28725109880305555
            },
            "question": "which of these states does not touch the mason-dixon line?",
            "rate_limited": false,
            "answers": [
                "west virginia",
                "delaware",
                "tennessee"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "tennessee": 0.30093639514780524,
                "delaware": 0.1519780347309273,
                "west virginia": 0.4578245978444114
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.5937689715284498,
                    1.785071790171333,
                    1.621159238300217
                ],
                "result_count_important_words": [
                    914000.0,
                    1150000.0,
                    1320000.0
                ],
                "wikipedia_search": [
                    0.6649440867502541,
                    0.7161696600022591,
                    0.6188862532474868
                ],
                "word_count_appended_bing": [
                    9.0,
                    49.0,
                    34.0
                ],
                "answer_relation_to_question_bing": [
                    1.1667483565909353,
                    0.7872722292699805,
                    1.0459794141390844
                ],
                "cosine_similarity_raw": [
                    0.0861324816942215,
                    0.1271471083164215,
                    0.032935597002506256
                ],
                "result_count_noun_chunks": [
                    138000.0,
                    117000.0,
                    208000.0
                ],
                "question_answer_similarity": [
                    5.432002059184015,
                    1.3971054386347532,
                    2.110989023465663
                ],
                "word_count_noun_chunks": [
                    5.0,
                    21.0,
                    2.0
                ],
                "word_count_raw": [
                    1.0,
                    5.0,
                    0.0
                ],
                "result_count_bing": [
                    642000.0,
                    330000.0,
                    330000.0
                ],
                "word_count_appended": [
                    72.0,
                    299.0,
                    230.0
                ],
                "answer_relation_to_question": [
                    0.6562106043752318,
                    2.647534297367445,
                    0.696255098257323
                ],
                "result_count": [
                    58500.0,
                    60600.0,
                    129000.0
                ]
            },
            "integer_answers": {
                "tennessee": 3,
                "delaware": 8,
                "west virginia": 3
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "To help first create Maps, Google acquired what company?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "waze"
            ],
            "lines": [
                [
                    0.26692973058689823,
                    0.5939431913116123,
                    0.10454451558148638,
                    0.0041149995010338536,
                    0.08806203671246463,
                    0.33896260554885405,
                    0.3701828766689869,
                    0.5285001476691589,
                    0.10114341399718757,
                    0.3416149068322981,
                    0.4148148148148148,
                    0.32122514180397893,
                    0.0,
                    0.03125,
                    1.0
                ],
                [
                    0.4924483111712738,
                    0.1919172932330827,
                    0.4769973710494258,
                    1.0679859411524217,
                    0.005668459256346024,
                    0.3220747889022919,
                    0.022337530848573057,
                    0.0020673682247058273,
                    0.7860637062877629,
                    0.06388642413487133,
                    0.014814814814814815,
                    0.3238938901683654,
                    0.0,
                    0.09375,
                    1.0
                ],
                [
                    0.2406219582418281,
                    0.21413951545530494,
                    0.4184581133690878,
                    -0.0721009406534556,
                    0.9062695040311893,
                    0.33896260554885405,
                    0.60747959248244,
                    0.46943248410613525,
                    0.11279287971504952,
                    0.5944986690328306,
                    0.5703703703703704,
                    0.3548809680276557,
                    1.0,
                    0.875,
                    1.0
                ]
            ],
            "fraction_answers": {
                "mapquest": 0.25037774150205533,
                "waze": 0.4736289799805207,
                "where 2 technologies": 0.2759932785174239
            },
            "question": "to help first create maps, google acquired what company?",
            "rate_limited": false,
            "answers": [
                "mapquest",
                "where 2 technologies",
                "waze"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "mapquest": 0.2920060920920267,
                "waze": 0.3772338305751047,
                "where 2 technologies": 0.11348671013722832
            },
            "integer_answers": {
                "mapquest": 3,
                "waze": 7,
                "where 2 technologies": 4
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9273508508238735,
                    1.9433633410101925,
                    2.129285808165934
                ],
                "result_count_important_words": [
                    117000.0,
                    7060.0,
                    192000.0
                ],
                "wikipedia_search": [
                    0.6068604839831254,
                    4.716382237726577,
                    0.6767572782902971
                ],
                "word_count_appended_bing": [
                    56.0,
                    2.0,
                    77.0
                ],
                "answer_relation_to_question_bing": [
                    3.563659147869674,
                    1.1515037593984963,
                    1.2848370927318296
                ],
                "cosine_similarity_raw": [
                    0.06674104928970337,
                    0.3045143485069275,
                    0.26714298129081726
                ],
                "result_count_noun_chunks": [
                    1700000.0,
                    6650.0,
                    1510000.0
                ],
                "question_answer_similarity": [
                    0.036335155775304884,
                    9.430240642279387,
                    -0.6366462279111147
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    4.0
                ],
                "result_count_bing": [
                    56200000.0,
                    53400000.0,
                    56200000.0
                ],
                "word_count_raw": [
                    1.0,
                    3.0,
                    28.0
                ],
                "result_count": [
                    103000.0,
                    6630.0,
                    1060000.0
                ],
                "answer_relation_to_question": [
                    1.6015783835213893,
                    2.9546898670276427,
                    1.4437317494509687
                ],
                "word_count_appended": [
                    385.0,
                    72.0,
                    670.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "In the classic novel \u201cA Confederacy of Dunces,\u201d where does the main character work?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "petting zoo"
            ],
            "question": "in the classic novel \u201ca confederacy of dunces,\u201d where does the main character work?",
            "lines": [
                [
                    0.45686274509803926,
                    0.3333333333333333,
                    0.39658234111834495,
                    0.2521594144739166,
                    0.5157894736842106,
                    0.4022346368715084,
                    0.027594665031427257,
                    0.028156073561446927,
                    0.0,
                    0.4583333333333333,
                    0.25,
                    0.3512417111144043,
                    0,
                    0,
                    3.0
                ],
                [
                    0.19215686274509802,
                    0.3333333333333333,
                    0.286029679333732,
                    0.33213411016006916,
                    0.0,
                    0.26256983240223464,
                    0.9535489805304308,
                    0.9706435885656703,
                    0.13020833333333331,
                    0.125,
                    0.25,
                    0.08396261612730603,
                    0,
                    0,
                    3.0
                ],
                [
                    0.3509803921568627,
                    0.3333333333333333,
                    0.317387979547923,
                    0.4157064753660142,
                    0.4842105263157895,
                    0.33519553072625696,
                    0.01885635443814196,
                    0.0012003378728827373,
                    0.8697916666666666,
                    0.4166666666666667,
                    0.5,
                    0.5647956727582898,
                    0,
                    0,
                    3.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "petting zoo",
                "canning facility",
                "pants factory"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "petting zoo": 0.4656809683578133,
                "pants factory": 0.41861011801300607,
                "canning facility": 0.10627979946466078
            },
            "categorical_data": {
                "question_type": 3
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.45869197780083,
                    0.5877383128911422,
                    3.953569709308028
                ],
                "result_count_important_words": [
                    1800.0,
                    62200.0,
                    1230.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.5208333333333333,
                    3.4791666666666665
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    4.0
                ],
                "answer_relation_to_question_bing": [
                    0.6666666666666666,
                    0.6666666666666666,
                    0.6666666666666666
                ],
                "cosine_similarity_raw": [
                    0.039399608969688416,
                    0.028416438028216362,
                    0.031531818211078644
                ],
                "result_count_noun_chunks": [
                    1900.0,
                    65500.0,
                    81.0
                ],
                "question_answer_similarity": [
                    3.7646648790687323,
                    4.9586632419377565,
                    6.206373738124967
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    43200.0,
                    28200.0,
                    36000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    11.0,
                    3.0,
                    10.0
                ],
                "answer_relation_to_question": [
                    2.2843137254901964,
                    0.9607843137254901,
                    1.7549019607843137
                ],
                "result_count": [
                    49.0,
                    0,
                    46.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Where do pearls come from?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "oysters"
            ],
            "question": "where do pearls come from?",
            "lines": [
                [
                    0.3904761904761905,
                    0.5,
                    0.8349200331534057,
                    0.34577023082129754,
                    0.47972872141756884,
                    0.01999560536145902,
                    0.4685408299866131,
                    0.49645390070921985,
                    0.75,
                    0.36780772032046616,
                    0.41304347826086957,
                    0.42582574672174855,
                    0.3333333333333333,
                    1.0,
                    3.0
                ],
                [
                    0.4666666666666667,
                    0.0,
                    0.11328980866552955,
                    0.27753771567523744,
                    7.335043719389893e-05,
                    0.5229619863766205,
                    0.26282909415439537,
                    0.2473404255319149,
                    0.16666666666666666,
                    0.42534595775673706,
                    0.375,
                    0.26913732141748103,
                    0.6666666666666666,
                    0.0,
                    3.0
                ],
                [
                    0.14285714285714285,
                    0.5,
                    0.05179015818106482,
                    0.3766920535034651,
                    0.5201979281452372,
                    0.45704240826192044,
                    0.2686300758589915,
                    0.25620567375886527,
                    0.08333333333333333,
                    0.20684632192279678,
                    0.21195652173913043,
                    0.3050369318607704,
                    0.0,
                    0.0,
                    3.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "oysters",
                "olives",
                "oranges"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "olives": 0.3390658906188082,
                "oysters": 0.9276932775067643,
                "oranges": 0.07671718731642062
            },
            "categorical_data": {
                "question_type": 3
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.8516514934434971,
                    0.5382746428349621,
                    0.6100738637215408
                ],
                "result_count_important_words": [
                    1050000.0,
                    589000.0,
                    602000.0
                ],
                "wikipedia_search": [
                    1.5,
                    0.3333333333333333,
                    0.16666666666666666
                ],
                "word_count_appended_bing": [
                    76.0,
                    69.0,
                    39.0
                ],
                "answer_relation_to_question_bing": [
                    1.0,
                    0.0,
                    1.0
                ],
                "cosine_similarity_raw": [
                    0.30714040994644165,
                    0.04167570173740387,
                    0.019051944836974144
                ],
                "result_count_noun_chunks": [
                    1120000.0,
                    558000.0,
                    578000.0
                ],
                "question_answer_similarity": [
                    1.0457068607211113,
                    0.8393524587154388,
                    1.13922318816185
                ],
                "word_count_noun_chunks": [
                    3.0,
                    6.0,
                    0.0
                ],
                "word_count_raw": [
                    26.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    4550000.0,
                    119000000.0,
                    104000000.0
                ],
                "result_count": [
                    569000.0,
                    87.0,
                    617000.0
                ],
                "answer_relation_to_question": [
                    0.780952380952381,
                    0.9333333333333333,
                    0.2857142857142857
                ],
                "word_count_appended": [
                    505.0,
                    584.0,
                    284.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these is typically used to find the longest side of a right triangle?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "pythagorean theorem"
            ],
            "question": "which of these is typically used to find the longest side of a right triangle?",
            "lines": [
                [
                    0.15295413437161764,
                    0.24315738025415445,
                    0.05293371565966835,
                    0.2291664559165872,
                    0.16338028169014085,
                    0.08608414239482201,
                    0.26910435497124074,
                    0.11600928074245939,
                    0.27872382535724183,
                    0.2914798206278027,
                    0.05357142857142857,
                    0.2863558632400651,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.7205521817772387,
                    0.5896532529597046,
                    0.9169532577286387,
                    0.15047140724898658,
                    0.6028169014084507,
                    0.7993527508090615,
                    0.5299917830731307,
                    0.7018561484918794,
                    0.6127617464275819,
                    0.5112107623318386,
                    0.8928571428571429,
                    0.38988402539611505,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.12649368385114373,
                    0.167189366786141,
                    0.030113026611692996,
                    0.6203621368344262,
                    0.23380281690140844,
                    0.1145631067961165,
                    0.2009038619556286,
                    0.18213457076566125,
                    0.10851442821517635,
                    0.19730941704035873,
                    0.05357142857142857,
                    0.3237601113638198,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "binomial theorem",
                "pythagorean theorem",
                "quadratic formula"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "binomial theorem": 0.009980100410429005,
                "pythagorean theorem": 1.058149057801674,
                "quadratic formula": -0.06309678101846747
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.4317793162003256,
                    1.9494201269805753,
                    1.618800556819099
                ],
                "result_count_important_words": [
                    65500.0,
                    129000.0,
                    48900.0
                ],
                "wikipedia_search": [
                    1.393619126786209,
                    3.0638087321379093,
                    0.5425721410758817
                ],
                "answer_relation_to_question": [
                    0.6118165374864706,
                    2.8822087271089547,
                    0.5059747354045749
                ],
                "word_count_appended_bing": [
                    3.0,
                    50.0,
                    3.0
                ],
                "answer_relation_to_question_bing": [
                    0.9726295210166178,
                    2.3586130118388184,
                    0.668757467144564
                ],
                "cosine_similarity_raw": [
                    0.029980968683958054,
                    0.5193504095077515,
                    0.017055626958608627
                ],
                "result_count_noun_chunks": [
                    20000.0,
                    121000.0,
                    31400.0
                ],
                "question_answer_similarity": [
                    1.8224437674507499,
                    1.1966222422197461,
                    4.933423198061064
                ],
                "word_count_noun_chunks": [
                    0.0,
                    26.0,
                    0.0
                ],
                "result_count_bing": [
                    266000.0,
                    2470000.0,
                    354000.0
                ],
                "word_count_raw": [
                    0.0,
                    9.0,
                    0.0
                ],
                "result_count": [
                    11600.0,
                    42800.0,
                    16600.0
                ],
                "word_count_appended": [
                    65.0,
                    114.0,
                    44.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What was the first popular home video game?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "pong"
            ],
            "lines": [
                [
                    0.4077353098048687,
                    0.21890547263681592,
                    0.09464189878389216,
                    0.15572708926335596,
                    0.013999321423829826,
                    0.4220985691573927,
                    0.01008233910267182,
                    0.013207329056123924,
                    0.09539914521327161,
                    0.15081521739130435,
                    0.13402061855670103,
                    0.3080632980973424,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.35019724461351126,
                    0.22307069304639593,
                    0.8519627478644958,
                    0.10277820441426974,
                    0.9647582941185148,
                    0.15580286168521462,
                    0.974626113258276,
                    0.9667071267556788,
                    0.7075650244795225,
                    0.7065217391304348,
                    0.7010309278350515,
                    0.43682004443533035,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.24206744558162002,
                    0.5580238343167881,
                    0.053395353351612086,
                    0.7414947063223744,
                    0.02124238445765537,
                    0.4220985691573927,
                    0.01529154763905226,
                    0.020085544188197215,
                    0.19703583030720578,
                    0.14266304347826086,
                    0.16494845360824742,
                    0.2551166574673273,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "tekken 2": 0.14462111489196933,
                "pong": 0.652988644402621,
                "half-life 3": 0.2023902407054095
            },
            "question": "what was the first popular home video game?",
            "rate_limited": false,
            "answers": [
                "tekken 2",
                "pong",
                "half-life 3"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "tekken 2": 0.09131179726462739,
                "pong": 0.6753844124645527,
                "half-life 3": 0.08928949855617384
            },
            "integer_answers": {
                "tekken 2": 2,
                "pong": 10,
                "half-life 3": 2
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2322531923893694,
                    1.7472801777413212,
                    1.020466629869309
                ],
                "result_count_important_words": [
                    120000.0,
                    11600000.0,
                    182000.0
                ],
                "wikipedia_search": [
                    0.38159658085308645,
                    2.83026009791809,
                    0.7881433212288231
                ],
                "word_count_appended_bing": [
                    13.0,
                    68.0,
                    16.0
                ],
                "answer_relation_to_question_bing": [
                    0.8756218905472637,
                    0.8922827721855837,
                    2.2320953372671526
                ],
                "cosine_similarity_raw": [
                    0.028733020648360252,
                    0.25865355134010315,
                    0.016210682690143585
                ],
                "result_count_noun_chunks": [
                    91400.0,
                    6690000.0,
                    139000.0
                ],
                "question_answer_similarity": [
                    2.11887746816501,
                    1.3984363451600075,
                    10.089037388563156
                ],
                "word_count_noun_chunks": [
                    0.0,
                    31.0,
                    0.0
                ],
                "result_count_bing": [
                    53100000.0,
                    19600000.0,
                    53100000.0
                ],
                "word_count_raw": [
                    0.0,
                    11.0,
                    0.0
                ],
                "result_count": [
                    94900.0,
                    6540000.0,
                    144000.0
                ],
                "answer_relation_to_question": [
                    1.6309412392194749,
                    1.400788978454045,
                    0.9682697823264801
                ],
                "word_count_appended": [
                    111.0,
                    520.0,
                    105.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Susan B. Anthony was arrested for voting in an election won by whom?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ulysses s. grant"
            ],
            "question": "susan b. anthony was arrested for voting in an election won by whom?",
            "lines": [
                [
                    0.3653225806451613,
                    0.10185185185185185,
                    0.2692930315265907,
                    0.2534064848888804,
                    0.6635718186783178,
                    0.4329896907216495,
                    0.5602152072003509,
                    0.08982855890891166,
                    0.397235122119023,
                    0.2482758620689655,
                    0.21428571428571427,
                    0.33865960668268374,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.5164650537634409,
                    0.8018518518518518,
                    0.6035501521591544,
                    0.3131376589306931,
                    0.03604587657018023,
                    0.38291605301914583,
                    0.002711230200884717,
                    0.006408500848989428,
                    0.13021765825873394,
                    0.2689655172413793,
                    0.2857142857142857,
                    0.31084780638109066,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.11821236559139785,
                    0.0962962962962963,
                    0.12715681631425485,
                    0.4334558561804265,
                    0.3003823047515019,
                    0.18409425625920472,
                    0.43707356259876434,
                    0.9037629402420989,
                    0.472547219622243,
                    0.4827586206896552,
                    0.5,
                    0.3504925869362257,
                    1.0,
                    1.0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "answers": [
                "abraham lincoln",
                "rutherford b. hayes",
                "ulysses s. grant"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "cosine_similarity_raw",
                "question_answer_similarity",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "rutherford b. hayes": 0.3247963043214433,
                "abraham lincoln": 0.08865253276242917,
                "ulysses s. grant": 0.7918594311911306
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.370617246778786,
                    2.1759346446676346,
                    2.4534481085535798
                ],
                "result_count_important_words": [
                    10600000.0,
                    51300.0,
                    8270000.0
                ],
                "wikipedia_search": [
                    1.9861756105951152,
                    0.6510882912936696,
                    2.362736098111215
                ],
                "answer_relation_to_question": [
                    1.4612903225806453,
                    2.0658602150537635,
                    0.4728494623655914
                ],
                "answer_relation_to_question_bing": [
                    0.2037037037037037,
                    1.6037037037037036,
                    0.1925925925925926
                ],
                "word_count_appended": [
                    36.0,
                    39.0,
                    70.0
                ],
                "cosine_similarity_raw": [
                    0.11434347182512283,
                    0.256271094083786,
                    0.053991563618183136
                ],
                "result_count_noun_chunks": [
                    1640000.0,
                    117000.0,
                    16500000.0
                ],
                "question_answer_similarity": [
                    3.4871994895511307,
                    4.309177347458899,
                    5.964910649694502
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    3.0
                ],
                "result_count_bing": [
                    294000.0,
                    260000.0,
                    125000.0
                ],
                "result_count": [
                    243000.0,
                    13200.0,
                    110000.0
                ],
                "word_count_appended_bing": [
                    3.0,
                    4.0,
                    7.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    }
}