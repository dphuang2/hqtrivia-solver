{
    "Which of these versions of the Old Testament typically contains the most books?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "catholic"
            ],
            "lines": [
                [
                    0.07977755308392315,
                    0.20067663817663817,
                    0.85,
                    1.0,
                    0.5655644241733181,
                    0.5761872608597794,
                    0.5773112636253532,
                    0.3501628664495114,
                    0.28238038277511956,
                    0.5523255813953488,
                    0.5739130434782609,
                    0.33269305816111916,
                    0.6927374301675978,
                    0.6861313868613139,
                    -1.0
                ],
                [
                    0.424469160768453,
                    0.42984330484330485,
                    0.15,
                    0.0,
                    0.37856328392246297,
                    0.2655863155525546,
                    0.36980218005652,
                    0.46579804560260585,
                    0.2269554568238779,
                    0.3918604651162791,
                    0.4,
                    0.33714086075573,
                    0.29608938547486036,
                    0.29927007299270075,
                    -1.0
                ],
                [
                    0.4957532861476238,
                    0.3694800569800569,
                    0.0,
                    0.0,
                    0.055872291904218926,
                    0.158226423587666,
                    0.05288655631812677,
                    0.18403908794788273,
                    0.4906641604010026,
                    0.05581395348837209,
                    0.02608695652173913,
                    0.3301660810831508,
                    0.0111731843575419,
                    0.014598540145985401,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "catholic": 0.5228472063719488,
                "protestant": 0.31681275227923916,
                "eastern orthodox": 0.16034004134881194
            },
            "question": "which of these versions of the old testament typically contains the most books?",
            "rate_limited": false,
            "answers": [
                "catholic",
                "protestant",
                "eastern orthodox"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "catholic": 0.567507377006703,
                "protestant": 0.29877619455105,
                "eastern orthodox": 0.26827335294492044
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.996158348966715,
                    2.02284516453438,
                    1.980996486498905
                ],
                "result_count_important_words": [
                    1430000.0,
                    916000.0,
                    131000.0
                ],
                "wikipedia_search": [
                    1.411901913875598,
                    1.1347772841193895,
                    2.453320802005013
                ],
                "word_count_appended_bing": [
                    66.0,
                    46.0,
                    3.0
                ],
                "answer_relation_to_question_bing": [
                    0.8027065527065527,
                    1.7193732193732194,
                    1.4779202279202277
                ],
                "question_related_to_answer": [
                    1.7,
                    0.3,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    4300000.0,
                    5720000.0,
                    2260000.0
                ],
                "word_count_noun_chunks": [
                    124.0,
                    53.0,
                    2.0
                ],
                "word_count_raw": [
                    94.0,
                    41.0,
                    2.0
                ],
                "result_count_bing": [
                    2560000.0,
                    1180000.0,
                    703000.0
                ],
                "word_count_appended": [
                    475.0,
                    337.0,
                    48.0
                ],
                "answer_relation_to_question": [
                    0.39888776541961574,
                    2.122345803842265,
                    2.478766430738119
                ],
                "result_count": [
                    2480000.0,
                    1660000.0,
                    245000.0
                ]
            },
            "integer_answers": {
                "catholic": 9,
                "protestant": 3,
                "eastern orthodox": 2
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "The material that forms images in an Etch A Sketch is also the main component in which item?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "u.s. nickels"
            ],
            "lines": [
                [
                    0.35748653345002235,
                    0.3008960573476702,
                    0,
                    0,
                    0.02666666666666667,
                    0.033246414602346806,
                    0.019417475728155338,
                    0.028169014084507043,
                    0.5069444444444444,
                    0.20833333333333334,
                    0.2857142857142857,
                    0.20863857202875435,
                    0.3333333333333333,
                    0.4,
                    -1.0
                ],
                [
                    0.4775951829704365,
                    0.42374551971326174,
                    0,
                    0,
                    0.4533333333333333,
                    0.026727509778357236,
                    0.39805825242718446,
                    0.5352112676056338,
                    0.23333333333333334,
                    0.25,
                    0.2857142857142857,
                    0.33054079653621643,
                    0.3333333333333333,
                    0.2,
                    -1.0
                ],
                [
                    0.16491828357954122,
                    0.27535842293906804,
                    0,
                    0,
                    0.52,
                    0.940026075619296,
                    0.5825242718446602,
                    0.43661971830985913,
                    0.25972222222222224,
                    0.5416666666666666,
                    0.42857142857142855,
                    0.4608206314350292,
                    0.3333333333333333,
                    0.4,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "zinc supplement tablets": 0.22573717756112666,
                "soda cans": 0.44529675454342527,
                "u.s. nickels": 0.32896606789544797
            },
            "question": "the material that forms images in an etch a sketch is also the main component in which item?",
            "rate_limited": false,
            "answers": [
                "zinc supplement tablets",
                "u.s. nickels",
                "soda cans"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "zinc supplement tablets": -0.0007460387518006817,
                "soda cans": 0.2520077516758037,
                "u.s. nickels": 0.2930052420301475
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6691085762300348,
                    2.6443263722897314,
                    3.6865650514802337
                ],
                "result_count_important_words": [
                    2.0,
                    41.0,
                    60.0
                ],
                "wikipedia_search": [
                    3.548611111111111,
                    1.6333333333333333,
                    1.8180555555555555
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    3.0
                ],
                "answer_relation_to_question_bing": [
                    1.8053763440860213,
                    2.5424731182795703,
                    1.6521505376344083
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    2.0,
                    38.0,
                    31.0
                ],
                "word_count_noun_chunks": [
                    1.0,
                    1.0,
                    1.0
                ],
                "word_count_raw": [
                    2.0,
                    1.0,
                    2.0
                ],
                "result_count_bing": [
                    2550.0,
                    2050.0,
                    72100.0
                ],
                "word_count_appended": [
                    5.0,
                    6.0,
                    13.0
                ],
                "answer_relation_to_question": [
                    2.1449192007001336,
                    2.8655710978226185,
                    0.9895097014772471
                ],
                "result_count": [
                    2.0,
                    34.0,
                    39.0
                ]
            },
            "integer_answers": {
                "zinc supplement tablets": 3,
                "soda cans": 6,
                "u.s. nickels": 3
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Who was NOT a wife of Henry VIII?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "catherine of york"
            ],
            "lines": [
                [
                    0.2925683585559209,
                    0.24002849002849003,
                    0,
                    0.5,
                    0.2442147610545598,
                    0.48843023255813955,
                    0.244214390367969,
                    0.07600797105014423,
                    0.4230769230769231,
                    0.26322418136020154,
                    0.24647887323943662,
                    0.2806215398804228,
                    0.13636363636363635,
                    0.28448275862068967,
                    0.0
                ],
                [
                    0.365034495291473,
                    0.4568420584045584,
                    0,
                    0.5,
                    0.49997681266955735,
                    0.023255813953488358,
                    0.49997753724108046,
                    0.4999906001767167,
                    0.3205128205128205,
                    0.47103274559193953,
                    0.47887323943661975,
                    0.44906477318830934,
                    0.5,
                    0.5,
                    0.0
                ],
                [
                    0.34239714615260614,
                    0.3031294515669516,
                    0,
                    0.0,
                    0.25580842627588285,
                    0.4883139534883721,
                    0.25580807239095055,
                    0.42400142877313907,
                    0.2564102564102564,
                    0.26574307304785894,
                    0.27464788732394363,
                    0.2703136869312679,
                    0.36363636363636365,
                    0.21551724137931033,
                    0.0
                ]
            ],
            "fraction_answers": {
                "catherine of york": 0.1439137082359133,
                "catherine parr": 0.42773659751437954,
                "catherine howard": 0.42834969424970715
            },
            "question": "who was not a wife of henry viii?",
            "rate_limited": false,
            "answers": [
                "catherine parr",
                "catherine of york",
                "catherine howard"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "catherine of york": 0.6882123662632578,
                "catherine parr": 0.08156239389585485,
                "catherine howard": -0.015074720992998733
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3162707607174633,
                    0.30561136087014384,
                    1.3781178784123929
                ],
                "result_count_important_words": [
                    353000.0,
                    31.0,
                    337000.0
                ],
                "wikipedia_search": [
                    0.46153846153846156,
                    1.0769230769230769,
                    1.4615384615384617
                ],
                "word_count_appended_bing": [
                    36.0,
                    3.0,
                    32.0
                ],
                "answer_relation_to_question_bing": [
                    1.5598290598290598,
                    0.2589476495726496,
                    1.1812232905982905
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_noun_chunks": [
                    2120000.0,
                    47.0,
                    380000.0
                ],
                "word_count_noun_chunks": [
                    8.0,
                    0.0,
                    3.0
                ],
                "word_count_raw": [
                    25.0,
                    0.0,
                    33.0
                ],
                "result_count_bing": [
                    1990000.0,
                    82000000.0,
                    2010000.0
                ],
                "word_count_appended": [
                    188.0,
                    23.0,
                    186.0
                ],
                "answer_relation_to_question": [
                    1.2445898486644749,
                    0.809793028251162,
                    0.9456171230843632
                ],
                "result_count": [
                    353000.0,
                    32.0,
                    337000.0
                ]
            },
            "integer_answers": {
                "catherine of york": 1,
                "catherine parr": 8,
                "catherine howard": 4
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "How many U.S. state names are only four letters long?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "50 states"
            ],
            "lines": [
                [
                    0.28509455456823873,
                    0.19297052154195007,
                    0.0,
                    0.8333333333333334,
                    0.17679431323091035,
                    0.23648983639067922,
                    0.2422062350119904,
                    0.23651844843897823,
                    0.3599468488990129,
                    0.2112676056338028,
                    0.2222222222222222,
                    0.31055081314158034,
                    0.0,
                    0,
                    5.0
                ],
                [
                    0.5024424698108909,
                    0.6201625094482237,
                    1.0,
                    0.16666666666666666,
                    0.5182297638156386,
                    0.2553296975706495,
                    0.4988009592326139,
                    0.5045726900031536,
                    0.5162743609212858,
                    0.5704225352112676,
                    0.5555555555555556,
                    0.36199566672181693,
                    1.0,
                    0,
                    5.0
                ],
                [
                    0.21246297562087035,
                    0.18686696900982616,
                    0.0,
                    0.0,
                    0.30497592295345105,
                    0.5081804660386713,
                    0.2589928057553957,
                    0.2589088615578682,
                    0.12377879017970134,
                    0.21830985915492956,
                    0.2222222222222222,
                    0.3274535201366027,
                    0.0,
                    0,
                    5.0
                ]
            ],
            "fraction_answers": {
                "12 states": 0.25441497941636143,
                "50 states": 0.5438809903813663,
                "3 states": 0.20170403020227218
            },
            "question": "how many u.s. state names are only four letters long?",
            "rate_limited": false,
            "answers": [
                "12 states",
                "50 states",
                "3 states"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "12 states": 0.3194444440305233,
                "50 states": 0.3952380970830009,
                "3 states": 0.29999999900658925
            },
            "integer_answers": {
                "12 states": 1,
                "50 states": 11,
                "3 states": 1
            },
            "categorical_data": {
                "question_type": 5
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.5527540657079018,
                    1.8099783336090847,
                    1.6372676006830136
                ],
                "result_count_important_words": [
                    1010000.0,
                    2080000.0,
                    1080000.0
                ],
                "wikipedia_search": [
                    1.0798405466970387,
                    1.5488230827638572,
                    0.371336370539104
                ],
                "word_count_appended_bing": [
                    2.0,
                    5.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.5789115646258503,
                    1.8604875283446711,
                    0.5606009070294785
                ],
                "question_related_to_answer": [
                    0.0,
                    1.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.8333333333333334,
                    0.16666666666666666,
                    0.0
                ],
                "result_count_noun_chunks": [
                    750000.0,
                    1600000.0,
                    821000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    5.0,
                    0.0
                ],
                "result_count_bing": [
                    95400000.0,
                    103000000.0,
                    205000000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    771000.0,
                    2260000.0,
                    1330000.0
                ],
                "answer_relation_to_question": [
                    1.140378218272955,
                    2.0097698792435637,
                    0.8498519024834814
                ],
                "word_count_appended": [
                    30.0,
                    81.0,
                    31.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What play has the stage direction, \u201cEnter a Messenger, with two heads and a hand\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "oedipus rex",
                "titus andronicus",
                "agamemnon"
            ],
            "lines": [
                [
                    0.33456334601071447,
                    0.37173370555804314,
                    1.0,
                    0,
                    0.03225806451612903,
                    0.0884504167375404,
                    0.011970359110773323,
                    0.16753817193055845,
                    0.22264552781439853,
                    0.037037037037037035,
                    0.05660377358490566,
                    0.1294206718287769,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.4689220136588557,
                    0.25967959503037047,
                    0.0,
                    0,
                    0.8225806451612904,
                    0.04972500992232239,
                    0.02058395085185889,
                    0.4078644635013596,
                    0.7545288609212119,
                    0.6296296296296297,
                    0.2641509433962264,
                    0.6759329388173737,
                    0.9545454545454546,
                    1.0,
                    1.0
                ],
                [
                    0.19651464033042979,
                    0.3685866994115865,
                    0.0,
                    0,
                    0.14516129032258066,
                    0.8618245733401372,
                    0.9674456900373678,
                    0.424597364568082,
                    0.022825611264389683,
                    0.3333333333333333,
                    0.6792452830188679,
                    0.19464638935384945,
                    0.045454545454545456,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "oedipus rex": 0.18863239031760587,
                "agamemnon": 0.32612580157193605,
                "titus andronicus": 0.4852418081104579
            },
            "question": "what play has the stage direction, \u201center a messenger, with two heads and a hand\u201d?",
            "rate_limited": false,
            "answers": [
                "oedipus rex",
                "titus andronicus",
                "agamemnon"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "oedipus rex": 0.0,
                "agamemnon": 0.0,
                "titus andronicus": 0.0
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.9059447028014384,
                    4.731530571721616,
                    1.3625247254769461
                ],
                "result_count_important_words": [
                    7560.0,
                    13000.0,
                    611000.0
                ],
                "wikipedia_search": [
                    1.5585186947007896,
                    5.281702026448483,
                    0.1597792788507278
                ],
                "word_count_appended_bing": [
                    3.0,
                    14.0,
                    36.0
                ],
                "answer_relation_to_question_bing": [
                    2.2304022333482587,
                    1.5580775701822227,
                    2.211520196469519
                ],
                "question_related_to_answer": [
                    1.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    801.0,
                    1950.0,
                    2030.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    21.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    25.0,
                    0.0
                ],
                "result_count_bing": [
                    156000.0,
                    87700.0,
                    1520000.0
                ],
                "word_count_appended": [
                    3.0,
                    51.0,
                    27.0
                ],
                "answer_relation_to_question": [
                    2.007380076064287,
                    2.8135320819531344,
                    1.1790878419825788
                ],
                "result_count": [
                    2.0,
                    51.0,
                    9.0
                ]
            },
            "integer_answers": {
                "oedipus rex": 2,
                "agamemnon": 4,
                "titus andronicus": 7
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Wrestling legend Ric Flair entered the ring to the same music used in what classic film?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "star wars: episode iv",
                "2001: a space odyssey",
                "back to the future"
            ],
            "lines": [
                [
                    0.16631381727499625,
                    0.275450796757059,
                    0,
                    0,
                    0.2152777777777778,
                    0.06958187041174593,
                    0.17647058823529413,
                    0.1986754966887417,
                    0.732760827637417,
                    0.20588235294117646,
                    0.25,
                    0.22333253037086426,
                    0.0,
                    0,
                    1.0
                ],
                [
                    0.5279174280252499,
                    0.3926125244618396,
                    0,
                    0,
                    0.3472222222222222,
                    0.14203638684966485,
                    0.32941176470588235,
                    0.4105960264900662,
                    0.0445646010268299,
                    0.35294117647058826,
                    0.5,
                    0.36241699768589225,
                    1.0,
                    0,
                    1.0
                ],
                [
                    0.3057687546997538,
                    0.3319366787811015,
                    0,
                    0,
                    0.4375,
                    0.7883817427385892,
                    0.49411764705882355,
                    0.39072847682119205,
                    0.22267457133575316,
                    0.4411764705882353,
                    0.25,
                    0.41425047194324344,
                    0.0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "star wars: episode iv": 0.22852236891773386,
                "back to the future": 0.370594073996972,
                "2001: a space odyssey": 0.4008835570852941
            },
            "question": "wrestling legend ric flair entered the ring to the same music used in what classic film?",
            "rate_limited": false,
            "answers": [
                "star wars: episode iv",
                "2001: a space odyssey",
                "back to the future"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "star wars: episode iv": 0.0,
                "back to the future": 0.0,
                "2001: a space odyssey": 0.0
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.0099927733377783,
                    3.26175297917303,
                    3.728254247489191
                ],
                "result_count_important_words": [
                    30.0,
                    56.0,
                    84.0
                ],
                "wikipedia_search": [
                    5.129325793461918,
                    0.31195220718780925,
                    1.5587219993502717
                ],
                "word_count_appended_bing": [
                    2.0,
                    4.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.1018031870282359,
                    1.5704500978473581,
                    1.3277467151244058
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    30.0,
                    62.0,
                    59.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    2180.0,
                    4450.0,
                    24700.0
                ],
                "word_count_appended": [
                    7.0,
                    12.0,
                    15.0
                ],
                "answer_relation_to_question": [
                    0.9978829036499774,
                    3.1675045681514993,
                    1.8346125281985228
                ],
                "result_count": [
                    31.0,
                    50.0,
                    63.0
                ]
            },
            "integer_answers": {
                "star wars: episode iv": 1,
                "back to the future": 5,
                "2001: a space odyssey": 5
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "The man famously known as the Science Guy holds a patent for which of these items?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "pulse rate monitor",
                "mechanical pencil",
                "ballet shoe"
            ],
            "lines": [
                [
                    0.3508838383838384,
                    0.3904320987654321,
                    0,
                    0,
                    0.004294344566562341,
                    0.4804015296367113,
                    0.011897498474679682,
                    0.0025503565294331962,
                    0.23822222222222225,
                    0.16216216216216217,
                    0.4,
                    0.21997105209640536,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.3041666666666667,
                    0.3114197530864198,
                    0,
                    0,
                    0.9898828153431837,
                    0.036806883365200764,
                    0.9731543624161074,
                    0.9941185655545725,
                    0.7137777777777778,
                    0.5405405405405406,
                    0.4,
                    0.49169969642828204,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.34494949494949495,
                    0.2981481481481481,
                    0,
                    0,
                    0.005822840090254021,
                    0.48279158699808794,
                    0.014948139109212935,
                    0.003331077915994379,
                    0.048,
                    0.2972972972972973,
                    0.2,
                    0.28832925147531263,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "pulse rate monitor": 0.22608151028374474,
                "mechanical pencil": 0.5755567061178751,
                "ballet shoe": 0.19836178359838028
            },
            "question": "the man famously known as the science guy holds a patent for which of these items?",
            "rate_limited": false,
            "answers": [
                "pulse rate monitor",
                "mechanical pencil",
                "ballet shoe"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "pulse rate monitor": 0.0,
                "mechanical pencil": 0.0,
                "ballet shoe": 0.0
            },
            "integer_answers": {
                "pulse rate monitor": 3,
                "mechanical pencil": 6,
                "ballet shoe": 1
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7597684167712428,
                    3.9335975714262563,
                    2.306634011802501
                ],
                "result_count_important_words": [
                    78.0,
                    6380.0,
                    98.0
                ],
                "wikipedia_search": [
                    1.1911111111111112,
                    3.568888888888889,
                    0.24
                ],
                "word_count_appended_bing": [
                    4.0,
                    4.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.1712962962962963,
                    0.9342592592592593,
                    0.8944444444444444
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    49.0,
                    19100.0,
                    64.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    201000.0,
                    15400.0,
                    202000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    59.0,
                    13600.0,
                    80.0
                ],
                "answer_relation_to_question": [
                    2.1053030303030305,
                    1.8250000000000002,
                    2.06969696969697
                ],
                "word_count_appended": [
                    12.0,
                    40.0,
                    22.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What advertising mascot wears epaulettes?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "cap'n crunch"
            ],
            "lines": [
                [
                    0.17065390749601275,
                    0.125,
                    0.0,
                    0,
                    0.08571428571428572,
                    0.041263788642244316,
                    0.08333333333333333,
                    0.07079646017699115,
                    0.08333333333333333,
                    0.14942528735632185,
                    0.09734513274336283,
                    0.19266601029850886,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.4481658692185008,
                    0.75,
                    1.0,
                    0,
                    0.580952380952381,
                    0.7081574288437968,
                    0.5648148148148148,
                    0.6283185840707964,
                    0.21794871794871795,
                    0.3448275862068966,
                    0.35398230088495575,
                    0.4172632669429408,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.38118022328548645,
                    0.125,
                    0.0,
                    0,
                    0.3333333333333333,
                    0.2505787825139589,
                    0.35185185185185186,
                    0.3008849557522124,
                    0.6987179487179488,
                    0.5057471264367817,
                    0.5486725663716814,
                    0.39007072275855037,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "sun-maid raisin girl": 0.08457934916110724,
                "mr. peanut": 0.4626485346064462,
                "cap'n crunch": 0.45277211623244656
            },
            "question": "what advertising mascot wears epaulettes?",
            "rate_limited": false,
            "answers": [
                "sun-maid raisin girl",
                "mr. peanut",
                "cap'n crunch"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sun-maid raisin girl": 0.29999999900658925,
                "mr. peanut": 0.34000000019868215,
                "cap'n crunch": 0.3952380970830009
            },
            "integer_answers": {
                "sun-maid raisin girl": 0,
                "mr. peanut": 8,
                "cap'n crunch": 5
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.7706640411940354,
                    1.6690530677717632,
                    1.5602828910342015
                ],
                "result_count_important_words": [
                    9.0,
                    61.0,
                    38.0
                ],
                "wikipedia_search": [
                    0.25,
                    0.6538461538461539,
                    2.0961538461538463
                ],
                "word_count_appended_bing": [
                    11.0,
                    40.0,
                    62.0
                ],
                "answer_relation_to_question_bing": [
                    0.25,
                    1.5,
                    0.25
                ],
                "question_related_to_answer": [
                    0.0,
                    1.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    8.0,
                    71.0,
                    34.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    7.0
                ],
                "result_count_bing": [
                    30300.0,
                    520000.0,
                    184000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    3.0
                ],
                "result_count": [
                    9.0,
                    61.0,
                    35.0
                ],
                "answer_relation_to_question": [
                    0.5119617224880383,
                    1.3444976076555024,
                    1.1435406698564594
                ],
                "word_count_appended": [
                    13.0,
                    30.0,
                    44.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which alum from \u201cThe Hills\u201d founded a wildly popular millennial skincare line?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "lauren conrad"
            ],
            "lines": [
                [
                    0.47388854531711677,
                    0.4420825542931109,
                    0.5,
                    0.5,
                    0.10227272727272728,
                    0.6821523081446633,
                    0.23841059602649006,
                    0.2356687898089172,
                    0.3932291666666667,
                    0.20930232558139536,
                    0.23333333333333334,
                    0.28355534131847254,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.25868893011750155,
                    0.1595857903063426,
                    0.3125,
                    0.5,
                    0.6704545454545454,
                    0.24669214936783299,
                    0.5099337748344371,
                    0.45222929936305734,
                    0.5166495901639344,
                    0.6046511627906976,
                    0.36666666666666664,
                    0.43667998909453637,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.26742252456538174,
                    0.3983316554005466,
                    0.1875,
                    0.0,
                    0.22727272727272727,
                    0.07115554248750368,
                    0.25165562913907286,
                    0.31210191082802546,
                    0.0901212431693989,
                    0.18604651162790697,
                    0.4,
                    0.279764669586991,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "emily weiss": 0.3578246406469078,
                "whitney port": 0.22261436783979624,
                "lauren conrad": 0.41956099151329607
            },
            "question": "which alum from \u201cthe hills\u201d founded a wildly popular millennial skincare line?",
            "rate_limited": false,
            "answers": [
                "emily weiss",
                "lauren conrad",
                "whitney port"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "emily weiss": 0.3194444440305233,
                "whitney port": 0.3194444440305233,
                "lauren conrad": 0.34000000019868215
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.268442730547781,
                    3.493439912756292,
                    2.2381173566959283
                ],
                "result_count_important_words": [
                    36.0,
                    77.0,
                    38.0
                ],
                "wikipedia_search": [
                    1.5729166666666667,
                    2.0665983606557377,
                    0.3604849726775956
                ],
                "word_count_appended_bing": [
                    7.0,
                    11.0,
                    12.0
                ],
                "answer_relation_to_question_bing": [
                    2.2104127714655544,
                    0.797928951531713,
                    1.991658277002733
                ],
                "question_related_to_answer": [
                    1.0,
                    0.625,
                    0.375
                ],
                "question_related_to_answer_bing": [
                    1.0,
                    1.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    37.0,
                    71.0,
                    49.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    23200.0,
                    8390.0,
                    2420.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    9.0,
                    26.0,
                    8.0
                ],
                "answer_relation_to_question": [
                    3.3172198172198173,
                    1.810822510822511,
                    1.871957671957672
                ],
                "result_count": [
                    9.0,
                    59.0,
                    20.0
                ]
            },
            "integer_answers": {
                "emily weiss": 5,
                "whitney port": 1,
                "lauren conrad": 6
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which color is NOT represented in the original electronic Simon game?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "blue",
                "orange",
                "green"
            ],
            "lines": [
                [
                    0.2917924528301887,
                    0.21775,
                    0.2582070707070707,
                    0.037499999999999985,
                    0.3245192307692308,
                    0.29394812680115273,
                    0.32713085234093636,
                    0.31940298507462683,
                    0.3572197484054904,
                    0.3100719424460432,
                    0.2844036697247706,
                    0.32412849047691517,
                    0.25,
                    0.2142857142857143,
                    -1.0
                ],
                [
                    0.38798742138364783,
                    0.42375,
                    0.4715909090909091,
                    0.48750000000000004,
                    0.34615384615384615,
                    0.4048991354466859,
                    0.3457382953181273,
                    0.3584221748400853,
                    0.4315209564118391,
                    0.35035971223021584,
                    0.3623853211009174,
                    0.348004256464326,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.32022012578616355,
                    0.3585,
                    0.2702020202020202,
                    0.47500000000000003,
                    0.3293269230769231,
                    0.3011527377521614,
                    0.32713085234093636,
                    0.32217484008528785,
                    0.21125929518267056,
                    0.339568345323741,
                    0.3532110091743119,
                    0.32786725305875897,
                    0.25,
                    0.2857142857142857,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "blue": 0.4556628165911229,
                "orange": 0.1830982816513429,
                "green": 0.3612389017575342
            },
            "question": "which color is not represented in the original electronic simon game?",
            "rate_limited": false,
            "answers": [
                "blue",
                "orange",
                "green"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "blue": 0.0,
                "orange": 0.0,
                "green": 0.0
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.1104581142770185,
                    1.8239489224280887,
                    2.065592963294893
                ],
                "result_count_important_words": [
                    2880000.0,
                    2570000.0,
                    2880000.0
                ],
                "wikipedia_search": [
                    1.1422420127560768,
                    0.5478323487052876,
                    2.3099256385386355
                ],
                "word_count_appended_bing": [
                    47.0,
                    30.0,
                    32.0
                ],
                "answer_relation_to_question_bing": [
                    2.258,
                    0.61,
                    1.1320000000000001
                ],
                "question_related_to_answer": [
                    0.9671717171717171,
                    0.11363636363636363,
                    0.9191919191919192
                ],
                "question_related_to_answer_bing": [
                    1.85,
                    0.05,
                    0.1
                ],
                "result_count_noun_chunks": [
                    8470000.0,
                    6640000.0,
                    8340000.0
                ],
                "word_count_noun_chunks": [
                    5.0,
                    0.0,
                    5.0
                ],
                "result_count_bing": [
                    2860000.0,
                    1320000.0,
                    2760000.0
                ],
                "word_count_raw": [
                    4.0,
                    0.0,
                    3.0
                ],
                "word_count_appended": [
                    264.0,
                    208.0,
                    223.0
                ],
                "answer_relation_to_question": [
                    2.082075471698113,
                    1.1201257861635219,
                    1.7977987421383648
                ],
                "result_count": [
                    2920000.0,
                    2560000.0,
                    2840000.0
                ]
            },
            "integer_answers": {
                "blue": 13,
                "orange": 0,
                "green": 1
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What car brand is sung about by Will Smith, Charli XCX and Janis Joplin?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "porsche"
            ],
            "lines": [
                [
                    0.4743273604033098,
                    0.29095880906465865,
                    0.16666666666666666,
                    0.0,
                    0.07384987893462469,
                    0.3333333333333333,
                    0.3375047402351157,
                    0.39285714285714285,
                    0.00904977375565611,
                    0.2857142857142857,
                    0.30337078651685395,
                    0.2967226589810448,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.2597950572634117,
                    0.3936249451569786,
                    0.1,
                    0.0,
                    0.5609362389023406,
                    0.3333333333333333,
                    0.3560864618885097,
                    0.3598901098901099,
                    0.23546170604994132,
                    0.3020408163265306,
                    0.3146067415730337,
                    0.38933842938167684,
                    0.14285714285714285,
                    0.0,
                    1.0
                ],
                [
                    0.2658775823332785,
                    0.31541624577836275,
                    0.7333333333333334,
                    1.0,
                    0.3652138821630347,
                    0.3333333333333333,
                    0.30640879787637465,
                    0.24725274725274726,
                    0.7554885201944025,
                    0.4122448979591837,
                    0.38202247191011235,
                    0.31393891163727833,
                    0.8571428571428571,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "porsche": 0.5205481129224498,
                "rolls-royce": 0.2117396740330494,
                "mercedes-benz": 0.2677122130445007
            },
            "question": "what car brand is sung about by will smith, charli xcx and janis joplin?",
            "rate_limited": false,
            "answers": [
                "rolls-royce",
                "mercedes-benz",
                "porsche"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "porsche": 0.3952380970830009,
                "rolls-royce": 0.3194444440305233,
                "mercedes-benz": 0.34000000019868215
            },
            "integer_answers": {
                "porsche": 7,
                "rolls-royce": 3,
                "mercedes-benz": 4
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.373781271848358,
                    3.1147074350534147,
                    2.5115112930982266
                ],
                "result_count_important_words": [
                    8900.0,
                    9390.0,
                    8080.0
                ],
                "wikipedia_search": [
                    0.027149321266968326,
                    0.706385118149824,
                    2.2664655605832076
                ],
                "word_count_appended_bing": [
                    27.0,
                    28.0,
                    34.0
                ],
                "answer_relation_to_question_bing": [
                    0.872876427193976,
                    1.1808748354709357,
                    0.9462487373350883
                ],
                "question_related_to_answer": [
                    0.3333333333333333,
                    0.2,
                    1.4666666666666668
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_noun_chunks": [
                    14300.0,
                    13100.0,
                    9000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    2.0,
                    12.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    160000.0,
                    160000.0,
                    160000.0
                ],
                "word_count_appended": [
                    70.0,
                    74.0,
                    101.0
                ],
                "answer_relation_to_question": [
                    0.9486547208066196,
                    0.5195901145268234,
                    0.531755164666557
                ],
                "result_count": [
                    1830.0,
                    13900.0,
                    9050.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which person is most likely to use a Reuleaux triangle at work?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "banksy"
            ],
            "lines": [
                [
                    0.4391427537979262,
                    0.36184968934800504,
                    0,
                    0,
                    0.06535947712418301,
                    0.35561497326203206,
                    0.06666666666666667,
                    0.096045197740113,
                    0.02611754966887417,
                    0.05970149253731343,
                    0.09375,
                    0.20759718889615852,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.1303619751895614,
                    0.35166928662325025,
                    0,
                    0,
                    0.3660130718954248,
                    0.3449197860962567,
                    0.38,
                    0.3785310734463277,
                    0.15378565970453387,
                    0.08955223880597014,
                    0.0625,
                    0.3127093442059191,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.4304952710125124,
                    0.28648102402874465,
                    0,
                    0,
                    0.5686274509803921,
                    0.2994652406417112,
                    0.5533333333333333,
                    0.5254237288135594,
                    0.820096790626592,
                    0.8507462686567164,
                    0.84375,
                    0.4796934668979224,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "banksy": 0.5658112574991484,
                "adam levine": 0.25700424359672436,
                "greta gerwig": 0.17718449890412719
            },
            "question": "which person is most likely to use a reuleaux triangle at work?",
            "rate_limited": false,
            "answers": [
                "greta gerwig",
                "adam levine",
                "banksy"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "banksy": 0.34000000019868215,
                "adam levine": 0.31499999945362406,
                "greta gerwig": 0.3194444440305233
            },
            "integer_answers": {
                "banksy": 7,
                "adam levine": 0,
                "greta gerwig": 3
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2455831333769511,
                    1.8762560652355147,
                    2.8781608013875344
                ],
                "result_count_important_words": [
                    10.0,
                    57.0,
                    83.0
                ],
                "wikipedia_search": [
                    0.07835264900662252,
                    0.46135697911360163,
                    2.460290371879776
                ],
                "word_count_appended_bing": [
                    3.0,
                    2.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    1.0855490680440152,
                    1.0550078598697508,
                    0.8594430720862339
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    17.0,
                    67.0,
                    93.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    133000.0,
                    129000.0,
                    112000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    10.0,
                    56.0,
                    87.0
                ],
                "answer_relation_to_question": [
                    1.7565710151917049,
                    0.5214479007582457,
                    1.7219810840500496
                ],
                "word_count_appended": [
                    4.0,
                    6.0,
                    57.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these songs was written by the man nicknamed \u201cSlowhand\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "lay down sally"
            ],
            "lines": [
                [
                    0.39170506912442393,
                    0.5319672131147541,
                    0,
                    0,
                    0.9992601631484381,
                    0.6149012567324955,
                    0.9992525915575342,
                    0.6109154929577465,
                    0.2256383712905452,
                    0.7105263157894737,
                    0.6842105263157895,
                    0.6301305675326563,
                    0,
                    1.0,
                    -1.0
                ],
                [
                    0.3824884792626728,
                    0.3524590163934426,
                    0,
                    0,
                    0.00044198045677719383,
                    0.2535906642728905,
                    0.0004305722548987749,
                    0.20246478873239437,
                    0.6922015182884748,
                    0.18421052631578946,
                    0.21052631578947367,
                    0.22346300701392083,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.22580645161290322,
                    0.11557377049180328,
                    0,
                    0,
                    0.0002978563947846306,
                    0.131508078994614,
                    0.00031683618756702303,
                    0.18661971830985916,
                    0.08216011042097998,
                    0.10526315789473684,
                    0.10526315789473684,
                    0.1464064254534228,
                    0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "lay down sally": 0.6725915970512597,
                "lover lay down": 0.09992868760503708,
                "lay lady lay": 0.22747971534370323
            },
            "question": "which of these songs was written by the man nicknamed \u201cslowhand\u201d?",
            "rate_limited": false,
            "answers": [
                "lay down sally",
                "lay lady lay",
                "lover lay down"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "lay down sally": 0.7636987992684696,
                "lover lay down": 0.0461501040143171,
                "lay lady lay": 0.09615650386823776
            },
            "integer_answers": {
                "lay down sally": 10,
                "lover lay down": 0,
                "lay lady lay": 1
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.150652837663282,
                    1.1173150350696044,
                    0.7320321272671142
                ],
                "result_count_important_words": [
                    123000.0,
                    53.0,
                    39.0
                ],
                "wikipedia_search": [
                    0.6769151138716356,
                    2.0766045548654244,
                    0.24648033126293994
                ],
                "word_count_appended_bing": [
                    13.0,
                    4.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    2.1278688524590166,
                    1.4098360655737705,
                    0.46229508196721314
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    347000.0,
                    115000.0,
                    106000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    137000.0,
                    56500.0,
                    29300.0
                ],
                "word_count_raw": [
                    3.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    104000.0,
                    46.0,
                    31.0
                ],
                "answer_relation_to_question": [
                    1.5668202764976957,
                    1.5299539170506913,
                    0.9032258064516129
                ],
                "word_count_appended": [
                    54.0,
                    14.0,
                    8.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these quantities is the largest?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "dozen",
                "two half-dozens",
                "baker's dozen"
            ],
            "lines": [
                [
                    1.0,
                    0.0,
                    0,
                    0,
                    0.6201850118818893,
                    0.46397694524495675,
                    0.6288785697593259,
                    0.6531558495328454,
                    0,
                    0.7654320987654321,
                    0.6829268292682927,
                    0.5224607557237699,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.0,
                    0.0,
                    0,
                    0,
                    5.132565615574257e-06,
                    0.26628242074927955,
                    5.434753071994174e-06,
                    1.2470756076999436e-06,
                    0,
                    0.0102880658436214,
                    0.04878048780487805,
                    0.07845235879299291,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.0,
                    1.0,
                    0,
                    0,
                    0.379809855552495,
                    0.2697406340057637,
                    0.37111599548760216,
                    0.34684290339154683,
                    0,
                    0.2242798353909465,
                    0.2682926829268293,
                    0.3990868854832371,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "two half-dozens": 0.04486834973167413,
                "baker's dozen": 0.3621298658042689,
                "dozen": 0.5930017844640569
            },
            "question": "which of these quantities is the largest?",
            "rate_limited": false,
            "answers": [
                "dozen",
                "two half-dozens",
                "baker's dozen"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "two half-dozens": 0.0,
                "baker's dozen": 0.0,
                "dozen": 0.0
            },
            "integer_answers": {
                "two half-dozens": 0,
                "baker's dozen": 1,
                "dozen": 8
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.0449215114475399,
                    0.15690471758598581,
                    0.7981737709664742
                ],
                "result_count_important_words": [
                    1620000.0,
                    14.0,
                    956000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    28.0,
                    2.0,
                    11.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    8380000.0,
                    16.0,
                    4450000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    16100000.0,
                    9240000.0,
                    9360000.0
                ],
                "result_count": [
                    1450000.0,
                    12.0,
                    888000.0
                ],
                "answer_relation_to_question": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    372.0,
                    5.0,
                    109.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "The actor who played Don Draper provides the voice for what car company\u2019s ads?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "mercedes-benz"
            ],
            "lines": [
                [
                    0.27268691083920804,
                    0.24122558942605382,
                    0.0625,
                    0,
                    0.1344333243170138,
                    0.40899795501022496,
                    0.1467236467236467,
                    0.1327986951202936,
                    0.3909018217215964,
                    0.33678756476683935,
                    0.37777777777777777,
                    0.34094977654972913,
                    0.65625,
                    0.75,
                    1.0
                ],
                [
                    0.30453160497828674,
                    0.4753624374851155,
                    0.875,
                    0,
                    0.4949959426562077,
                    0.3593339176161262,
                    0.47150997150997154,
                    0.49476688867745006,
                    0.5380267000417188,
                    0.3177892918825561,
                    0.3111111111111111,
                    0.32882349419430107,
                    0.09375,
                    0.25,
                    1.0
                ],
                [
                    0.4227814841825051,
                    0.2834119730888307,
                    0.0625,
                    0,
                    0.37057073302677845,
                    0.23166812737364884,
                    0.3817663817663818,
                    0.37243441620225637,
                    0.07107147823668475,
                    0.3454231433506045,
                    0.3111111111111111,
                    0.3302267292559699,
                    0.25,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "jaguar": 0.2640742751995978,
                "bmw": 0.4088462584732957,
                "mercedes-benz": 0.3270794663271065
            },
            "question": "the actor who played don draper provides the voice for what car company\u2019s ads?",
            "rate_limited": false,
            "answers": [
                "mercedes-benz",
                "bmw",
                "jaguar"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "jaguar": 0.34000000019868215,
                "bmw": 0.34000000019868215,
                "mercedes-benz": 0.354545455177625
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.7275982123978326,
                    2.630587953554408,
                    2.641813834047759
                ],
                "result_count_important_words": [
                    1030000.0,
                    3310000.0,
                    2680000.0
                ],
                "wikipedia_search": [
                    1.954509108607982,
                    2.690133500208594,
                    0.35535739118342374
                ],
                "word_count_appended_bing": [
                    34.0,
                    28.0,
                    28.0
                ],
                "answer_relation_to_question_bing": [
                    0.9649023577042153,
                    1.901449749940462,
                    1.1336478923553228
                ],
                "question_related_to_answer": [
                    0.125,
                    1.75,
                    0.125
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    977000.0,
                    3640000.0,
                    2740000.0
                ],
                "word_count_noun_chunks": [
                    21.0,
                    3.0,
                    8.0
                ],
                "word_count_raw": [
                    9.0,
                    3.0,
                    0.0
                ],
                "result_count_bing": [
                    140000.0,
                    123000.0,
                    79300.0
                ],
                "word_count_appended": [
                    195.0,
                    184.0,
                    200.0
                ],
                "answer_relation_to_question": [
                    1.6361214650352482,
                    1.8271896298697206,
                    2.5366889050950308
                ],
                "result_count": [
                    994000.0,
                    3660000.0,
                    2740000.0
                ]
            },
            "integer_answers": {
                "jaguar": 2,
                "bmw": 6,
                "mercedes-benz": 5
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these substances expands when it freezes?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "sodium chloride",
                "carbon dioxide",
                "dihydrogen monoxide"
            ],
            "lines": [
                [
                    0.3333333333333333,
                    0,
                    0,
                    0,
                    0.5089156256301431,
                    0.27055702917771884,
                    0.456551932782354,
                    0.4030576789437109,
                    0.0,
                    0.28308823529411764,
                    0.28,
                    0.3279208824755103,
                    0,
                    0,
                    2.0
                ],
                [
                    0.0,
                    0,
                    0,
                    0,
                    0.489711262398817,
                    0.5450928381962865,
                    0.5390613182249481,
                    0.5466759323604354,
                    1.0,
                    0.39338235294117646,
                    0.24,
                    0.3619478345356449,
                    0,
                    0,
                    2.0
                ],
                [
                    0.6666666666666666,
                    0,
                    0,
                    0,
                    0.0013731119710398202,
                    0.1843501326259947,
                    0.00438674899269792,
                    0.0502663886958536,
                    0.0,
                    0.3235294117647059,
                    0.48,
                    0.31013128298884485,
                    0,
                    0,
                    2.0
                ]
            ],
            "fraction_answers": {
                "sodium chloride": 0.31815830195965433,
                "carbon dioxide": 0.45731905985081206,
                "dihydrogen monoxide": 0.2245226381895337
            },
            "question": "which of these substances expands when it freezes?",
            "rate_limited": false,
            "answers": [
                "sodium chloride",
                "carbon dioxide",
                "dihydrogen monoxide"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sodium chloride": 0.0,
                "carbon dioxide": 0.0,
                "dihydrogen monoxide": 0.0
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 2
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.9837626474265309,
                    1.0858435036069345,
                    0.9303938489665344
                ],
                "result_count_important_words": [
                    332000.0,
                    392000.0,
                    3190.0
                ],
                "wikipedia_search": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    7.0,
                    6.0,
                    12.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    5220000.0,
                    7080000.0,
                    651000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    2040000.0,
                    4110000.0,
                    1390000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    77.0,
                    107.0,
                    88.0
                ],
                "answer_relation_to_question": [
                    0.3333333333333333,
                    0.0,
                    0.6666666666666666
                ],
                "result_count": [
                    1060000.0,
                    1020000.0,
                    2860.0
                ]
            },
            "integer_answers": {
                "sodium chloride": 1,
                "carbon dioxide": 6,
                "dihydrogen monoxide": 2
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which U.S. president's wife was NOT born in North America?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "rutherford b. hayes"
            ],
            "lines": [
                [
                    0.2612015649418876,
                    0.29218690400508585,
                    0,
                    0,
                    0.005324054665884148,
                    0.4033163265306123,
                    0.17539003522898844,
                    0.17662910338069576,
                    0.34878386668759187,
                    0.3205128205128205,
                    0.3235294117647059,
                    0.33278729763920023,
                    0.0,
                    0.0625,
                    -1.0
                ],
                [
                    0.3649943204905364,
                    0.4029794447976266,
                    0,
                    0,
                    0.4977320365557139,
                    0.1326530612244898,
                    0.425138399597383,
                    0.42283194512493877,
                    0.39652199759360457,
                    0.37564102564102564,
                    0.35294117647058826,
                    0.3367291990347747,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.373804114567576,
                    0.30483365119728756,
                    0,
                    0,
                    0.49694390877840194,
                    0.464030612244898,
                    0.3994715651736286,
                    0.4005389514943655,
                    0.25469413571880367,
                    0.3038461538461539,
                    0.3235294117647059,
                    0.33048350332602505,
                    0.5,
                    0.4375,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "martin van buren": 0.23505399864802567,
                "john quincy adams": 0.5496397691070879,
                "rutherford b. hayes": 0.2153062322448864
            },
            "question": "which u.s. president's wife was not born in north america?",
            "rate_limited": false,
            "answers": [
                "john quincy adams",
                "rutherford b. hayes",
                "martin van buren"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "martin van buren": 0.34000000019868215,
                "john quincy adams": 0.3194444440305233,
                "rutherford b. hayes": 0.354545455177625
            },
            "integer_answers": {
                "martin van buren": 3,
                "john quincy adams": 8,
                "rutherford b. hayes": 1
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.0065524283295972,
                    1.9592496115827038,
                    2.034197960087699
                ],
                "result_count_important_words": [
                    258000.0,
                    59500.0,
                    79900.0
                ],
                "wikipedia_search": [
                    1.5121613331240817,
                    1.034780024063955,
                    2.4530586428119636
                ],
                "word_count_appended_bing": [
                    30.0,
                    25.0,
                    30.0
                ],
                "answer_relation_to_question_bing": [
                    2.0781309599491418,
                    0.9702055520237337,
                    1.9516634880271244
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    264000.0,
                    63000.0,
                    81200.0
                ],
                "word_count_noun_chunks": [
                    5.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    3790000.0,
                    14400000.0,
                    1410000.0
                ],
                "word_count_raw": [
                    7.0,
                    0.0,
                    1.0
                ],
                "result_count": [
                    11800000.0,
                    54100.0,
                    72900.0
                ],
                "answer_relation_to_question": [
                    2.865581220697349,
                    1.620068154113563,
                    1.5143506251890884
                ],
                "word_count_appended": [
                    140.0,
                    97.0,
                    153.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Who was the president of the Screen Actors Guild before its merger with AFTRA?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ken howard"
            ],
            "lines": [
                [
                    0.3988927391346473,
                    0.27875816993464053,
                    0,
                    0,
                    0.4695889093677378,
                    0.08722255548890222,
                    0.23881821912187115,
                    0.1441349867859321,
                    0.3790243902439025,
                    0.33062330623306235,
                    0.2459016393442623,
                    0.3252465353259187,
                    0.24390243902439024,
                    0.2222222222222222,
                    0.0
                ],
                [
                    0.08035195714942835,
                    0.3333333333333333,
                    0,
                    0,
                    0.0015225578972674091,
                    0.038152369526094784,
                    0.04718916700861715,
                    0.3821915023378736,
                    0.10941734417344173,
                    0.21680216802168023,
                    0.11475409836065574,
                    0.3137752683196342,
                    0.0,
                    0.044444444444444446,
                    0.0
                ],
                [
                    0.5207553037159243,
                    0.3879084967320261,
                    0,
                    0,
                    0.5288885327349948,
                    0.874625074985003,
                    0.7139926138695117,
                    0.47367351087619436,
                    0.5115582655826558,
                    0.45257452574525747,
                    0.639344262295082,
                    0.36097819635444717,
                    0.7560975609756098,
                    0.7333333333333333,
                    0.0
                ]
            ],
            "fraction_answers": {
                "gabrielle carteris": 0.2803613426856241,
                "ken howard": 0.5794774731000033,
                "melissa gilbert": 0.1401611842143726
            },
            "question": "who was the president of the screen actors guild before its merger with aftra?",
            "rate_limited": false,
            "answers": [
                "gabrielle carteris",
                "melissa gilbert",
                "ken howard"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "gabrielle carteris": 0.38791626985019206,
                "ken howard": 0.8588547324253105,
                "melissa gilbert": 0.08551667991439485
            },
            "integer_answers": {
                "gabrielle carteris": 0,
                "ken howard": 12,
                "melissa gilbert": 0
            },
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9514792119555122,
                    1.8826516099178052,
                    2.165869178126683
                ],
                "result_count_important_words": [
                    5820.0,
                    1150.0,
                    17400.0
                ],
                "wikipedia_search": [
                    2.274146341463415,
                    0.6565040650406504,
                    3.069349593495935
                ],
                "answer_relation_to_question": [
                    2.3933564348078837,
                    0.4821117428965701,
                    3.1245318222955456
                ],
                "word_count_appended_bing": [
                    15.0,
                    7.0,
                    39.0
                ],
                "answer_relation_to_question_bing": [
                    1.3937908496732025,
                    1.6666666666666665,
                    1.9395424836601305
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    7090.0,
                    18800.0,
                    23300.0
                ],
                "word_count_noun_chunks": [
                    10.0,
                    0.0,
                    31.0
                ],
                "result_count_bing": [
                    72700.0,
                    31800.0,
                    729000.0
                ],
                "word_count_raw": [
                    10.0,
                    2.0,
                    33.0
                ],
                "word_count_appended": [
                    122.0,
                    80.0,
                    167.0
                ],
                "result_count": [
                    29300.0,
                    95.0,
                    33000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Jean Valjean, the protagonist of \u201cLes Mis\u00e9rables,\u201d is identified by what prisoner number?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "24601"
            ],
            "lines": [
                [
                    0.004604051565377532,
                    0.02222222222222222,
                    0.0,
                    0,
                    0.004383737940950245,
                    0.29301745635910226,
                    0.034521158129175944,
                    0.002565993216339773,
                    0.07152317880794704,
                    0.22486772486772486,
                    0.25961538461538464,
                    0.26832657635163487,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.7997237569060774,
                    0.8962962962962964,
                    1.0,
                    0,
                    0.9953900370506291,
                    0.4164588528678304,
                    0.961395694135115,
                    0.996903111635452,
                    0.8211920529801325,
                    0.6375661375661376,
                    0.4807692307692308,
                    0.574348800322715,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.19567219152854512,
                    0.08148148148148147,
                    0.0,
                    0,
                    0.00022622500842059754,
                    0.29052369077306733,
                    0.004083147735708983,
                    0.0005308951482082289,
                    0.10728476821192054,
                    0.13756613756613756,
                    0.25961538461538464,
                    0.1573246233256501,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "y2k": 0.09120365262121995,
                "24601": 0.8138495361945858,
                "867-5309": 0.09494681118419418
            },
            "question": "jean valjean, the protagonist of \u201cles mis\u00e9rables,\u201d is identified by what prisoner number?",
            "rate_limited": false,
            "answers": [
                "y2k",
                "24601",
                "867-5309"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "y2k": 0.29999999900658925,
                "24601": 0.3952380970830009,
                "867-5309": 0.29999999900658925
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6099594581098093,
                    3.44609280193629,
                    0.9439477399539006
                ],
                "result_count_important_words": [
                    93.0,
                    2590.0,
                    11.0
                ],
                "wikipedia_search": [
                    0.3576158940397351,
                    4.105960264900662,
                    0.5364238410596026
                ],
                "answer_relation_to_question": [
                    0.027624309392265192,
                    4.798342541436464,
                    1.1740331491712708
                ],
                "answer_relation_to_question_bing": [
                    0.1111111111111111,
                    4.481481481481482,
                    0.4074074074074074
                ],
                "word_count_appended": [
                    85.0,
                    241.0,
                    52.0
                ],
                "question_related_to_answer": [
                    0.0,
                    2.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    87.0,
                    33800.0,
                    18.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    30.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    15.0,
                    0.0
                ],
                "result_count_bing": [
                    23500.0,
                    33400.0,
                    23300.0
                ],
                "result_count": [
                    872.0,
                    198000.0,
                    45.0
                ],
                "word_count_appended_bing": [
                    27.0,
                    50.0,
                    27.0
                ]
            },
            "integer_answers": {
                "y2k": 0,
                "24601": 13,
                "867-5309": 0
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What is the grammatically correct way to announce people have arrived?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "there here"
            ],
            "lines": [
                [
                    0.34996138996139,
                    0.3,
                    0,
                    0,
                    0.042469297425347545,
                    0.5331664580725908,
                    0.05316751233833031,
                    0.8602150537634409,
                    0.0,
                    0.5918367346938775,
                    0.3333333333333333,
                    0.35938099937190743,
                    0,
                    0,
                    1.0
                ],
                [
                    0.4531274131274131,
                    0.3,
                    0,
                    0,
                    0.95458512561561,
                    0.23404255319148937,
                    0.9439159799196323,
                    0.09867172675521822,
                    0.5,
                    0.10204081632653061,
                    0.3333333333333333,
                    0.36543120961869674,
                    0,
                    0,
                    1.0
                ],
                [
                    0.1969111969111969,
                    0.4,
                    0,
                    0,
                    0.0029455769590424536,
                    0.2327909887359199,
                    0.0029165077420373947,
                    0.041113219481340925,
                    0.5,
                    0.30612244897959184,
                    0.3333333333333333,
                    0.2751877910093959,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "they're here": 0.34235307789602176,
                "there here": 0.42851481578879236,
                "their here": 0.22913210631518588
            },
            "question": "what is the grammatically correct way to announce people have arrived?",
            "rate_limited": false,
            "answers": [
                "they're here",
                "there here",
                "their here"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "they're here": 0.14806638294475769,
                "there here": 0.3992900029040363,
                "their here": 0.17818112057557206
            },
            "integer_answers": {
                "they're here": 4,
                "there here": 5,
                "their here": 1
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.1562859962314445,
                    2.1925872577121805,
                    1.6511267460563752
                ],
                "result_count_important_words": [
                    138000.0,
                    2450000.0,
                    7570.0
                ],
                "wikipedia_search": [
                    0.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question": [
                    1.74980694980695,
                    2.2656370656370655,
                    0.9845559845559846
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.2,
                    1.2,
                    1.6
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    2040000.0,
                    234000.0,
                    97500.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    426000.0,
                    187000.0,
                    186000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    109000.0,
                    2450000.0,
                    7560.0
                ],
                "word_count_appended": [
                    29.0,
                    5.0,
                    15.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these is NOT a Slavic language?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "bulgarian",
                "serbian",
                "hungarian"
            ],
            "lines": [
                [
                    0.3194630872483222,
                    0.22391952309985097,
                    0.22499999999999998,
                    0.0,
                    0.37767220902612825,
                    0.47231096911608095,
                    0.37664473684210525,
                    0.42980769230769234,
                    0.1857976653696498,
                    0.329778156996587,
                    0.31845238095238093,
                    0.3295740704308479,
                    0.22499999999999998,
                    0.1923076923076923,
                    -1.0
                ],
                [
                    0.2778523489932886,
                    0.37853949329359166,
                    0.275,
                    0.5,
                    0.29453681710213775,
                    0.05626553070642526,
                    0.287828947368421,
                    0.24230769230769234,
                    0.43093385214007784,
                    0.3417235494880546,
                    0.3392857142857143,
                    0.32059971145648897,
                    0.275,
                    0.3269230769230769,
                    -1.0
                ],
                [
                    0.40268456375838924,
                    0.3975409836065574,
                    0.5,
                    0.5,
                    0.32779097387173395,
                    0.4714235001774938,
                    0.3355263157894737,
                    0.3278846153846154,
                    0.38326848249027234,
                    0.32849829351535836,
                    0.34226190476190477,
                    0.3498262181126631,
                    0.5,
                    0.4807692307692308,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "bulgarian": 0.42775311661466603,
                "hungarian": 0.1932178453946153,
                "serbian": 0.3790290379907187
            },
            "question": "which of these is not a slavic language?",
            "rate_limited": false,
            "answers": [
                "bulgarian",
                "serbian",
                "hungarian"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "bulgarian": 0.0,
                "hungarian": 0.0,
                "serbian": 0.0
            },
            "integer_answers": {
                "bulgarian": 7,
                "hungarian": 1,
                "serbian": 6
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.6817037182766084,
                    0.7176011541740441,
                    0.6006951275493475
                ],
                "result_count_important_words": [
                    1500000.0,
                    2580000.0,
                    2000000.0
                ],
                "wikipedia_search": [
                    1.2568093385214008,
                    0.27626459143968873,
                    0.4669260700389105
                ],
                "word_count_appended_bing": [
                    61.0,
                    54.0,
                    53.0
                ],
                "answer_relation_to_question_bing": [
                    1.1043219076005961,
                    0.4858420268256334,
                    0.4098360655737705
                ],
                "question_related_to_answer": [
                    0.55,
                    0.45,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    730000.0,
                    2680000.0,
                    1790000.0
                ],
                "word_count_noun_chunks": [
                    11.0,
                    9.0,
                    0.0
                ],
                "result_count_bing": [
                    1560000.0,
                    25000000.0,
                    1610000.0
                ],
                "word_count_raw": [
                    16.0,
                    9.0,
                    1.0
                ],
                "result_count": [
                    1030000.0,
                    1730000.0,
                    1450000.0
                ],
                "answer_relation_to_question": [
                    0.7221476510067114,
                    0.8885906040268456,
                    0.38926174496644295
                ],
                "word_count_appended": [
                    399.0,
                    371.0,
                    402.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Basketball is NOT a major theme of which of these 90s movies?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "white men can't jump",
                "point break",
                "eddie"
            ],
            "lines": [
                [
                    0.19423483255422258,
                    0.2005718954248366,
                    0,
                    0,
                    0.4796857641644183,
                    0.40154922538730636,
                    0.4691482226693494,
                    0.4796857641644183,
                    0.28125,
                    0.39378238341968913,
                    0.3557692307692308,
                    0.33086842652211124,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.3182975043241908,
                    0.3954248366013072,
                    0,
                    0,
                    0.45675289636565625,
                    0.39580209895052476,
                    0.3829644533869886,
                    0.45675289636565625,
                    0.28125,
                    0.41968911917098445,
                    0.4326923076923077,
                    0.3517189320689592,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.4874676631215867,
                    0.4040032679738562,
                    0,
                    0,
                    0.06356133946992543,
                    0.20264867566216893,
                    0.147887323943662,
                    0.06356133946992543,
                    0.4375,
                    0.18652849740932642,
                    0.21153846153846156,
                    0.31741264140892955,
                    0.5,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "white men can't jump": 0.4022423758207362,
                "point break": 0.18477582584557084,
                "eddie": 0.412981798333693
            },
            "question": "basketball is not a major theme of which of these 90s movies?",
            "rate_limited": false,
            "answers": [
                "white men can't jump",
                "point break",
                "eddie"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "white men can't jump": 0.0,
                "point break": 0.0,
                "eddie": 0.0
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6913157347788874,
                    1.482810679310408,
                    1.8258735859107045
                ],
                "result_count_important_words": [
                    92000.0,
                    349000.0,
                    1050000.0
                ],
                "wikipedia_search": [
                    1.75,
                    1.75,
                    0.5
                ],
                "answer_relation_to_question": [
                    3.057651674457774,
                    1.8170249567580923,
                    0.12532336878413336
                ],
                "answer_relation_to_question_bing": [
                    1.7965686274509802,
                    0.6274509803921569,
                    0.5759803921568627
                ],
                "word_count_appended": [
                    82.0,
                    62.0,
                    242.0
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    51200.0,
                    109000.0,
                    1100000.0
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    788000.0,
                    834000.0,
                    2380000.0
                ],
                "result_count": [
                    51200.0,
                    109000.0,
                    1100000.0
                ],
                "word_count_appended_bing": [
                    15.0,
                    7.0,
                    30.0
                ]
            },
            "integer_answers": {
                "white men can't jump": 5,
                "point break": 0,
                "eddie": 7
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What generation of the iPod was the first to offer video?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "u2 special edition"
            ],
            "lines": [
                [
                    0.24033687266060652,
                    0.2264191848442399,
                    1.0,
                    0,
                    0.5010989010989011,
                    0.46119463632669644,
                    0.40265866209262435,
                    0.4238921001926782,
                    0.3333333333333333,
                    0.24444444444444444,
                    0.375,
                    0.3322439745730873,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.526050227217087,
                    0.4575168434393969,
                    0.0,
                    0,
                    0.24395604395604395,
                    0.05729378301503454,
                    0.16423670668953688,
                    0.31695568400770713,
                    0.6666666666666666,
                    0.5388888888888889,
                    0.3125,
                    0.30941284628585614,
                    0.75,
                    1.0,
                    1.0
                ],
                [
                    0.23361290012230645,
                    0.31606397171636313,
                    0.0,
                    0,
                    0.2549450549450549,
                    0.481511580658269,
                    0.43310463121783876,
                    0.2591522157996146,
                    0.0,
                    0.21666666666666667,
                    0.3125,
                    0.3583431791410567,
                    0.25,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "third generation": 0.3492786238128162,
                "u2 special edition": 0.4110367453974014,
                "fifth generation": 0.23968463078978233
            },
            "question": "what generation of the ipod was the first to offer video?",
            "rate_limited": false,
            "answers": [
                "third generation",
                "u2 special edition",
                "fifth generation"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "third generation": 0.14450238146022445,
                "u2 special edition": 0.556911905419778,
                "fifth generation": 0.35497909727208826
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.328975898292349,
                    1.2376513851434243,
                    1.4333727165642265
                ],
                "result_count_important_words": [
                    939000.0,
                    383000.0,
                    1010000.0
                ],
                "wikipedia_search": [
                    1.0,
                    2.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    6.0,
                    5.0,
                    5.0
                ],
                "answer_relation_to_question_bing": [
                    0.9056767393769596,
                    1.8300673737575877,
                    1.2642558868654525
                ],
                "question_related_to_answer": [
                    1.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    440000.0,
                    329000.0,
                    269000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    3.0,
                    1.0
                ],
                "result_count_bing": [
                    22700000.0,
                    2820000.0,
                    23700000.0
                ],
                "word_count_raw": [
                    0.0,
                    3.0,
                    0.0
                ],
                "word_count_appended": [
                    44.0,
                    97.0,
                    39.0
                ],
                "answer_relation_to_question": [
                    0.9613474906424261,
                    2.104200908868348,
                    0.9344516004892258
                ],
                "result_count": [
                    228000.0,
                    111000.0,
                    116000.0
                ]
            },
            "integer_answers": {
                "third generation": 4,
                "u2 special edition": 6,
                "fifth generation": 3
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these actresses is NOT mentioned in Madonna\u2019s song \u201cVogue\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "jean harlow",
                "audrey hepburn",
                "rita hayworth"
            ],
            "lines": [
                [
                    0.3734126984126984,
                    0.5,
                    0,
                    0.5,
                    2.759106997429761e-05,
                    0.3443092340730136,
                    0.34431443618716323,
                    0.4366,
                    0.5000000000000001,
                    0.3080357142857143,
                    0.33333333333333337,
                    0.2957625836759094,
                    0.5,
                    0.25,
                    -1.0
                ],
                [
                    0.28055555555555556,
                    0.25,
                    0,
                    0.5,
                    0.4999892598519563,
                    0.3258768790264853,
                    0.49989661505528055,
                    0.09000000000000002,
                    0.14871794871794872,
                    0.3839285714285714,
                    0.4285714285714286,
                    0.4029464732957495,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.346031746031746,
                    0.25,
                    0,
                    0.0,
                    0.4999831490780694,
                    0.32981388690050106,
                    0.15578894875755622,
                    0.4734,
                    0.3512820512820513,
                    0.3080357142857143,
                    0.23809523809523808,
                    0.3012909430283412,
                    0.0,
                    0.25,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "jean harlow": 0.2791083706095682,
                "audrey hepburn": 0.2599257336149268,
                "rita hayworth": 0.46096589577550495
            },
            "question": "which of these actresses is not mentioned in madonna\u2019s song \u201cvogue\u201d?",
            "rate_limited": false,
            "answers": [
                "jean harlow",
                "audrey hepburn",
                "rita hayworth"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "jean harlow": 0.0,
                "audrey hepburn": 0.0,
                "rita hayworth": 0.0
            },
            "integer_answers": {
                "jean harlow": 4,
                "audrey hepburn": 5,
                "rita hayworth": 4
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.0423741632409063,
                    0.9705352670425056,
                    1.9870905697165884
                ],
                "result_count_important_words": [
                    128000.0,
                    85.0,
                    283000.0
                ],
                "wikipedia_search": [
                    0.0,
                    2.1076923076923078,
                    0.8923076923076922
                ],
                "word_count_appended_bing": [
                    7.0,
                    3.0,
                    11.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    1.0,
                    1.0
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_noun_chunks": [
                    317000.0,
                    2050000.0,
                    133000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    87000.0,
                    97300.0,
                    95100.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    1.0
                ],
                "result_count": [
                    2700000.0,
                    58.0,
                    91.0
                ],
                "answer_relation_to_question": [
                    0.7595238095238095,
                    1.3166666666666667,
                    0.9238095238095239
                ],
                "word_count_appended": [
                    43.0,
                    26.0,
                    43.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these is NOT a step in the famous Korean 10-step skin care regime?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "spelunking"
            ],
            "lines": [
                [
                    0.2726478673753639,
                    0.42333333333333334,
                    0.09999999999999998,
                    0,
                    0.21864957352832887,
                    0.13669242150254807,
                    0.18890046899426782,
                    0.18653489808523782,
                    0.4184626436781609,
                    0.288527397260274,
                    0.324468085106383,
                    0.2967721618047594,
                    0.10869565217391303,
                    0.08695652173913043,
                    -1.0
                ],
                [
                    0.24471580248462138,
                    0.14333333333333337,
                    0.4,
                    0,
                    0.28186658293307476,
                    0.490547427256288,
                    0.31917665450755606,
                    0.3213918056413424,
                    0.3315373563218391,
                    0.3407534246575342,
                    0.3191489361702128,
                    0.2958060958990807,
                    0.391304347826087,
                    0.41304347826086957,
                    -1.0
                ],
                [
                    0.4826363301400147,
                    0.43333333333333335,
                    0.5,
                    0,
                    0.49948384353859643,
                    0.3727601512411639,
                    0.4919228764981761,
                    0.4920732962734198,
                    0.25,
                    0.3707191780821918,
                    0.35638297872340424,
                    0.40742174229616,
                    0.5,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "moisturizing": 0.33959611610894785,
                "spelunking": 0.1297332722882369,
                "cleansing": 0.5306706116028154
            },
            "question": "which of these is not a step in the famous korean 10-step skin care regime?",
            "rate_limited": false,
            "answers": [
                "cleansing",
                "moisturizing",
                "spelunking"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "moisturizing": 0.3194444440305233,
                "spelunking": 0.354545455177625,
                "cleansing": 0.3194444440305233
            },
            "integer_answers": {
                "moisturizing": 4,
                "spelunking": 1,
                "cleansing": 8
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.845189734733369,
                    2.8587146574128712,
                    1.2960956078537602
                ],
                "result_count_important_words": [
                    597000.0,
                    347000.0,
                    15500.0
                ],
                "wikipedia_search": [
                    0.6522988505747127,
                    1.3477011494252873,
                    2.0
                ],
                "answer_relation_to_question": [
                    1.8188170609970888,
                    2.042273580123029,
                    0.1389093588798821
                ],
                "word_count_appended_bing": [
                    33.0,
                    34.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    0.7666666666666666,
                    3.5666666666666664,
                    0.6666666666666666
                ],
                "question_related_to_answer": [
                    0.8,
                    0.2,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    609000.0,
                    347000.0,
                    15400.0
                ],
                "word_count_noun_chunks": [
                    18.0,
                    5.0,
                    0.0
                ],
                "result_count_bing": [
                    2210000.0,
                    57500.0,
                    774000.0
                ],
                "word_count_raw": [
                    19.0,
                    4.0,
                    0.0
                ],
                "result_count": [
                    405000.0,
                    314000.0,
                    743.0
                ],
                "word_count_appended": [
                    247.0,
                    186.0,
                    151.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Napa cabbage is named after what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "city in california"
            ],
            "question": "napa cabbage is named after what?",
            "answers": [
                "city in california",
                "scientist edward napa",
                "japanese for \u201cvegetable\u201d"
            ],
            "integer_answers": {
                "city in california": 4,
                "scientist edward napa": 5,
                "japanese for \u201cvegetable\u201d": 1
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.45870361522535436,
                    1.2471571906354515,
                    0.29413919413919415
                ],
                "result_count_important_words": [
                    46.0,
                    2300000.0,
                    19.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    0.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_appended": [
                    25.0,
                    7.0,
                    0.0
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    24.0,
                    100.0,
                    21.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    1040000.0,
                    538000.0,
                    385000.0
                ],
                "result_count": [
                    46.0,
                    72800.0,
                    16.0
                ],
                "answer_relation_to_question": [
                    1.0,
                    0.0,
                    1.0
                ]
            },
            "negative_question": false,
            "fraction_answers": {
                "city in california": 0.270657170339042,
                "scientist edward napa": 0.5305174884557062,
                "japanese for \u201cvegetable\u201d": 0.19882534120525183
            },
            "lines": [
                [
                    0.5,
                    0.0,
                    0,
                    0,
                    0.000631330460322253,
                    0.5298013245033113,
                    1.999943479858178e-05,
                    0.16551724137931034,
                    0.0,
                    0.78125,
                    0.5,
                    0.22935180761267718,
                    0,
                    0,
                    1.0
                ],
                [
                    0.0,
                    1.0,
                    0,
                    0,
                    0.9991490763360874,
                    0.2740703005603668,
                    0.9999717399290889,
                    0.6896551724137931,
                    0.0,
                    0.21875,
                    0.5,
                    0.6235785953177257,
                    0,
                    0,
                    1.0
                ],
                [
                    0.5,
                    0.0,
                    0,
                    0,
                    0.00021959320359034887,
                    0.19612837493632196,
                    8.260636112457692e-06,
                    0.14482758620689656,
                    1.0,
                    0.0,
                    0.0,
                    0.14706959706959707,
                    0,
                    0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "city in california": 0.36282307164070493,
                "scientist edward napa": 0.28266697233115806,
                "japanese for \u201cvegetable\u201d": 0.2582800118469649
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Lonnie Lynn's only Academy Award win was in what category?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "best original song"
            ],
            "lines": [
                [
                    0.33318100277593343,
                    0.25653206875662576,
                    0.038461538461538464,
                    0,
                    0.41491085899513774,
                    0.0668495029139527,
                    0.17455928102315935,
                    0.4019448946515397,
                    0.3178261827799001,
                    0.29375,
                    0.4230769230769231,
                    0.32539169130880613,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.29862244079356015,
                    0.3498012267151295,
                    0.0,
                    0,
                    0.3371150729335494,
                    0.15152553993829276,
                    0.1462150017283097,
                    0.3338735818476499,
                    0.25884373731861243,
                    0.1875,
                    0.07692307692307693,
                    0.3299634349604141,
                    0.043478260869565216,
                    0.047619047619047616,
                    1.0
                ],
                [
                    0.3681965564305064,
                    0.3936667045282447,
                    0.9615384615384616,
                    0,
                    0.2479740680713128,
                    0.7816249571477546,
                    0.679225717248531,
                    0.26418152350081037,
                    0.4233300799014875,
                    0.51875,
                    0.5,
                    0.34464487373077973,
                    0.9565217391304348,
                    0.9523809523809523,
                    1.0
                ]
            ],
            "fraction_answers": {
                "best adapted screenplay": 0.23434491882642436,
                "best original song": 0.568618125662252,
                "best cinematography": 0.19703695551132366
            },
            "question": "lonnie lynn's only academy award win was in what category?",
            "rate_limited": false,
            "answers": [
                "best adapted screenplay",
                "best cinematography",
                "best original song"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "best adapted screenplay": -0.026517513638686152,
                "best original song": 0.7243373744966606,
                "best cinematography": 0.2695316043971992
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.952350147852837,
                    1.9797806097624844,
                    2.0678692423846785
                ],
                "result_count_important_words": [
                    10100.0,
                    8460.0,
                    39300.0
                ],
                "wikipedia_search": [
                    1.2713047311196004,
                    1.0353749492744497,
                    1.69332031960595
                ],
                "word_count_appended_bing": [
                    11.0,
                    2.0,
                    13.0
                ],
                "answer_relation_to_question_bing": [
                    1.026128275026503,
                    1.399204906860518,
                    1.5746668181129788
                ],
                "question_related_to_answer": [
                    0.038461538461538464,
                    0.0,
                    0.9615384615384616
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    12400.0,
                    10300.0,
                    8150.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    22.0
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    20.0
                ],
                "result_count_bing": [
                    9750.0,
                    22100.0,
                    114000.0
                ],
                "word_count_appended": [
                    47.0,
                    30.0,
                    83.0
                ],
                "answer_relation_to_question": [
                    1.3327240111037337,
                    1.1944897631742406,
                    1.4727862257220257
                ],
                "result_count": [
                    12800.0,
                    10400.0,
                    7650.0
                ]
            },
            "integer_answers": {
                "best adapted screenplay": 2,
                "best original song": 11,
                "best cinematography": 0
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these products was featured on \u201cShark Tank\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "instant pot",
                "scrub daddy",
                "sticky buddy"
            ],
            "lines": [
                [
                    0.009615384615384616,
                    0.10416666666666666,
                    0.0,
                    0,
                    0.4488404953869171,
                    0.6087719298245614,
                    0.9776523173411111,
                    0.34625492772667543,
                    0.03125,
                    0.14979757085020243,
                    0.08695652173913043,
                    0.28532933535860583,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.8153846153846154,
                    0.75,
                    1.0,
                    0,
                    0.5485828276951209,
                    0.20701754385964913,
                    0.022248264698919953,
                    0.28252299605781866,
                    0.84375,
                    0.6882591093117408,
                    0.8260869565217391,
                    0.5317933738629858,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.175,
                    0.14583333333333331,
                    0.0,
                    0,
                    0.0025766769179619317,
                    0.18421052631578946,
                    9.941795996888803e-05,
                    0.3712220762155059,
                    0.125,
                    0.16194331983805668,
                    0.08695652173913043,
                    0.18287729077840856,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "scrub daddy": 0.6550496682609684,
                "sticky buddy": 0.11043993562293501,
                "instant pot": 0.23451039611609656
            },
            "question": "which of these products was featured on \u201cshark tank\u201d?",
            "rate_limited": false,
            "answers": [
                "instant pot",
                "scrub daddy",
                "sticky buddy"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "scrub daddy": 0.0,
                "sticky buddy": 0.0,
                "instant pot": 0.0
            },
            "integer_answers": {
                "scrub daddy": 10,
                "sticky buddy": 1,
                "instant pot": 2
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.141317341434423,
                    2.1271734954519426,
                    0.7315091631136341
                ],
                "result_count_important_words": [
                    7690000.0,
                    175000.0,
                    782.0
                ],
                "wikipedia_search": [
                    0.125,
                    3.375,
                    0.5
                ],
                "word_count_appended_bing": [
                    2.0,
                    19.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.41666666666666663,
                    3.0,
                    0.5833333333333333
                ],
                "question_related_to_answer": [
                    0.0,
                    1.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    52700.0,
                    43000.0,
                    56500.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    4.0,
                    0.0
                ],
                "result_count_bing": [
                    3470000.0,
                    1180000.0,
                    1050000.0
                ],
                "word_count_raw": [
                    0.0,
                    4.0,
                    0.0
                ],
                "result_count": [
                    10800.0,
                    13200.0,
                    62.0
                ],
                "answer_relation_to_question": [
                    0.038461538461538464,
                    3.2615384615384615,
                    0.7
                ],
                "word_count_appended": [
                    37.0,
                    170.0,
                    40.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What are the first words spoken by God in the King James Bible?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "let there be light"
            ],
            "lines": [
                [
                    0.24572649572649574,
                    0.45,
                    1.0,
                    0,
                    0.9984204095013859,
                    0.34684684684684686,
                    0.9993811171980654,
                    0.5737583833131444,
                    0.5000000000000001,
                    0.8235294117647058,
                    0.7333333333333333,
                    0.6957774037452991,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.4349626068376068,
                    0.2833333333333333,
                    0.0,
                    0,
                    0.00035764313176169047,
                    0.32432432432432434,
                    0.00013752951154101818,
                    0.0002068130114928945,
                    0.33333333333333337,
                    0.04201680672268908,
                    0.13333333333333333,
                    0.09805414765924063,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.31931089743589747,
                    0.26666666666666666,
                    0.0,
                    0,
                    0.0012219473668524424,
                    0.32882882882882886,
                    0.00048135329039356364,
                    0.42603480367536267,
                    0.16666666666666669,
                    0.13445378151260504,
                    0.13333333333333333,
                    0.20616844859546032,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "hello, my children": 0.12692768239989669,
                "let there be light": 0.7205210308791751,
                "this is my gift": 0.15255128672092824
            },
            "question": "what are the first words spoken by god in the king james bible?",
            "rate_limited": false,
            "answers": [
                "let there be light",
                "hello, my children",
                "this is my gift"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "hello, my children": 0.19039401467265704,
                "let there be light": 0.789407289519009,
                "this is my gift": 0.0743357920875469
            },
            "integer_answers": {
                "hello, my children": 1,
                "let there be light": 12,
                "this is my gift": 0
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    4.174664422471794,
                    0.5883248859554437,
                    1.237010691572762
                ],
                "result_count_important_words": [
                    87200.0,
                    12.0,
                    42.0
                ],
                "wikipedia_search": [
                    1.0,
                    0.6666666666666666,
                    0.3333333333333333
                ],
                "word_count_appended_bing": [
                    11.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.8,
                    1.1333333333333333,
                    1.0666666666666667
                ],
                "question_related_to_answer": [
                    1.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    97100.0,
                    35.0,
                    72100.0
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    1540000.0,
                    1440000.0,
                    1460000.0
                ],
                "word_count_raw": [
                    10.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    33500.0,
                    12.0,
                    41.0
                ],
                "answer_relation_to_question": [
                    1.2286324786324787,
                    2.174813034188034,
                    1.5965544871794872
                ],
                "word_count_appended": [
                    98.0,
                    5.0,
                    16.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these grape varieties is typically white?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "riesling",
                "merlot",
                "concord"
            ],
            "lines": [
                [
                    0.605659532762244,
                    0.7098039215686275,
                    0.34615384615384615,
                    0.0,
                    0.4411764705882353,
                    0.4740820734341253,
                    0.16246362754607177,
                    0.6516434620353093,
                    0.646306572358468,
                    0.341227125941873,
                    0.43617021276595747,
                    0.35855694110256403,
                    0.34615384615384615,
                    0.6129032258064516,
                    -1.0
                ],
                [
                    0.3589621441293715,
                    0.2531590413943355,
                    0.38461538461538464,
                    1.0,
                    0.27389705882352944,
                    0.12526997840172785,
                    0.11978661493695442,
                    0.3272570690397589,
                    0.3536934276415321,
                    0.3573735199138859,
                    0.20212765957446807,
                    0.3207151442751638,
                    0.38461538461538464,
                    0.16129032258064516,
                    -1.0
                ],
                [
                    0.03537832310838446,
                    0.037037037037037035,
                    0.2692307692307692,
                    0.0,
                    0.2849264705882353,
                    0.4006479481641469,
                    0.7177497575169738,
                    0.021099468924931822,
                    0.0,
                    0.3013993541442411,
                    0.3617021276595745,
                    0.32072791462227224,
                    0.2692307692307692,
                    0.22580645161290322,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "riesling": 0.43802148987268713,
                "concord": 0.23178117084573133,
                "merlot": 0.33019733928158157
            },
            "question": "which of these grape varieties is typically white?",
            "rate_limited": false,
            "answers": [
                "riesling",
                "merlot",
                "concord"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "riesling": 0.0,
                "concord": 0.0,
                "merlot": 0.0
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.4342277644102561,
                    1.2828605771006552,
                    1.282911658489089
                ],
                "result_count_important_words": [
                    335000.0,
                    247000.0,
                    1480000.0
                ],
                "wikipedia_search": [
                    2.585226289433872,
                    1.4147737105661284,
                    0.0
                ],
                "word_count_appended_bing": [
                    82.0,
                    38.0,
                    68.0
                ],
                "answer_relation_to_question_bing": [
                    2.1294117647058823,
                    0.7594771241830065,
                    0.1111111111111111
                ],
                "question_related_to_answer": [
                    0.34615384615384615,
                    0.38461538461538464,
                    0.2692307692307692
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    4540000.0,
                    2280000.0,
                    147000.0
                ],
                "word_count_noun_chunks": [
                    9.0,
                    10.0,
                    7.0
                ],
                "result_count_bing": [
                    4390000.0,
                    1160000.0,
                    3710000.0
                ],
                "word_count_raw": [
                    19.0,
                    5.0,
                    7.0
                ],
                "result_count": [
                    240000.0,
                    149000.0,
                    155000.0
                ],
                "answer_relation_to_question": [
                    1.8169785982867324,
                    1.0768864323881147,
                    0.10613496932515339
                ],
                "word_count_appended": [
                    317.0,
                    332.0,
                    280.0
                ]
            },
            "integer_answers": {
                "riesling": 9,
                "concord": 1,
                "merlot": 4
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "The inventor of the Erector Set made another toy that contained what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "uranium ore"
            ],
            "lines": [
                [
                    0.15470761122935037,
                    0.38388625592417064,
                    0,
                    0,
                    0.3825503355704698,
                    0.35658914728682173,
                    0.4222222222222222,
                    0.4778761061946903,
                    0.2516666666666667,
                    0.6666666666666666,
                    0.5,
                    0.5341229295152219,
                    0,
                    0,
                    1.0
                ],
                [
                    0.29228742272220537,
                    0.3080568720379147,
                    0,
                    0,
                    0.5906040268456376,
                    0.35271317829457366,
                    0.4888888888888889,
                    0.40707964601769914,
                    0.12666666666666668,
                    0.27450980392156865,
                    0.25,
                    0.3643992065868361,
                    0,
                    0,
                    1.0
                ],
                [
                    0.5530049660484443,
                    0.3080568720379147,
                    0,
                    0,
                    0.026845637583892617,
                    0.29069767441860467,
                    0.08888888888888889,
                    0.11504424778761062,
                    0.6216666666666667,
                    0.058823529411764705,
                    0.25,
                    0.10147786389794206,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "asbestos powder": 0.24145063467417294,
                "uranium ore": 0.413028794127628,
                "live ants": 0.34552057119819907
            },
            "question": "the inventor of the erector set made another toy that contained what?",
            "rate_limited": false,
            "answers": [
                "uranium ore",
                "live ants",
                "asbestos powder"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "asbestos powder": 0.20599475691559924,
                "uranium ore": 0.26579246891885355,
                "live ants": 0.22361073185960526
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.6706146475761092,
                    1.8219960329341802,
                    0.5073893194897102
                ],
                "result_count_important_words": [
                    38.0,
                    44.0,
                    8.0
                ],
                "wikipedia_search": [
                    0.5033333333333334,
                    0.25333333333333335,
                    1.2433333333333334
                ],
                "word_count_appended_bing": [
                    4.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.38388625592417064,
                    0.3080568720379147,
                    0.3080568720379147
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    54.0,
                    46.0,
                    13.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    18400.0,
                    18200.0,
                    15000.0
                ],
                "word_count_appended": [
                    34.0,
                    14.0,
                    3.0
                ],
                "answer_relation_to_question": [
                    0.4641228336880511,
                    0.876862268166616,
                    1.6590148981453328
                ],
                "result_count": [
                    57.0,
                    88.0,
                    4.0
                ]
            },
            "integer_answers": {
                "asbestos powder": 2,
                "uranium ore": 6,
                "live ants": 2
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these outfits can be defined as a \u201cCanadian tuxedo\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "jean jacket / jeans"
            ],
            "lines": [
                [
                    0.0,
                    0.3235294117647059,
                    0,
                    0,
                    0.0,
                    0.26953748006379585,
                    0.875,
                    0.33389318105475313,
                    0.0,
                    0.2727272727272727,
                    0.3333333333333333,
                    0.2532920631281287,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.72,
                    0.3235294117647059,
                    0,
                    0,
                    0.0,
                    0.29558745348219034,
                    0.0,
                    0.32683909976486397,
                    0.1111111111111111,
                    0.2727272727272727,
                    0.3333333333333333,
                    0.28495357101914476,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.28,
                    0.35294117647058826,
                    0,
                    0,
                    1.0,
                    0.4348750664540138,
                    0.125,
                    0.33926771918038295,
                    0.8888888888888888,
                    0.45454545454545453,
                    0.3333333333333333,
                    0.46175436585272656,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "wool sweater / slacks": 0.26613127420719895,
                "cotton t-shirt / shorts": 0.2668081253202622,
                "jean jacket / jeans": 0.46706060047253883
            },
            "question": "which of these outfits can be defined as a \u201ccanadian tuxedo\u201d?",
            "rate_limited": false,
            "answers": [
                "wool sweater / slacks",
                "cotton t-shirt / shorts",
                "jean jacket / jeans"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "wool sweater / slacks": 0.017598721694299997,
                "cotton t-shirt / shorts": 0.15922138029717248,
                "jean jacket / jeans": 0.46304221437608745
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.0131682525125147,
                    1.139814284076579,
                    1.8470174634109062
                ],
                "result_count_important_words": [
                    91.0,
                    0,
                    13.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.3333333333333333,
                    2.6666666666666665
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.3235294117647059,
                    0.3235294117647059,
                    0.35294117647058826
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    994000.0,
                    973000.0,
                    1010000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    50700.0,
                    55600.0,
                    81800.0
                ],
                "word_count_appended": [
                    3.0,
                    3.0,
                    5.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    1.44,
                    0.56
                ],
                "result_count": [
                    0,
                    0,
                    15.0
                ]
            },
            "integer_answers": {
                "wool sweater / slacks": 2,
                "cotton t-shirt / shorts": 1,
                "jean jacket / jeans": 7
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these creatures is most likely to bark?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "dog"
            ],
            "lines": [
                [
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0,
                    0.08029647930821494,
                    0.1878114220007666,
                    0.012188623950979086,
                    0.11511005353955979,
                    0.038461538461538464,
                    0.20113851992409867,
                    0.2605042016806723,
                    0.2940918527385438,
                    0.3333333333333333,
                    0.05106382978723404,
                    -1.0
                ],
                [
                    0.75,
                    0.6666666666666666,
                    0.0,
                    0,
                    0.3631871525633107,
                    0.10310463779225756,
                    0.03536699080857866,
                    0.15466983938132065,
                    0.2692307692307692,
                    0.15654648956356737,
                    0.025210084033613446,
                    0.29137772602086576,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.25,
                    0.3333333333333333,
                    0.6666666666666666,
                    0,
                    0.5565163681284744,
                    0.7090839402069758,
                    0.9524443852404423,
                    0.7302201070791196,
                    0.6923076923076923,
                    0.642314990512334,
                    0.7142857142857143,
                    0.4145304212405905,
                    0.6666666666666666,
                    0.948936170212766,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "blue whale": 0.2165661812354577,
                "mime": 0.14671793754294418,
                "dog": 0.6367158812215982
            },
            "question": "which of these creatures is most likely to bark?",
            "rate_limited": false,
            "answers": [
                "mime",
                "blue whale",
                "dog"
            ],
            "ml_answers": {
                "blue whale": 0.4049232987687449,
                "mime": 0.042510595841808684,
                "dog": 0.6324801792149606
            },
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "integer_answers": {
                "blue whale": 2,
                "mime": 0,
                "dog": 11
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.8822755582156314,
                    0.8741331780625973,
                    1.2435912637217714
                ],
                "result_count_important_words": [
                    183000.0,
                    531000.0,
                    14300000.0
                ],
                "wikipedia_search": [
                    0.07692307692307693,
                    0.5384615384615384,
                    1.3846153846153846
                ],
                "answer_relation_to_question": [
                    0.0,
                    1.5,
                    0.5
                ],
                "result_count": [
                    130000.0,
                    588000.0,
                    901000.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.6666666666666666,
                    0.3333333333333333
                ],
                "question_related_to_answer": [
                    0.3333333333333333,
                    0.0,
                    0.6666666666666666
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    774000.0,
                    1040000.0,
                    4910000.0
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    4.0
                ],
                "result_count_bing": [
                    4900000.0,
                    2690000.0,
                    18500000.0
                ],
                "word_count_raw": [
                    12.0,
                    0.0,
                    223.0
                ],
                "word_count_appended": [
                    212.0,
                    165.0,
                    677.0
                ],
                "word_count_appended_bing": [
                    31.0,
                    3.0,
                    85.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these phrases, written backwards, is a hip hop group?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "beat chefs"
            ],
            "question": "which of these phrases, written backwards, is a hip hop group?",
            "answers": [
                "drummers ear",
                "beat chefs",
                "blues rhythm"
            ],
            "integer_answers": {
                "beat chefs": 1,
                "drummers ear": 3,
                "blues rhythm": 6
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.4736506728301488,
                    0.8676776290515065,
                    3.658671698118345
                ],
                "result_count_important_words": [
                    56.0,
                    6.0,
                    5760.0
                ],
                "wikipedia_search": [
                    1.8170583369824358,
                    0.0,
                    2.182941663017564
                ],
                "word_count_appended_bing": [
                    3.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.7179487179487178,
                    0.5641025641025641,
                    0.717948717948718
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    53.0,
                    40.0,
                    1980000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    70400.0,
                    197000.0,
                    301000.0
                ],
                "word_count_appended": [
                    13.0,
                    10.0,
                    9.0
                ],
                "answer_relation_to_question": [
                    2.324116424116424,
                    3.2587058212058215,
                    0.4171777546777547
                ],
                "result_count": [
                    42.0,
                    6.0,
                    5600.0
                ]
            },
            "negative_question": false,
            "fraction_answers": {
                "beat chefs": 0.18226790571212392,
                "drummers ear": 0.26356349221973735,
                "blues rhythm": 0.5541686020681388
            },
            "lines": [
                [
                    0.3873527373527373,
                    0.5726495726495726,
                    0,
                    0,
                    0.007436260623229462,
                    0.12385643912737508,
                    0.009618687736173137,
                    2.676641955706121e-05,
                    0.45426458424560895,
                    0.40625,
                    0.42857142857142855,
                    0.24560844547169147,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.5431176368676369,
                    0.18803418803418803,
                    0,
                    0,
                    0.0010623229461756375,
                    0.34658691062631947,
                    0.0010305736860185502,
                    2.020107136381978e-05,
                    0.0,
                    0.3125,
                    0.2857142857142857,
                    0.1446129381752511,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.06952962577962578,
                    0.23931623931623933,
                    0,
                    0,
                    0.9915014164305949,
                    0.5295566502463054,
                    0.9893507385778083,
                    0.9999530325090791,
                    0.545735415754391,
                    0.28125,
                    0.2857142857142857,
                    0.6097786163530575,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "beat chefs": 0.3745963029465561,
                "drummers ear": 0.22691194734306122,
                "blues rhythm": 0.27723292139540057
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these is a French territory?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "french guiana",
                "french stewart",
                "french cyprus"
            ],
            "lines": [
                [
                    0.6994505494505494,
                    0.7230769230769231,
                    1.0,
                    1.0,
                    0.9989548363123343,
                    0.41522491349480967,
                    0.997884140474069,
                    0.9981086929115174,
                    0.48484848484848486,
                    0.6629213483146067,
                    0.7333333333333333,
                    0.515238505510533,
                    1.0,
                    0,
                    -1.0
                ],
                [
                    0.18928571428571428,
                    0.18076923076923077,
                    0.0,
                    0.0,
                    0.0010296329118996612,
                    0.2975778546712803,
                    0.0021114944172798207,
                    0.0018873328011417784,
                    0.18181818181818182,
                    0.27340823970037453,
                    0.13333333333333333,
                    0.31635649090203366,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.11126373626373626,
                    0.09615384615384616,
                    0.0,
                    0.0,
                    1.5530775766084274e-05,
                    0.28719723183391005,
                    4.365108651107322e-06,
                    3.97428734086586e-06,
                    0.3333333333333333,
                    0.06367041198501873,
                    0.13333333333333333,
                    0.1684050035874334,
                    0.0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "french guiana": 0.7868493636713201,
                "french stewart": 0.12135211581619001,
                "french cyprus": 0.09179852051248996
            },
            "question": "which of these is a french territory?",
            "rate_limited": false,
            "answers": [
                "french guiana",
                "french stewart",
                "french cyprus"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "french guiana": 0.0,
                "french stewart": 0.0,
                "french cyprus": 0.0
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.030477011021066,
                    0.6327129818040673,
                    0.3368100071748668
                ],
                "result_count_important_words": [
                    98300000.0,
                    208000.0,
                    430.0
                ],
                "wikipedia_search": [
                    0.9696969696969697,
                    0.36363636363636365,
                    0.6666666666666666
                ],
                "word_count_appended_bing": [
                    11.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.4461538461538461,
                    0.36153846153846153,
                    0.19230769230769232
                ],
                "question_related_to_answer": [
                    1.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    110000000.0,
                    208000.0,
                    438.0
                ],
                "word_count_noun_chunks": [
                    7.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    24000000.0,
                    17200000.0,
                    16600000.0
                ],
                "word_count_appended": [
                    177.0,
                    73.0,
                    17.0
                ],
                "answer_relation_to_question": [
                    1.3989010989010988,
                    0.37857142857142856,
                    0.22252747252747251
                ],
                "result_count": [
                    5210000.0,
                    5370.0,
                    81.0
                ]
            },
            "integer_answers": {
                "french guiana": 13,
                "french stewart": 0,
                "french cyprus": 0
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these phrases appears in a Shakespeare play?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "in such a pickle"
            ],
            "lines": [
                [
                    0.7681836248012719,
                    0.7731829573934837,
                    0,
                    0,
                    4.1344364907287664e-05,
                    0.3449074074074074,
                    0.00010910585509845682,
                    7.129108148570614e-05,
                    0.813953488372093,
                    0.3469387755102041,
                    0.6363636363636364,
                    0.3323304927771308,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.14874801271860094,
                    0.14661654135338345,
                    0,
                    0,
                    0.9999567326413761,
                    0.47337962962962965,
                    0.999887904943392,
                    0.999926857202112,
                    0.13953488372093023,
                    0.6122448979591837,
                    0.18181818181818182,
                    0.6017327900898998,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.08306836248012718,
                    0.08020050125313283,
                    0,
                    0,
                    1.922993716618031e-06,
                    0.18171296296296297,
                    2.9892015095467623e-06,
                    1.8517164022261334e-06,
                    0.046511627906976744,
                    0.04081632653061224,
                    0.18181818181818182,
                    0.06593671713296945,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "at a loss": 0.530384643207669,
                "in such a pickle": 0.40160821239267186,
                "up a dark creek": 0.06800714439965916
            },
            "question": "which of these phrases appears in a shakespeare play?",
            "rate_limited": false,
            "answers": [
                "in such a pickle",
                "at a loss",
                "up a dark creek"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "at a loss": 0.2089114038948246,
                "in such a pickle": 0.5294730579209699,
                "up a dark creek": 0.035774280971791736
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3293219711085231,
                    2.406931160359599,
                    0.2637468685318778
                ],
                "result_count_important_words": [
                    73.0,
                    669000.0,
                    2.0
                ],
                "wikipedia_search": [
                    0.813953488372093,
                    0.13953488372093023,
                    0.046511627906976744
                ],
                "word_count_appended_bing": [
                    7.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    2.319548872180451,
                    0.4398496240601504,
                    0.24060150375939848
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    77.0,
                    1080000.0,
                    2.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    2980000.0,
                    4090000.0,
                    1570000.0
                ],
                "word_count_appended": [
                    34.0,
                    60.0,
                    4.0
                ],
                "answer_relation_to_question": [
                    3.0727344992050876,
                    0.5949920508744038,
                    0.3322734499205087
                ],
                "result_count": [
                    43.0,
                    1040000.0,
                    2.0
                ]
            },
            "integer_answers": {
                "at a loss": 6,
                "in such a pickle": 4,
                "up a dark creek": 0
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What term describes a person from the state between New York and Rhode Island?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "hoosier"
            ],
            "lines": [
                [
                    0.30267977150537634,
                    0.2780481423338566,
                    0.0,
                    0,
                    0.08215707977224232,
                    0.33820459290187893,
                    0.020425677995193958,
                    0.5062368378422161,
                    0.28334892787524363,
                    0.27074235807860264,
                    0.32926829268292684,
                    0.2844436386542451,
                    0,
                    0,
                    1.0
                ],
                [
                    0.44588013632872503,
                    0.2678244374672946,
                    0.5,
                    0,
                    0.0038454077615618972,
                    0.32254697286012524,
                    0.009783728115345005,
                    0.005750850477887575,
                    0.0659337231968811,
                    0.2925764192139738,
                    0.32926829268292684,
                    0.32100423364936276,
                    0,
                    0,
                    1.0
                ],
                [
                    0.25144009216589863,
                    0.4541274201988488,
                    0.5,
                    0,
                    0.9139975124661958,
                    0.33924843423799583,
                    0.969790593889461,
                    0.48801231167989634,
                    0.6507173489278752,
                    0.4366812227074236,
                    0.34146341463414637,
                    0.3945521276963922,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "hoosier": 0.5218209526003758,
                "nutmegger": 0.2331285637958258,
                "cheesehead": 0.24505048360379836
            },
            "question": "what term describes a person from the state between new york and rhode island?",
            "rate_limited": false,
            "answers": [
                "cheesehead",
                "nutmegger",
                "hoosier"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "hoosier": 0.4804981315612494,
                "nutmegger": 0.18630944904884883,
                "cheesehead": 0.442670690704762
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.275549109233961,
                    2.568033869194902,
                    3.1564170215711376
                ],
                "result_count_important_words": [
                    7140.0,
                    3420.0,
                    339000.0
                ],
                "wikipedia_search": [
                    1.4167446393762182,
                    0.32966861598440544,
                    3.2535867446393762
                ],
                "word_count_appended_bing": [
                    27.0,
                    27.0,
                    28.0
                ],
                "answer_relation_to_question_bing": [
                    1.1121925693354264,
                    1.0712977498691785,
                    1.816509680795395
                ],
                "question_related_to_answer": [
                    0.0,
                    0.5,
                    0.5
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    250000.0,
                    2840.0,
                    241000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    32400000.0,
                    30900000.0,
                    32500000.0
                ],
                "word_count_appended": [
                    124.0,
                    134.0,
                    200.0
                ],
                "answer_relation_to_question": [
                    1.816078629032258,
                    2.67528081797235,
                    1.5086405529953917
                ],
                "result_count": [
                    72000.0,
                    3370.0,
                    801000.0
                ]
            },
            "integer_answers": {
                "hoosier": 8,
                "nutmegger": 2,
                "cheesehead": 1
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Where in the home does the Maillard reaction typically occur?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "kitchen"
            ],
            "lines": [
                [
                    0.6462585034013606,
                    0.24242424242424243,
                    0.9772727272727273,
                    0.6666666666666666,
                    0.7806291777684289,
                    0.45013850415512463,
                    0.7811769108429226,
                    0.638328530259366,
                    0.7032784793978824,
                    0.4642857142857143,
                    0.35398230088495575,
                    0.4238794129610711,
                    1.0,
                    1.0,
                    3.0
                ],
                [
                    0.1598639455782313,
                    0.48484848484848486,
                    0.0,
                    0.0,
                    0.04174262609264713,
                    0.21329639889196675,
                    0.041538029750210495,
                    0.15417867435158503,
                    0.12590891695369308,
                    0.25487012987012986,
                    0.24778761061946902,
                    0.2728025962042836,
                    0.0,
                    0.0,
                    3.0
                ],
                [
                    0.19387755102040816,
                    0.2727272727272727,
                    0.022727272727272728,
                    0.3333333333333333,
                    0.17762819613892394,
                    0.3365650969529086,
                    0.17728505940686687,
                    0.207492795389049,
                    0.17081260364842454,
                    0.28084415584415584,
                    0.39823008849557523,
                    0.3033179908346453,
                    0.0,
                    0.0,
                    3.0
                ]
            ],
            "fraction_answers": {
                "bathroom": 0.20534581546563116,
                "kitchen": 0.6520229407371758,
                "bedroom": 0.14263124379719294
            },
            "question": "where in the home does the maillard reaction typically occur?",
            "rate_limited": false,
            "answers": [
                "kitchen",
                "bedroom",
                "bathroom"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "bathroom": 0.2014366514184146,
                "kitchen": 0.9740155836701279,
                "bedroom": 0.05935943462826665
            },
            "integer_answers": {
                "bathroom": 1,
                "kitchen": 12,
                "bedroom": 1
            },
            "categorical_data": {
                "question_type": 3
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.1193970648053555,
                    1.3640129810214179,
                    1.5165899541732264
                ],
                "result_count_important_words": [
                    167000.0,
                    8880.0,
                    37900.0
                ],
                "wikipedia_search": [
                    2.109835438193647,
                    0.3777267508610792,
                    0.5124378109452736
                ],
                "word_count_appended_bing": [
                    40.0,
                    28.0,
                    45.0
                ],
                "answer_relation_to_question_bing": [
                    0.24242424242424243,
                    0.48484848484848486,
                    0.2727272727272727
                ],
                "question_related_to_answer": [
                    1.9545454545454546,
                    0.0,
                    0.045454545454545456
                ],
                "question_related_to_answer_bing": [
                    0.6666666666666666,
                    0.0,
                    0.3333333333333333
                ],
                "result_count_noun_chunks": [
                    443000.0,
                    107000.0,
                    144000.0
                ],
                "word_count_noun_chunks": [
                    5.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    325000.0,
                    154000.0,
                    243000.0
                ],
                "word_count_raw": [
                    25.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    167000.0,
                    8930.0,
                    38000.0
                ],
                "answer_relation_to_question": [
                    1.2925170068027212,
                    0.3197278911564626,
                    0.3877551020408163
                ],
                "word_count_appended": [
                    286.0,
                    157.0,
                    173.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Who is the director of \u201cTyler Perry\u2019s Madea\u2019s Family Reunion\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "tyler perry",
                "george lucas",
                "abraham lincoln"
            ],
            "lines": [
                [
                    0.8830677087739621,
                    0.7416666666666666,
                    1.0,
                    1.0,
                    0.6242038216560509,
                    0.5378442854044676,
                    0.2849264705882353,
                    0.5855263157894737,
                    0.6392543859649122,
                    0.8366013071895425,
                    0.7,
                    0.7432977214952499,
                    0.9948717948717949,
                    0.9958677685950413,
                    0.0
                ],
                [
                    0.08745382210855357,
                    0.18333333333333332,
                    0.0,
                    0.0,
                    0.2356687898089172,
                    0.21492084146605941,
                    0.30514705882352944,
                    0.2565789473684211,
                    0.15021929824561403,
                    0.08496732026143791,
                    0.05,
                    0.1320423904900246,
                    0.0,
                    0.0,
                    0.0
                ],
                [
                    0.029478469117484366,
                    0.075,
                    0.0,
                    0.0,
                    0.14012738853503184,
                    0.247234873129473,
                    0.4099264705882353,
                    0.15789473684210525,
                    0.21052631578947367,
                    0.0784313725490196,
                    0.25,
                    0.12465988801472554,
                    0.005128205128205128,
                    0.004132231404958678,
                    0.0
                ]
            ],
            "fraction_answers": {
                "tyler perry": 0.7547948747853855,
                "abraham lincoln": 0.123752853649908,
                "george lucas": 0.12145227156470648
            },
            "question": "who is the director of \u201ctyler perry\u2019s madea\u2019s family reunion\u201d?",
            "rate_limited": false,
            "answers": [
                "tyler perry",
                "george lucas",
                "abraham lincoln"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "tyler perry": 0.0,
                "abraham lincoln": 0.0,
                "george lucas": 0.0
            },
            "integer_answers": {
                "tyler perry": 13,
                "abraham lincoln": 1,
                "george lucas": 0
            },
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    4.459786328971499,
                    0.7922543429401476,
                    0.7479593280883532
                ],
                "result_count_important_words": [
                    155000.0,
                    166000.0,
                    223000.0
                ],
                "wikipedia_search": [
                    3.8355263157894735,
                    0.9013157894736842,
                    1.263157894736842
                ],
                "word_count_appended_bing": [
                    28.0,
                    2.0,
                    10.0
                ],
                "answer_relation_to_question_bing": [
                    3.708333333333333,
                    0.9166666666666666,
                    0.375
                ],
                "question_related_to_answer": [
                    3.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    2.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    89.0,
                    39.0,
                    24.0
                ],
                "word_count_noun_chunks": [
                    194.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    248000.0,
                    99100.0,
                    114000.0
                ],
                "word_count_raw": [
                    241.0,
                    0.0,
                    1.0
                ],
                "result_count": [
                    98.0,
                    37.0,
                    22.0
                ],
                "answer_relation_to_question": [
                    5.2984062526437725,
                    0.5247229326513214,
                    0.1768708147049062
                ],
                "word_count_appended": [
                    256.0,
                    26.0,
                    24.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "The first person to lead an expedition to the South Pole came from what country?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "norway"
            ],
            "lines": [
                [
                    0.12075860647289219,
                    0.27499999999999997,
                    0.3166666666666667,
                    0.0,
                    0.026581179456354773,
                    0.1945227547321788,
                    0.2682926829268293,
                    0.2732049036777583,
                    0.3841568718276065,
                    0.30741190765492105,
                    0.358695652173913,
                    0.3460618036967216,
                    0.75,
                    0.5,
                    1.0
                ],
                [
                    0.5504844361987219,
                    0.275,
                    0.3541666666666667,
                    1.0,
                    0.015027048687637748,
                    0.1288763592428514,
                    0.2109038737446198,
                    0.19614711033274956,
                    0.3842689900612428,
                    0.38517618469015796,
                    0.31521739130434784,
                    0.32188791277542833,
                    0.0,
                    0.0625,
                    1.0
                ],
                [
                    0.3287569573283859,
                    0.4499999999999999,
                    0.32916666666666666,
                    0.0,
                    0.9583917718560074,
                    0.6766008860249698,
                    0.5208034433285509,
                    0.5306479859894921,
                    0.23157413811115066,
                    0.30741190765492105,
                    0.32608695652173914,
                    0.3320502835278501,
                    0.25,
                    0.4375,
                    1.0
                ]
            ],
            "fraction_answers": {
                "canada": 0.4056422140721238,
                "iceland": 0.2999754266931732,
                "norway": 0.294382359234703
            },
            "question": "the first person to lead an expedition to the south pole came from what country?",
            "rate_limited": false,
            "answers": [
                "norway",
                "iceland",
                "canada"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "canada": 0.15447214833415635,
                "iceland": 0.35107610898637565,
                "norway": 0.46281720642920665
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.422432625877051,
                    2.2532153894279983,
                    2.3243519846949505
                ],
                "result_count_important_words": [
                    187000.0,
                    147000.0,
                    363000.0
                ],
                "wikipedia_search": [
                    2.6890981027932455,
                    2.6898829304286997,
                    1.6210189667780546
                ],
                "answer_relation_to_question": [
                    0.8453102453102453,
                    3.853391053391053,
                    2.3012987012987014
                ],
                "answer_relation_to_question_bing": [
                    1.3749999999999998,
                    1.375,
                    2.2499999999999996
                ],
                "word_count_appended": [
                    253.0,
                    317.0,
                    253.0
                ],
                "question_related_to_answer": [
                    1.2666666666666668,
                    1.4166666666666667,
                    1.3166666666666667
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    156000.0,
                    112000.0,
                    303000.0
                ],
                "word_count_noun_chunks": [
                    6.0,
                    0.0,
                    2.0
                ],
                "word_count_raw": [
                    8.0,
                    1.0,
                    7.0
                ],
                "result_count_bing": [
                    483000.0,
                    320000.0,
                    1680000.0
                ],
                "result_count": [
                    79600.0,
                    45000.0,
                    2870000.0
                ],
                "word_count_appended_bing": [
                    33.0,
                    29.0,
                    30.0
                ]
            },
            "integer_answers": {
                "canada": 5,
                "iceland": 5,
                "norway": 4
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these modes of transportation has only one wheel?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "bus"
            ],
            "lines": [
                [
                    0.0676328502415459,
                    0.0,
                    0.0,
                    0.0,
                    0.2632375189107413,
                    0.1279576587795766,
                    0.02529033302310525,
                    0.26939970717423134,
                    0.09574468085106383,
                    0.10869565217391304,
                    0.03296703296703297,
                    0.26889222587519795,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.6653491436100132,
                    0.5,
                    0.7619047619047619,
                    0.9333333333333332,
                    0.5128593040847201,
                    0.7067247820672479,
                    0.9630558815198479,
                    0.5109809663250366,
                    0.5425531914893617,
                    0.39473684210526316,
                    0.3516483516483517,
                    0.3783555593559136,
                    0.8,
                    0.6170212765957447,
                    -1.0
                ],
                [
                    0.26701800614844096,
                    0.5,
                    0.2380952380952381,
                    0.06666666666666667,
                    0.22390317700453857,
                    0.1653175591531756,
                    0.011653785457046898,
                    0.21961932650073207,
                    0.3617021276595745,
                    0.4965675057208238,
                    0.6153846153846154,
                    0.3527522147688884,
                    0.2,
                    0.3829787234042553,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "unicycle": 0.29297563899742834,
                "bus": 0.6170373852885425,
                "monster truck": 0.08998697571402915
            },
            "question": "which of these modes of transportation has only one wheel?",
            "rate_limited": false,
            "answers": [
                "monster truck",
                "bus",
                "unicycle"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "unicycle": 0.5891255675765179,
                "bus": 0.8020118237402313,
                "monster truck": 0.007062122984029546
            },
            "integer_answers": {
                "unicycle": 2,
                "bus": 12,
                "monster truck": 0
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.8066766776255939,
                    1.1350666780677408,
                    1.0582566443066652
                ],
                "result_count_important_words": [
                    1250000.0,
                    47600000.0,
                    576000.0
                ],
                "wikipedia_search": [
                    0.19148936170212766,
                    1.0851063829787233,
                    0.723404255319149
                ],
                "word_count_appended_bing": [
                    3.0,
                    32.0,
                    56.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    1.0,
                    1.0
                ],
                "question_related_to_answer": [
                    0.0,
                    2.2857142857142856,
                    0.7142857142857143
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    2.8,
                    0.2
                ],
                "result_count_noun_chunks": [
                    1840000.0,
                    3490000.0,
                    1500000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    24.0,
                    6.0
                ],
                "result_count_bing": [
                    411000.0,
                    2270000.0,
                    531000.0
                ],
                "word_count_raw": [
                    0.0,
                    29.0,
                    18.0
                ],
                "word_count_appended": [
                    95.0,
                    345.0,
                    434.0
                ],
                "answer_relation_to_question": [
                    0.2028985507246377,
                    1.9960474308300395,
                    0.8010540184453229
                ],
                "result_count": [
                    1740000.0,
                    3390000.0,
                    1480000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these astronomical objects orbits the Earth?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "sun"
            ],
            "lines": [
                [
                    0.4770833333333333,
                    0.673469387755102,
                    0.44221698113207547,
                    0.369047619047619,
                    0.4126679462571977,
                    0.20155038759689922,
                    0.33111480865224624,
                    0.3751937984496124,
                    0.40514200711569137,
                    0.49281767955801103,
                    0.4573170731707317,
                    0.3365762163093491,
                    0.47619047619047616,
                    0.37058823529411766,
                    -1.0
                ],
                [
                    0.37916666666666665,
                    0.30612244897959184,
                    0.04716981132075472,
                    0.0,
                    0.28790786948176583,
                    0.5904392764857881,
                    0.2795341098169717,
                    0.27286821705426356,
                    0.3276821862348178,
                    0.09281767955801105,
                    0.2073170731707317,
                    0.32384384204739247,
                    0.047619047619047616,
                    0.01764705882352941,
                    -1.0
                ],
                [
                    0.14375,
                    0.02040816326530612,
                    0.5106132075471699,
                    0.6309523809523809,
                    0.29942418426103645,
                    0.20801033591731266,
                    0.389351081530782,
                    0.351937984496124,
                    0.26717580664949087,
                    0.4143646408839779,
                    0.3353658536585366,
                    0.33957994164325844,
                    0.47619047619047616,
                    0.611764705882353,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "sun": 0.35706348306272895,
                "milky way": 0.22715252051852375,
                "moon": 0.4157839964187473
            },
            "question": "which of these astronomical objects orbits the earth?",
            "rate_limited": false,
            "answers": [
                "moon",
                "milky way",
                "sun"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sun": 0.4550199985230922,
                "milky way": 0.12082506578302002,
                "moon": 0.42435344774202255
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3463048652373963,
                    1.2953753681895699,
                    1.3583197665730338
                ],
                "result_count_important_words": [
                    796000.0,
                    672000.0,
                    936000.0
                ],
                "wikipedia_search": [
                    1.6205680284627655,
                    1.3107287449392713,
                    1.0687032265979635
                ],
                "word_count_appended_bing": [
                    75.0,
                    34.0,
                    55.0
                ],
                "answer_relation_to_question_bing": [
                    2.693877551020408,
                    1.2244897959183674,
                    0.08163265306122448
                ],
                "question_related_to_answer": [
                    0.8844339622641509,
                    0.09433962264150944,
                    1.0212264150943398
                ],
                "question_related_to_answer_bing": [
                    0.738095238095238,
                    0.0,
                    1.2619047619047619
                ],
                "result_count_noun_chunks": [
                    2420000.0,
                    1760000.0,
                    2270000.0
                ],
                "word_count_noun_chunks": [
                    60.0,
                    6.0,
                    60.0
                ],
                "word_count_raw": [
                    63.0,
                    3.0,
                    104.0
                ],
                "result_count_bing": [
                    156000.0,
                    457000.0,
                    161000.0
                ],
                "word_count_appended": [
                    446.0,
                    84.0,
                    375.0
                ],
                "answer_relation_to_question": [
                    1.9083333333333332,
                    1.5166666666666666,
                    0.575
                ],
                "result_count": [
                    2150000.0,
                    1500000.0,
                    1560000.0
                ]
            },
            "integer_answers": {
                "sun": 5,
                "milky way": 1,
                "moon": 8
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Made famous in a documentary, where is the \u201cGrey Gardens\u201d home located?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "east hampton, ny"
            ],
            "lines": [
                [
                    0.26619510757441794,
                    0.43434343434343436,
                    0,
                    0.0,
                    0.16842105263157894,
                    0.07301687431004573,
                    0.0008121182161646835,
                    0.757507709811671,
                    0.3482142857142857,
                    0.15151515151515152,
                    0.2,
                    0.1464328120294846,
                    0.0,
                    0.0,
                    3.0
                ],
                [
                    0.5254936634246979,
                    0.25883838383838387,
                    0,
                    1.0,
                    0.4631578947368421,
                    0.07065131682699889,
                    0.001694855407648035,
                    0.22246175035180693,
                    0.38125,
                    0.6363636363636364,
                    0.3,
                    0.5757562936438623,
                    1.0,
                    1.0,
                    3.0
                ],
                [
                    0.20831122900088417,
                    0.3068181818181818,
                    0,
                    0.0,
                    0.3684210526315789,
                    0.8563318088629553,
                    0.9974930263761873,
                    0.020030539836522053,
                    0.27053571428571427,
                    0.21212121212121213,
                    0.5,
                    0.27781089432665323,
                    0.0,
                    0.0,
                    3.0
                ]
            ],
            "fraction_answers": {
                "east hampton, ny": 0.4950513688149136,
                "savannah, ga": 0.309067204558453,
                "cape cod, ma": 0.19588142662663344
            },
            "question": "made famous in a documentary, where is the \u201cgrey gardens\u201d home located?",
            "rate_limited": false,
            "answers": [
                "cape cod, ma",
                "east hampton, ny",
                "savannah, ga"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "east hampton, ny": 0.3952380970830009,
                "savannah, ga": 0.31499999945362406,
                "cape cod, ma": 0.3194444440305233
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 3
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.8785968721769075,
                    3.454537761863173,
                    1.666865365959919
                ],
                "result_count_important_words": [
                    46.0,
                    96.0,
                    56500.0
                ],
                "wikipedia_search": [
                    1.3928571428571428,
                    1.525,
                    1.082142857142857
                ],
                "word_count_appended_bing": [
                    2.0,
                    3.0,
                    5.0
                ],
                "answer_relation_to_question_bing": [
                    1.7373737373737375,
                    1.0353535353535355,
                    1.2272727272727273
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    253000.0,
                    74300.0,
                    6690.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count_bing": [
                    463000.0,
                    448000.0,
                    5430000.0
                ],
                "word_count_appended": [
                    5.0,
                    21.0,
                    7.0
                ],
                "answer_relation_to_question": [
                    0.7985853227232538,
                    1.576480990274094,
                    0.6249336870026525
                ],
                "result_count": [
                    32.0,
                    88.0,
                    70.0
                ]
            },
            "integer_answers": {
                "east hampton, ny": 8,
                "savannah, ga": 3,
                "cape cod, ma": 2
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these is NOT a suit in a traditional Tarot deck?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "gloves"
            ],
            "lines": [
                [
                    0.4875,
                    0,
                    0.5,
                    0,
                    0.42201834862385323,
                    0.30207328833172614,
                    0.4260407440212578,
                    0.4257777777777778,
                    0.24043062200956938,
                    0.4015984015984016,
                    0.40878378378378377,
                    0.3806529314285849,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.060317460317460325,
                    0,
                    0.2,
                    0,
                    0.268348623853211,
                    0.4047733847637416,
                    0.283436669619132,
                    0.2813333333333333,
                    0.3181818181818182,
                    0.28621378621378624,
                    0.2736486486486487,
                    0.3100319071826884,
                    0.1781609195402299,
                    0.2013888888888889,
                    -1.0
                ],
                [
                    0.4521825396825397,
                    0,
                    0.3,
                    0,
                    0.30963302752293576,
                    0.29315332690453233,
                    0.2905225863596103,
                    0.29288888888888887,
                    0.44138755980861244,
                    0.3121878121878122,
                    0.31756756756756754,
                    0.3093151613887268,
                    0.3218390804597701,
                    0.2986111111111111,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "swords": 0.4890274265761769,
                "cups": 0.34345188968631546,
                "gloves": 0.1675206837375076
            },
            "question": "which of these is not a suit in a traditional tarot deck?",
            "rate_limited": false,
            "answers": [
                "gloves",
                "swords",
                "cups"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "swords": 0.05743171046707154,
                "cups": 0.12626333857941313,
                "gloves": 0.4966184294607129
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.9547765485713209,
                    1.5197447425384931,
                    1.5254787088901858
                ],
                "result_count_important_words": [
                    167000.0,
                    489000.0,
                    473000.0
                ],
                "wikipedia_search": [
                    1.0382775119617225,
                    0.7272727272727273,
                    0.23444976076555024
                ],
                "answer_relation_to_question": [
                    0.1,
                    3.5174603174603174,
                    0.38253968253968257
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    197.0,
                    428.0,
                    376.0
                ],
                "question_related_to_answer": [
                    0.0,
                    0.6,
                    0.4
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    167000.0,
                    492000.0,
                    466000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    56.0,
                    31.0
                ],
                "word_count_raw": [
                    0.0,
                    43.0,
                    29.0
                ],
                "result_count_bing": [
                    821000.0,
                    395000.0,
                    858000.0
                ],
                "result_count": [
                    340000.0,
                    1010000.0,
                    830000.0
                ],
                "word_count_appended_bing": [
                    27.0,
                    67.0,
                    54.0
                ]
            },
            "integer_answers": {
                "swords": 9,
                "cups": 2,
                "gloves": 1
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Dominique Ansel is credited with creating what food craze?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "rainbow bagel",
                "ramen burger",
                "cronut"
            ],
            "lines": [
                [
                    0.185881126173097,
                    0.32900900900900903,
                    0.0,
                    0.0,
                    0.20465116279069767,
                    0.20413699559172602,
                    0.0007250504945880159,
                    0.04188991719434973,
                    0.0,
                    0.04900181488203267,
                    0.021505376344086023,
                    0.1623143805646842,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.2712648206078863,
                    0.15603603603603605,
                    0.0,
                    0.0,
                    0.40930232558139534,
                    0.3380807053238386,
                    0.002330519446890051,
                    0.19581100828056502,
                    0.24393939393939396,
                    0.10707803992740472,
                    0.10752688172043011,
                    0.3754108341980232,
                    0.0,
                    0.010676156583629894,
                    1.0
                ],
                [
                    0.5428540532190167,
                    0.5149549549549549,
                    1.0,
                    1.0,
                    0.386046511627907,
                    0.4577822990844354,
                    0.996944430058522,
                    0.7622990745250853,
                    0.7560606060606061,
                    0.8439201451905626,
                    0.8709677419354839,
                    0.46227478523729254,
                    1.0,
                    0.9893238434163701,
                    1.0
                ]
            ],
            "fraction_answers": {
                "ramen burger": 0.15838976583182096,
                "cronut": 0.7559591746650168,
                "rainbow bagel": 0.08565105950316217
            },
            "question": "dominique ansel is credited with creating what food craze?",
            "rate_limited": false,
            "answers": [
                "rainbow bagel",
                "ramen burger",
                "cronut"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "ramen burger": 0.0,
                "cronut": 0.0,
                "rainbow bagel": 0.0
            },
            "integer_answers": {
                "ramen burger": 1,
                "cronut": 13,
                "rainbow bagel": 0
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.9738862833881051,
                    2.252465005188139,
                    2.773648711423755
                ],
                "result_count_important_words": [
                    28.0,
                    90.0,
                    38500.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.9757575757575758,
                    3.0242424242424244
                ],
                "word_count_appended_bing": [
                    2.0,
                    10.0,
                    81.0
                ],
                "answer_relation_to_question_bing": [
                    1.6450450450450451,
                    0.7801801801801802,
                    2.5747747747747747
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    1.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_noun_chunks": [
                    17200.0,
                    80400.0,
                    313000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    131.0
                ],
                "result_count_bing": [
                    6020.0,
                    9970.0,
                    13500.0
                ],
                "word_count_raw": [
                    0.0,
                    3.0,
                    278.0
                ],
                "result_count": [
                    44.0,
                    88.0,
                    83.0
                ],
                "answer_relation_to_question": [
                    0.9294056308654849,
                    1.3563241030394315,
                    2.7142702660950833
                ],
                "word_count_appended": [
                    27.0,
                    59.0,
                    465.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which person is most famous for being a children\u2019s book writer?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "dr. seuss"
            ],
            "lines": [
                [
                    0.2500872777896766,
                    0.20784313725490197,
                    0.0,
                    0,
                    0.23070641077991016,
                    0.8005071085755683,
                    0.3848714875482754,
                    0.27405857740585776,
                    0.42870234186222983,
                    0.2231404958677686,
                    0.44,
                    0.29660655864613034,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.6764644463482544,
                    0.5458823529411765,
                    1.0,
                    0,
                    0.7227439771335239,
                    0.1974101240604908,
                    0.506059395392196,
                    0.639569635385535,
                    0.34343663124324664,
                    0.3884297520661157,
                    0.4,
                    0.4048032322084235,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.07344827586206897,
                    0.24627450980392157,
                    0.0,
                    0,
                    0.046549612086565946,
                    0.0020827673639409582,
                    0.10906911705952857,
                    0.08637178720860729,
                    0.2278610268945235,
                    0.3884297520661157,
                    0.16,
                    0.2985902091454461,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "tiger woods": 0.12605208134543988,
                "paris hilton": 0.27204026121002456,
                "dr. seuss": 0.6019076574445354
            },
            "question": "which person is most famous for being a children\u2019s book writer?",
            "rate_limited": false,
            "answers": [
                "paris hilton",
                "dr. seuss",
                "tiger woods"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "tiger woods": 0.29999999900658925,
                "paris hilton": 0.3194444440305233,
                "dr. seuss": 0.3952380970830009
            },
            "integer_answers": {
                "tiger woods": 0,
                "paris hilton": 3,
                "dr. seuss": 10
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.4830327932306517,
                    2.0240161610421175,
                    1.4929510457272306
                ],
                "result_count_important_words": [
                    2890000.0,
                    3800000.0,
                    819000.0
                ],
                "wikipedia_search": [
                    2.143511709311149,
                    1.7171831562162332,
                    1.1393051344726175
                ],
                "word_count_appended_bing": [
                    33.0,
                    30.0,
                    12.0
                ],
                "answer_relation_to_question_bing": [
                    1.0392156862745099,
                    2.7294117647058824,
                    1.231372549019608
                ],
                "question_related_to_answer": [
                    0.0,
                    1.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    917000.0,
                    2140000.0,
                    289000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    2.0,
                    0.0
                ],
                "result_count_bing": [
                    44200000.0,
                    10900000.0,
                    115000.0
                ],
                "result_count": [
                    1130000.0,
                    3540000.0,
                    228000.0
                ],
                "answer_relation_to_question": [
                    1.250436388948383,
                    3.3823222317412722,
                    0.36724137931034484
                ],
                "word_count_appended": [
                    81.0,
                    141.0,
                    141.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "How does the second verse of \u201cThe Star-Spangled Banner\u201d begin?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "travels far",
                "on the shore",
                "where the foe"
            ],
            "lines": [
                [
                    0.24868421052631579,
                    0.22889784946236558,
                    0,
                    0,
                    0.0034045187248529867,
                    0.21348314606741572,
                    0.010008621659107096,
                    0.0022300553916984388,
                    0.5833333333333334,
                    0.03488372093023256,
                    0.1111111111111111,
                    0.044165092963120135,
                    0.0,
                    0.0,
                    5.0
                ],
                [
                    0.3767543859649123,
                    0.5317876344086022,
                    0,
                    0,
                    0.659238625812442,
                    0.5730337078651685,
                    0.94838250178056,
                    0.8632472483993957,
                    0.06250000000000001,
                    0.5,
                    0.4444444444444444,
                    0.4795406292718973,
                    0.7142857142857143,
                    1.0,
                    5.0
                ],
                [
                    0.37456140350877193,
                    0.23931451612903223,
                    0,
                    0,
                    0.33735685546270505,
                    0.21348314606741572,
                    0.04160887656033287,
                    0.13452269620890583,
                    0.3541666666666667,
                    0.46511627906976744,
                    0.4444444444444444,
                    0.4762942777649826,
                    0.2857142857142857,
                    0.0,
                    5.0
                ]
            ],
            "fraction_answers": {
                "on the shore": 0.5961012410194281,
                "travels far": 0.12335013834746274,
                "where the foe": 0.2805486206331092
            },
            "question": "how does the second verse of \u201cthe star-spangled banner\u201d begin?",
            "rate_limited": false,
            "answers": [
                "travels far",
                "on the shore",
                "where the foe"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "on the shore": 0.0,
                "travels far": 0.0,
                "where the foe": 0.0
            },
            "integer_answers": {
                "on the shore": 11,
                "travels far": 1,
                "where the foe": 0
            },
            "categorical_data": {
                "question_type": 5
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.2649905577787208,
                    2.8772437756313836,
                    2.857765666589896
                ],
                "result_count_important_words": [
                    2670.0,
                    253000.0,
                    11100.0
                ],
                "wikipedia_search": [
                    2.333333333333333,
                    0.25,
                    1.4166666666666665
                ],
                "word_count_appended_bing": [
                    2.0,
                    8.0,
                    8.0
                ],
                "answer_relation_to_question_bing": [
                    0.6866935483870967,
                    1.5953629032258065,
                    0.7179435483870967
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    31.0,
                    12000.0,
                    1870.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    5.0,
                    2.0
                ],
                "result_count_bing": [
                    114000.0,
                    306000.0,
                    114000.0
                ],
                "word_count_raw": [
                    0.0,
                    2.0,
                    0.0
                ],
                "result_count": [
                    11.0,
                    2130.0,
                    1090.0
                ],
                "answer_relation_to_question": [
                    0.9947368421052631,
                    1.5070175438596491,
                    1.4982456140350877
                ],
                "word_count_appended": [
                    3.0,
                    43.0,
                    40.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these substances is both artificially made and found in nature?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "nylon"
            ],
            "lines": [
                [
                    0.5333333333333333,
                    0.16666666666666666,
                    0,
                    0,
                    0.24947589098532494,
                    0.3431013431013431,
                    0.305338078291815,
                    0.3136902623068631,
                    0.2833333333333333,
                    0.28942307692307695,
                    0.3805309734513274,
                    0.3292226814160622,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.3333333333333333,
                    0.08333333333333333,
                    0,
                    0,
                    0.40670859538784065,
                    0.32844932844932845,
                    0.3516014234875445,
                    0.3406395975565936,
                    0.09999999999999999,
                    0.30865384615384617,
                    0.336283185840708,
                    0.33643905416123965,
                    0,
                    1.0,
                    -1.0
                ],
                [
                    0.13333333333333333,
                    0.75,
                    0,
                    0,
                    0.3438155136268344,
                    0.32844932844932845,
                    0.34306049822064055,
                    0.3456701401365433,
                    0.6166666666666667,
                    0.40192307692307694,
                    0.2831858407079646,
                    0.33433826442269815,
                    0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "latex": 0.3527675147715534,
                "teflon": 0.29037414907355874,
                "nylon": 0.35685833615488793
            },
            "question": "which of these substances is both artificially made and found in nature?",
            "rate_limited": false,
            "answers": [
                "teflon",
                "nylon",
                "latex"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "latex": 0.31499999945362406,
                "teflon": 0.3194444440305233,
                "nylon": 0.354545455177625
            },
            "integer_answers": {
                "latex": 4,
                "teflon": 3,
                "nylon": 4
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3168907256642488,
                    1.3457562166449586,
                    1.3373530576907926
                ],
                "result_count_important_words": [
                    429000.0,
                    494000.0,
                    482000.0
                ],
                "wikipedia_search": [
                    0.85,
                    0.3,
                    1.85
                ],
                "word_count_appended_bing": [
                    43.0,
                    38.0,
                    32.0
                ],
                "answer_relation_to_question_bing": [
                    0.3333333333333333,
                    0.16666666666666666,
                    1.5
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    873000.0,
                    948000.0,
                    962000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    2810000.0,
                    2690000.0,
                    2690000.0
                ],
                "word_count_raw": [
                    0.0,
                    4.0,
                    0.0
                ],
                "result_count": [
                    1190000.0,
                    1940000.0,
                    1640000.0
                ],
                "answer_relation_to_question": [
                    0.5333333333333333,
                    0.3333333333333333,
                    0.13333333333333333
                ],
                "word_count_appended": [
                    301.0,
                    321.0,
                    418.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Every U.S. state that starts with which of these letters has a Democratic governor?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "v"
            ],
            "lines": [
                [
                    0.22393822393822393,
                    0.2441890166028097,
                    0.2377584300543481,
                    0.212969855908523,
                    0.31885073580939033,
                    0.3305719921104536,
                    0.32748538011695905,
                    0.2823779193205945,
                    0.18421052631578946,
                    0.23584108199492815,
                    0.22188995215311005,
                    0.32826067750794236,
                    0.21898790864308107,
                    0.2272944550669216,
                    -1.0
                ],
                [
                    0.12113899613899615,
                    0.2803746275010643,
                    0.4656541167253135,
                    0.5848596862185288,
                    0.4176594253679047,
                    0.33727810650887574,
                    0.35477582846003897,
                    0.42038216560509556,
                    0.5,
                    0.4606931530008453,
                    0.5823365231259968,
                    0.32268204833922737,
                    0.4815942678011644,
                    0.4669375398342894,
                    -1.0
                ],
                [
                    0.6549227799227799,
                    0.47543635589612593,
                    0.29658745322033847,
                    0.2021704578729483,
                    0.263489838822705,
                    0.33214990138067063,
                    0.31773879142300193,
                    0.29723991507430997,
                    0.3157894736842105,
                    0.30346576500422656,
                    0.19577352472089316,
                    0.34905727415283033,
                    0.2994178235557546,
                    0.30576800509878904,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "c": 0.41402617747338155,
                "w": 0.25675901111021965,
                "v": 0.32921481141639886
            },
            "question": "every u.s. state that starts with which of these letters has a democratic governor?",
            "rate_limited": false,
            "answers": [
                "w",
                "c",
                "v"
            ],
            "ml_answers": {
                "c": 0.1536083937439484,
                "w": 0.06932687231445121,
                "v": 0.3765111570806934
            },
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "integer_answers": {
                "c": 11,
                "w": 0,
                "v": 3
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9695640650476542,
                    1.936092290035364,
                    2.094343644916982
                ],
                "result_count_important_words": [
                    1680000.0,
                    1820000.0,
                    1630000.0
                ],
                "wikipedia_search": [
                    0.3684210526315789,
                    1.0,
                    0.631578947368421
                ],
                "word_count_appended_bing": [
                    1113.0,
                    2921.0,
                    982.0
                ],
                "answer_relation_to_question_bing": [
                    0.7325670498084291,
                    0.8411238825031928,
                    1.4263090676883778
                ],
                "question_related_to_answer": [
                    0.9510337202173923,
                    1.8626164669012537,
                    1.1863498128813537
                ],
                "question_related_to_answer_bing": [
                    0.851879423634092,
                    2.339438744874115,
                    0.8086818314917932
                ],
                "result_count_noun_chunks": [
                    2660000.0,
                    3960000.0,
                    2800000.0
                ],
                "word_count_noun_chunks": [
                    2445.0,
                    5377.0,
                    3343.0
                ],
                "result_count_bing": [
                    8380000.0,
                    8550000.0,
                    8420000.0
                ],
                "word_count_raw": [
                    2853.0,
                    5861.0,
                    3838.0
                ],
                "result_count": [
                    4550000.0,
                    5960000.0,
                    3760000.0
                ],
                "answer_relation_to_question": [
                    0.8957528957528957,
                    0.4845559845559846,
                    2.6196911196911197
                ],
                "word_count_appended": [
                    3069.0,
                    5995.0,
                    3949.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What does a rattlesnake typically do when it feels threatened?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "rattles its tail"
            ],
            "lines": [
                [
                    0.6071428571428571,
                    1.0,
                    0,
                    0,
                    0.00010134108029591596,
                    0.33865814696485624,
                    0.00033942472982886424,
                    1.0,
                    0.5,
                    0.6785714285714286,
                    0.5555555555555556,
                    0.2342798894038809,
                    0,
                    0,
                    1.0
                ],
                [
                    0.2222222222222222,
                    0.0,
                    0,
                    0,
                    0.9998986589197041,
                    0.3306709265175719,
                    0.9996605752701712,
                    0.0,
                    0.0,
                    0.21428571428571427,
                    0.2222222222222222,
                    0.6858436950493556,
                    0,
                    0,
                    1.0
                ],
                [
                    0.1706349206349206,
                    0.0,
                    0,
                    0,
                    0.0,
                    0.3306709265175719,
                    0.0,
                    0.0,
                    0.5,
                    0.10714285714285714,
                    0.2222222222222222,
                    0.07987641554676345,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "sends an angry email": 0.14105473420643355,
                "rattles its tail": 0.49146486434487036,
                "eats its feelings": 0.36748040144869615
            },
            "question": "what does a rattlesnake typically do when it feels threatened?",
            "rate_limited": false,
            "answers": [
                "rattles its tail",
                "eats its feelings",
                "sends an angry email"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sends an angry email": 0.29999999900658925,
                "rattles its tail": 0.3194444440305233,
                "eats its feelings": 0.31499999945362406
            },
            "integer_answers": {
                "sends an angry email": 0,
                "rattles its tail": 7,
                "eats its feelings": 3
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.9371195576155236,
                    2.7433747801974224,
                    0.3195056621870538
                ],
                "result_count_important_words": [
                    31.0,
                    91300.0,
                    0
                ],
                "wikipedia_search": [
                    1.0,
                    0.0,
                    1.0
                ],
                "word_count_appended_bing": [
                    5.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    2.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    1060.0,
                    0,
                    0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    424000.0,
                    414000.0,
                    414000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    30.0,
                    296000.0,
                    0
                ],
                "answer_relation_to_question": [
                    1.8214285714285714,
                    0.6666666666666666,
                    0.5119047619047619
                ],
                "word_count_appended": [
                    19.0,
                    6.0,
                    3.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these was a name the ancient Greeks gave the planet Venus?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "phosphorus"
            ],
            "lines": [
                [
                    0.0,
                    0.0,
                    0,
                    0,
                    0.00011462037752468585,
                    0.3317901234567901,
                    0.001445870233146575,
                    0.00018183726769141815,
                    0.14285714285714288,
                    0.14072847682119205,
                    0.27835051546391754,
                    0.20024394100978768,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.3333333333333333,
                    0.0,
                    0,
                    0,
                    0.9784666374058548,
                    0.3333333333333333,
                    0.7568227001626604,
                    0.9694163482602418,
                    0.634920634920635,
                    0.5612582781456954,
                    0.4536082474226804,
                    0.4352245688306434,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.6666666666666666,
                    1.0,
                    0,
                    0,
                    0.02141874221662047,
                    0.33487654320987653,
                    0.24173142960419303,
                    0.030401814472066756,
                    0.22222222222222224,
                    0.2980132450331126,
                    0.26804123711340205,
                    0.364531490159569,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "antimony": 0.2873252825581441,
                "flourine": 0.0913093772905994,
                "phosphorus": 0.6213653401512566
            },
            "question": "which of these was a name the ancient greeks gave the planet venus?",
            "rate_limited": false,
            "answers": [
                "flourine",
                "phosphorus",
                "antimony"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "antimony": 0.3989552451379332,
                "flourine": 0.04651931217285919,
                "phosphorus": 0.79707519012099
            },
            "integer_answers": {
                "antimony": 3,
                "flourine": 0,
                "phosphorus": 9
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.0012197050489384,
                    2.176122844153217,
                    1.8226574507978448
                ],
                "result_count_important_words": [
                    640.0,
                    335000.0,
                    107000.0
                ],
                "wikipedia_search": [
                    0.42857142857142855,
                    1.9047619047619047,
                    0.6666666666666666
                ],
                "answer_relation_to_question": [
                    0.0,
                    1.0,
                    2.0
                ],
                "word_count_appended_bing": [
                    27.0,
                    44.0,
                    26.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    634.0,
                    3380000.0,
                    106000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    4.0,
                    0.0
                ],
                "result_count_bing": [
                    215000.0,
                    216000.0,
                    217000.0
                ],
                "word_count_raw": [
                    0.0,
                    5.0,
                    0.0
                ],
                "result_count": [
                    533.0,
                    4550000.0,
                    99600.0
                ],
                "word_count_appended": [
                    85.0,
                    339.0,
                    180.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these film composers most recently won an Oscar?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "hans zimmer"
            ],
            "lines": [
                [
                    0.2879127799002726,
                    0.3958333333333333,
                    0.3,
                    0,
                    0.06958762886597938,
                    0.010850668425154367,
                    0.2870662460567823,
                    0.31064670996893534,
                    0.6345354645354646,
                    0.3412887828162291,
                    0.3617021276595745,
                    0.3533259873785647,
                    0.25,
                    0.0,
                    -1.0
                ],
                [
                    0.2684196517008711,
                    0.5729166666666666,
                    0.7,
                    0,
                    0.1056701030927835,
                    0.4556081769678077,
                    0.526813880126183,
                    0.48009036995199095,
                    0.23106617520410624,
                    0.35799522673031026,
                    0.32978723404255317,
                    0.32566757439239435,
                    0.5,
                    1.0,
                    -1.0
                ],
                [
                    0.4436675683988563,
                    0.03125,
                    0.0,
                    0,
                    0.8247422680412371,
                    0.533541154607038,
                    0.1861198738170347,
                    0.2092629200790737,
                    0.13439836026042923,
                    0.30071599045346065,
                    0.30851063829787234,
                    0.321006438229041,
                    0.25,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "ennio morricone": 0.27713459453386846,
                "danny elfman": 0.27255501632184953,
                "hans zimmer": 0.4503103891442821
            },
            "question": "which of these film composers most recently won an oscar?",
            "rate_limited": false,
            "answers": [
                "ennio morricone",
                "hans zimmer",
                "danny elfman"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "ennio morricone": 0.13425192552721504,
                "danny elfman": 0.024380996959522998,
                "hans zimmer": 0.4696253752439399
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7666299368928233,
                    1.6283378719619717,
                    1.605032191145205
                ],
                "result_count_important_words": [
                    182000.0,
                    334000.0,
                    118000.0
                ],
                "wikipedia_search": [
                    3.172677322677323,
                    1.1553308760205312,
                    0.6719918013021462
                ],
                "word_count_appended_bing": [
                    34.0,
                    31.0,
                    29.0
                ],
                "answer_relation_to_question_bing": [
                    1.1875,
                    1.71875,
                    0.09375
                ],
                "question_related_to_answer": [
                    0.3,
                    0.7,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    110000.0,
                    170000.0,
                    74100.0
                ],
                "word_count_noun_chunks": [
                    2.0,
                    4.0,
                    2.0
                ],
                "word_count_raw": [
                    0.0,
                    5.0,
                    0.0
                ],
                "result_count_bing": [
                    36200.0,
                    1520000.0,
                    1780000.0
                ],
                "word_count_appended": [
                    143.0,
                    150.0,
                    126.0
                ],
                "answer_relation_to_question": [
                    1.4395638995013629,
                    1.3420982585043557,
                    2.2183378419942814
                ],
                "result_count": [
                    108000.0,
                    164000.0,
                    1280000.0
                ]
            },
            "integer_answers": {
                "ennio morricone": 3,
                "danny elfman": 3,
                "hans zimmer": 7
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these figure skating jumps was invented the most recently?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "salchow"
            ],
            "lines": [
                [
                    0.07040487062404871,
                    0.03472222222222222,
                    0.2191780821917808,
                    0,
                    0.1744186046511628,
                    0.25097024579560157,
                    0.22006302521008403,
                    0.2354373858331642,
                    0.013447286636385369,
                    0.33287671232876714,
                    0.2754491017964072,
                    0.33006241861184704,
                    0.2191780821917808,
                    0.20408163265306123,
                    -1.0
                ],
                [
                    0.8126362252663621,
                    0.8958333333333334,
                    0.2602739726027397,
                    0,
                    0.4318936877076412,
                    0.44372574385511,
                    0.3797268907563025,
                    0.11913943576212706,
                    0.9759144154912742,
                    0.29041095890410956,
                    0.2694610778443114,
                    0.3488688002818582,
                    0.2602739726027397,
                    0.10204081632653061,
                    -1.0
                ],
                [
                    0.11695890410958905,
                    0.06944444444444443,
                    0.5205479452054794,
                    0,
                    0.39368770764119604,
                    0.3053040103492885,
                    0.40021008403361347,
                    0.6454231784047088,
                    0.010638297872340425,
                    0.3767123287671233,
                    0.4550898203592814,
                    0.32106878110629483,
                    0.5205479452054794,
                    0.6938775510204082,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "salchow": 0.43001533313341844,
                "lutz": 0.19848382082663948,
                "axel": 0.37150084603994205
            },
            "question": "which of these figure skating jumps was invented the most recently?",
            "rate_limited": false,
            "answers": [
                "lutz",
                "salchow",
                "axel"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "salchow": 0.34000000019868215,
                "lutz": 0.29999999900658925,
                "axel": 0.31499999945362406
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6503120930592352,
                    1.7443440014092908,
                    1.6053439055314742
                ],
                "result_count_important_words": [
                    41900.0,
                    72300.0,
                    76200.0
                ],
                "wikipedia_search": [
                    0.053789146545541476,
                    3.903657661965097,
                    0.0425531914893617
                ],
                "word_count_appended_bing": [
                    46.0,
                    45.0,
                    76.0
                ],
                "answer_relation_to_question_bing": [
                    0.10416666666666666,
                    2.6875,
                    0.20833333333333331
                ],
                "question_related_to_answer": [
                    0.2191780821917808,
                    0.2602739726027397,
                    0.5205479452054794
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    116000.0,
                    58700.0,
                    318000.0
                ],
                "word_count_noun_chunks": [
                    16.0,
                    19.0,
                    38.0
                ],
                "word_count_raw": [
                    10.0,
                    5.0,
                    34.0
                ],
                "result_count_bing": [
                    19400.0,
                    34300.0,
                    23600.0
                ],
                "word_count_appended": [
                    243.0,
                    212.0,
                    275.0
                ],
                "answer_relation_to_question": [
                    0.35202435312024355,
                    4.063181126331811,
                    0.5847945205479452
                ],
                "result_count": [
                    105000.0,
                    260000.0,
                    237000.0
                ]
            },
            "integer_answers": {
                "salchow": 6,
                "lutz": 0,
                "axel": 7
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Romaine, Iceberg and Butterhead are all varieties of what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "lettuce",
                "disney dwarfs",
                "race cars"
            ],
            "lines": [
                [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.9905163329820864,
                    0.7310447023956532,
                    0.9993121617587895,
                    0.9975470155355682,
                    0.9895833333333334,
                    0.98,
                    0.875,
                    0.6612952456641891,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.009272918861959958,
                    0.13460113608298346,
                    0.0005905026410392846,
                    0.0021025581123700504,
                    0.0,
                    0.006666666666666667,
                    0.06770833333333333,
                    0.2605163741466498,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.00021074815595363542,
                    0.1343541615213633,
                    9.733560017131066e-05,
                    0.00035042635206167504,
                    0.010416666666666666,
                    0.013333333333333334,
                    0.057291666666666664,
                    0.0781883801891612,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "lettuce": 0.9445927708335443,
                "race cars": 0.02101733703466984,
                "disney dwarfs": 0.03438989213178589
            },
            "question": "romaine, iceberg and butterhead are all varieties of what?",
            "rate_limited": false,
            "answers": [
                "lettuce",
                "disney dwarfs",
                "race cars"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "lettuce": 0.0,
                "race cars": 0.0,
                "disney dwarfs": 0.0
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.6451809826567563,
                    1.0420654965865992,
                    0.3127535207566448
                ],
                "result_count_important_words": [
                    154000.0,
                    91.0,
                    15.0
                ],
                "wikipedia_search": [
                    3.9583333333333335,
                    0.0,
                    0.041666666666666664
                ],
                "word_count_appended_bing": [
                    168.0,
                    13.0,
                    11.0
                ],
                "answer_relation_to_question_bing": [
                    4.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer": [
                    4.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    3.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    42700.0,
                    90.0,
                    15.0
                ],
                "word_count_noun_chunks": [
                    669.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    660.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    296000.0,
                    54500.0,
                    54400.0
                ],
                "word_count_appended": [
                    882.0,
                    6.0,
                    12.0
                ],
                "answer_relation_to_question": [
                    4.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    65800.0,
                    616.0,
                    14.0
                ]
            },
            "integer_answers": {
                "lettuce": 14,
                "race cars": 0,
                "disney dwarfs": 0
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Rejected in the late 1700s, what was the name of the proposed 14th U.S. colony?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "vandalia"
            ],
            "lines": [
                [
                    0.29022137619853355,
                    0.2644736842105263,
                    0.023255813953488372,
                    0.0,
                    0.7118353344768439,
                    0.27117741076185403,
                    0.639412997903564,
                    0.8829697986577181,
                    0.008620689655172414,
                    0.5809859154929577,
                    0.5294117647058824,
                    0.43764461076307276,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.6475958826847152,
                    0.6755263157894736,
                    0.9767441860465116,
                    1.0,
                    0.275871926815323,
                    0.37133724027703785,
                    0.23270440251572327,
                    0.08200503355704698,
                    0.24233716475095785,
                    0.3732394366197183,
                    0.4411764705882353,
                    0.3024169030548017,
                    0.9333333333333333,
                    1.0,
                    1.0
                ],
                [
                    0.06218274111675127,
                    0.06,
                    0.0,
                    0.0,
                    0.012292738707833047,
                    0.3574853489611082,
                    0.1278825995807128,
                    0.0350251677852349,
                    0.7490421455938697,
                    0.045774647887323945,
                    0.029411764705882353,
                    0.2599384861821255,
                    0.06666666666666667,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "roanoke": 0.33142924262711526,
                "new albion": 0.12897873622767916,
                "vandalia": 0.5395920211452055
            },
            "question": "rejected in the late 1700s, what was the name of the proposed 14th u.s. colony?",
            "rate_limited": false,
            "answers": [
                "roanoke",
                "vandalia",
                "new albion"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "roanoke": 0.10266898087726048,
                "new albion": 0.12675563739171236,
                "vandalia": 0.6004182401078704
            },
            "integer_answers": {
                "roanoke": 6,
                "new albion": 1,
                "vandalia": 7
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.6258676645784367,
                    1.8145014183288102,
                    1.559630917092753
                ],
                "result_count_important_words": [
                    6100.0,
                    2220.0,
                    1220.0
                ],
                "wikipedia_search": [
                    0.017241379310344827,
                    0.4846743295019157,
                    1.4980842911877394
                ],
                "word_count_appended_bing": [
                    36.0,
                    30.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.0578947368421052,
                    2.7021052631578946,
                    0.24
                ],
                "question_related_to_answer": [
                    0.023255813953488372,
                    0.9767441860465116,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    42100.0,
                    3910.0,
                    1670.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    14.0,
                    1.0
                ],
                "result_count_bing": [
                    50900.0,
                    69700.0,
                    67100.0
                ],
                "word_count_raw": [
                    0.0,
                    7.0,
                    0.0
                ],
                "result_count": [
                    4980.0,
                    1930.0,
                    86.0
                ],
                "answer_relation_to_question": [
                    1.1608855047941342,
                    2.590383530738861,
                    0.24873096446700507
                ],
                "word_count_appended": [
                    165.0,
                    106.0,
                    13.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these are you most likely to find in a toolbox?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "hammer"
            ],
            "lines": [
                [
                    0.42810457516339867,
                    0.2549019607843137,
                    1.0,
                    0,
                    0.9452411994784876,
                    0.47546561604584525,
                    0.7436170212765958,
                    0.9716901803232738,
                    0.05555555555555555,
                    0.8089622641509434,
                    0.581081081081081,
                    0.4665375211742481,
                    0,
                    1.0,
                    -1.0
                ],
                [
                    0.1568627450980392,
                    0.3137254901960784,
                    0.0,
                    0,
                    0.030421555845284658,
                    0.4781518624641834,
                    0.11063829787234042,
                    0.01644398766700925,
                    0.8888888888888888,
                    0.1179245283018868,
                    0.02702702702702703,
                    0.29292617304834295,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.4150326797385621,
                    0.43137254901960786,
                    0.0,
                    0,
                    0.024337244676227728,
                    0.046382521489971344,
                    0.14574468085106382,
                    0.011865832009716902,
                    0.05555555555555555,
                    0.07311320754716981,
                    0.3918918918918919,
                    0.24053630577740892,
                    0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "hammerhead shark": 0.15298603904643135,
                "hammer": 0.6442630812528118,
                "mc hammer": 0.20275087970075675
            },
            "question": "which of these are you most likely to find in a toolbox?",
            "rate_limited": false,
            "answers": [
                "hammer",
                "mc hammer",
                "hammerhead shark"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "hammerhead shark": 0.3194444440305233,
                "hammer": 0.354545455177625,
                "mc hammer": 0.29999999900658925
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3996125635227443,
                    0.8787785191450289,
                    0.7216089173322268
                ],
                "result_count_important_words": [
                    699000.0,
                    104000.0,
                    137000.0
                ],
                "wikipedia_search": [
                    0.1111111111111111,
                    1.7777777777777777,
                    0.1111111111111111
                ],
                "word_count_appended_bing": [
                    43.0,
                    2.0,
                    29.0
                ],
                "answer_relation_to_question_bing": [
                    0.5098039215686274,
                    0.6274509803921569,
                    0.8627450980392157
                ],
                "question_related_to_answer": [
                    1.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    10400000.0,
                    176000.0,
                    127000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    53100000.0,
                    53400000.0,
                    5180000.0
                ],
                "word_count_appended": [
                    343.0,
                    50.0,
                    31.0
                ],
                "answer_relation_to_question": [
                    1.284313725490196,
                    0.47058823529411764,
                    1.2450980392156863
                ],
                "result_count": [
                    4350000.0,
                    140000.0,
                    112000.0
                ]
            },
            "integer_answers": {
                "hammerhead shark": 1,
                "hammer": 9,
                "mc hammer": 2
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these is usually found on the ocean floor?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "sea cucumber"
            ],
            "lines": [
                [
                    0.07291666666666667,
                    0.05555555555555555,
                    0,
                    0,
                    0.7540444200712915,
                    0.41280287167215074,
                    0.7725147387680423,
                    0.69613457408733,
                    0.28108974358974365,
                    0.349862258953168,
                    0.16,
                    0.29885299516161856,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.05952380952380953,
                    0.3055555555555556,
                    0,
                    0,
                    0.07046887853029887,
                    0.34998504337421477,
                    0.07033949989835332,
                    0.06227630637079456,
                    0.03541666666666667,
                    0.1349862258953168,
                    0.08,
                    0.2838764137576301,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.8675595238095238,
                    0.6388888888888888,
                    0,
                    0,
                    0.17548670139840966,
                    0.23721208495363447,
                    0.1571457613336044,
                    0.24158911954187545,
                    0.6834935897435898,
                    0.5151515151515151,
                    0.76,
                    0.41727059108075143,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "sweet potato": 0.3853773824525567,
                "cherry tomato": 0.145242839957264,
                "sea cucumber": 0.46937977759017924
            },
            "question": "which of these is usually found on the ocean floor?",
            "rate_limited": false,
            "answers": [
                "sweet potato",
                "cherry tomato",
                "sea cucumber"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sweet potato": 0.31499999945362406,
                "cherry tomato": 0.29999999900658925,
                "sea cucumber": 0.3194444440305233
            },
            "integer_answers": {
                "sweet potato": 4,
                "cherry tomato": 0,
                "sea cucumber": 6
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.195411980646474,
                    1.1355056550305203,
                    1.6690823643230055
                ],
                "result_count_important_words": [
                    380000.0,
                    34600.0,
                    77300.0
                ],
                "wikipedia_search": [
                    1.1243589743589744,
                    0.14166666666666666,
                    2.7339743589743586
                ],
                "answer_relation_to_question": [
                    0.29166666666666663,
                    0.23809523809523808,
                    3.470238095238095
                ],
                "word_count_appended_bing": [
                    4.0,
                    2.0,
                    19.0
                ],
                "answer_relation_to_question_bing": [
                    0.2222222222222222,
                    1.2222222222222223,
                    2.5555555555555554
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    389000.0,
                    34800.0,
                    135000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    1380000.0,
                    1170000.0,
                    793000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    275000.0,
                    25700.0,
                    64000.0
                ],
                "word_count_appended": [
                    127.0,
                    49.0,
                    187.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "The '90s band The Lightning Seeds took their name from which song?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "raspberry beret",
                "purple rain",
                "when doves cry"
            ],
            "lines": [
                [
                    0.15462789243277048,
                    0.4285714285714286,
                    1.0,
                    0,
                    0.04851500843008315,
                    0.25264677574590955,
                    0.03929359628913391,
                    0.011608492476708919,
                    0.241156116068292,
                    0.43283582089552236,
                    0.2857142857142857,
                    0.35005349406874176,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.2294871794871795,
                    0.4444444444444445,
                    0.0,
                    0,
                    0.9510863038768775,
                    0.5293551491819056,
                    0.9603143487703131,
                    0.9879568065284186,
                    0.4220824843673154,
                    0.31343283582089554,
                    0.42857142857142855,
                    0.368300793312618,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.61588492808005,
                    0.126984126984127,
                    0.0,
                    0,
                    0.0003986876930392972,
                    0.2179980750721848,
                    0.0003920549405530177,
                    0.0004347009948725042,
                    0.33676139956439266,
                    0.2537313432835821,
                    0.2857142857142857,
                    0.2816457126186403,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "when doves cry": 0.1630727165342867,
                "raspberry beret": 0.40346330082252896,
                "purple rain": 0.43346398264318436
            },
            "question": "the '90s band the lightning seeds took their name from which song?",
            "rate_limited": false,
            "answers": [
                "raspberry beret",
                "purple rain",
                "when doves cry"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "when doves cry": 0.0,
                "raspberry beret": 0.0,
                "purple rain": 0.0
            },
            "integer_answers": {
                "when doves cry": 1,
                "raspberry beret": 4,
                "purple rain": 8
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.1003209644124503,
                    2.209804759875708,
                    1.6898742757118415
                ],
                "result_count_important_words": [
                    8920.0,
                    218000.0,
                    89.0
                ],
                "wikipedia_search": [
                    0.964624464273168,
                    1.6883299374692615,
                    1.3470455982575706
                ],
                "word_count_appended_bing": [
                    2.0,
                    3.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.2857142857142856,
                    1.3333333333333333,
                    0.38095238095238093
                ],
                "question_related_to_answer": [
                    1.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    2350.0,
                    200000.0,
                    88.0
                ],
                "word_count_noun_chunks": [
                    3.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    105000.0,
                    220000.0,
                    90600.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    10100.0,
                    198000.0,
                    83.0
                ],
                "answer_relation_to_question": [
                    0.6185115697310819,
                    0.917948717948718,
                    2.4635397123202
                ],
                "word_count_appended": [
                    29.0,
                    21.0,
                    17.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which Las Vegas hotel features a replica of the Rialto Bridge?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "the venetian"
            ],
            "lines": [
                [
                    0.24430803327815515,
                    0.23634594518110685,
                    0.25,
                    0.42857142857142855,
                    0.1453437771975631,
                    0.45161290322580644,
                    0.01609455820117129,
                    0.19860417628218577,
                    0.1981892595339231,
                    0.5233918128654971,
                    0.43478260869565216,
                    0.3139373003668024,
                    0.025974025974025976,
                    0.014084507042253521,
                    -1.0
                ],
                [
                    0.36084569600334826,
                    0.2200219423499472,
                    0.625,
                    0.42857142857142855,
                    0.3237597911227154,
                    0.15241935483870966,
                    0.9789253195400334,
                    0.7972139470482105,
                    0.0324301175737756,
                    0.38596491228070173,
                    0.43478260869565216,
                    0.36243162861671807,
                    0.9090909090909091,
                    0.9577464788732394,
                    -1.0
                ],
                [
                    0.3948462707184966,
                    0.5436321124689459,
                    0.125,
                    0.14285714285714285,
                    0.5308964316797214,
                    0.3959677419354839,
                    0.004980122258795366,
                    0.004181876669603771,
                    0.7693806228923012,
                    0.09064327485380116,
                    0.13043478260869565,
                    0.3236310710164795,
                    0.06493506493506493,
                    0.028169014084507043,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "caesars palace": 0.25353968064135995,
                "luxor": 0.24866002402968362,
                "the venetian": 0.49780029532895637
            },
            "question": "which las vegas hotel features a replica of the rialto bridge?",
            "rate_limited": false,
            "answers": [
                "luxor",
                "the venetian",
                "caesars palace"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "caesars palace": 0.34000000019868215,
                "luxor": 0.3194444440305233,
                "the venetian": 0.3952380970830009
            },
            "integer_answers": {
                "caesars palace": 4,
                "luxor": 4,
                "the venetian": 6
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.197561102567617,
                    2.5370214003170264,
                    2.2654174971153562
                ],
                "result_count_important_words": [
                    7530.0,
                    458000.0,
                    2330.0
                ],
                "wikipedia_search": [
                    1.1891355572035387,
                    0.19458070544265357,
                    4.6162837373538075
                ],
                "word_count_appended_bing": [
                    30.0,
                    30.0,
                    9.0
                ],
                "answer_relation_to_question_bing": [
                    0.9453837807244274,
                    0.8800877693997888,
                    2.1745284498757838
                ],
                "question_related_to_answer": [
                    0.5,
                    1.25,
                    0.25
                ],
                "question_related_to_answer_bing": [
                    0.42857142857142855,
                    0.42857142857142855,
                    0.14285714285714285
                ],
                "result_count_noun_chunks": [
                    142000.0,
                    570000.0,
                    2990.0
                ],
                "word_count_noun_chunks": [
                    2.0,
                    70.0,
                    5.0
                ],
                "word_count_raw": [
                    1.0,
                    68.0,
                    2.0
                ],
                "result_count_bing": [
                    112000.0,
                    37800.0,
                    98200.0
                ],
                "result_count": [
                    3340.0,
                    7440.0,
                    12200.0
                ],
                "answer_relation_to_question": [
                    0.9772321331126206,
                    1.443382784013393,
                    1.5793850828739864
                ],
                "word_count_appended": [
                    179.0,
                    132.0,
                    31.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What TV series derived from a nearly 20-year-old Michael Crichton screenplay?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "the expanse",
                "numb3rs",
                "er"
            ],
            "lines": [
                [
                    0.21490531807828805,
                    0.3170821683752718,
                    0.0,
                    0.0,
                    0.00040384930235942587,
                    0.24879227053140096,
                    0.03571428571428571,
                    0.00041709493378617925,
                    0.022727272727272728,
                    0.009689922480620155,
                    0.002162162162162162,
                    0.3279182037830206,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.17222787223899572,
                    0.10754504504504503,
                    0.0,
                    0.0,
                    0.0002813604749170775,
                    0.37681159420289856,
                    0.052336028751123094,
                    0.00029295953682600685,
                    0.30612627286125815,
                    0.07364341085271318,
                    0.02918918918918919,
                    0.2517782414285476,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.6128668096827162,
                    0.5753727865796832,
                    1.0,
                    1.0,
                    0.9993147902227235,
                    0.3743961352657005,
                    0.9119496855345912,
                    0.9992899455293878,
                    0.6711464544114691,
                    0.9166666666666666,
                    0.9686486486486486,
                    0.42030355478843173,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "numb3rs": 0.09787371247010815,
                "the expanse": 0.084272324863462,
                "er": 0.8178539626664298
            },
            "question": "what tv series derived from a nearly 20-year-old michael crichton screenplay?",
            "rate_limited": false,
            "answers": [
                "the expanse",
                "numb3rs",
                "er"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "numb3rs": 0.0,
                "the expanse": 0.0,
                "er": 0.0
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.9512638340471855,
                    2.2660041728569285,
                    3.7827319930958856
                ],
                "result_count_important_words": [
                    1590.0,
                    2330.0,
                    40600.0
                ],
                "wikipedia_search": [
                    0.09090909090909091,
                    1.2245050914450326,
                    2.6845858176458766
                ],
                "word_count_appended_bing": [
                    2.0,
                    27.0,
                    896.0
                ],
                "answer_relation_to_question_bing": [
                    1.9024930102516309,
                    0.6452702702702702,
                    3.452236719478099
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    2.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    2.0
                ],
                "result_count_noun_chunks": [
                    3360.0,
                    2360.0,
                    8050000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    1024.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1120.0
                ],
                "result_count_bing": [
                    10300.0,
                    15600.0,
                    15500.0
                ],
                "word_count_appended": [
                    15.0,
                    114.0,
                    1419.0
                ],
                "answer_relation_to_question": [
                    1.2894319084697283,
                    1.0333672334339743,
                    3.677200858096297
                ],
                "result_count": [
                    3330.0,
                    2320.0,
                    8240000.0
                ]
            },
            "integer_answers": {
                "numb3rs": 1,
                "the expanse": 0,
                "er": 13
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What makeup item often contains dried cochineal bugs as an ingredient?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "lipstick"
            ],
            "question": "what makeup item often contains dried cochineal bugs as an ingredient?",
            "answers": [
                "mascara",
                "eyeliner",
                "lipstick"
            ],
            "integer_answers": {
                "lipstick": 10,
                "mascara": 3,
                "eyeliner": 1
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.3414894130361863,
                    2.0102153796880096,
                    2.648295207275804
                ],
                "result_count_important_words": [
                    8870.0,
                    5680.0,
                    62200.0
                ],
                "wikipedia_search": [
                    1.3928571428571428,
                    0.5119047619047619,
                    5.095238095238096
                ],
                "answer_relation_to_question": [
                    1.6724703704375918,
                    0.9712398134199685,
                    1.3562898161424397
                ],
                "answer_relation_to_question_bing": [
                    1.5951680672268909,
                    0.8845588235294117,
                    1.5202731092436976
                ],
                "word_count_appended": [
                    191.0,
                    153.0,
                    382.0
                ],
                "question_related_to_answer": [
                    0.46511627906976744,
                    0.18604651162790697,
                    1.3488372093023255
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    1.0,
                    1.0
                ],
                "result_count_noun_chunks": [
                    42300.0,
                    5750.0,
                    63900.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    60.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    46.0
                ],
                "result_count_bing": [
                    93700.0,
                    15000.0,
                    90400.0
                ],
                "result_count": [
                    4940.0,
                    3040.0,
                    6830.0
                ],
                "word_count_appended_bing": [
                    29.0,
                    27.0,
                    66.0
                ]
            },
            "negative_question": false,
            "fraction_answers": {
                "lipstick": 0.5958634463851443,
                "mascara": 0.24304330099245255,
                "eyeliner": 0.1610932526224032
            },
            "lines": [
                [
                    0.41811759260939796,
                    0.3987920168067227,
                    0.23255813953488372,
                    0.0,
                    0.33355840648210666,
                    0.47061778001004523,
                    0.1155700325732899,
                    0.3778472532380527,
                    0.1989795918367347,
                    0.2630853994490358,
                    0.23770491803278687,
                    0.33449848757659806,
                    0.0,
                    0.02127659574468085,
                    1.0
                ],
                [
                    0.24280995335499211,
                    0.22113970588235293,
                    0.09302325581395349,
                    0.5,
                    0.20526671168129643,
                    0.07533902561526871,
                    0.07400651465798046,
                    0.051362215274676194,
                    0.07312925170068027,
                    0.21074380165289255,
                    0.22131147540983606,
                    0.28717362566971566,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.3390724540356099,
                    0.3800682773109244,
                    0.6744186046511628,
                    0.5,
                    0.4611748818365969,
                    0.4540431943746861,
                    0.8104234527687296,
                    0.5707905314872711,
                    0.7278911564625851,
                    0.5261707988980716,
                    0.5409836065573771,
                    0.3783278867536863,
                    1.0,
                    0.9787234042553191,
                    1.0
                ]
            ],
            "rate_limited": false,
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "lipstick": 0.9192713355367721,
                "mascara": 0.08472301724447584,
                "eyeliner": 0.04414951997863305
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these video games was NOT produced by FromSoftware?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "beyond: two souls"
            ],
            "lines": [
                [
                    0.2456149552436171,
                    0.3184027777777778,
                    0.06521739130434784,
                    0.0,
                    0.11838575957083636,
                    0.13993926686429037,
                    0.11287106778910055,
                    0.11847879083373963,
                    0.3576882323610072,
                    0.22976190476190478,
                    0.12195121951219512,
                    0.3191844899090234,
                    0.0625,
                    0.1031746031746032,
                    -1.0
                ],
                [
                    0.3768462007934086,
                    0.41979166666666673,
                    0.5,
                    0.5,
                    0.4586686174103877,
                    0.3611814040922565,
                    0.4602348249889233,
                    0.45867869332033157,
                    0.4148591373722264,
                    0.45,
                    0.45121951219512196,
                    0.3401794005514862,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.37753884396297427,
                    0.26180555555555557,
                    0.43478260869565216,
                    0.5,
                    0.4229456230187759,
                    0.4988793290434531,
                    0.42689410722197607,
                    0.4228425158459288,
                    0.22745263026676638,
                    0.3202380952380952,
                    0.4268292682926829,
                    0.3406361095394904,
                    0.4375,
                    0.39682539682539686,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "demon's souls": 0.2149757023561789,
                "beyond: two souls": 0.11547722037274159,
                "dark souls": 0.6695470772710795
            },
            "question": "which of these video games was not produced by fromsoftware?",
            "rate_limited": false,
            "answers": [
                "dark souls",
                "beyond: two souls",
                "demon's souls"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "demon's souls": 0.34000000019868215,
                "beyond: two souls": 0.354545455177625,
                "dark souls": 0.3194444440305233
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.4465240807278132,
                    1.2785647955881103,
                    1.2749111236840764
                ],
                "result_count_important_words": [
                    699000.0,
                    71800.0,
                    132000.0
                ],
                "wikipedia_search": [
                    0.8538706058339567,
                    0.5108451757666417,
                    1.6352842183994016
                ],
                "word_count_appended_bing": [
                    31.0,
                    4.0,
                    6.0
                ],
                "answer_relation_to_question_bing": [
                    1.4527777777777777,
                    0.6416666666666666,
                    1.9055555555555554
                ],
                "question_related_to_answer": [
                    0.8695652173913043,
                    0.0,
                    0.13043478260869565
                ],
                "question_related_to_answer_bing": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    313000.0,
                    33900.0,
                    63300.0
                ],
                "word_count_noun_chunks": [
                    63.0,
                    0.0,
                    9.0
                ],
                "word_count_raw": [
                    50.0,
                    0.0,
                    13.0
                ],
                "result_count_bing": [
                    49800000.0,
                    19200000.0,
                    155000.0
                ],
                "word_count_appended": [
                    227.0,
                    42.0,
                    151.0
                ],
                "answer_relation_to_question": [
                    2.0350803580510632,
                    0.9852303936527311,
                    0.9796892482962059
                ],
                "result_count": [
                    313000.0,
                    33900.0,
                    63200.0
                ]
            },
            "integer_answers": {
                "demon's souls": 2,
                "beyond: two souls": 0,
                "dark souls": 12
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which NBA franchise has NOT retired any jersey numbers?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "brooklyn nets",
                "dallas mavericks",
                "los angeles clippers"
            ],
            "lines": [
                [
                    0.34206050680296596,
                    0.2762416794674859,
                    0.5,
                    0,
                    0.3316708229426434,
                    0.40881913303437967,
                    0.33491895701198027,
                    0.31850533807829184,
                    0.36394707455711417,
                    0.3047619047619048,
                    0.23958333333333331,
                    0.3295581213508649,
                    0.5,
                    0.375,
                    -1.0
                ],
                [
                    0.32465940177093083,
                    0.2793138760880697,
                    0.25,
                    0,
                    0.3092269326683292,
                    0.40059790732436473,
                    0.33738548273431995,
                    0.3149466192170819,
                    0.3047349146498657,
                    0.33333333333333337,
                    0.30208333333333337,
                    0.3390122094990391,
                    0.33333333333333337,
                    0.5,
                    -1.0
                ],
                [
                    0.33328009142610326,
                    0.4444444444444444,
                    0.25,
                    0,
                    0.35910224438902744,
                    0.1905829596412556,
                    0.3276955602536998,
                    0.3665480427046264,
                    0.33131801079302015,
                    0.3619047619047619,
                    0.4583333333333333,
                    0.33142966915009614,
                    0.16666666666666669,
                    0.125,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "dallas mavericks": 0.3340573316996922,
                "brooklyn nets": 0.28845125056292853,
                "los angeles clippers": 0.37749141773737926
            },
            "question": "which nba franchise has not retired any jersey numbers?",
            "rate_limited": false,
            "answers": [
                "brooklyn nets",
                "dallas mavericks",
                "los angeles clippers"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "dallas mavericks": 0.0,
                "brooklyn nets": 0.0,
                "los angeles clippers": 0.0
            },
            "integer_answers": {
                "dallas mavericks": 5,
                "brooklyn nets": 4,
                "los angeles clippers": 4
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7044187864913516,
                    1.6098779050096095,
                    1.685703308499039
                ],
                "result_count_important_words": [
                    937000.0,
                    923000.0,
                    978000.0
                ],
                "wikipedia_search": [
                    1.360529254428858,
                    1.9526508535013432,
                    1.6868198920697985
                ],
                "word_count_appended_bing": [
                    25.0,
                    19.0,
                    4.0
                ],
                "answer_relation_to_question_bing": [
                    1.3425499231950844,
                    1.3241167434715821,
                    0.3333333333333333
                ],
                "question_related_to_answer": [
                    0.0,
                    0.5,
                    0.5
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    2040000.0,
                    2080000.0,
                    1500000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    2.0,
                    4.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    3.0
                ],
                "result_count_bing": [
                    1220000.0,
                    1330000.0,
                    4140000.0
                ],
                "result_count": [
                    135000.0,
                    153000.0,
                    113000.0
                ],
                "answer_relation_to_question": [
                    1.2635159455762723,
                    1.4027247858325533,
                    1.333759268591174
                ],
                "word_count_appended": [
                    82.0,
                    70.0,
                    58.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these is NOT a marsupial?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "quintana roo"
            ],
            "lines": [
                [
                    0.5,
                    0.5,
                    0.5,
                    0.5,
                    0.49238335132541683,
                    0.3365724381625442,
                    0.451615548630474,
                    0.4560044714148834,
                    0.5,
                    0.47206005004170143,
                    0.4661016949152542,
                    0.4049079754601227,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.2246376811594203,
                    0.2857142857142857,
                    0.4782608695652174,
                    0.5,
                    0.46541521730438606,
                    0.3833922261484099,
                    0.45760209939314417,
                    0.4544075375279463,
                    0.5,
                    0.2894078398665555,
                    0.3135593220338983,
                    0.3057259713701431,
                    0.4782608695652174,
                    0.4285714285714286,
                    -1.0
                ],
                [
                    0.2753623188405797,
                    0.2142857142857143,
                    0.021739130434782594,
                    0.0,
                    0.04220143137019711,
                    0.28003533568904593,
                    0.09078235197638185,
                    0.08958799105717025,
                    0.0,
                    0.23853211009174313,
                    0.22033898305084748,
                    0.2893660531697342,
                    0.021739130434782594,
                    0.07142857142857145,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "cuscus": 0.20500637882570677,
                "quintana roo": 0.06005063857851477,
                "wombat": 0.7349429825957785
            },
            "question": "which of these is not a marsupial?",
            "rate_limited": false,
            "answers": [
                "quintana roo",
                "cuscus",
                "wombat"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "cuscus": 0.05455510802359393,
                "quintana roo": 0.8327578312047792,
                "wombat": 0.15966823464421254
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.1901840490797546,
                    0.3885480572597137,
                    0.4212678936605317
                ],
                "result_count_important_words": [
                    59000.0,
                    51700.0,
                    499000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_appended_bing": [
                    12.0,
                    66.0,
                    99.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.42857142857142855,
                    0.5714285714285714
                ],
                "question_related_to_answer": [
                    0.0,
                    0.043478260869565216,
                    0.9565217391304348
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_noun_chunks": [
                    55100.0,
                    57100.0,
                    514000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    22.0
                ],
                "word_count_raw": [
                    0.0,
                    2.0,
                    12.0
                ],
                "result_count_bing": [
                    1850000.0,
                    1320000.0,
                    2490000.0
                ],
                "word_count_appended": [
                    67.0,
                    505.0,
                    627.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    0.5507246376811594,
                    0.4492753623188406
                ],
                "result_count": [
                    38100.0,
                    173000.0,
                    2290000.0
                ]
            },
            "integer_answers": {
                "cuscus": 1,
                "quintana roo": 0,
                "wombat": 13
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these is NOT the title of a current TV show?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "chicago med"
            ],
            "lines": [
                [
                    0.3075040670582072,
                    0.3447702607470049,
                    0,
                    0,
                    0.4093705631803124,
                    0.36953671328671334,
                    0.4372766597705473,
                    0.27793696275071633,
                    0.3672551537269646,
                    0.19724770642201833,
                    0.1891891891891892,
                    0.33179091791148185,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.33597829983180305,
                    0.28425088090204376,
                    0,
                    0,
                    0.20539517274017985,
                    0.2705419580419581,
                    0.20566108707918002,
                    0.24212034383954156,
                    0.3010275009634972,
                    0.4954128440366973,
                    0.45945945945945943,
                    0.32669821419562906,
                    0.5,
                    0,
                    -1.0
                ],
                [
                    0.3565176331099898,
                    0.37097885835095135,
                    0,
                    0,
                    0.38523426407950784,
                    0.3599213286713287,
                    0.35706225315027273,
                    0.4799426934097421,
                    0.3317173453095382,
                    0.3073394495412844,
                    0.3513513513513513,
                    0.3415108678928892,
                    0.5,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "chicago med": 0.4123857829012445,
                "chicago police": 0.24698617366057168,
                "chicage fire": 0.3406280434381838
            },
            "question": "which of these is not the title of a current tv show?",
            "rate_limited": false,
            "answers": [
                "chicago med",
                "chicage fire",
                "chicago police"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "chicago med": 0.47808184878376886,
                "chicago police": 0.2536167343845853,
                "chicage fire": 0.23525285030422155
            },
            "integer_answers": {
                "chicago med": 4,
                "chicago police": 0,
                "chicage fire": 7
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.009254492531109,
                    1.039810714826226,
                    0.9509347926426649
                ],
                "result_count_important_words": [
                    66700.0,
                    313000.0,
                    152000.0
                ],
                "wikipedia_search": [
                    0.7964690776382126,
                    1.1938349942190167,
                    1.0096959281427706
                ],
                "word_count_appended_bing": [
                    23.0,
                    3.0,
                    11.0
                ],
                "answer_relation_to_question_bing": [
                    0.9313784355179704,
                    1.2944947145877377,
                    0.7741268498942918
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    1240000.0,
                    1440000.0,
                    112000.0
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    597000.0,
                    1050000.0,
                    641000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    76600.0,
                    249000.0,
                    97000.0
                ],
                "answer_relation_to_question": [
                    1.1549755976507567,
                    0.9841302010091819,
                    0.8608942013400612
                ],
                "word_count_appended": [
                    132.0,
                    2.0,
                    84.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these things is NOT found inside an atom?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "wonton"
            ],
            "lines": [
                [
                    0.3504075787618418,
                    0.28541666666666665,
                    0.23282442748091603,
                    0.25,
                    0.2765628714048015,
                    0.33322724379376195,
                    0.26930792377131396,
                    0.2552771978659244,
                    0.32073643410852715,
                    0.3203753351206434,
                    0.29225352112676056,
                    0.31036661007240085,
                    0.2272727272727273,
                    0.1724137931034483,
                    -1.0
                ],
                [
                    0.303921568627451,
                    0.4,
                    0.5,
                    0.5,
                    0.46945566912289044,
                    0.33322724379376195,
                    0.48019057171514545,
                    0.45349106935745764,
                    0.29761904761904767,
                    0.36371760500446826,
                    0.38380281690140844,
                    0.3856495476878987,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.3456708526107072,
                    0.3145833333333333,
                    0.26717557251908397,
                    0.25,
                    0.25398145947230805,
                    0.33354551241247615,
                    0.2505015045135406,
                    0.29123173277661796,
                    0.38164451827242524,
                    0.3159070598748883,
                    0.323943661971831,
                    0.30398384223970043,
                    0.2727272727272727,
                    0.3275862068965517,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "wonton": 0.16127498002435292,
                "neutron": 0.39535963862560913,
                "proton": 0.44336538135003806
            },
            "question": "which of these things is not found inside an atom?",
            "rate_limited": false,
            "answers": [
                "proton",
                "wonton",
                "neutron"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "wonton": 0.354545455177625,
                "neutron": 0.3194444440305233,
                "proton": 0.3194444440305233
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1378003395655951,
                    0.6861027138726077,
                    1.1760969465617972
                ],
                "result_count_important_words": [
                    1840000.0,
                    158000.0,
                    1990000.0
                ],
                "wikipedia_search": [
                    1.0755813953488373,
                    1.2142857142857142,
                    0.7101328903654485
                ],
                "word_count_appended_bing": [
                    59.0,
                    33.0,
                    50.0
                ],
                "answer_relation_to_question_bing": [
                    0.8583333333333334,
                    0.4,
                    0.7416666666666667
                ],
                "question_related_to_answer": [
                    0.5343511450381679,
                    0.0,
                    0.46564885496183206
                ],
                "question_related_to_answer_bing": [
                    0.5,
                    0.0,
                    0.5
                ],
                "result_count_noun_chunks": [
                    211000.0,
                    40100.0,
                    180000.0
                ],
                "word_count_noun_chunks": [
                    30.0,
                    0.0,
                    25.0
                ],
                "word_count_raw": [
                    76.0,
                    0.0,
                    40.0
                ],
                "result_count_bing": [
                    52400000.0,
                    52400000.0,
                    52300000.0
                ],
                "word_count_appended": [
                    402.0,
                    305.0,
                    412.0
                ],
                "answer_relation_to_question": [
                    0.8975545274289491,
                    1.1764705882352942,
                    0.9259748843357567
                ],
                "result_count": [
                    188000.0,
                    25700.0,
                    207000.0
                ]
            },
            "integer_answers": {
                "wonton": 2,
                "neutron": 4,
                "proton": 8
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What actor famously yelled \"Not the bees! Not the bees!\" in a 2006 film?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "oprah winfrey"
            ],
            "lines": [
                [
                    0.09210332646452751,
                    0.24144144144144142,
                    0.0,
                    0,
                    0.125,
                    0.3116045845272206,
                    0.3326463686142367,
                    0.2866025010538148,
                    0.3038935430478885,
                    0.24074074074074076,
                    0.1935483870967742,
                    0.24287077772016813,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.42123564910214895,
                    0.31576576576576576,
                    0.5,
                    0,
                    0.5,
                    0.26504297994269344,
                    0.49980444070040314,
                    0.4979274975410988,
                    0.3826627183370782,
                    0.4444444444444444,
                    0.41935483870967744,
                    0.41078891093234865,
                    0.5,
                    0.5,
                    1.0
                ],
                [
                    0.48666102443332354,
                    0.4427927927927928,
                    0.5,
                    0,
                    0.375,
                    0.42335243553008595,
                    0.16754919068536012,
                    0.21547000140508643,
                    0.3134437386150333,
                    0.3148148148148148,
                    0.3870967741935484,
                    0.3463403113474832,
                    0.5,
                    0.5,
                    1.0
                ]
            ],
            "fraction_answers": {
                "macauley culkin": 0.12968811608066788,
                "nicolas cage": 0.6353151275835672,
                "oprah winfrey": 0.23499675633576486
            },
            "question": "what actor famously yelled \"not the bees! not the bees!\" in a 2006 film?",
            "rate_limited": false,
            "answers": [
                "nicolas cage",
                "macauley culkin",
                "oprah winfrey"
            ],
            "ml_answers": {
                "macauley culkin": 0.524166062694452,
                "nicolas cage": 0.05187275766323328,
                "oprah winfrey": 0.7912946766305766
            },
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "integer_answers": {
                "macauley culkin": 1,
                "nicolas cage": 10,
                "oprah winfrey": 2
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.5998091119176463,
                    1.248955246947119,
                    2.151235641135235
                ],
                "result_count_important_words": [
                    44500.0,
                    52.0,
                    88400.0
                ],
                "wikipedia_search": [
                    1.176638741712669,
                    0.7040236899775306,
                    1.1193375683098006
                ],
                "word_count_appended_bing": [
                    19.0,
                    5.0,
                    7.0
                ],
                "answer_relation_to_question_bing": [
                    1.0342342342342343,
                    0.736936936936937,
                    0.22882882882882882
                ],
                "question_related_to_answer": [
                    3.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    243000.0,
                    2360.0,
                    324000.0
                ],
                "word_count_noun_chunks": [
                    8.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    263000.0,
                    328000.0,
                    107000.0
                ],
                "word_count_raw": [
                    4.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    12.0,
                    0,
                    4.0
                ],
                "answer_relation_to_question": [
                    3.26317338828378,
                    0.6301148071828083,
                    0.10671180453341184
                ],
                "word_count_appended": [
                    14.0,
                    3.0,
                    10.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What form of transportation counts Jay-Z as a prominent investor?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "aviation",
                "e-bikes",
                "boats"
            ],
            "lines": [
                [
                    0.3441009775725416,
                    0.5466379310344828,
                    0.3333333333333333,
                    0,
                    0.9053981304752117,
                    0.3691187938653496,
                    0.795939874646403,
                    0.21706908917045184,
                    0.16511018786127168,
                    0.46633416458852867,
                    0.3870967741935484,
                    0.43381541918790995,
                    0.3333333333333333,
                    0.3333333333333333,
                    1.0
                ],
                [
                    0.20618493539109653,
                    0.23413793103448272,
                    0.3333333333333333,
                    0,
                    0.013360739979445015,
                    0.22537041850792824,
                    0.01630706084641411,
                    0.5785321539564906,
                    0.4782153179190752,
                    0.1546134663341646,
                    0.2903225806451613,
                    0.19614206282107668,
                    0.3333333333333333,
                    0.3333333333333333,
                    1.0
                ],
                [
                    0.4497140870363619,
                    0.21922413793103446,
                    0.3333333333333333,
                    0,
                    0.08124112954534332,
                    0.4055107876267221,
                    0.18775306450718288,
                    0.2043987568730576,
                    0.35667449421965314,
                    0.3790523690773067,
                    0.3225806451612903,
                    0.37004251799101334,
                    0.3333333333333333,
                    0.3333333333333333,
                    1.0
                ]
            ],
            "fraction_answers": {
                "boats": 0.3058609223053051,
                "aviation": 0.43312471866120766,
                "e-bikes": 0.2610143590334873
            },
            "question": "what form of transportation counts jay-z as a prominent investor?",
            "rate_limited": false,
            "answers": [
                "aviation",
                "e-bikes",
                "boats"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "boats": 0.0,
                "aviation": 0.0,
                "e-bikes": 0.0
            },
            "integer_answers": {
                "boats": 2,
                "aviation": 9,
                "e-bikes": 2
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.03670793431537,
                    1.3729944397475369,
                    2.5902976259370933
                ],
                "result_count_important_words": [
                    2870000.0,
                    58800.0,
                    677000.0
                ],
                "wikipedia_search": [
                    0.6604407514450867,
                    1.9128612716763007,
                    1.4266979768786126
                ],
                "word_count_appended_bing": [
                    36.0,
                    27.0,
                    30.0
                ],
                "answer_relation_to_question_bing": [
                    2.186551724137931,
                    0.9365517241379309,
                    0.8768965517241378
                ],
                "question_related_to_answer": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    908000.0,
                    2420000.0,
                    855000.0
                ],
                "word_count_noun_chunks": [
                    1.0,
                    1.0,
                    1.0
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    1.0
                ],
                "result_count_bing": [
                    142000.0,
                    86700.0,
                    156000.0
                ],
                "result_count": [
                    1850000.0,
                    27300.0,
                    166000.0
                ],
                "answer_relation_to_question": [
                    2.0646058654352495,
                    1.2371096123465792,
                    2.6982845222181715
                ],
                "word_count_appended": [
                    187.0,
                    62.0,
                    152.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Mardi Gras is celebrated right before what other observance?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "lent",
                "kwanzaa",
                "ramadan"
            ],
            "lines": [
                [
                    0.5043126684636119,
                    0.31450872359963267,
                    1.0,
                    1.0,
                    0.23315444245676803,
                    0.2524850894632207,
                    0.5718222528418877,
                    0.5852842809364549,
                    0.4016225749559083,
                    0.4664310954063604,
                    0.38095238095238093,
                    0.39747539650937913,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.291644204851752,
                    0.45041322314049587,
                    0.0,
                    0.0,
                    0.27787716159809184,
                    0.595427435387674,
                    0.15260075783672064,
                    0.1426978818283166,
                    0.28557319223985894,
                    0.287396937573616,
                    0.30952380952380953,
                    0.29693363798167255,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.20404312668463614,
                    0.23507805325987144,
                    0.0,
                    0.0,
                    0.48896839594514013,
                    0.15208747514910537,
                    0.2755769893213917,
                    0.27201783723522854,
                    0.3128042328042328,
                    0.24617196702002356,
                    0.30952380952380953,
                    0.3055909655089483,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "kwanzaa": 0.220720588711572,
                "ramadan": 0.20013306088945623,
                "lent": 0.5791463503989718
            },
            "question": "mardi gras is celebrated right before what other observance?",
            "rate_limited": false,
            "answers": [
                "lent",
                "kwanzaa",
                "ramadan"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "kwanzaa": 0.0,
                "ramadan": 0.0,
                "lent": 0.0
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9873769825468957,
                    1.4846681899083627,
                    1.5279548275447417
                ],
                "result_count_important_words": [
                    166000.0,
                    44300.0,
                    80000.0
                ],
                "wikipedia_search": [
                    1.2048677248677249,
                    0.8567195767195768,
                    0.9384126984126984
                ],
                "word_count_appended_bing": [
                    48.0,
                    39.0,
                    39.0
                ],
                "answer_relation_to_question_bing": [
                    0.943526170798898,
                    1.3512396694214877,
                    0.7052341597796143
                ],
                "question_related_to_answer": [
                    2.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    210000.0,
                    51200.0,
                    97600.0
                ],
                "word_count_noun_chunks": [
                    68.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    254000.0,
                    599000.0,
                    153000.0
                ],
                "word_count_raw": [
                    107.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    396.0,
                    244.0,
                    209.0
                ],
                "answer_relation_to_question": [
                    2.5215633423180592,
                    1.45822102425876,
                    1.0202156334231807
                ],
                "result_count": [
                    39100.0,
                    46600.0,
                    82000.0
                ]
            },
            "integer_answers": {
                "kwanzaa": 2,
                "ramadan": 1,
                "lent": 11
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "In which state is happy hour currently banned?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "illinois"
            ],
            "lines": [
                [
                    0.4613299232736573,
                    0.34182590233545646,
                    0.5,
                    0,
                    0.35575942915392456,
                    0.27654867256637167,
                    0.36307519640852975,
                    0.44068551079456936,
                    0.39244089834515367,
                    0.4979123173277662,
                    0.5765765765765766,
                    0.3760718131256954,
                    0.8928571428571429,
                    0.92,
                    -1.0
                ],
                [
                    0.429002557544757,
                    0.3036093418259023,
                    0.5,
                    0,
                    0.5147808358817533,
                    0.3141592920353982,
                    0.5145903479236813,
                    0.360560872468284,
                    0.2851418439716312,
                    0.4091858037578288,
                    0.2972972972972973,
                    0.27379762042670924,
                    0.10714285714285714,
                    0.02,
                    -1.0
                ],
                [
                    0.10966751918158568,
                    0.3545647558386412,
                    0.0,
                    0,
                    0.12945973496432212,
                    0.4092920353982301,
                    0.122334455667789,
                    0.19875361673714667,
                    0.32241725768321516,
                    0.09290187891440502,
                    0.12612612612612611,
                    0.35013056644759544,
                    0.0,
                    0.06,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "arizona": 0.33302066694431537,
                "illinois": 0.4919294909819111,
                "rhode island": 0.17504984207377358
            },
            "question": "in which state is happy hour currently banned?",
            "rate_limited": false,
            "answers": [
                "illinois",
                "arizona",
                "rhode island"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "arizona": 0.34000000019868215,
                "illinois": 0.3952380970830009,
                "rhode island": 0.29999999900658925
            },
            "integer_answers": {
                "arizona": 2,
                "illinois": 9,
                "rhode island": 2
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.880359065628477,
                    1.3689881021335462,
                    1.750652832237977
                ],
                "result_count_important_words": [
                    647000.0,
                    917000.0,
                    218000.0
                ],
                "wikipedia_search": [
                    1.5697635933806147,
                    1.140567375886525,
                    1.2896690307328607
                ],
                "word_count_appended_bing": [
                    64.0,
                    33.0,
                    14.0
                ],
                "answer_relation_to_question_bing": [
                    0.6836518046709129,
                    0.6072186836518046,
                    0.7091295116772824
                ],
                "question_related_to_answer": [
                    1.0,
                    1.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    1980000.0,
                    1620000.0,
                    893000.0
                ],
                "word_count_noun_chunks": [
                    25.0,
                    3.0,
                    0.0
                ],
                "result_count_bing": [
                    1250000.0,
                    1420000.0,
                    1850000.0
                ],
                "word_count_raw": [
                    46.0,
                    1.0,
                    3.0
                ],
                "result_count": [
                    698000.0,
                    1010000.0,
                    254000.0
                ],
                "answer_relation_to_question": [
                    1.8453196930946292,
                    1.716010230179028,
                    0.43867007672634273
                ],
                "word_count_appended": [
                    477.0,
                    392.0,
                    89.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "In baking, yeast helps bread rise, but scientifically yeast is what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "fungus"
            ],
            "lines": [
                [
                    0.7555555555555555,
                    0.5,
                    0.6216216216216216,
                    0.8333333333333334,
                    0.1657271702367531,
                    0.03678251563116093,
                    0.22139597769139766,
                    0.12944245190740136,
                    0.8807241145950824,
                    0.3309659090909091,
                    0.29357798165137616,
                    0.33380251989060794,
                    0.5,
                    0.75,
                    1.0
                ],
                [
                    0.2,
                    0.0,
                    0.13513513513513514,
                    0.16666666666666666,
                    0.7001127395715896,
                    0.5187855573705853,
                    0.6202467466621598,
                    0.6080860776002608,
                    0.06468531468531469,
                    0.3196022727272727,
                    0.3119266055045872,
                    0.3243524121069436,
                    0.375,
                    0.15,
                    1.0
                ],
                [
                    0.04444444444444444,
                    0.5,
                    0.24324324324324326,
                    0.0,
                    0.13416009019165728,
                    0.4444319269982538,
                    0.15835727564644245,
                    0.26247147049233777,
                    0.05459057071960298,
                    0.3494318181818182,
                    0.3944954128440367,
                    0.3418450680024484,
                    0.125,
                    0.1,
                    1.0
                ]
            ],
            "fraction_answers": {
                "fungus": 0.4537806536575143,
                "plant": 0.3210428234307511,
                "bacteria": 0.22517652291173462
            },
            "question": "in baking, yeast helps bread rise, but scientifically yeast is what?",
            "rate_limited": false,
            "answers": [
                "fungus",
                "plant",
                "bacteria"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "fungus": 0.4785868977413464,
                "plant": 0.012367768392178917,
                "bacteria": 0.4313062533993353
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.3366176392342557,
                    2.2704668847486054,
                    2.392915476017139
                ],
                "result_count_important_words": [
                    131000.0,
                    367000.0,
                    93700.0
                ],
                "wikipedia_search": [
                    3.5228964583803295,
                    0.25874125874125875,
                    0.21836228287841192
                ],
                "word_count_appended_bing": [
                    32.0,
                    34.0,
                    43.0
                ],
                "answer_relation_to_question_bing": [
                    2.0,
                    0.0,
                    2.0
                ],
                "question_related_to_answer": [
                    1.2432432432432432,
                    0.2702702702702703,
                    0.4864864864864865
                ],
                "question_related_to_answer_bing": [
                    1.6666666666666667,
                    0.3333333333333333,
                    0.0
                ],
                "result_count_noun_chunks": [
                    794000.0,
                    3730000.0,
                    1610000.0
                ],
                "word_count_noun_chunks": [
                    8.0,
                    6.0,
                    2.0
                ],
                "word_count_raw": [
                    15.0,
                    3.0,
                    2.0
                ],
                "result_count_bing": [
                    65300.0,
                    921000.0,
                    789000.0
                ],
                "word_count_appended": [
                    233.0,
                    225.0,
                    246.0
                ],
                "answer_relation_to_question": [
                    3.7777777777777777,
                    1.0,
                    0.2222222222222222
                ],
                "result_count": [
                    147000.0,
                    621000.0,
                    119000.0
                ]
            },
            "integer_answers": {
                "fungus": 7,
                "plant": 4,
                "bacteria": 3
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these is classified as a neurological condition or disorder?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "multiple sclerosis"
            ],
            "lines": [
                [
                    0.18389516107507942,
                    0.2532467532467533,
                    0.0,
                    0.0,
                    0.00572010607970884,
                    0.11345454545454546,
                    0.530932594644506,
                    0.004156605043347452,
                    0.28756674294431733,
                    0.5117845117845118,
                    0.3855421686746988,
                    0.29379053371940095,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.21952651666800116,
                    0.5238095238095238,
                    0.0,
                    0.0,
                    0.05024501431579209,
                    0.05381818181818182,
                    0.1809787626962142,
                    0.022010213372392225,
                    0.20404271548436306,
                    0.234006734006734,
                    0.3493975903614458,
                    0.31339478081930155,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.5965783222569194,
                    0.22294372294372294,
                    1.0,
                    1.0,
                    0.944034879604499,
                    0.8327272727272728,
                    0.2880886426592798,
                    0.9738331815842604,
                    0.5083905415713196,
                    0.2542087542087542,
                    0.26506024096385544,
                    0.3928146854612975,
                    1.0,
                    1.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "halitosis": 0.18357783733334784,
                "cystic fibrosis": 0.15365928809656784,
                "multiple sclerosis": 0.6627628745700843
            },
            "question": "which of these is classified as a neurological condition or disorder?",
            "rate_limited": false,
            "answers": [
                "halitosis",
                "cystic fibrosis",
                "multiple sclerosis"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "halitosis": 0.31499999945362406,
                "cystic fibrosis": 0.29999999900658925,
                "multiple sclerosis": 0.3952380970830009
            },
            "integer_answers": {
                "halitosis": 3,
                "cystic fibrosis": 1,
                "multiple sclerosis": 10
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1751621348776038,
                    1.2535791232772062,
                    1.57125874184519
                ],
                "result_count_important_words": [
                    1150000.0,
                    392000.0,
                    624000.0
                ],
                "wikipedia_search": [
                    0.8627002288329519,
                    0.6121281464530892,
                    1.5251716247139586
                ],
                "word_count_appended_bing": [
                    32.0,
                    29.0,
                    22.0
                ],
                "answer_relation_to_question_bing": [
                    0.5064935064935066,
                    1.0476190476190477,
                    0.4458874458874459
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    1.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_noun_chunks": [
                    105000.0,
                    556000.0,
                    24600000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    2.0
                ],
                "result_count_bing": [
                    624000.0,
                    296000.0,
                    4580000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    7.0
                ],
                "result_count": [
                    92100.0,
                    809000.0,
                    15200000.0
                ],
                "answer_relation_to_question": [
                    0.5516854832252382,
                    0.6585795500040035,
                    1.7897349667707583
                ],
                "word_count_appended": [
                    304.0,
                    139.0,
                    151.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these Uranus moons is NOT named after a Shakespearean character?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "trinculo"
            ],
            "lines": [
                [
                    0.4670920254919693,
                    0.32847222222222217,
                    0.10393258426966291,
                    0.2857142857142857,
                    0.11466081190070793,
                    0.11340206185567009,
                    0.11838036713765704,
                    0.11463066916154326,
                    0.4713541666666667,
                    0.28426051560379917,
                    0.3457943925233645,
                    0.32354998579093885,
                    0.1896551724137931,
                    0.2213114754098361,
                    -1.0
                ],
                [
                    0.210209997441196,
                    0.2011574074074074,
                    0.401685393258427,
                    0.2142857142857143,
                    0.3967201362129223,
                    0.4520618556701031,
                    0.39392931230185135,
                    0.3964180700347501,
                    0.20208333333333334,
                    0.3344640434192673,
                    0.3037383177570093,
                    0.3302245983486921,
                    0.3448275862068966,
                    0.29508196721311475,
                    -1.0
                ],
                [
                    0.3226979770668347,
                    0.4703703703703704,
                    0.49438202247191015,
                    0.5,
                    0.4886190518863697,
                    0.4345360824742268,
                    0.4876903205604916,
                    0.4889512608037067,
                    0.3265625,
                    0.3812754409769335,
                    0.35046728971962615,
                    0.346225415860369,
                    0.46551724137931033,
                    0.48360655737704916,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "oberon": 0.5025413234054119,
                "trinculo": 0.13701406700754312,
                "umbriel": 0.360444609587045
            },
            "question": "which of these uranus moons is not named after a shakespearean character?",
            "rate_limited": false,
            "answers": [
                "oberon",
                "umbriel",
                "trinculo"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "oberon": 0.21596199642771757,
                "trinculo": 0.2797046267505514,
                "umbriel": 0.09288448353463304
            },
            "integer_answers": {
                "oberon": 9,
                "trinculo": 0,
                "umbriel": 5
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7645001420906112,
                    1.6977540165130787,
                    1.53774584139631
                ],
                "result_count_important_words": [
                    19500.0,
                    5420.0,
                    629.0
                ],
                "wikipedia_search": [
                    0.22916666666666666,
                    2.3833333333333333,
                    1.3875
                ],
                "word_count_appended_bing": [
                    33.0,
                    42.0,
                    32.0
                ],
                "answer_relation_to_question_bing": [
                    1.3722222222222222,
                    2.3907407407407404,
                    0.23703703703703705
                ],
                "question_related_to_answer": [
                    1.5842696629213484,
                    0.39325842696629215,
                    0.02247191011235955
                ],
                "question_related_to_answer_bing": [
                    0.42857142857142855,
                    0.5714285714285714,
                    0.0
                ],
                "result_count_noun_chunks": [
                    17300.0,
                    4650.0,
                    496.0
                ],
                "word_count_noun_chunks": [
                    36.0,
                    18.0,
                    4.0
                ],
                "result_count_bing": [
                    225000.0,
                    27900.0,
                    38100.0
                ],
                "word_count_raw": [
                    34.0,
                    25.0,
                    2.0
                ],
                "result_count": [
                    17200.0,
                    4610.0,
                    508.0
                ],
                "answer_relation_to_question": [
                    0.32907974508030674,
                    2.89790002558804,
                    1.7730202293316533
                ],
                "word_count_appended": [
                    318.0,
                    244.0,
                    175.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these knots is typically used to add another line to a rope?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "bowline",
                "rolling hitch",
                "grantchester"
            ],
            "lines": [
                [
                    0.5011134253541472,
                    0.3741635807831446,
                    1.0,
                    0,
                    0.5193675889328063,
                    0.7197220383851753,
                    0.41534391534391535,
                    0.9051715915377425,
                    0.5455273736757995,
                    0.6027944111776448,
                    0.47692307692307695,
                    0.43738738050393067,
                    0.8421052631578947,
                    0.9285714285714286,
                    -1.0
                ],
                [
                    0.38074279084103446,
                    0.42515218265344656,
                    0.0,
                    0,
                    0.3027667984189723,
                    0.1902713434811383,
                    0.36331569664902996,
                    0.09395089754106178,
                    0.39576091075341446,
                    0.16766467065868262,
                    0.1076923076923077,
                    0.354876565132097,
                    0.15789473684210525,
                    0.07142857142857142,
                    -1.0
                ],
                [
                    0.11814378380481834,
                    0.20068423656340886,
                    0.0,
                    0,
                    0.17786561264822134,
                    0.0900066181336863,
                    0.22134038800705466,
                    0.000877510921195704,
                    0.058711715570786034,
                    0.22954091816367264,
                    0.4153846153846154,
                    0.2077360543639723,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "grantchester": 0.13233011181241783,
                "rolling hitch": 0.23165519016091246,
                "bowline": 0.6360146980266698
            },
            "question": "which of these knots is typically used to add another line to a rope?",
            "rate_limited": false,
            "answers": [
                "bowline",
                "rolling hitch",
                "grantchester"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "grantchester": 0.0,
                "rolling hitch": 0.0,
                "bowline": 0.0
            },
            "integer_answers": {
                "grantchester": 0,
                "rolling hitch": 1,
                "bowline": 12
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.1869369025196534,
                    1.774382825660485,
                    1.0386802718198616
                ],
                "result_count_important_words": [
                    47100.0,
                    41200.0,
                    25100.0
                ],
                "wikipedia_search": [
                    2.182109494703198,
                    1.5830436430136579,
                    0.23484686228314414
                ],
                "answer_relation_to_question": [
                    2.0044537014165886,
                    1.5229711633641378,
                    0.47257513521927336
                ],
                "word_count_appended_bing": [
                    31.0,
                    7.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    1.4966543231325784,
                    1.7006087306137863,
                    0.8027369462536355
                ],
                "question_related_to_answer": [
                    1.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    94900.0,
                    9850.0,
                    92.0
                ],
                "word_count_noun_chunks": [
                    16.0,
                    3.0,
                    0.0
                ],
                "result_count_bing": [
                    435000.0,
                    115000.0,
                    54400.0
                ],
                "word_count_raw": [
                    13.0,
                    1.0,
                    0.0
                ],
                "result_count": [
                    65700.0,
                    38300.0,
                    22500.0
                ],
                "word_count_appended": [
                    302.0,
                    84.0,
                    115.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these celebrities is known for having aviophobia?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "angelina jolie",
                "john madden",
                "john travolta"
            ],
            "lines": [
                [
                    0.22456140350877193,
                    0.33333333333333337,
                    0.5,
                    0,
                    0.41208791208791207,
                    0.2967121090617482,
                    0.4343891402714932,
                    0.42786069651741293,
                    0.8344907407407408,
                    0.47435897435897434,
                    0.32142857142857145,
                    0.3777098741445135,
                    0.16666666666666666,
                    0.25,
                    -1.0
                ],
                [
                    0.5368421052631579,
                    0.4523809523809524,
                    0.25,
                    0,
                    0.2692307692307692,
                    0.34723336006415395,
                    0.2986425339366516,
                    0.2835820895522388,
                    0.07423941798941798,
                    0.34615384615384615,
                    0.2857142857142857,
                    0.3786359322538703,
                    0.6666666666666666,
                    0.5,
                    -1.0
                ],
                [
                    0.23859649122807017,
                    0.2142857142857143,
                    0.25,
                    0,
                    0.31868131868131866,
                    0.35605453087409783,
                    0.2669683257918552,
                    0.2885572139303483,
                    0.09126984126984126,
                    0.1794871794871795,
                    0.39285714285714285,
                    0.2436541936016162,
                    0.16666666666666666,
                    0.25,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "angelina jolie": 0.3887384170861645,
                "john travolta": 0.25054450912875775,
                "john madden": 0.3607170737850777
            },
            "question": "which of these celebrities is known for having aviophobia?",
            "rate_limited": false,
            "answers": [
                "angelina jolie",
                "john madden",
                "john travolta"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "angelina jolie": 0.0,
                "john travolta": 0.0,
                "john madden": 0.0
            },
            "integer_answers": {
                "angelina jolie": 6,
                "john travolta": 2,
                "john madden": 5
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.510839496578054,
                    1.5145437290154813,
                    0.9746167744064648
                ],
                "result_count_important_words": [
                    96.0,
                    66.0,
                    59.0
                ],
                "wikipedia_search": [
                    2.5034722222222223,
                    0.22271825396825395,
                    0.2738095238095238
                ],
                "word_count_appended_bing": [
                    9.0,
                    8.0,
                    11.0
                ],
                "answer_relation_to_question_bing": [
                    0.6666666666666666,
                    0.9047619047619047,
                    0.42857142857142855
                ],
                "question_related_to_answer": [
                    1.0,
                    0.5,
                    0.5
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    86.0,
                    57.0,
                    58.0
                ],
                "word_count_noun_chunks": [
                    1.0,
                    4.0,
                    1.0
                ],
                "word_count_raw": [
                    1.0,
                    2.0,
                    1.0
                ],
                "result_count_bing": [
                    3700000.0,
                    4330000.0,
                    4440000.0
                ],
                "result_count": [
                    75.0,
                    49.0,
                    58.0
                ],
                "answer_relation_to_question": [
                    0.6736842105263158,
                    1.6105263157894738,
                    0.7157894736842105
                ],
                "word_count_appended": [
                    37.0,
                    27.0,
                    14.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "By definition, an Anglophile would be most interested in which of these things?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "geometry"
            ],
            "lines": [
                [
                    0.4166666666666667,
                    1.0,
                    0.5,
                    0,
                    0.409270584725537,
                    0.9491390111656891,
                    0.08603711805152968,
                    0.40881785820231387,
                    0.26666666666666666,
                    0.6964285714285714,
                    0.7352941176470589,
                    0.4049046825284622,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.4166666666666667,
                    0.0,
                    0.5,
                    0,
                    0.5901327565632458,
                    0.005621652788095845,
                    0.9136322535948151,
                    0.5905146840700088,
                    0.7333333333333334,
                    0.26339285714285715,
                    0.22058823529411764,
                    0.4252612385620575,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.16666666666666666,
                    0.0,
                    0.0,
                    0,
                    0.0005966587112171838,
                    0.045239336046215085,
                    0.00033062835365516404,
                    0.0006674577276772471,
                    0.0,
                    0.04017857142857143,
                    0.04411764705882353,
                    0.16983407890948032,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "geometry": 0.5339295706438631,
                "downton abbey": 0.42355851618319984,
                "trout fishing": 0.04251191317293696
            },
            "question": "by definition, an anglophile would be most interested in which of these things?",
            "rate_limited": false,
            "answers": [
                "geometry",
                "downton abbey",
                "trout fishing"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "geometry": 0.38960508165004526,
                "downton abbey": 0.27121963667554705,
                "trout fishing": 0.04725316800553263
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6196187301138487,
                    1.70104495424823,
                    0.6793363156379213
                ],
                "result_count_important_words": [
                    210000.0,
                    2230000.0,
                    807.0
                ],
                "wikipedia_search": [
                    0.8,
                    2.2,
                    0.0
                ],
                "word_count_appended_bing": [
                    50.0,
                    15.0,
                    3.0
                ],
                "answer_relation_to_question_bing": [
                    1.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer": [
                    1.0,
                    1.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    44100.0,
                    63700.0,
                    72.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    8560000.0,
                    50700.0,
                    408000.0
                ],
                "word_count_appended": [
                    156.0,
                    59.0,
                    9.0
                ],
                "answer_relation_to_question": [
                    1.25,
                    1.25,
                    0.5
                ],
                "result_count": [
                    43900.0,
                    63300.0,
                    64.0
                ]
            },
            "integer_answers": {
                "geometry": 6,
                "downton abbey": 5,
                "trout fishing": 0
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Who wrote a #1 hit song for the Monkees?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "james taylor",
                "neil diamond",
                "jackson browne"
            ],
            "lines": [
                [
                    0.24062849893515764,
                    0.28328878623143444,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.33236994219653176,
                    0.27307206068268014,
                    0.3333333333333333,
                    0.5366170441366657,
                    0,
                    0,
                    0.33460594491409873,
                    0,
                    0,
                    0.0
                ],
                [
                    0.3480019030066023,
                    0.33500950100897114,
                    1.0,
                    1.0,
                    0.3333333333333333,
                    0.33429672447013487,
                    0.5562579013906448,
                    0.3333333333333333,
                    0.2946559946743365,
                    0,
                    0,
                    0.33249250068175623,
                    0,
                    0,
                    0.0
                ],
                [
                    0.41136959805824014,
                    0.38170171275959436,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.1706700379266751,
                    0.3333333333333333,
                    0.16872696118899783,
                    0,
                    0,
                    0.3329015544041451,
                    0,
                    0,
                    0.0
                ]
            ],
            "fraction_answers": {
                "neil diamond": 0.4867381191899113,
                "james taylor": 0.26672489437632346,
                "jackson browne": 0.2465369864337652
            },
            "question": "who wrote a #1 hit song for the monkees?",
            "rate_limited": false,
            "answers": [
                "james taylor",
                "neil diamond",
                "jackson browne"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "neil diamond": 0.0,
                "james taylor": 0.0,
                "jackson browne": 0.0
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3384237796563947,
                    1.3299700027270247,
                    1.3316062176165802
                ],
                "result_count_important_words": [
                    108000.0,
                    220000.0,
                    67500.0
                ],
                "wikipedia_search": [
                    2.1464681765466627,
                    1.178623978697346,
                    0.6749078447559913
                ],
                "word_count_appended_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "answer_relation_to_question_bing": [
                    0.8498663586943034,
                    1.0050285030269135,
                    1.1451051382787831
                ],
                "question_related_to_answer": [
                    0.0,
                    1.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    7220000000.0,
                    7220000000.0,
                    7220000000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    34500000.0,
                    34700000.0,
                    34600000.0
                ],
                "word_count_appended": [
                    0.0,
                    0.0,
                    0.0
                ],
                "answer_relation_to_question": [
                    0.9625139957406306,
                    1.392007612026409,
                    1.6454783922329606
                ],
                "result_count": [
                    64600000.0,
                    64600000.0,
                    64600000.0
                ]
            },
            "integer_answers": {
                "neil diamond": 4,
                "james taylor": 4,
                "jackson browne": 2
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Anne of Green Gables literally means Anne of what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "green pastures"
            ],
            "lines": [
                [
                    0.41732131854590265,
                    0.3262290423415743,
                    0,
                    0,
                    0.5604395604395604,
                    0.3336439888164026,
                    0.6,
                    0.7596141160290573,
                    0.08333333333333333,
                    0.5,
                    0.3333333333333333,
                    0.5029420648838103,
                    0,
                    0,
                    1.0
                ],
                [
                    0.403573629081947,
                    0.3245240125035522,
                    0,
                    0,
                    0.04395604395604396,
                    0.33457595526561046,
                    0.041666666666666664,
                    0.00010794516385676077,
                    0.430952380952381,
                    0.07894736842105263,
                    0.3333333333333333,
                    0.13157286445958602,
                    0,
                    0,
                    1.0
                ],
                [
                    0.17910505237215033,
                    0.3492469451548735,
                    0,
                    0,
                    0.3956043956043956,
                    0.3317800559179869,
                    0.35833333333333334,
                    0.240277938807086,
                    0.4857142857142857,
                    0.42105263157894735,
                    0.3333333333333333,
                    0.3654850706566037,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "green pastures": 0.4416856757722975,
                "green walls": 0.3459933042472996,
                "green jars": 0.212321019980403
            },
            "question": "anne of green gables literally means anne of what?",
            "rate_limited": false,
            "answers": [
                "green pastures",
                "green jars",
                "green walls"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "green pastures": 0.34000000019868215,
                "green walls": 0.31499999945362406,
                "green jars": 0.3194444440305233
            },
            "integer_answers": {
                "green pastures": 7,
                "green walls": 2,
                "green jars": 1
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.017652389302862,
                    0.7894371867575161,
                    2.192910423939622
                ],
                "result_count_important_words": [
                    72.0,
                    5.0,
                    43.0
                ],
                "wikipedia_search": [
                    0.3333333333333333,
                    1.723809523809524,
                    1.9428571428571428
                ],
                "word_count_appended_bing": [
                    13.0,
                    13.0,
                    13.0
                ],
                "answer_relation_to_question_bing": [
                    0.9786871270247228,
                    0.9735720375106565,
                    1.0477408354646205
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    190000.0,
                    27.0,
                    60100.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    358000.0,
                    359000.0,
                    356000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    51.0,
                    4.0,
                    36.0
                ],
                "answer_relation_to_question": [
                    1.6692852741836106,
                    1.614294516327788,
                    0.7164202094886013
                ],
                "word_count_appended": [
                    19.0,
                    3.0,
                    16.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What Japanese word means \u201cempty orchestra\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "karaoke"
            ],
            "lines": [
                [
                    0.3851214574898785,
                    0.5861486486486487,
                    0.0,
                    0.0,
                    0.05079505300353357,
                    0.21712707182320443,
                    0.34457939721097613,
                    0.12413793103448276,
                    0.15625,
                    0.19114470842332612,
                    0.37254901960784315,
                    0.3004098416568604,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.30145074224021595,
                    0.23817567567567569,
                    0.44125939849624063,
                    0.0,
                    0.05256183745583039,
                    0.6464088397790055,
                    0.30364372469635625,
                    0.36354679802955664,
                    0.4270833333333333,
                    0.21166306695464362,
                    0.24183006535947713,
                    0.3080617858134042,
                    0.017793594306049824,
                    0.012539184952978056,
                    1.0
                ],
                [
                    0.3134278002699055,
                    0.17567567567567569,
                    0.5587406015037594,
                    1.0,
                    0.8966431095406361,
                    0.13646408839779006,
                    0.35177687809266756,
                    0.5123152709359606,
                    0.4166666666666667,
                    0.5971922246220303,
                    0.38562091503267976,
                    0.3915283725297354,
                    0.9822064056939501,
                    0.987460815047022,
                    1.0
                ]
            ],
            "fraction_answers": {
                "sake": 0.1948759377784824,
                "anime": 0.2547155747923405,
                "karaoke": 0.550408487429177
            },
            "question": "what japanese word means \u201cempty orchestra\u201d?",
            "rate_limited": false,
            "answers": [
                "sake",
                "anime",
                "karaoke"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sake": 0.21661070797834364,
                "anime": 0.050593896909943564,
                "karaoke": 0.8525033400654742
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2016393666274416,
                    1.2322471432536168,
                    1.5661134901189415
                ],
                "result_count_important_words": [
                    766000.0,
                    675000.0,
                    782000.0
                ],
                "wikipedia_search": [
                    0.625,
                    1.7083333333333333,
                    1.6666666666666667
                ],
                "word_count_appended_bing": [
                    57.0,
                    37.0,
                    59.0
                ],
                "answer_relation_to_question_bing": [
                    1.1722972972972974,
                    0.47635135135135137,
                    0.35135135135135137
                ],
                "question_related_to_answer": [
                    0.0,
                    0.8825187969924813,
                    1.1174812030075187
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_noun_chunks": [
                    12600.0,
                    36900.0,
                    52000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    5.0,
                    276.0
                ],
                "word_count_raw": [
                    0.0,
                    4.0,
                    315.0
                ],
                "result_count_bing": [
                    3930000.0,
                    11700000.0,
                    2470000.0
                ],
                "word_count_appended": [
                    177.0,
                    196.0,
                    553.0
                ],
                "answer_relation_to_question": [
                    1.540485829959514,
                    1.2058029689608638,
                    1.253711201079622
                ],
                "result_count": [
                    1150.0,
                    1190.0,
                    20300.0
                ]
            },
            "integer_answers": {
                "sake": 2,
                "anime": 2,
                "karaoke": 10
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What is Telluride, Colorado named after?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "a governor"
            ],
            "lines": [
                [
                    0.3333333333333333,
                    0,
                    0,
                    0,
                    0.9790565035675526,
                    0.3541666666666667,
                    0.9955561108112514,
                    0.8635254364557913,
                    0,
                    0.5391304347826087,
                    0.3333333333333333,
                    0.4607648372584035,
                    0,
                    0,
                    1.0
                ],
                [
                    0.2222222222222222,
                    0,
                    0,
                    0,
                    0.02082804165608331,
                    0.2664930555555556,
                    0.004411680020457702,
                    0.08954383330204618,
                    0,
                    0.3565217391304348,
                    0.3333333333333333,
                    0.4430928386407318,
                    0,
                    0,
                    1.0
                ],
                [
                    0.4444444444444444,
                    0,
                    0,
                    0,
                    0.00011545477636409818,
                    0.3793402777777778,
                    3.220916829095225e-05,
                    0.046930730242162565,
                    0,
                    0.10434782608695652,
                    0.3333333333333333,
                    0.0961423241008646,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "a european city": 0.1755858249912743,
                "an element": 0.6073583320261176,
                "a governor": 0.2170558429826081
            },
            "question": "what is telluride, colorado named after?",
            "rate_limited": false,
            "answers": [
                "an element",
                "a governor",
                "a european city"
            ],
            "ml_answers": {
                "a european city": 0.23980873748359383,
                "an element": 0.2073451443044786,
                "a governor": 0.3580451856510281
            },
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "integer_answers": {
                "a european city": 2,
                "an element": 6,
                "a governor": 0
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3822945117752108,
                    1.3292785159221956,
                    0.28842697230259384
                ],
                "result_count_important_words": [
                    1020000.0,
                    4520.0,
                    33.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    46000000.0,
                    4770000.0,
                    2500000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    408000.0,
                    307000.0,
                    437000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    212000.0,
                    4510.0,
                    25.0
                ],
                "answer_relation_to_question": [
                    0.3333333333333333,
                    0.2222222222222222,
                    0.4444444444444444
                ],
                "word_count_appended": [
                    62.0,
                    41.0,
                    12.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these is NOT a machine used for printing?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "hydraulophone",
                "hectograph",
                "spirit duplicator"
            ],
            "lines": [
                [
                    0.49836601307189543,
                    0.5,
                    0,
                    0,
                    0.49004767082043993,
                    0.33312236286919833,
                    0.4940382067609727,
                    0.468537028623353,
                    0.5,
                    0.3282828282828283,
                    0.37786259541984735,
                    0.4129677499471872,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.34977532679738566,
                    0.27692307692307694,
                    0,
                    0,
                    0.04001839926402945,
                    0.33312236286919833,
                    0.09613658703363392,
                    0.101317582916856,
                    0.30978260869565216,
                    0.26262626262626265,
                    0.23282442748091603,
                    0.2762019189595026,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.15185866013071891,
                    0.22307692307692306,
                    0,
                    0,
                    0.4699339299155306,
                    0.3337552742616034,
                    0.4098252062053934,
                    0.430145388459791,
                    0.19021739130434784,
                    0.40909090909090906,
                    0.3893129770992366,
                    0.3108303310933102,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "spirit duplicator": 0.3363906018724472,
                "hectograph": 0.5442542892866973,
                "hydraulophone": 0.11935510884085557
            },
            "question": "which of these is not a machine used for printing?",
            "rate_limited": false,
            "answers": [
                "hydraulophone",
                "hectograph",
                "spirit duplicator"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "spirit duplicator": 0.0,
                "hectograph": 0.0,
                "hydraulophone": 0.0
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.34812900021125115,
                    0.8951923241619897,
                    0.7566786756267592
                ],
                "result_count_important_words": [
                    2790.0,
                    189000.0,
                    42200.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.7608695652173914,
                    1.2391304347826086
                ],
                "word_count_appended_bing": [
                    32.0,
                    70.0,
                    29.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.8923076923076922,
                    1.1076923076923078
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    27700.0,
                    351000.0,
                    61500.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    79100000.0,
                    79100000.0,
                    78800000.0
                ],
                "word_count_appended": [
                    204.0,
                    282.0,
                    108.0
                ],
                "answer_relation_to_question": [
                    0.006535947712418301,
                    0.6008986928104575,
                    1.3925653594771243
                ],
                "result_count": [
                    2380.0,
                    110000.0,
                    7190.0
                ]
            },
            "integer_answers": {
                "spirit duplicator": 3,
                "hectograph": 6,
                "hydraulophone": 1
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Table tennis is also known as what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ping-pong"
            ],
            "lines": [
                [
                    0.712106135986733,
                    0.8462962962962962,
                    1.0,
                    1.0,
                    0.9888163990211942,
                    0.3713163064833006,
                    0.988616482000233,
                    0.9862924874784798,
                    0.20198170731707318,
                    0.4957410562180579,
                    0.42857142857142855,
                    0.6212648045421375,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.14975124378109453,
                    0.11666666666666665,
                    0.0,
                    0.0,
                    0.011113246254485988,
                    0.2583497053045187,
                    0.011311197586849512,
                    0.01347607557148814,
                    0.6676829268292683,
                    0.2776831345826235,
                    0.2857142857142857,
                    0.20788309787153103,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.13814262023217247,
                    0.037037037037037035,
                    0.0,
                    0.0,
                    7.035472431973807e-05,
                    0.3703339882121807,
                    7.232041291749452e-05,
                    0.00023143695003207892,
                    0.13033536585365854,
                    0.22657580919931858,
                    0.2857142857142857,
                    0.1708520975863315,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "hunky-dory": 0.14283082715448658,
                "argle-bargle": 0.09709752256587527,
                "ping-pong": 0.7600716502796381
            },
            "question": "table tennis is also known as what?",
            "rate_limited": false,
            "answers": [
                "ping-pong",
                "hunky-dory",
                "argle-bargle"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "hunky-dory": 0.04240828637399469,
                "argle-bargle": 0.011536512546490553,
                "ping-pong": 0.6310963661454397
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.8637944136264124,
                    0.623649293614593,
                    0.5125562927589944
                ],
                "result_count_important_words": [
                    11100000.0,
                    127000.0,
                    812.0
                ],
                "wikipedia_search": [
                    0.40396341463414637,
                    1.3353658536585367,
                    0.2606707317073171
                ],
                "word_count_appended_bing": [
                    42.0,
                    28.0,
                    28.0
                ],
                "answer_relation_to_question_bing": [
                    2.5388888888888888,
                    0.35,
                    0.1111111111111111
                ],
                "question_related_to_answer": [
                    1.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    10100000.0,
                    138000.0,
                    2370.0
                ],
                "word_count_noun_chunks": [
                    21.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    84.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    3780000.0,
                    2630000.0,
                    3770000.0
                ],
                "word_count_appended": [
                    291.0,
                    163.0,
                    133.0
                ],
                "answer_relation_to_question": [
                    2.136318407960199,
                    0.44925373134328356,
                    0.4144278606965174
                ],
                "result_count": [
                    11300000.0,
                    127000.0,
                    804.0
                ]
            },
            "integer_answers": {
                "hunky-dory": 1,
                "argle-bargle": 0,
                "ping-pong": 13
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these countries is closest to the International Date Line?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "spain"
            ],
            "lines": [
                [
                    0.32279693486590033,
                    0.1754636141063708,
                    0.9285714285714286,
                    0,
                    0.3763171098845961,
                    0.3189619087484303,
                    0.2667834208990076,
                    0.20695807314897413,
                    0.32236730863275115,
                    0.2938856015779093,
                    0.32558139534883723,
                    0.3513212213792766,
                    0.5,
                    0.3333333333333333,
                    -1.0
                ],
                [
                    0.33725908510391267,
                    0.4089245715476376,
                    0.07142857142857142,
                    0,
                    0.2824887104867035,
                    0.33528673084972793,
                    0.3432574430823117,
                    0.45316681534344333,
                    0.3777416673453997,
                    0.3609467455621302,
                    0.3488372093023256,
                    0.32361984823269774,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.33994398003018694,
                    0.41561181434599154,
                    0.0,
                    0,
                    0.34119417962870047,
                    0.3457513604018418,
                    0.38995913601868065,
                    0.33987511150758254,
                    0.29989102402184914,
                    0.34516765285996054,
                    0.32558139534883723,
                    0.32505893038802564,
                    0.5,
                    0.6666666666666666,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "brazil": 0.2802274921757586,
                "japan": 0.3632570269612935,
                "spain": 0.35651548086294793
            },
            "question": "which of these countries is closest to the international date line?",
            "rate_limited": false,
            "answers": [
                "japan",
                "brazil",
                "spain"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "brazil": 0.3194444440305233,
                "japan": 0.34000000019868215,
                "spain": 0.354545455177625
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7566061068963832,
                    1.6180992411634887,
                    1.6252946519401281
                ],
                "result_count_important_words": [
                    4570000.0,
                    5880000.0,
                    6680000.0
                ],
                "wikipedia_search": [
                    1.2894692345310046,
                    1.5109666693815988,
                    1.1995640960873966
                ],
                "word_count_appended_bing": [
                    28.0,
                    30.0,
                    28.0
                ],
                "answer_relation_to_question_bing": [
                    0.7018544564254832,
                    1.6356982861905505,
                    1.6624472573839661
                ],
                "question_related_to_answer": [
                    1.8571428571428572,
                    0.14285714285714285,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    2320000.0,
                    5080000.0,
                    3810000.0
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    76200000.0,
                    80100000.0,
                    82600000.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    2.0
                ],
                "word_count_appended": [
                    298.0,
                    366.0,
                    350.0
                ],
                "answer_relation_to_question": [
                    1.2911877394636013,
                    1.3490363404156507,
                    1.3597759201207478
                ],
                "result_count": [
                    7500000.0,
                    5630000.0,
                    6800000.0
                ]
            },
            "integer_answers": {
                "brazil": 4,
                "japan": 4,
                "spain": 5
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these is NOT a name of one of the Florida Keys?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "turtle key"
            ],
            "question": "which of these is not a name of one of the florida keys?",
            "answers": [
                "fat deer key",
                "turtle key",
                "pigeon key"
            ],
            "integer_answers": {
                "pigeon key": 10,
                "turtle key": 0,
                "fat deer key": 1
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.3980709043912827,
                    0.484444627678453,
                    1.1174844679302645
                ],
                "result_count_important_words": [
                    22500.0,
                    18500.0,
                    123000.0
                ],
                "wikipedia_search": [
                    0.3333333333333333,
                    0.0,
                    0.6666666666666666
                ],
                "word_count_appended_bing": [
                    3.0,
                    12.0,
                    33.0
                ],
                "answer_relation_to_question_bing": [
                    0.5447564721369021,
                    0.5344449319877139,
                    0.9207985958753839
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_noun_chunks": [
                    53.0,
                    15400.0,
                    649000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    3570000.0,
                    3590000.0,
                    3610000.0
                ],
                "word_count_appended": [
                    29.0,
                    32.0,
                    114.0
                ],
                "answer_relation_to_question": [
                    1.0851178652735074,
                    0.10691319057077811,
                    0.8079689441557145
                ],
                "result_count": [
                    32.0,
                    1920.0,
                    3990000.0
                ]
            },
            "negative_question": true,
            "fraction_answers": {
                "pigeon key": 0.6809241707123136,
                "turtle key": 0.13323224146885662,
                "fat deer key": 0.1858435878188298
            },
            "lines": [
                [
                    0.22872053368162315,
                    0.3638108819657745,
                    0,
                    0.5,
                    0.49999599193577476,
                    0.3342618384401114,
                    0.43140243902439024,
                    0.4999601175703925,
                    0.33333333333333337,
                    0.41714285714285715,
                    0.46875,
                    0.40048227390217933,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.4732717023573055,
                    0.36638876700307155,
                    0,
                    0.5,
                    0.4997595161464867,
                    0.33333333333333337,
                    0.44359756097560976,
                    0.4884115204536664,
                    0.5,
                    0.4085714285714286,
                    0.375,
                    0.37888884308038673,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.29800776396107137,
                    0.269800351031154,
                    0,
                    0.0,
                    0.00024449191773850965,
                    0.33240482822655526,
                    0.125,
                    0.011628361975941137,
                    0.16666666666666669,
                    0.17428571428571427,
                    0.15625,
                    0.22062888301743389,
                    0,
                    0,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "pigeon key": 0.08479638884094878,
                "turtle key": 0.2927830274103262,
                "fat deer key": 0.24183528604458304
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "The '70s sitcom \u201cThree's Company\u201d was about three people who were what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "roommates"
            ],
            "lines": [
                [
                    0.21428571428571427,
                    0.0,
                    1.0,
                    1.0,
                    0.9998695822284049,
                    0.44043585752220493,
                    0.9081290219028049,
                    0.9991619087528154,
                    0.28205128205128205,
                    0.9547511312217195,
                    0.8775510204081632,
                    0.7927939419642405,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.6428571428571428,
                    0.5,
                    0.0,
                    0.0,
                    0.0,
                    0.477062540060434,
                    2.1119279579134998e-06,
                    0.0,
                    0.3333333333333333,
                    0.01809954751131222,
                    0.061224489795918366,
                    0.06304518069900707,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.14285714285714285,
                    0.5,
                    0.0,
                    0.0,
                    0.00013041777159500934,
                    0.08250160241736104,
                    0.09186886616923723,
                    0.0008380912471845372,
                    0.3846153846153846,
                    0.027149321266968326,
                    0.061224489795918366,
                    0.1441608773367524,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "trivia show hosts": 0.14968745329893612,
                "roommates": 0.747787818595525,
                "panda bears": 0.10252472810553888
            },
            "question": "the '70s sitcom \u201cthree's company\u201d was about three people who were what?",
            "rate_limited": false,
            "answers": [
                "roommates",
                "trivia show hosts",
                "panda bears"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "trivia show hosts": 0.23520778037803095,
                "roommates": 0.8192662584847863,
                "panda bears": 0.06947489132243118
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.3783818258927214,
                    0.18913554209702121,
                    0.43248263201025716
                ],
                "result_count_important_words": [
                    1720000.0,
                    4.0,
                    174000.0
                ],
                "wikipedia_search": [
                    0.8461538461538461,
                    1.0,
                    1.1538461538461537
                ],
                "word_count_appended_bing": [
                    43.0,
                    3.0,
                    3.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    1.0,
                    1.0
                ],
                "question_related_to_answer": [
                    1.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    76300.0,
                    0,
                    64.0
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    6.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    48100000.0,
                    52100000.0,
                    9010000.0
                ],
                "word_count_appended": [
                    211.0,
                    4.0,
                    6.0
                ],
                "answer_relation_to_question": [
                    0.42857142857142855,
                    1.2857142857142856,
                    0.2857142857142857
                ],
                "result_count": [
                    184000.0,
                    0,
                    24.0
                ]
            },
            "integer_answers": {
                "trivia show hosts": 3,
                "roommates": 10,
                "panda bears": 1
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "How did Mason jars get their name?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "invented in mason, al"
            ],
            "lines": [
                [
                    0.0,
                    0.0,
                    0,
                    0,
                    4.804778833027329e-06,
                    0.32207207207207206,
                    4.065021647256527e-06,
                    5.740144851044752e-06,
                    0.0,
                    0.45,
                    0.3333333333333333,
                    0.056239856044033594,
                    0,
                    0,
                    5.0
                ],
                [
                    0.42335473515248795,
                    0.35443037974683544,
                    0,
                    0,
                    0.9999945946238128,
                    0.3130630630630631,
                    0.9999953252251057,
                    0.9999936556293751,
                    0.0,
                    0.3,
                    0.3333333333333333,
                    0.876709477101122,
                    0,
                    0,
                    5.0
                ],
                [
                    0.576645264847512,
                    0.6455696202531646,
                    0,
                    0,
                    6.005973541284161e-07,
                    0.36486486486486486,
                    6.09753247088479e-07,
                    6.042257737941844e-07,
                    1.0,
                    0.25,
                    0.3333333333333333,
                    0.0670506668548444,
                    0,
                    0,
                    5.0
                ]
            ],
            "fraction_answers": {
                "named after inventor": 0.11616598713947704,
                "invented in mason, al": 0.5600874563875136,
                "masons used them": 0.32374655647300943
            },
            "question": "how did mason jars get their name?",
            "rate_limited": false,
            "answers": [
                "named after inventor",
                "invented in mason, al",
                "masons used them"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "named after inventor": 0.29999999900658925,
                "invented in mason, al": 0.34000000019868215,
                "masons used them": 0.3194444440305233
            },
            "integer_answers": {
                "named after inventor": 2,
                "invented in mason, al": 4,
                "masons used them": 4
            },
            "categorical_data": {
                "question_type": 5
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.11247971208806719,
                    1.753418954202244,
                    0.1341013337096888
                ],
                "result_count_important_words": [
                    20.0,
                    4920000.0,
                    3.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.35443037974683544,
                    0.6455696202531646
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    19.0,
                    3310000.0,
                    2.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    1430000.0,
                    1390000.0,
                    1620000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    16.0,
                    3330000.0,
                    2.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    0.8467094703049759,
                    1.153290529695024
                ],
                "word_count_appended": [
                    9.0,
                    6.0,
                    5.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which two countries have almost perfectly identical flags?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "indonesia / monaco"
            ],
            "lines": [
                [
                    0.06468531468531469,
                    0.1111111111111111,
                    0,
                    0,
                    0.2698412698412698,
                    0.10118130457113508,
                    0.4048140043763676,
                    0.288135593220339,
                    0.00315955766192733,
                    0.16666666666666666,
                    0.3333333333333333,
                    0.2744533701607454,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.10372960372960373,
                    0.037037037037037035,
                    0,
                    0,
                    0.38095238095238093,
                    0.7986646122239343,
                    0.5725747629467542,
                    0.4067796610169492,
                    0.00315955766192733,
                    0.16666666666666666,
                    0.3333333333333333,
                    0.3166248959113437,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.8315850815850816,
                    0.8518518518518517,
                    0,
                    0,
                    0.3492063492063492,
                    0.10015408320493066,
                    0.02261123267687819,
                    0.3050847457627119,
                    0.9936808846761452,
                    0.6666666666666666,
                    0.3333333333333333,
                    0.4089217339279109,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "luxembourg / russia": 0.311952251147993,
                "indonesia / monaco": 0.4863095962891859,
                "armenia / romania": 0.20173815256282102
            },
            "question": "which two countries have almost perfectly identical flags?",
            "rate_limited": false,
            "answers": [
                "armenia / romania",
                "luxembourg / russia",
                "indonesia / monaco"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "luxembourg / russia": 0.07553038919738239,
                "indonesia / monaco": 0.5356919214042968,
                "armenia / romania": 0.03349094454929791
            },
            "integer_answers": {
                "luxembourg / russia": 4,
                "indonesia / monaco": 5,
                "armenia / romania": 1
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.0978134806429816,
                    1.2664995836453747,
                    1.6356869357116437
                ],
                "result_count_important_words": [
                    1110.0,
                    1570.0,
                    62.0
                ],
                "wikipedia_search": [
                    0.009478672985781991,
                    0.009478672985781991,
                    2.9810426540284363
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.3333333333333333,
                    0.1111111111111111,
                    2.5555555555555554
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    17.0,
                    24.0,
                    18.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    39400.0,
                    311000.0,
                    39000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    17.0,
                    24.0,
                    22.0
                ],
                "answer_relation_to_question": [
                    0.25874125874125875,
                    0.4149184149184149,
                    3.3263403263403264
                ],
                "word_count_appended": [
                    3.0,
                    3.0,
                    12.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Catherine O'Hara and Eugene Levy do NOT kiss in which Christopher Guest film?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "best in show"
            ],
            "lines": [
                [
                    0.38742479673228,
                    0.349664783583882,
                    0.2,
                    0.25,
                    0.04251303253640121,
                    0.11563775174410951,
                    0.04247779671660534,
                    0.0842577918647649,
                    0.4393939393939394,
                    0.350328947368421,
                    0.32978723404255317,
                    0.3343514561783716,
                    0.2894736842105263,
                    0.38095238095238093,
                    -1.0
                ],
                [
                    0.3097220980196931,
                    0.31334272598310964,
                    0.3666666666666667,
                    0.5,
                    0.4962250584217149,
                    0.4416216927734632,
                    0.4961873149726384,
                    0.46619123085050185,
                    0.0896464646464647,
                    0.26480263157894735,
                    0.30851063829787234,
                    0.3287609605337944,
                    0.34210526315789475,
                    0.17142857142857143,
                    -1.0
                ],
                [
                    0.302853105248027,
                    0.3369924904330084,
                    0.43333333333333335,
                    0.25,
                    0.46126190904188386,
                    0.4427405554824273,
                    0.46133488831075625,
                    0.44955097728473326,
                    0.47095959595959597,
                    0.3848684210526316,
                    0.36170212765957444,
                    0.33688758328783414,
                    0.368421052631579,
                    0.44761904761904764,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "a mighty wind": 0.3006840975240953,
                "best in show": 0.48624805781082353,
                "waiting for guffman": 0.2130678446650811
            },
            "question": "catherine o'hara and eugene levy do not kiss in which christopher guest film?",
            "rate_limited": false,
            "answers": [
                "best in show",
                "a mighty wind",
                "waiting for guffman"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "a mighty wind": 0.03177608322221471,
                "best in show": 0.42684381196023546,
                "waiting for guffman": 0.3439401294455731
            },
            "integer_answers": {
                "a mighty wind": 6,
                "best in show": 7,
                "waiting for guffman": 1
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.6503767011460555,
                    2.7398246314592902,
                    2.6097986673946543
                ],
                "result_count_important_words": [
                    510000.0,
                    4250.0,
                    43100.0
                ],
                "wikipedia_search": [
                    0.36363636363636365,
                    2.462121212121212,
                    0.17424242424242425
                ],
                "word_count_appended_bing": [
                    16.0,
                    18.0,
                    13.0
                ],
                "answer_relation_to_question_bing": [
                    2.1046930298256528,
                    2.613201836236465,
                    2.282105133937882
                ],
                "question_related_to_answer": [
                    1.2,
                    0.5333333333333333,
                    0.26666666666666666
                ],
                "question_related_to_answer_bing": [
                    0.5,
                    0.0,
                    0.5
                ],
                "result_count_noun_chunks": [
                    787000.0,
                    64000.0,
                    95500.0
                ],
                "word_count_noun_chunks": [
                    32.0,
                    24.0,
                    20.0
                ],
                "result_count_bing": [
                    584000.0,
                    88700.0,
                    87000.0
                ],
                "word_count_raw": [
                    25.0,
                    69.0,
                    11.0
                ],
                "result_count": [
                    509000.0,
                    4200.0,
                    43100.0
                ],
                "answer_relation_to_question": [
                    1.8012032522835204,
                    3.044446431684911,
                    3.154350316031568
                ],
                "word_count_appended": [
                    91.0,
                    143.0,
                    70.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What SportsCenter anchor shares their last name with Linus & Lucy from \u201cPeanuts\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "scott van pelt"
            ],
            "lines": [
                [
                    0.353527735880677,
                    0.21777777777777776,
                    0.8979591836734694,
                    0.8,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.4642857142857143,
                    0.0032258064516129032,
                    0.2833333333333333,
                    0.16666666666666666,
                    0.5,
                    0.33333333333333337,
                    0.5,
                    0.16666666666666666,
                    1.0
                ],
                [
                    0.3636668342550695,
                    0.4944444444444444,
                    0.08163265306122448,
                    0.0,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.34523809523809523,
                    0.24946236559139784,
                    0.20833333333333331,
                    0.6666666666666666,
                    0.5,
                    0.33333333333333337,
                    0.0,
                    0.6666666666666666,
                    1.0
                ],
                [
                    0.28280542986425333,
                    0.28777777777777774,
                    0.02040816326530612,
                    0.2,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.19047619047619047,
                    0.7473118279569892,
                    0.5083333333333333,
                    0.16666666666666666,
                    0.0,
                    0.33333333333333337,
                    0.5,
                    0.16666666666666666,
                    1.0
                ]
            ],
            "fraction_answers": {
                "jemele hill": 0.38238877748113703,
                "kenny mayne": 0.29074614685765604,
                "scott van pelt": 0.326865075661207
            },
            "question": "what sportscenter anchor shares their last name with linus & lucy from \u201cpeanuts\u201d?",
            "rate_limited": false,
            "answers": [
                "jemele hill",
                "scott van pelt",
                "kenny mayne"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "jemele hill": 0.3194444440305233,
                "kenny mayne": 0.3194444440305233,
                "scott van pelt": 0.354545455177625
            },
            "integer_answers": {
                "jemele hill": 8,
                "kenny mayne": 2,
                "scott van pelt": 4
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9999999999999998,
                    1.9999999999999998,
                    1.9999999999999998
                ],
                "result_count_important_words": [
                    39.0,
                    29.0,
                    16.0
                ],
                "wikipedia_search": [
                    0.5666666666666667,
                    0.41666666666666663,
                    1.0166666666666666
                ],
                "word_count_appended_bing": [
                    1.0,
                    1.0,
                    0.0
                ],
                "answer_relation_to_question_bing": [
                    0.6533333333333333,
                    1.4833333333333332,
                    0.8633333333333333
                ],
                "question_related_to_answer": [
                    0.8979591836734694,
                    0.08163265306122448,
                    0.02040816326530612
                ],
                "question_related_to_answer_bing": [
                    0.8,
                    0.0,
                    0.2
                ],
                "result_count_noun_chunks": [
                    60.0,
                    4640.0,
                    13900.0
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    244000.0,
                    244000.0,
                    244000.0
                ],
                "word_count_raw": [
                    1.0,
                    4.0,
                    1.0
                ],
                "result_count": [
                    161000.0,
                    161000.0,
                    161000.0
                ],
                "answer_relation_to_question": [
                    1.0605832076420312,
                    1.0910005027652088,
                    0.8484162895927602
                ],
                "word_count_appended": [
                    1.0,
                    4.0,
                    1.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "How do you let someone on Tinder know you're interested?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "swipe right"
            ],
            "lines": [
                [
                    0.4755892255892256,
                    0.7901234567901234,
                    1.0,
                    0,
                    0.9994909999537273,
                    0.28164196123147095,
                    0.06425970843002822,
                    0.3700349585658224,
                    0.9128571428571428,
                    0.8317757009345794,
                    0.7333333333333333,
                    0.7163918282207955,
                    1.0,
                    1.0,
                    5.0
                ],
                [
                    0.20244107744107742,
                    0.1111111111111111,
                    0.0,
                    0,
                    0.0,
                    0.5222348916761688,
                    0.9357115438056741,
                    0.6297086136997329,
                    0.06469387755102039,
                    0.028037383177570093,
                    0.13333333333333333,
                    0.07056726288024595,
                    0.0,
                    0.0,
                    5.0
                ],
                [
                    0.32196969696969696,
                    0.09876543209876543,
                    0.0,
                    0,
                    0.0005090000462727314,
                    0.1961231470923603,
                    2.8747764297644204e-05,
                    0.00025642773444473657,
                    0.02244897959183673,
                    0.14018691588785046,
                    0.13333333333333333,
                    0.21304090889895838,
                    0.0,
                    0.0,
                    5.0
                ]
            ],
            "fraction_answers": {
                "draw circle around face": 0.2075260842058411,
                "shake phone": 0.08666635303213975,
                "swipe right": 0.7058075627620191
            },
            "question": "how do you let someone on tinder know you're interested?",
            "rate_limited": false,
            "answers": [
                "swipe right",
                "draw circle around face",
                "shake phone"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "draw circle around face": 0.07049144675710893,
                "shake phone": 0.04353584445305037,
                "swipe right": 0.8868415478403734
            },
            "integer_answers": {
                "draw circle around face": 3,
                "shake phone": 0,
                "swipe right": 10
            },
            "categorical_data": {
                "question_type": 5
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.5819591411039777,
                    0.35283631440122976,
                    1.065204544494792
                ],
                "result_count_important_words": [
                    114000.0,
                    1660000.0,
                    51.0
                ],
                "wikipedia_search": [
                    4.564285714285715,
                    0.32346938775510203,
                    0.11224489795918367
                ],
                "word_count_appended_bing": [
                    11.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    2.3703703703703702,
                    0.3333333333333333,
                    0.2962962962962963
                ],
                "question_related_to_answer": [
                    1.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    2280000.0,
                    3880000.0,
                    1580.0
                ],
                "word_count_noun_chunks": [
                    16.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    4940000.0,
                    9160000.0,
                    3440000.0
                ],
                "word_count_raw": [
                    8.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    108000.0,
                    0,
                    55.0
                ],
                "answer_relation_to_question": [
                    1.9023569023569025,
                    0.8097643097643097,
                    1.2878787878787878
                ],
                "word_count_appended": [
                    89.0,
                    3.0,
                    15.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Blue spruce, red cedar & white pine are all kinds of what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "evergreen trees"
            ],
            "lines": [
                [
                    0.2919999289356334,
                    0.1496931985135252,
                    0,
                    0.0,
                    0.3333333333333333,
                    0.33519553072625696,
                    0.001050498987018834,
                    0.04468802698145025,
                    0.26850152905198776,
                    0.0,
                    0,
                    0.3333333333333333,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.06091215555989838,
                    0.13509636159363927,
                    0,
                    0.0,
                    0.3333333333333333,
                    0.3372905027932961,
                    0.0009754633450889173,
                    0.13743676222596965,
                    0.08402504732779963,
                    0.0,
                    0,
                    0.3333333333333333,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.6470879155044681,
                    0.7152104398928356,
                    0,
                    1.0,
                    0.3333333333333333,
                    0.32751396648044695,
                    0.9979740376678923,
                    0.8178752107925801,
                    0.6474734236202127,
                    1.0,
                    0,
                    0.3333333333333333,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "marvel supervillains": 0.11853357995936321,
                "evergreen trees": 0.7349834717187586,
                "colgate flavors": 0.14648294832187828
            },
            "question": "blue spruce, red cedar & white pine are all kinds of what?",
            "rate_limited": false,
            "answers": [
                "colgate flavors",
                "marvel supervillains",
                "evergreen trees"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "marvel supervillains": 0.2298776757254656,
                "evergreen trees": 0.6733006475195951,
                "colgate flavors": 0.23244943769689602
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.333333333333333,
                    2.333333333333333,
                    2.333333333333333
                ],
                "result_count_important_words": [
                    56.0,
                    52.0,
                    53200.0
                ],
                "wikipedia_search": [
                    1.8795107033639145,
                    0.5881753312945974,
                    4.532313965341489
                ],
                "word_count_appended_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "answer_relation_to_question_bing": [
                    1.0478523895946763,
                    0.9456745311554748,
                    5.006473079249849
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_noun_chunks": [
                    5300.0,
                    16300.0,
                    97000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    10.0
                ],
                "result_count_bing": [
                    480000.0,
                    483000.0,
                    469000.0
                ],
                "word_count_appended": [
                    0.0,
                    0.0,
                    10.0
                ],
                "answer_relation_to_question": [
                    2.043999502549434,
                    0.42638508891928867,
                    4.529615408531277
                ],
                "result_count": [
                    846000.0,
                    846000.0,
                    846000.0
                ]
            },
            "integer_answers": {
                "marvel supervillains": 1,
                "evergreen trees": 9,
                "colgate flavors": 2
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these actors was a high school cheerleader?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "george clooney"
            ],
            "lines": [
                [
                    0.5777777777777778,
                    0.1904761904761905,
                    0,
                    0,
                    0.2057001239157373,
                    0.3282208588957055,
                    0.2654370489174018,
                    0.21765417170495768,
                    0.36481481481481487,
                    0.4028776978417266,
                    0.43478260869565216,
                    0.31677398359927994,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.17777777777777778,
                    0.2916666666666667,
                    0,
                    0,
                    0.6530359355638166,
                    0.33588957055214724,
                    0.6206896551724138,
                    0.6360338573155986,
                    0.10555555555555556,
                    0.28776978417266186,
                    0.057971014492753624,
                    0.32751846382604144,
                    1.0,
                    0,
                    -1.0
                ],
                [
                    0.24444444444444446,
                    0.5178571428571429,
                    0,
                    0,
                    0.1412639405204461,
                    0.33588957055214724,
                    0.11387329591018444,
                    0.14631197097944376,
                    0.5296296296296297,
                    0.30935251798561153,
                    0.5072463768115942,
                    0.3557075525746786,
                    0.0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "john travolta": 0.2910524038423021,
                "george clooney": 0.30041047969447676,
                "michael douglas": 0.40853711646322116
            },
            "question": "which of these actors was a high school cheerleader?",
            "rate_limited": false,
            "answers": [
                "george clooney",
                "michael douglas",
                "john travolta"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "john travolta": 0.4594682636477864,
                "george clooney": 0.48163695551526803,
                "michael douglas": 0.29816552259012646
            },
            "integer_answers": {
                "john travolta": 4,
                "george clooney": 2,
                "michael douglas": 5
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2670959343971198,
                    1.3100738553041658,
                    1.4228302102987145
                ],
                "result_count_important_words": [
                    331000.0,
                    774000.0,
                    142000.0
                ],
                "wikipedia_search": [
                    1.0944444444444446,
                    0.31666666666666665,
                    1.588888888888889
                ],
                "word_count_appended_bing": [
                    30.0,
                    4.0,
                    35.0
                ],
                "answer_relation_to_question_bing": [
                    0.38095238095238093,
                    0.5833333333333333,
                    1.0357142857142856
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    180000.0,
                    526000.0,
                    121000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    21400000.0,
                    21900000.0,
                    21900000.0
                ],
                "result_count": [
                    166000.0,
                    527000.0,
                    114000.0
                ],
                "answer_relation_to_question": [
                    1.7333333333333334,
                    0.5333333333333333,
                    0.7333333333333334
                ],
                "word_count_appended": [
                    56.0,
                    40.0,
                    43.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these restaurant brands has its original location in Europe?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "benihana"
            ],
            "lines": [
                [
                    0.2151462566407556,
                    0.38637790332705585,
                    0,
                    0,
                    0.22623723487824038,
                    0.33117932148626816,
                    0.29449754585378457,
                    0.2300201477501679,
                    0.6653508771929825,
                    0.7995735607675906,
                    0.8285714285714286,
                    0.36745199864631173,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.5331148788484379,
                    0.30688951663527936,
                    0,
                    0,
                    0.6425765907305577,
                    0.3344103392568659,
                    0.545078791010075,
                    0.6363331094694425,
                    0.2655701754385965,
                    0.11940298507462686,
                    0.11428571428571428,
                    0.31946800680659315,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.25173886451080657,
                    0.3067325800376648,
                    0,
                    0,
                    0.1311861743912019,
                    0.3344103392568659,
                    0.16042366313614054,
                    0.13364674278038952,
                    0.06907894736842105,
                    0.08102345415778252,
                    0.05714285714285714,
                    0.3130799945470952,
                    0.5,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "mr. chow": 0.23653863477743545,
                "benihana": 0.44536718959288213,
                "p.f. chang's": 0.31809417562968245
            },
            "question": "which of these restaurant brands has its original location in europe?",
            "rate_limited": false,
            "answers": [
                "benihana",
                "p.f. chang's",
                "mr. chow"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "mr. chow": 0.22882626481656582,
                "benihana": 0.8767936323182485,
                "p.f. chang's": 0.20494109337265065
            },
            "integer_answers": {
                "mr. chow": 0,
                "benihana": 7,
                "p.f. chang's": 5
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.8372599932315585,
                    1.5973400340329658,
                    1.565399972735476
                ],
                "result_count_important_words": [
                    114000.0,
                    211000.0,
                    62100.0
                ],
                "wikipedia_search": [
                    2.66140350877193,
                    1.062280701754386,
                    0.2763157894736842
                ],
                "word_count_appended_bing": [
                    29.0,
                    4.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.1591337099811674,
                    0.920668549905838,
                    0.9201977401129943
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    137000.0,
                    379000.0,
                    79600.0
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    410000.0,
                    414000.0,
                    414000.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    1.0
                ],
                "result_count": [
                    144000.0,
                    409000.0,
                    83500.0
                ],
                "answer_relation_to_question": [
                    0.8605850265630224,
                    2.1324595153937516,
                    1.0069554580432263
                ],
                "word_count_appended": [
                    375.0,
                    56.0,
                    38.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "In which of these movies is the title NOT spoken by any character?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "gravity"
            ],
            "lines": [
                [
                    0.16572547822547823,
                    0.15520334928229662,
                    0.0,
                    0,
                    0.39448275862068966,
                    0.28636263006420193,
                    0.3594704684317719,
                    0.2625615763546798,
                    0.10241739211831885,
                    0.317115551694179,
                    0.2235576923076923,
                    0.3286820008511565,
                    0.5,
                    0.25,
                    -1.0
                ],
                [
                    0.4197954822954823,
                    0.4163875598086124,
                    0.5,
                    0,
                    0.36758620689655175,
                    0.4261678104936905,
                    0.3238289205702648,
                    0.36403940886699504,
                    0.42334784236637646,
                    0.3201563857515204,
                    0.34615384615384615,
                    0.33758856556537437,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.41447903947903947,
                    0.4284090909090909,
                    0.5,
                    0,
                    0.2379310344827586,
                    0.2874695594421076,
                    0.3167006109979633,
                    0.37339901477832516,
                    0.4742347655153047,
                    0.36272806255430057,
                    0.43028846153846156,
                    0.3337294335834692,
                    0.0,
                    0.25,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "inception": 0.48529555416146686,
                "speed": 0.32163552718756605,
                "gravity": 0.19306891865096706
            },
            "question": "in which of these movies is the title not spoken by any character?",
            "rate_limited": false,
            "answers": [
                "inception",
                "gravity",
                "speed"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "inception": 0.31499999945362406,
                "speed": 0.3194444440305233,
                "gravity": 0.354545455177625
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3705439931907482,
                    1.299291475477005,
                    1.3301645313322468
                ],
                "result_count_important_words": [
                    1380000.0,
                    1730000.0,
                    1800000.0
                ],
                "wikipedia_search": [
                    2.385495647290087,
                    0.4599129458017411,
                    0.15459140690817186
                ],
                "answer_relation_to_question": [
                    2.0056471306471306,
                    0.48122710622710624,
                    0.5131257631257631
                ],
                "result_count": [
                    4590000.0,
                    5760000.0,
                    11400000.0
                ],
                "answer_relation_to_question_bing": [
                    1.3791866028708133,
                    0.3344497607655502,
                    0.2863636363636364
                ],
                "question_related_to_answer": [
                    1.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    4820000.0,
                    2760000.0,
                    2570000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    19300000.0,
                    6670000.0,
                    19200000.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    1.0
                ],
                "word_count_appended": [
                    421.0,
                    414.0,
                    316.0
                ],
                "word_count_appended_bing": [
                    115.0,
                    64.0,
                    29.0
                ]
            },
            "integer_answers": {
                "inception": 10,
                "speed": 3,
                "gravity": 0
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which player won Rookie of the Year in their sport most recently?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "mike trout"
            ],
            "lines": [
                [
                    0.4318586063942705,
                    0.3816891284815813,
                    0,
                    0,
                    0.271,
                    0.3675840978593272,
                    0.4643748958361526,
                    0.4618226600985222,
                    0.39216705261113155,
                    0.3157894736842105,
                    0.5272727272727272,
                    0.3588050936774277,
                    0.5,
                    1.0,
                    -1.0
                ],
                [
                    0.21064875693694504,
                    0.1461558208188936,
                    0,
                    0,
                    0.109,
                    0.290519877675841,
                    2.706479454259785e-05,
                    0.07943349753694581,
                    0.16188290419375945,
                    0.3462603878116344,
                    0.10909090909090909,
                    0.2990817894614563,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.3574926366687845,
                    0.4721550506995252,
                    0,
                    0,
                    0.62,
                    0.3418960244648318,
                    0.5355980393693048,
                    0.45874384236453203,
                    0.445950043195109,
                    0.3379501385041551,
                    0.36363636363636365,
                    0.34211311686111595,
                    0.5,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "mike trout": 0.4560303113262792,
                "von miller": 0.14600841736007728,
                "blake griffin": 0.39796127131364356
            },
            "question": "which player won rookie of the year in their sport most recently?",
            "rate_limited": false,
            "answers": [
                "mike trout",
                "von miller",
                "blake griffin"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "mike trout": 0.7550574348905095,
                "von miller": 0.1343227443118945,
                "blake griffin": 0.37463447163473873
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.1528305620645662,
                    1.794490736768738,
                    2.0526787011666956
                ],
                "result_count_important_words": [
                    1630000.0,
                    95.0,
                    1880000.0
                ],
                "wikipedia_search": [
                    1.9608352630556578,
                    0.8094145209687973,
                    2.229750215975545
                ],
                "word_count_appended_bing": [
                    29.0,
                    6.0,
                    20.0
                ],
                "answer_relation_to_question_bing": [
                    1.9084456424079066,
                    0.730779104094468,
                    2.360775253497626
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    1500000.0,
                    258000.0,
                    1490000.0
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    2.0
                ],
                "result_count_bing": [
                    601000.0,
                    475000.0,
                    559000.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    114.0,
                    125.0,
                    122.0
                ],
                "answer_relation_to_question": [
                    2.591151638365623,
                    1.2638925416216702,
                    2.144955820012707
                ],
                "result_count": [
                    542000.0,
                    218000.0,
                    1240000.0
                ]
            },
            "integer_answers": {
                "mike trout": 7,
                "von miller": 1,
                "blake griffin": 4
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "In computing, what unit is half a byte?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "nibble",
                "demibyte",
                "octet"
            ],
            "lines": [
                [
                    0.3245225448872865,
                    0.26171875,
                    0.9407894736842105,
                    0.8297872340425532,
                    0.4796380090497738,
                    0.5757334476633796,
                    0.4774774774774775,
                    0.4774774774774775,
                    1.0,
                    0.6670454545454545,
                    0.4690721649484536,
                    0.495694451598782,
                    0.8514285714285714,
                    0.8587570621468926,
                    1.0
                ],
                [
                    0.24759207357991553,
                    0.109375,
                    0.0,
                    0.0,
                    0.0,
                    0.4134256140135971,
                    0.0,
                    0.0,
                    0.0,
                    0.01818181818181818,
                    0.15979381443298968,
                    0.029819844036339502,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.4278853815327979,
                    0.62890625,
                    0.05921052631578947,
                    0.1702127659574468,
                    0.5203619909502263,
                    0.010840938323023214,
                    0.5225225225225225,
                    0.5225225225225225,
                    0.0,
                    0.31477272727272726,
                    0.3711340206185567,
                    0.4744857043648785,
                    0.14857142857142858,
                    0.14124293785310735,
                    1.0
                ]
            ],
            "fraction_answers": {
                "demibyte": 0.06987058316033286,
                "nibble": 0.6220815799250223,
                "octet": 0.30804783691464477
            },
            "question": "in computing, what unit is half a byte?",
            "rate_limited": false,
            "answers": [
                "nibble",
                "demibyte",
                "octet"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "demibyte": 0.0,
                "nibble": 0.0,
                "octet": 0.0
            },
            "integer_answers": {
                "demibyte": 0,
                "nibble": 9,
                "octet": 5
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.982777806395128,
                    0.11927937614535801,
                    1.897942817459514
                ],
                "result_count_important_words": [
                    1060000.0,
                    0,
                    1160000.0
                ],
                "wikipedia_search": [
                    4.0,
                    0.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    91.0,
                    31.0,
                    72.0
                ],
                "answer_relation_to_question_bing": [
                    1.046875,
                    0.4375,
                    2.515625
                ],
                "question_related_to_answer": [
                    0.9407894736842105,
                    0.0,
                    0.05921052631578947
                ],
                "question_related_to_answer_bing": [
                    0.8297872340425532,
                    0.0,
                    0.1702127659574468
                ],
                "result_count_noun_chunks": [
                    1060000.0,
                    0,
                    1160000.0
                ],
                "word_count_noun_chunks": [
                    149.0,
                    0.0,
                    26.0
                ],
                "result_count_bing": [
                    1880000.0,
                    1350000.0,
                    35400.0
                ],
                "word_count_raw": [
                    152.0,
                    0.0,
                    25.0
                ],
                "result_count": [
                    1060000.0,
                    0,
                    1150000.0
                ],
                "answer_relation_to_question": [
                    1.298090179549146,
                    0.9903682943196621,
                    1.7115415261311917
                ],
                "word_count_appended": [
                    587.0,
                    16.0,
                    277.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these has NEVER been named Pantone\u2019s Color of the Year?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "cucumber"
            ],
            "lines": [
                [
                    0.41728395061728396,
                    0.3470824949698189,
                    0,
                    0,
                    0.4192853825376856,
                    0.2795071335927367,
                    0.23267326732673266,
                    0.24606884827879305,
                    0.3277777777777778,
                    0.1934931506849315,
                    0.16666666666666669,
                    0.37801438610528615,
                    0,
                    0.33333333333333337,
                    -1.0
                ],
                [
                    0.3080246913580247,
                    0.32595573440643866,
                    0,
                    0,
                    0.08289932316960441,
                    0.4409857328145266,
                    0.3795379537953795,
                    0.43348916277093075,
                    0.40416666666666673,
                    0.4006849315068493,
                    0.39743589743589747,
                    0.3149437432917652,
                    0,
                    0.33333333333333337,
                    -1.0
                ],
                [
                    0.27469135802469136,
                    0.32696177062374243,
                    0,
                    0,
                    0.49781529429271,
                    0.2795071335927367,
                    0.38778877887788776,
                    0.3204419889502762,
                    0.26805555555555555,
                    0.4058219178082192,
                    0.4358974358974359,
                    0.30704187060294863,
                    0,
                    0.33333333333333337,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "chili pepper": 0.3022988295346296,
                "cucumber": 0.3925115651107189,
                "sand dollar": 0.3051896053546515
            },
            "question": "which of these has never been named pantone\u2019s color of the year?",
            "rate_limited": false,
            "answers": [
                "cucumber",
                "sand dollar",
                "chili pepper"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "chili pepper": 0.08820887305475689,
                "cucumber": 0.6542038913821703,
                "sand dollar": 0.03663215141521861
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.9758849111577107,
                    1.4804500536658782,
                    1.543665035176411
                ],
                "result_count_important_words": [
                    324000.0,
                    146000.0,
                    136000.0
                ],
                "wikipedia_search": [
                    1.0333333333333334,
                    0.575,
                    1.3916666666666668
                ],
                "word_count_appended_bing": [
                    26.0,
                    8.0,
                    5.0
                ],
                "answer_relation_to_question_bing": [
                    0.3058350100603622,
                    0.34808853118712274,
                    0.3460764587525151
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    23900000.0,
                    6260000.0,
                    16900000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    1.0
                ],
                "result_count_bing": [
                    17000000.0,
                    4550000.0,
                    17000000.0
                ],
                "word_count_appended": [
                    179.0,
                    58.0,
                    55.0
                ],
                "answer_relation_to_question": [
                    0.4962962962962963,
                    1.151851851851852,
                    1.3518518518518519
                ],
                "result_count": [
                    3580000.0,
                    18500000.0,
                    96900.0
                ]
            },
            "integer_answers": {
                "chili pepper": 3,
                "cucumber": 6,
                "sand dollar": 2
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these best-selling authors uses his/her given last name?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "j.d. robb",
                "r.l. stine",
                "e.l. james"
            ],
            "lines": [
                [
                    0.3396116138763198,
                    0.521885521885522,
                    0,
                    0,
                    0.2156710628394104,
                    0.31985940246045697,
                    0.08914450035945364,
                    0.2043445314473352,
                    0.265625,
                    0.2540983606557377,
                    0.2702702702702703,
                    0.3410575098287906,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.32867108381814264,
                    0.28998316498316495,
                    0,
                    0,
                    0.15205585725368503,
                    0.34797891036906853,
                    0.098490294751977,
                    0.20207123010861328,
                    0.5587121212121212,
                    0.4262295081967213,
                    0.5675675675675675,
                    0.3174604800309107,
                    1.0,
                    0,
                    -1.0
                ],
                [
                    0.3317173023055376,
                    0.18813131313131312,
                    0,
                    0,
                    0.6322730799069046,
                    0.3321616871704745,
                    0.8123652048885693,
                    0.5935842384440515,
                    0.17566287878787878,
                    0.319672131147541,
                    0.16216216216216217,
                    0.3414820101402986,
                    0.0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "r.l. stine": 0.38992911075381564,
                "e.l. james": 0.35356472800770283,
                "j.d. robb": 0.2565061612384815
            },
            "question": "which of these best-selling authors uses his/her given last name?",
            "rate_limited": false,
            "answers": [
                "j.d. robb",
                "r.l. stine",
                "e.l. james"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "r.l. stine": 0.0,
                "e.l. james": 0.0,
                "j.d. robb": 0.0
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.705287549143953,
                    1.5873024001545535,
                    1.707410050701493
                ],
                "result_count_important_words": [
                    124000.0,
                    137000.0,
                    1130000.0
                ],
                "wikipedia_search": [
                    1.0625,
                    2.234848484848485,
                    0.7026515151515151
                ],
                "word_count_appended_bing": [
                    10.0,
                    21.0,
                    6.0
                ],
                "answer_relation_to_question_bing": [
                    1.5656565656565657,
                    0.8699494949494949,
                    0.5643939393939393
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    80900.0,
                    80000.0,
                    235000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    3.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    1820000.0,
                    1980000.0,
                    1890000.0
                ],
                "word_count_appended": [
                    31.0,
                    52.0,
                    39.0
                ],
                "answer_relation_to_question": [
                    1.6980580693815988,
                    1.6433554190907131,
                    1.6585865115276879
                ],
                "result_count": [
                    27800.0,
                    19600.0,
                    81500.0
                ]
            },
            "integer_answers": {
                "r.l. stine": 5,
                "e.l. james": 4,
                "j.d. robb": 2
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these things was created by a person who chose to remain anonymous?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "bitcoin"
            ],
            "lines": [
                [
                    0.5041208791208792,
                    0.6583333333333333,
                    1.0,
                    0,
                    0.995109867816951,
                    0.4063312071816679,
                    0.9699279966116052,
                    0.9711401294853506,
                    0.8973684210526316,
                    0.7429643527204502,
                    0.6764705882352942,
                    0.3964520387589661,
                    0,
                    1.0,
                    0.0
                ],
                [
                    0.2087912087912088,
                    0.2333333333333333,
                    0.0,
                    0,
                    0.0025625146810736936,
                    0.5102763997165131,
                    0.010059296908089793,
                    0.006638867551849007,
                    0.08947368421052632,
                    0.19136960600375236,
                    0.22058823529411764,
                    0.3289200353751181,
                    0,
                    0.0,
                    0.0
                ],
                [
                    0.2870879120879121,
                    0.10833333333333334,
                    0.0,
                    0,
                    0.002327617501975272,
                    0.08339239310181903,
                    0.020012706480304957,
                    0.022221002962800395,
                    0.013157894736842105,
                    0.06566604127579738,
                    0.10294117647058823,
                    0.2746279258659158,
                    0,
                    0.0,
                    0.0
                ]
            ],
            "fraction_answers": {
                "hoverboards": 0.15016776515546518,
                "fidget spinners": 0.08164733365144071,
                "bitcoin": 0.7681849011930941
            },
            "question": "which of these things was created by a person who chose to remain anonymous?",
            "rate_limited": false,
            "answers": [
                "bitcoin",
                "hoverboards",
                "fidget spinners"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "hoverboards": 0.29999999900658925,
                "fidget spinners": 0.3194444440305233,
                "bitcoin": 0.354545455177625
            },
            "integer_answers": {
                "hoverboards": 1,
                "fidget spinners": 0,
                "bitcoin": 11
            },
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.378712232553797,
                    1.9735202122507087,
                    1.6477675551954947
                ],
                "result_count_important_words": [
                    4580000.0,
                    47500.0,
                    94500.0
                ],
                "wikipedia_search": [
                    3.5894736842105264,
                    0.35789473684210527,
                    0.05263157894736842
                ],
                "word_count_appended_bing": [
                    92.0,
                    30.0,
                    14.0
                ],
                "answer_relation_to_question_bing": [
                    1.975,
                    0.7,
                    0.325
                ],
                "question_related_to_answer": [
                    1.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    17700000.0,
                    121000.0,
                    405000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    1720000.0,
                    2160000.0,
                    353000.0
                ],
                "word_count_raw": [
                    20.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    4660000.0,
                    12000.0,
                    10900.0
                ],
                "answer_relation_to_question": [
                    2.0164835164835164,
                    0.8351648351648351,
                    1.1483516483516483
                ],
                "word_count_appended": [
                    792.0,
                    204.0,
                    70.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "In which town were a President, Governor, Senator, NFL owner and late night host all born?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "brookline, ma",
                "hope, ar",
                "muncie, in"
            ],
            "lines": [
                [
                    0.18830449386004944,
                    0.3361904761904762,
                    0,
                    0,
                    0.3919933277731443,
                    0.322690992018244,
                    0.6211075576502272,
                    0.23437995559784333,
                    0.38285929032272115,
                    0.4,
                    0.3333333333333333,
                    0.4204177943117866,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.3564208425319536,
                    0.2704761904761904,
                    0,
                    0,
                    0.021406727828746176,
                    0.338654503990878,
                    0.011950850025248275,
                    0.023469711385981605,
                    0.17637664562531583,
                    0.2,
                    0.3333333333333333,
                    0.25278965631523465,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.4552746636079969,
                    0.3933333333333333,
                    0,
                    0,
                    0.5865999443981096,
                    0.338654503990878,
                    0.3669415923245245,
                    0.7421503330161751,
                    0.44076406405196294,
                    0.4,
                    0.3333333333333333,
                    0.32679254937297875,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "muncie, in": 0.43838443174292924,
                "hope, ar": 0.1984878461512882,
                "brookline, ma": 0.36312772210578254
            },
            "question": "in which town were a president, governor, senator, nfl owner and late night host all born?",
            "rate_limited": false,
            "answers": [
                "brookline, ma",
                "hope, ar",
                "muncie, in"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "muncie, in": 0.0,
                "hope, ar": 0.0,
                "brookline, ma": 0.0
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    4.204177943117866,
                    2.5278965631523462,
                    3.2679254937297877
                ],
                "result_count_important_words": [
                    3690.0,
                    71.0,
                    2180.0
                ],
                "wikipedia_search": [
                    3.062874322581769,
                    1.4110131650025266,
                    3.5261125124157036
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.680952380952381,
                    1.3523809523809522,
                    1.9666666666666666
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    739.0,
                    74.0,
                    2340.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    283000.0,
                    297000.0,
                    297000.0
                ],
                "word_count_appended": [
                    6.0,
                    3.0,
                    6.0
                ],
                "answer_relation_to_question": [
                    1.6947404447404448,
                    3.2077875827875824,
                    4.097471972471972
                ],
                "result_count": [
                    1410.0,
                    77.0,
                    2110.0
                ]
            },
            "integer_answers": {
                "muncie, in": 5,
                "hope, ar": 1,
                "brookline, ma": 4
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Though it now conveys something different, which of these words originally meant \u201cparrot\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "popinjay"
            ],
            "lines": [
                [
                    0.1796875,
                    0.08333333333333333,
                    0,
                    0,
                    0.3370288248337029,
                    0.4810379241516966,
                    0.485006518904824,
                    0.0635822136029921,
                    0.0,
                    0.3290155440414508,
                    0.32926829268292684,
                    0.3490094907861929,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.2578125,
                    0.0,
                    0,
                    0,
                    0.2328159645232816,
                    0.2554890219560878,
                    0.32333767926988266,
                    0.8366809807452555,
                    0.041666666666666664,
                    0.36787564766839376,
                    0.32926829268292684,
                    0.357570714978834,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.5625,
                    0.9166666666666667,
                    0,
                    0,
                    0.43015521064301554,
                    0.2634730538922156,
                    0.19165580182529335,
                    0.09973680565175232,
                    0.9583333333333334,
                    0.30310880829015546,
                    0.34146341463414637,
                    0.293419794234973,
                    0,
                    1.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "thespian": 0.2729561334992117,
                "popinjay": 0.4873193535610501,
                "warble": 0.2397245129397381
            },
            "question": "though it now conveys something different, which of these words originally meant \u201cparrot\u201d?",
            "rate_limited": false,
            "answers": [
                "warble",
                "thespian",
                "popinjay"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "thespian": 0.3194444440305233,
                "popinjay": 0.354545455177625,
                "warble": 0.31499999945362406
            },
            "integer_answers": {
                "thespian": 3,
                "popinjay": 6,
                "warble": 2
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.0940569447171575,
                    2.145424289873004,
                    1.760518765409838
                ],
                "result_count_important_words": [
                    18600.0,
                    12400.0,
                    7350.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.125,
                    2.875
                ],
                "answer_relation_to_question": [
                    0.71875,
                    1.03125,
                    2.25
                ],
                "result_count": [
                    15200.0,
                    10500.0,
                    19400.0
                ],
                "answer_relation_to_question_bing": [
                    0.16666666666666666,
                    0.0,
                    1.8333333333333335
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    459000.0,
                    6040000.0,
                    720000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    24100.0,
                    12800.0,
                    13200.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_appended": [
                    127.0,
                    142.0,
                    117.0
                ],
                "word_count_appended_bing": [
                    27.0,
                    27.0,
                    28.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What gargantuan fruit is the subject of a Roald Dahl children's book?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "dragonfruit",
                "loquat",
                "peach"
            ],
            "lines": [
                [
                    0.2150543144086903,
                    0.4084856691491763,
                    0.0,
                    0.0,
                    0.003463203463203463,
                    0.08145454545454546,
                    0.0010757507844016136,
                    0.005185825410544511,
                    0.18108318034175963,
                    0.24603174603174602,
                    0.1724137931034483,
                    0.1706405034364316,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.16286209605793536,
                    0.2177273753103137,
                    0.0,
                    0.0,
                    0.005194805194805195,
                    0.11127272727272727,
                    0.0038547736441057823,
                    0.005185825410544511,
                    0.33319799463352634,
                    0.10714285714285714,
                    0.15517241379310345,
                    0.11841839577779258,
                    0.1,
                    0.25,
                    1.0
                ],
                [
                    0.6220835895333743,
                    0.37378695554051,
                    1.0,
                    1.0,
                    0.9913419913419913,
                    0.8072727272727273,
                    0.9950694755714926,
                    0.989628349178911,
                    0.48571882502471403,
                    0.6468253968253969,
                    0.6724137931034483,
                    0.7109411007857759,
                    0.9,
                    0.75,
                    1.0
                ]
            ],
            "fraction_answers": {
                "loquat": 0.11214494744555081,
                "dragonfruit": 0.10606346654171049,
                "peach": 0.7817915860127387
            },
            "question": "what gargantuan fruit is the subject of a roald dahl children's book?",
            "rate_limited": false,
            "answers": [
                "dragonfruit",
                "loquat",
                "peach"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "loquat": 0.0,
                "dragonfruit": 0.0,
                "peach": 0.0
            },
            "integer_answers": {
                "loquat": 0,
                "dragonfruit": 1,
                "peach": 13
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1944835240550213,
                    0.828928770444548,
                    4.976587705500431
                ],
                "result_count_important_words": [
                    12.0,
                    43.0,
                    11100.0
                ],
                "wikipedia_search": [
                    0.7243327213670385,
                    1.3327919785341054,
                    1.9428753000988561
                ],
                "word_count_appended_bing": [
                    30.0,
                    27.0,
                    117.0
                ],
                "answer_relation_to_question_bing": [
                    1.6339426765967051,
                    0.8709095012412548,
                    1.49514782216204
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    1.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_noun_chunks": [
                    12.0,
                    12.0,
                    2290.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    9.0
                ],
                "result_count_bing": [
                    11200.0,
                    15300.0,
                    111000.0
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    3.0
                ],
                "result_count": [
                    8.0,
                    12.0,
                    2290.0
                ],
                "answer_relation_to_question": [
                    0.8602172576347612,
                    0.6514483842317415,
                    2.488334358133497
                ],
                "word_count_appended": [
                    62.0,
                    27.0,
                    163.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "The only person who owns more U.S. land than Ted Turner made his fortune in what business?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "pharmaceuticals",
                "cable tv",
                "fast food"
            ],
            "lines": [
                [
                    0.1767993940974162,
                    0.38766666666666666,
                    0.0,
                    0,
                    0.08976848394324123,
                    0.3872549019607843,
                    0.5174193548387097,
                    0.11839291170180263,
                    0.33905134292231065,
                    0.8723404255319149,
                    0.8387096774193549,
                    0.32785724257883914,
                    0,
                    0,
                    1.0
                ],
                [
                    0.6261767298642927,
                    0.1750952380952381,
                    1.0,
                    0,
                    0.06631814787154593,
                    0.2181372549019608,
                    0.33419354838709675,
                    0.8386801099908341,
                    0.3952115626309175,
                    0.0797872340425532,
                    0.0967741935483871,
                    0.36716128268392184,
                    0,
                    0,
                    1.0
                ],
                [
                    0.19702387603829105,
                    0.43723809523809526,
                    0.0,
                    0,
                    0.8439133681852129,
                    0.3946078431372549,
                    0.14838709677419354,
                    0.04292697830736328,
                    0.26573709444677185,
                    0.047872340425531915,
                    0.06451612903225806,
                    0.304981474737239,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "pharmaceuticals": 0.3686600365146401,
                "cable tv": 0.3815941183651589,
                "fast food": 0.2497458451202011
            },
            "question": "the only person who owns more u.s. land than ted turner made his fortune in what business?",
            "rate_limited": false,
            "answers": [
                "pharmaceuticals",
                "cable tv",
                "fast food"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "pharmaceuticals": 0.0,
                "cable tv": 0.0,
                "fast food": 0.0
            },
            "integer_answers": {
                "pharmaceuticals": 3,
                "cable tv": 5,
                "fast food": 3
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.295000698051874,
                    2.570128978787453,
                    2.134870323160673
                ],
                "result_count_important_words": [
                    4010000.0,
                    2590000.0,
                    1150000.0
                ],
                "wikipedia_search": [
                    1.6952567146115534,
                    1.9760578131545874,
                    1.3286854722338592
                ],
                "word_count_appended_bing": [
                    26.0,
                    3.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.9383333333333332,
                    0.8754761904761905,
                    2.1861904761904762
                ],
                "question_related_to_answer": [
                    0.0,
                    1.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    775000.0,
                    5490000.0,
                    281000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    316000.0,
                    178000.0,
                    322000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    601000.0,
                    444000.0,
                    5650000.0
                ],
                "answer_relation_to_question": [
                    1.0607963645844973,
                    3.757060379185756,
                    1.1821432562297463
                ],
                "word_count_appended": [
                    164.0,
                    15.0,
                    9.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which game is an example of combinatorics?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "risk",
                "crossword puzzles",
                "sudoku"
            ],
            "lines": [
                [
                    0.6054347826086957,
                    0.5730337078651686,
                    1.0,
                    0,
                    0.05317228932389282,
                    0.018563501437389968,
                    0.20273480808827413,
                    0.21698576337751596,
                    0.14516129032258066,
                    0.30885311871227367,
                    0.37142857142857144,
                    0.3732635218338543,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.1532608695652174,
                    0.2921348314606742,
                    0.0,
                    0,
                    0.42898792169929195,
                    0.9560314665834689,
                    0.7655350826249934,
                    0.7682866961217477,
                    0.3544142614601019,
                    0.06338028169014084,
                    0.09285714285714286,
                    0.2942124269119154,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.24130434782608695,
                    0.13483146067415733,
                    0.0,
                    0,
                    0.5178397889768153,
                    0.02540503197914113,
                    0.031730109286732484,
                    0.014727540500736377,
                    0.5004244482173175,
                    0.6277665995975855,
                    0.5357142857142857,
                    0.3325240512542303,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "sudoku": 0.26929706036609896,
                "risk": 0.3516937595452925,
                "crossword puzzles": 0.3790091800886086
            },
            "question": "which game is an example of combinatorics?",
            "rate_limited": false,
            "answers": [
                "risk",
                "crossword puzzles",
                "sudoku"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sudoku": 0.0,
                "risk": 0.0,
                "crossword puzzles": 0.0
            },
            "integer_answers": {
                "sudoku": 4,
                "risk": 4,
                "crossword puzzles": 3
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.119790565501563,
                    0.8826372807357462,
                    0.9975721537626908
                ],
                "result_count_important_words": [
                    384000.0,
                    1450000.0,
                    60100.0
                ],
                "wikipedia_search": [
                    0.2903225806451613,
                    0.7088285229202038,
                    1.000848896434635
                ],
                "answer_relation_to_question": [
                    1.2108695652173913,
                    0.3065217391304348,
                    0.4826086956521739
                ],
                "word_count_appended_bing": [
                    52.0,
                    13.0,
                    75.0
                ],
                "answer_relation_to_question_bing": [
                    0.5730337078651685,
                    0.29213483146067415,
                    0.1348314606741573
                ],
                "question_related_to_answer": [
                    1.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    884000.0,
                    3130000.0,
                    60000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    833000.0,
                    42900000.0,
                    1140000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    383000.0,
                    3090000.0,
                    3730000.0
                ],
                "word_count_appended": [
                    307.0,
                    63.0,
                    624.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these mammals averages the largest litter?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "naked mole rat",
                "burmese cat",
                "jackrabbit"
            ],
            "lines": [
                [
                    0.24836601307189543,
                    0.2857142857142857,
                    1.0,
                    0,
                    0.6669768884752598,
                    0.03215957663342153,
                    0.6840713813615333,
                    0.2303218301667313,
                    0.7863247863247863,
                    0.16780045351473924,
                    0.13636363636363635,
                    0.36629368155530945,
                    1.0,
                    0,
                    -1.0
                ],
                [
                    0.08021390374331551,
                    0.0,
                    0.0,
                    0,
                    0.0006426023178000842,
                    0.6747404844290658,
                    0.1232650363516193,
                    0.30438154323381156,
                    0.1111111111111111,
                    0.06802721088435375,
                    0.12121212121212122,
                    0.14061000369139906,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.6714200831847891,
                    0.7142857142857143,
                    0.0,
                    0,
                    0.3323805092069401,
                    0.29309993893751274,
                    0.19266358228684732,
                    0.46529662659945714,
                    0.10256410256410257,
                    0.764172335600907,
                    0.7424242424242424,
                    0.4930963147532916,
                    0.0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "naked mole rat": 0.4670327110984665,
                "burmese cat": 0.13535033474788313,
                "jackrabbit": 0.39761695415365034
            },
            "question": "which of these mammals averages the largest litter?",
            "rate_limited": false,
            "answers": [
                "naked mole rat",
                "burmese cat",
                "jackrabbit"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "naked mole rat": 0.0,
                "burmese cat": 0.0,
                "jackrabbit": 0.0
            },
            "integer_answers": {
                "naked mole rat": 5,
                "burmese cat": 1,
                "jackrabbit": 6
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.4651747262212376,
                    0.5624400147655961,
                    1.972385259013166
                ],
                "result_count_important_words": [
                    207000.0,
                    37300.0,
                    58300.0
                ],
                "wikipedia_search": [
                    2.358974358974359,
                    0.3333333333333333,
                    0.3076923076923077
                ],
                "word_count_appended_bing": [
                    9.0,
                    8.0,
                    49.0
                ],
                "answer_relation_to_question_bing": [
                    0.8571428571428571,
                    0.0,
                    2.142857142857143
                ],
                "question_related_to_answer": [
                    2.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    59400.0,
                    78500.0,
                    120000.0
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    31600.0,
                    663000.0,
                    288000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    30100.0,
                    29.0,
                    15000.0
                ],
                "answer_relation_to_question": [
                    0.7450980392156863,
                    0.24064171122994654,
                    2.014260249554367
                ],
                "word_count_appended": [
                    74.0,
                    30.0,
                    337.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Queen Victoria is credited with starting what fashion trend?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "white wedding dress"
            ],
            "question": "queen victoria is credited with starting what fashion trend?",
            "answers": [
                "mini dress",
                "little black dress",
                "white wedding dress"
            ],
            "integer_answers": {
                "white wedding dress": 8,
                "mini dress": 2,
                "little black dress": 1
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.016752370134215,
                    1.3802150483779874,
                    2.6030325814877973
                ],
                "result_count_important_words": [
                    76.0,
                    82.0,
                    89.0
                ],
                "wikipedia_search": [
                    2.3746031746031746,
                    0.4761904761904762,
                    1.1492063492063491
                ],
                "word_count_appended_bing": [
                    17.0,
                    16.0,
                    19.0
                ],
                "answer_relation_to_question_bing": [
                    1.833333333333333,
                    2.0666666666666664,
                    2.0999999999999996
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    2000000.0,
                    1150000.0,
                    704000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    8.0
                ],
                "result_count_bing": [
                    213000.0,
                    330000.0,
                    273000.0
                ],
                "word_count_appended": [
                    22.0,
                    34.0,
                    49.0
                ],
                "answer_relation_to_question": [
                    1.0188678530141946,
                    1.6978168173290125,
                    3.283315329656793
                ],
                "result_count": [
                    36.0,
                    24.0,
                    46.0
                ]
            },
            "negative_question": false,
            "fraction_answers": {
                "white wedding dress": 0.43290210795019496,
                "mini dress": 0.30626142364582853,
                "little black dress": 0.2608364684039765
            },
            "lines": [
                [
                    0.1698113088356991,
                    0.3055555555555555,
                    0,
                    0,
                    0.33962264150943394,
                    0.2610294117647059,
                    0.3076923076923077,
                    0.5189413596263622,
                    0.5936507936507937,
                    0.20952380952380953,
                    0.3269230769230769,
                    0.3361253950223692,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.28296946955483543,
                    0.34444444444444444,
                    0,
                    0,
                    0.22641509433962265,
                    0.40441176470588236,
                    0.3319838056680162,
                    0.2983912817851583,
                    0.11904761904761905,
                    0.3238095238095238,
                    0.3076923076923077,
                    0.23003584139633124,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.5472192216094655,
                    0.35,
                    0,
                    0,
                    0.4339622641509434,
                    0.33455882352941174,
                    0.3603238866396761,
                    0.1826673585884795,
                    0.2873015873015873,
                    0.4666666666666667,
                    0.36538461538461536,
                    0.4338387635812995,
                    0,
                    1.0,
                    1.0
                ]
            ],
            "rate_limited": false,
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "white wedding dress": 0.7501663759313315,
                "mini dress": 0.20444821671140992,
                "little black dress": 0.15730111195931215
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Who is the only actor to appear as both a student and a guest on \u201cInside the Actors Studio\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ryan gosling",
                "tobey maguire",
                "bradley cooper"
            ],
            "lines": [
                [
                    0.21612952478670064,
                    0.3922743922743923,
                    0.0,
                    0.0,
                    0.23529411764705882,
                    0.32819383259911894,
                    0.45354330708661417,
                    0.04402408543512838,
                    0.288332481689661,
                    0.16981132075471697,
                    0.22727272727272727,
                    0.22676700723518275,
                    0.0,
                    0.0,
                    0.0
                ],
                [
                    0.556924824566225,
                    0.21134421134421133,
                    0.0,
                    0.0,
                    0.27450980392156865,
                    0.3436123348017621,
                    0.15118110236220472,
                    0.013008407180186322,
                    0.12284321239993186,
                    0.1320754716981132,
                    0.18181818181818182,
                    0.2532404655453302,
                    0.0,
                    0.0,
                    0.0
                ],
                [
                    0.22694565064707425,
                    0.3963813963813964,
                    1.0,
                    1.0,
                    0.49019607843137253,
                    0.32819383259911894,
                    0.3952755905511811,
                    0.9429675073846853,
                    0.5888243059104071,
                    0.6981132075471698,
                    0.5909090909090909,
                    0.5199925272194871,
                    1.0,
                    1.0,
                    0.0
                ]
            ],
            "fraction_answers": {
                "tobey maguire": 0.1600398582598368,
                "bradley cooper": 0.6555570848272131,
                "ryan gosling": 0.18440305691295014
            },
            "question": "who is the only actor to appear as both a student and a guest on \u201cinside the actors studio\u201d?",
            "rate_limited": false,
            "answers": [
                "ryan gosling",
                "tobey maguire",
                "bradley cooper"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "tobey maguire": 0.0,
                "bradley cooper": 0.0,
                "ryan gosling": 0.0
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3606020434110966,
                    1.5194427932719812,
                    3.1199551633169227
                ],
                "result_count_important_words": [
                    57600.0,
                    19200.0,
                    50200.0
                ],
                "wikipedia_search": [
                    1.7299948901379663,
                    0.7370592743995912,
                    3.5329458354624426
                ],
                "word_count_appended_bing": [
                    5.0,
                    4.0,
                    13.0
                ],
                "answer_relation_to_question_bing": [
                    1.176823176823177,
                    0.634032634032634,
                    1.1891441891441892
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    2.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_noun_chunks": [
                    77500.0,
                    22900.0,
                    1660000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    3.0
                ],
                "result_count_bing": [
                    149000.0,
                    156000.0,
                    149000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    5.0
                ],
                "word_count_appended": [
                    9.0,
                    7.0,
                    37.0
                ],
                "answer_relation_to_question": [
                    1.0806476239335032,
                    2.7846241228311253,
                    1.1347282532353713
                ],
                "result_count": [
                    24.0,
                    28.0,
                    50.0
                ]
            },
            "integer_answers": {
                "tobey maguire": 2,
                "bradley cooper": 11,
                "ryan gosling": 1
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "To help first create Maps, Google acquired what company?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "mapquest"
            ],
            "lines": [
                [
                    0.2488700955621157,
                    0.6307563781247992,
                    0.0,
                    0,
                    0.0587462851613795,
                    0.33913565426170467,
                    0.3738046942915097,
                    0.6682523502095368,
                    0.10114341399718757,
                    0.36980491942324,
                    0.3779527559055118,
                    0.325066812603083,
                    0.0,
                    0.09803921568627451,
                    1.0
                ],
                [
                    0.5335010955864262,
                    0.21856564488143437,
                    0.0,
                    0,
                    0.003287817303989811,
                    0.3217286914765906,
                    0.02057374674007534,
                    0.0025295427945784725,
                    0.7860637062877629,
                    0.05597964376590331,
                    0.06299212598425197,
                    0.3247127644895263,
                    0.0,
                    0.09803921568627451,
                    1.0
                ],
                [
                    0.2176288088514581,
                    0.15067797699376648,
                    1.0,
                    0,
                    0.9379658975346307,
                    0.33913565426170467,
                    0.605621558968415,
                    0.3292181069958848,
                    0.11279287971504952,
                    0.5742154368108566,
                    0.5590551181102362,
                    0.3502204229073907,
                    1.0,
                    0.803921568627451,
                    1.0
                ]
            ],
            "fraction_answers": {
                "mapquest": 0.27627481347894944,
                "waze": 0.5369579561366803,
                "where 2 technologies": 0.18676723038437032
            },
            "question": "to help first create maps, google acquired what company?",
            "rate_limited": false,
            "answers": [
                "mapquest",
                "where 2 technologies",
                "waze"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "mapquest": 0.5383563796085685,
                "waze": 0.32699156783361355,
                "where 2 technologies": 0.4571968873029763
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.950400875618498,
                    1.9482765869371579,
                    2.1013225374443443
                ],
                "result_count_important_words": [
                    129000.0,
                    7100.0,
                    209000.0
                ],
                "wikipedia_search": [
                    0.6068604839831254,
                    4.716382237726577,
                    0.6767572782902971
                ],
                "word_count_appended_bing": [
                    48.0,
                    8.0,
                    71.0
                ],
                "answer_relation_to_question_bing": [
                    3.784538268748795,
                    1.3113938692886062,
                    0.9040678619625988
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    1.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    1770000.0,
                    6700.0,
                    872000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    17.0
                ],
                "word_count_raw": [
                    5.0,
                    5.0,
                    41.0
                ],
                "result_count_bing": [
                    56500000.0,
                    53600000.0,
                    56500000.0
                ],
                "word_count_appended": [
                    436.0,
                    66.0,
                    677.0
                ],
                "answer_relation_to_question": [
                    1.4932205733726942,
                    3.201006573518557,
                    1.3057728531087487
                ],
                "result_count": [
                    119000.0,
                    6660.0,
                    1900000.0
                ]
            },
            "integer_answers": {
                "mapquest": 3,
                "waze": 8,
                "where 2 technologies": 2
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "According to the old saying, what kind of animal can NOT change its spots?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "zebra"
            ],
            "lines": [
                [
                    0.3184206711603972,
                    0.42301094319958316,
                    0.49382716049382713,
                    0.5,
                    0.48056218924950084,
                    0.36566440349175555,
                    0.3765880217785844,
                    0.47713280562884786,
                    0.4863121874177415,
                    0.37063492063492065,
                    0.3633093525179856,
                    0.33505895936078517,
                    0.4724409448818898,
                    0.5,
                    1.0
                ],
                [
                    0.2691074146553599,
                    0.16924127149557058,
                    0.009259259259259245,
                    0.0,
                    0.48336139059624944,
                    0.3676042677012609,
                    0.33030852994555354,
                    0.4736147757255937,
                    0.3476472195281712,
                    0.301984126984127,
                    0.262589928057554,
                    0.33044728475198626,
                    0.0748031496062992,
                    0.013513513513513487,
                    1.0
                ],
                [
                    0.41247191418424295,
                    0.4077477853048463,
                    0.49691358024691357,
                    0.5,
                    0.03607642015424972,
                    0.26673132880698347,
                    0.2931034482758621,
                    0.04925241864555846,
                    0.1660405930540873,
                    0.3273809523809524,
                    0.37410071942446044,
                    0.3344937558872285,
                    0.45275590551181105,
                    0.4864864864864865,
                    1.0
                ]
            ],
            "fraction_answers": {
                "tiger": 0.34234924166233105,
                "leopard": 0.5095025525970718,
                "zebra": 0.1481482057405973
            },
            "question": "according to the old saying, what kind of animal can not change its spots?",
            "rate_limited": false,
            "answers": [
                "zebra",
                "leopard",
                "tiger"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "tiger": 0.18890416467366344,
                "leopard": 0.27334897462146995,
                "zebra": 0.531636676079299
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.3091745689490075,
                    2.373738013472192,
                    2.3170874175788003
                ],
                "result_count_important_words": [
                    1360000.0,
                    1870000.0,
                    2280000.0
                ],
                "wikipedia_search": [
                    0.16425375098710188,
                    1.8282333656619458,
                    4.007512883350953
                ],
                "answer_relation_to_question": [
                    2.178951946075234,
                    2.7707110241356814,
                    1.0503370297890846
                ],
                "answer_relation_to_question_bing": [
                    0.7698905680041688,
                    3.307587285044294,
                    0.9225221469515373
                ],
                "word_count_appended": [
                    326.0,
                    499.0,
                    435.0
                ],
                "question_related_to_answer": [
                    0.012345679012345678,
                    0.9814814814814815,
                    0.006172839506172839
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    1040000.0,
                    1200000.0,
                    20500000.0
                ],
                "word_count_noun_chunks": [
                    7.0,
                    108.0,
                    12.0
                ],
                "word_count_raw": [
                    0.0,
                    72.0,
                    2.0
                ],
                "result_count_bing": [
                    27700000.0,
                    27300000.0,
                    48100000.0
                ],
                "result_count": [
                    993000.0,
                    850000.0,
                    23700000.0
                ],
                "word_count_appended_bing": [
                    38.0,
                    66.0,
                    35.0
                ]
            },
            "integer_answers": {
                "tiger": 5,
                "leopard": 9,
                "zebra": 0
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Who defeated Napoleon at the Battle of Waterloo?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "the duke of wellington"
            ],
            "lines": [
                [
                    0.0,
                    0.0,
                    0,
                    0.0,
                    0.07153308405137376,
                    0.3333333333333333,
                    0.00032784248714919306,
                    0.0002007628990162618,
                    0.0375,
                    0.14285714285714285,
                    0.2222222222222222,
                    0.14041858896519907,
                    0.0,
                    0.0,
                    0.0
                ],
                [
                    1.0,
                    1.0,
                    0,
                    1.0,
                    0.9158402427789519,
                    0.33537832310838445,
                    0.9835274614475792,
                    0.9829016931004484,
                    0.9625,
                    0.42424242424242425,
                    0.5555555555555556,
                    0.5529491851944098,
                    1.0,
                    1.0,
                    0.0
                ],
                [
                    0.0,
                    0.0,
                    0,
                    0.0,
                    0.012626673169674307,
                    0.3312883435582822,
                    0.016144696065271583,
                    0.016897544000535367,
                    0.0,
                    0.4329004329004329,
                    0.2222222222222222,
                    0.30663222584039124,
                    0.0,
                    0.0,
                    0.0
                ]
            ],
            "fraction_answers": {
                "jack skellington": 0.07295330590887975,
                "the duke of wellington": 0.8240688373405964,
                "beef wellington": 0.10297785675052382
            },
            "question": "who defeated napoleon at the battle of waterloo?",
            "rate_limited": false,
            "answers": [
                "jack skellington",
                "the duke of wellington",
                "beef wellington"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "jack skellington": 0.0708078320896507,
                "the duke of wellington": 0.948492446121844,
                "beef wellington": 0.4942506061412352
            },
            "integer_answers": {
                "jack skellington": 0,
                "the duke of wellington": 12,
                "beef wellington": 1
            },
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.5616743558607962,
                    2.211796740777639,
                    1.2265289033615647
                ],
                "result_count_important_words": [
                    53.0,
                    159000.0,
                    2610.0
                ],
                "wikipedia_search": [
                    0.15,
                    3.85,
                    0.0
                ],
                "word_count_appended_bing": [
                    2.0,
                    5.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    4.0,
                    0.0
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    48.0,
                    235000.0,
                    4040.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    14.0,
                    0.0
                ],
                "result_count_bing": [
                    326000.0,
                    328000.0,
                    324000.0
                ],
                "word_count_raw": [
                    0.0,
                    13.0,
                    0.0
                ],
                "result_count": [
                    13200.0,
                    169000.0,
                    2330.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    4.0,
                    0.0
                ],
                "word_count_appended": [
                    33.0,
                    98.0,
                    100.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What soda is named for a medical condition?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "pepsi"
            ],
            "lines": [
                [
                    0.27478376580172986,
                    0.3448275862068966,
                    0.0,
                    0.0,
                    0.03130032206119163,
                    0.27835051546391754,
                    0.061682818027995696,
                    0.13246753246753246,
                    0.024390243902439025,
                    0.2675760755508919,
                    0.4,
                    0.2894126873437267,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.5894876912840985,
                    0.5,
                    1.0,
                    0.6666666666666666,
                    0.1736111111111111,
                    0.3943298969072165,
                    0.24996154437778803,
                    0.7047619047619048,
                    0.8048780487804879,
                    0.4669464847848898,
                    0.32222222222222224,
                    0.37611537762316205,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.13572854291417166,
                    0.15517241379310345,
                    0.0,
                    0.3333333333333333,
                    0.7950885668276972,
                    0.327319587628866,
                    0.6883556375942163,
                    0.16277056277056276,
                    0.17073170731707318,
                    0.2654774396642183,
                    0.2777777777777778,
                    0.3344719350331113,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "faygo": 0.15034225334473725,
                "pepsi": 0.5892129248942534,
                "fanta": 0.26044482176100936
            },
            "question": "what soda is named for a medical condition?",
            "rate_limited": false,
            "answers": [
                "faygo",
                "pepsi",
                "fanta"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "faygo": 0.19169810444721042,
                "pepsi": 0.9559882796290521,
                "fanta": 0.22174976600296573
            },
            "integer_answers": {
                "faygo": 1,
                "pepsi": 11,
                "fanta": 2
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1576507493749069,
                    1.5044615104926482,
                    1.3378877401324452
                ],
                "result_count_important_words": [
                    80200.0,
                    325000.0,
                    895000.0
                ],
                "wikipedia_search": [
                    0.04878048780487805,
                    1.6097560975609757,
                    0.34146341463414637
                ],
                "word_count_appended_bing": [
                    72.0,
                    58.0,
                    50.0
                ],
                "answer_relation_to_question_bing": [
                    0.6896551724137931,
                    1.0,
                    0.3103448275862069
                ],
                "question_related_to_answer": [
                    0.0,
                    1.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.6666666666666666,
                    0.3333333333333333
                ],
                "result_count_noun_chunks": [
                    76500.0,
                    407000.0,
                    94000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    5.0,
                    0.0
                ],
                "result_count_bing": [
                    10800000.0,
                    15300000.0,
                    12700000.0
                ],
                "word_count_raw": [
                    0.0,
                    13.0,
                    0.0
                ],
                "result_count": [
                    62200.0,
                    345000.0,
                    1580000.0
                ],
                "answer_relation_to_question": [
                    0.8243512974051896,
                    1.7684630738522955,
                    0.40718562874251496
                ],
                "word_count_appended": [
                    255.0,
                    445.0,
                    253.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these is NOT a geometric shape?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "octagon",
                "tarragon",
                "hexagon"
            ],
            "lines": [
                [
                    0.40909090909090906,
                    0.25,
                    0.33333333333333337,
                    0,
                    0.30164560681751396,
                    0.2870521172638436,
                    0.4843016836875518,
                    0.48165367284171373,
                    0.35,
                    0.3411172161172161,
                    0.2700534759358289,
                    0.28321934939269355,
                    0.33333333333333337,
                    0.5,
                    -1.0
                ],
                [
                    0.5,
                    0.5,
                    0.5,
                    0,
                    0.4760505436379665,
                    0.36441368078175895,
                    0.49527325420921886,
                    0.49399184607681856,
                    0.4,
                    0.39056776556776557,
                    0.4278074866310161,
                    0.40310609494927385,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.09090909090909094,
                    0.25,
                    0.16666666666666669,
                    0,
                    0.22230384954451954,
                    0.3485342019543974,
                    0.020425062103229363,
                    0.024354481081467705,
                    0.25,
                    0.2683150183150183,
                    0.3021390374331551,
                    0.31367455565803254,
                    0.16666666666666669,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "hexagon": 0.6270786722565778,
                "tarragon": 0.08442912740710487,
                "octagon": 0.2884922003363174
            },
            "question": "which of these is not a geometric shape?",
            "rate_limited": false,
            "answers": [
                "octagon",
                "tarragon",
                "hexagon"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "hexagon": 0.0,
                "tarragon": 0.0,
                "octagon": 0.0
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.8671226024292258,
                    0.3875756202029045,
                    0.7453017773678698
                ],
                "result_count_important_words": [
                    455000.0,
                    137000.0,
                    13900000.0
                ],
                "wikipedia_search": [
                    0.3,
                    0.2,
                    0.5
                ],
                "answer_relation_to_question": [
                    0.36363636363636365,
                    0.0,
                    1.6363636363636362
                ],
                "answer_relation_to_question_bing": [
                    0.5,
                    0.0,
                    0.5
                ],
                "word_count_appended": [
                    347.0,
                    239.0,
                    506.0
                ],
                "question_related_to_answer": [
                    0.3333333333333333,
                    0.0,
                    0.6666666666666666
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    513000.0,
                    168000.0,
                    13300000.0
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    2.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    4.0
                ],
                "result_count_bing": [
                    5230000.0,
                    3330000.0,
                    3720000.0
                ],
                "result_count": [
                    1350000.0,
                    163000.0,
                    1890000.0
                ],
                "word_count_appended_bing": [
                    86.0,
                    27.0,
                    74.0
                ]
            },
            "integer_answers": {
                "hexagon": 9,
                "tarragon": 0,
                "octagon": 4
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What tech mogul became a billionaire the youngest?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "mark zuckerberg"
            ],
            "lines": [
                [
                    0.5766317016317016,
                    0.6853070175438597,
                    0.0,
                    0.0,
                    0.00912930474333983,
                    0.2522567703109328,
                    0.010373951894240773,
                    0.8298945437872536,
                    0.4163832199546485,
                    0.41284403669724773,
                    0.8148148148148148,
                    0.32844044881212725,
                    0.0,
                    0.32142857142857145,
                    1.0
                ],
                [
                    0.30477855477855476,
                    0.15899122807017546,
                    1.0,
                    1.0,
                    0.9649122807017544,
                    0.506519558676028,
                    0.9639512822082135,
                    0.08069692801467217,
                    0.34268707482993194,
                    0.3669724770642202,
                    0.1111111111111111,
                    0.35418939634719215,
                    1.0,
                    0.6071428571428571,
                    1.0
                ],
                [
                    0.11858974358974358,
                    0.15570175438596492,
                    0.0,
                    0.0,
                    0.025958414554905784,
                    0.24122367101303913,
                    0.025674765897545748,
                    0.08940852819807428,
                    0.24092970521541948,
                    0.22018348623853212,
                    0.07407407407407407,
                    0.3173701548406806,
                    0.0,
                    0.07142857142857142,
                    1.0
                ]
            ],
            "fraction_answers": {
                "mark zuckerberg": 0.5544251963531937,
                "evan spiegel": 0.33267888440133847,
                "larry page": 0.11289591924546796
            },
            "question": "what tech mogul became a billionaire the youngest?",
            "rate_limited": false,
            "answers": [
                "evan spiegel",
                "mark zuckerberg",
                "larry page"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "mark zuckerberg": 0.3952380970830009,
                "evan spiegel": 0.3194444440305233,
                "larry page": 0.29999999900658925
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.313761795248509,
                    1.4167575853887686,
                    1.2694806193627224
                ],
                "result_count_important_words": [
                    3390.0,
                    315000.0,
                    8390.0
                ],
                "wikipedia_search": [
                    1.2491496598639455,
                    1.0280612244897958,
                    0.7227891156462585
                ],
                "answer_relation_to_question": [
                    2.3065268065268065,
                    1.219114219114219,
                    0.47435897435897434
                ],
                "answer_relation_to_question_bing": [
                    2.7412280701754383,
                    0.6359649122807017,
                    0.6228070175438596
                ],
                "word_count_appended": [
                    90.0,
                    80.0,
                    48.0
                ],
                "question_related_to_answer": [
                    0.0,
                    2.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    1810000.0,
                    176000.0,
                    195000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    7.0,
                    0.0
                ],
                "word_count_raw": [
                    9.0,
                    17.0,
                    2.0
                ],
                "result_count_bing": [
                    50300.0,
                    101000.0,
                    48100.0
                ],
                "result_count": [
                    2810.0,
                    297000.0,
                    7990.0
                ],
                "word_count_appended_bing": [
                    22.0,
                    3.0,
                    2.0
                ]
            },
            "integer_answers": {
                "mark zuckerberg": 8,
                "evan spiegel": 6,
                "larry page": 0
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which actor currently stars in a show that is both one word long and pluralized?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "william h. macy"
            ],
            "lines": [
                [
                    0.5293261617040705,
                    0.22751322751322753,
                    0,
                    0,
                    0.1073738680465718,
                    0.2994250331711632,
                    0.012242701466433472,
                    0.2555257870060281,
                    0.30431887366818877,
                    0.3728813559322034,
                    0.38571428571428573,
                    0.3228992009310289,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.23387310781698076,
                    0.16534391534391535,
                    0,
                    0,
                    0.09314359637774904,
                    0.18310482087571872,
                    0.011031884837885107,
                    0.06463496316141996,
                    0.33656773211567736,
                    0.3389830508474576,
                    0.4,
                    0.25181844176248774,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.23680073047894878,
                    0.6071428571428571,
                    0,
                    0,
                    0.7994825355756792,
                    0.5174701459531181,
                    0.9767254136956814,
                    0.679839249832552,
                    0.359113394216134,
                    0.288135593220339,
                    0.21428571428571427,
                    0.4252823573064834,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "william h. macy": 0.2817220495153202,
                "taraji p. henson": 0.20785015131392917,
                "paul giamatti": 0.5104277991707507
            },
            "question": "which actor currently stars in a show that is both one word long and pluralized?",
            "rate_limited": false,
            "answers": [
                "william h. macy",
                "taraji p. henson",
                "paul giamatti"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "william h. macy": 0.3194444440305233,
                "taraji p. henson": 0.29999999900658925,
                "paul giamatti": 0.31499999945362406
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9373952055861736,
                    1.5109106505749264,
                    2.5516941438389003
                ],
                "result_count_important_words": [
                    91.0,
                    82.0,
                    7260.0
                ],
                "wikipedia_search": [
                    1.217275494672755,
                    1.3462709284627095,
                    1.436453576864536
                ],
                "answer_relation_to_question": [
                    2.6466308085203525,
                    1.1693655390849038,
                    1.184003652394744
                ],
                "answer_relation_to_question_bing": [
                    0.6825396825396826,
                    0.49603174603174605,
                    1.8214285714285714
                ],
                "word_count_appended": [
                    22.0,
                    20.0,
                    17.0
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    763000.0,
                    193000.0,
                    2030000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    67700.0,
                    41400.0,
                    117000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    83.0,
                    72.0,
                    618.0
                ],
                "word_count_appended_bing": [
                    27.0,
                    28.0,
                    15.0
                ]
            },
            "integer_answers": {
                "william h. macy": 2,
                "taraji p. henson": 1,
                "paul giamatti": 7
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which animal is specifically mentioned in the Judeo-Christian Ten Commandments?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ox"
            ],
            "lines": [
                [
                    0.20833333333333334,
                    0.46428571428571436,
                    0.09375,
                    0.0,
                    0.11159125571936959,
                    0.49298245614035086,
                    0.3422222222222222,
                    0.3751503006012024,
                    0.0989010989010989,
                    0.3502626970227671,
                    0.18620689655172415,
                    0.31393348605383836,
                    0.0,
                    0.041666666666666664,
                    -1.0
                ],
                [
                    0.075,
                    0.4285714285714286,
                    0.796875,
                    1.0,
                    0.2910523640061007,
                    0.2535087719298246,
                    0.29333333333333333,
                    0.24208416833667334,
                    0.054945054945054944,
                    0.35376532399299476,
                    0.6275862068965518,
                    0.3563532245402157,
                    1.0,
                    0.9583333333333334,
                    -1.0
                ],
                [
                    0.7166666666666667,
                    0.10714285714285715,
                    0.109375,
                    0.0,
                    0.5973563802745298,
                    0.2535087719298246,
                    0.36444444444444446,
                    0.38276553106212424,
                    0.8461538461538461,
                    0.29597197898423816,
                    0.18620689655172415,
                    0.32971328940594585,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "ox": 0.4808148721346793,
                "goat": 0.29923611875830003,
                "pig": 0.21994900910702056
            },
            "question": "which animal is specifically mentioned in the judeo-christian ten commandments?",
            "rate_limited": false,
            "answers": [
                "pig",
                "ox",
                "goat"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "ox": 0.7923562311541543,
                "goat": 0.22303461632506688,
                "pig": 0.21945511876811416
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.8836009163230303,
                    2.138119347241294,
                    1.9782797364356752
                ],
                "result_count_important_words": [
                    15400.0,
                    13200.0,
                    16400.0
                ],
                "wikipedia_search": [
                    0.2967032967032967,
                    0.16483516483516483,
                    2.5384615384615383
                ],
                "word_count_appended_bing": [
                    27.0,
                    91.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    0.4642857142857143,
                    0.42857142857142855,
                    0.10714285714285714
                ],
                "question_related_to_answer": [
                    0.1875,
                    1.59375,
                    0.21875
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    2.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    93600.0,
                    60400.0,
                    95500.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    18.0,
                    0.0
                ],
                "result_count_bing": [
                    56200.0,
                    28900.0,
                    28900.0
                ],
                "word_count_raw": [
                    1.0,
                    23.0,
                    0.0
                ],
                "word_count_appended": [
                    200.0,
                    202.0,
                    169.0
                ],
                "answer_relation_to_question": [
                    0.4166666666666667,
                    0.15,
                    1.4333333333333333
                ],
                "result_count": [
                    8780.0,
                    22900.0,
                    47000.0
                ]
            },
            "integer_answers": {
                "ox": 7,
                "goat": 5,
                "pig": 2
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which human sense is most closely associated with the bony labyrinth?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "hearing"
            ],
            "lines": [
                [
                    0.4039855072463768,
                    0.5588235294117647,
                    0.37349397590361444,
                    0.5833333333333334,
                    0.3401842664776754,
                    0.625,
                    0.3492753623188406,
                    0.3403973509933775,
                    0.3904761904761905,
                    0.3053545586107091,
                    0.3577981651376147,
                    0.3349741416802674,
                    0.37349397590361444,
                    0.041666666666666664,
                    -1.0
                ],
                [
                    0.3695652173913044,
                    0.29411764705882354,
                    0.2891566265060241,
                    0.20833333333333334,
                    0.3405386250885897,
                    0.1875,
                    0.32753623188405795,
                    0.3403973509933775,
                    0.5,
                    0.4081041968162084,
                    0.3761467889908257,
                    0.34264691640294503,
                    0.2891566265060241,
                    0.9375,
                    -1.0
                ],
                [
                    0.22644927536231885,
                    0.14705882352941177,
                    0.3373493975903614,
                    0.20833333333333334,
                    0.3192771084337349,
                    0.1875,
                    0.32318840579710145,
                    0.31920529801324504,
                    0.10952380952380951,
                    0.2865412445730825,
                    0.26605504587155965,
                    0.32237894191678756,
                    0.3373493975903614,
                    0.020833333333333332,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "touch": 0.3841612160114319,
                "sight": 0.2436459582048886,
                "hearing": 0.37219282578367957
            },
            "question": "which human sense is most closely associated with the bony labyrinth?",
            "rate_limited": false,
            "answers": [
                "touch",
                "hearing",
                "sight"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "touch": 0.3194444440305233,
                "sight": 0.31499999945362406,
                "hearing": 0.354545455177625
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.0098448500816044,
                    2.05588149841767,
                    1.9342736515007255
                ],
                "result_count_important_words": [
                    241000.0,
                    226000.0,
                    223000.0
                ],
                "wikipedia_search": [
                    1.9523809523809523,
                    2.5,
                    0.5476190476190476
                ],
                "word_count_appended_bing": [
                    39.0,
                    41.0,
                    29.0
                ],
                "answer_relation_to_question_bing": [
                    1.1176470588235294,
                    0.5882352941176471,
                    0.29411764705882354
                ],
                "question_related_to_answer": [
                    0.37349397590361444,
                    0.2891566265060241,
                    0.3373493975903614
                ],
                "question_related_to_answer_bing": [
                    0.5833333333333334,
                    0.20833333333333334,
                    0.20833333333333334
                ],
                "result_count_noun_chunks": [
                    257000000.0,
                    257000000.0,
                    241000000.0
                ],
                "word_count_noun_chunks": [
                    31.0,
                    24.0,
                    28.0
                ],
                "word_count_raw": [
                    2.0,
                    45.0,
                    1.0
                ],
                "result_count_bing": [
                    102000.0,
                    30600.0,
                    30600.0
                ],
                "word_count_appended": [
                    211.0,
                    282.0,
                    198.0
                ],
                "answer_relation_to_question": [
                    1.2119565217391304,
                    1.108695652173913,
                    0.6793478260869565
                ],
                "result_count": [
                    96000.0,
                    96100.0,
                    90100.0
                ]
            },
            "integer_answers": {
                "touch": 8,
                "sight": 0,
                "hearing": 6
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these utensils is tined?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "spoon"
            ],
            "lines": [
                [
                    0.0,
                    0.0,
                    0.3466666666666667,
                    0.0,
                    0.05605935696619951,
                    0.17658108954289292,
                    0.4018369690011481,
                    0.31083265966046886,
                    0.8333333333333334,
                    0.41218925421010427,
                    0.4777777777777778,
                    0.34351256721270473,
                    0.3466666666666667,
                    0.675,
                    -1.0
                ],
                [
                    0.2,
                    1.0,
                    0.09333333333333334,
                    0.0,
                    0.47197032151690027,
                    0.20788979336255478,
                    0.2893226176808266,
                    0.3229587712206952,
                    0.16666666666666669,
                    0.2702485966319166,
                    0.22777777777777777,
                    0.3165399524821808,
                    0.09333333333333334,
                    0.1,
                    -1.0
                ],
                [
                    0.8,
                    0.0,
                    0.56,
                    1.0,
                    0.47197032151690027,
                    0.6155291170945523,
                    0.30884041331802525,
                    0.36620856911883587,
                    0.0,
                    0.31756214915797915,
                    0.29444444444444445,
                    0.3399474803051144,
                    0.56,
                    0.225,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "fork": 0.31288973864556885,
                "spoon": 0.41853589249684653,
                "knife": 0.2685743688575847
            },
            "question": "which of these utensils is tined?",
            "rate_limited": false,
            "answers": [
                "fork",
                "knife",
                "spoon"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "fork": 0.4821724402932378,
                "spoon": 0.49017474386992727,
                "knife": 0.17591279738771257
            },
            "integer_answers": {
                "fork": 6,
                "spoon": 6,
                "knife": 2
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.6870251344254095,
                    0.6330799049643616,
                    0.6798949606102288
                ],
                "result_count_important_words": [
                    350000.0,
                    252000.0,
                    269000.0
                ],
                "wikipedia_search": [
                    1.6666666666666665,
                    0.3333333333333333,
                    0.0
                ],
                "word_count_appended_bing": [
                    86.0,
                    41.0,
                    53.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    1.0,
                    0.0
                ],
                "question_related_to_answer": [
                    0.3466666666666667,
                    0.09333333333333334,
                    0.56
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_noun_chunks": [
                    7690000.0,
                    7990000.0,
                    9060000.0
                ],
                "word_count_noun_chunks": [
                    26.0,
                    7.0,
                    42.0
                ],
                "result_count_bing": [
                    2820000.0,
                    3320000.0,
                    9830000.0
                ],
                "word_count_raw": [
                    162.0,
                    24.0,
                    54.0
                ],
                "result_count": [
                    272000.0,
                    2290000.0,
                    2290000.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    0.2,
                    0.8
                ],
                "word_count_appended": [
                    514.0,
                    337.0,
                    396.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which TV comedy centers on a vice president?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "young sheldon",
                "superstore",
                "veep"
            ],
            "lines": [
                [
                    0.18933706816059753,
                    0.1842361111111111,
                    0.0,
                    0,
                    0.06348208673790069,
                    0.09033391915641477,
                    0.06550760633343682,
                    0.06601504925917306,
                    0.312767094017094,
                    0.06624605678233439,
                    0.08866995073891626,
                    0.30533079170188226,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.22434523809523807,
                    0.19972222222222225,
                    0.0,
                    0,
                    0.8485229415461973,
                    0.81195079086116,
                    0.8460105557280347,
                    0.8455511597238383,
                    0.027136752136752137,
                    0.28391167192429023,
                    0.21674876847290642,
                    0.3244649644442362,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.5863176937441643,
                    0.6160416666666666,
                    1.0,
                    0,
                    0.08799497171590195,
                    0.0977152899824253,
                    0.0884818379385284,
                    0.0884337910169886,
                    0.6600961538461538,
                    0.6498422712933754,
                    0.6945812807881774,
                    0.37020424385388145,
                    1.0,
                    1.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "young sheldon": 0.11014813338452775,
                "veep": 0.5338237846804817,
                "superstore": 0.35602808193499047
            },
            "question": "which tv comedy centers on a vice president?",
            "rate_limited": false,
            "answers": [
                "young sheldon",
                "superstore",
                "veep"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "young sheldon": 0.0,
                "veep": 0.0,
                "superstore": 0.0
            },
            "integer_answers": {
                "young sheldon": 0,
                "veep": 9,
                "superstore": 4
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.5266539585094112,
                    1.6223248222211812,
                    1.8510212192694073
                ],
                "result_count_important_words": [
                    84400.0,
                    1090000.0,
                    114000.0
                ],
                "wikipedia_search": [
                    1.251068376068376,
                    0.10854700854700855,
                    2.6403846153846153
                ],
                "word_count_appended_bing": [
                    18.0,
                    44.0,
                    141.0
                ],
                "answer_relation_to_question_bing": [
                    0.7369444444444444,
                    0.798888888888889,
                    2.4641666666666664
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    1.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    85100.0,
                    1090000.0,
                    114000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    23.0
                ],
                "result_count_bing": [
                    2570000.0,
                    23100000.0,
                    2780000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    40.0
                ],
                "result_count": [
                    80800.0,
                    1080000.0,
                    112000.0
                ],
                "answer_relation_to_question": [
                    0.9466853408029877,
                    1.1217261904761904,
                    2.931588468720822
                ],
                "word_count_appended": [
                    63.0,
                    270.0,
                    618.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which organization began as the North West Police Agency?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "pinkerton"
            ],
            "question": "which organization began as the north west police agency?",
            "answers": [
                "fbi",
                "nra",
                "pinkerton"
            ],
            "integer_answers": {
                "pinkerton": 5,
                "fbi": 6,
                "nra": 2
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.144611142451421,
                    1.67072136992143,
                    2.1846674876271486
                ],
                "result_count_important_words": [
                    685000.0,
                    429000.0,
                    106000.0
                ],
                "wikipedia_search": [
                    5.0,
                    0.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    66.0,
                    74.0,
                    66.0
                ],
                "answer_relation_to_question_bing": [
                    0.5064935064935066,
                    0.6363636363636364,
                    0.8571428571428571
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    1.1739130434782608,
                    0.0,
                    0.8260869565217391
                ],
                "result_count_noun_chunks": [
                    767000.0,
                    408000.0,
                    161000.0
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    12.0,
                    1.0,
                    21.0
                ],
                "result_count_bing": [
                    1150000.0,
                    1560000.0,
                    1410000.0
                ],
                "word_count_appended": [
                    438.0,
                    451.0,
                    508.0
                ],
                "answer_relation_to_question": [
                    1.9186112906701145,
                    0.808785822021116,
                    3.2726028873087696
                ],
                "result_count": [
                    692000.0,
                    429000.0,
                    106000.0
                ]
            },
            "negative_question": false,
            "fraction_answers": {
                "pinkerton": 0.32221911899957995,
                "fbi": 0.4602266254206579,
                "nra": 0.21755425557976218
            },
            "lines": [
                [
                    0.31976854844501906,
                    0.2532467532467533,
                    0,
                    0.5869565217391304,
                    0.5639771801140995,
                    0.279126213592233,
                    0.5614754098360656,
                    0.5741017964071856,
                    1.0,
                    0.31352899069434503,
                    0.32038834951456313,
                    0.3574351904085702,
                    0.5,
                    0.35294117647058826,
                    -1.0
                ],
                [
                    0.13479763700351935,
                    0.3181818181818182,
                    0,
                    0.0,
                    0.34963325183374083,
                    0.3786407766990291,
                    0.3516393442622951,
                    0.30538922155688625,
                    0.0,
                    0.3228346456692913,
                    0.3592233009708738,
                    0.27845356165357166,
                    0.0,
                    0.029411764705882353,
                    -1.0
                ],
                [
                    0.5454338145514616,
                    0.42857142857142855,
                    0,
                    0.41304347826086957,
                    0.08638956805215973,
                    0.3422330097087379,
                    0.08688524590163935,
                    0.12050898203592815,
                    0.0,
                    0.36363636363636365,
                    0.32038834951456313,
                    0.3641112479378581,
                    0.5,
                    0.6176470588235294,
                    -1.0
                ]
            ],
            "rate_limited": false,
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "pinkerton": 0.8656481108613021,
                "fbi": 0.43371535840779213,
                "nra": 0.16174835380807548
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Mount Rushmore was named after a person with what profession?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "lawyer"
            ],
            "lines": [
                [
                    0.2776679841897233,
                    0.15625,
                    0.0,
                    0.0,
                    0.18171765573166238,
                    0.27125506072874495,
                    0.023172120010282397,
                    0.11236442516268981,
                    0.265625,
                    0.2900662251655629,
                    0.3191489361702128,
                    0.3014597479072882,
                    0.0,
                    0.16666666666666666,
                    1.0
                ],
                [
                    0.22233201581027667,
                    0.33035714285714285,
                    0.6785714285714286,
                    0.0,
                    0.37596756358274974,
                    0.20917678812415655,
                    0.5545150747310051,
                    0.5292841648590022,
                    0.6437190594059405,
                    0.33112582781456956,
                    0.3723404255319149,
                    0.37173623807054745,
                    0.0,
                    0.8333333333333334,
                    1.0
                ],
                [
                    0.5,
                    0.5133928571428572,
                    0.3214285714285714,
                    1.0,
                    0.4423147806855879,
                    0.5195681511470985,
                    0.4223128052587125,
                    0.358351409978308,
                    0.0906559405940594,
                    0.37880794701986753,
                    0.30851063829787234,
                    0.3268040140221644,
                    1.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "architect": 0.4415819368267928,
                "prospector": 0.1689567015523452,
                "lawyer": 0.3894613616208619
            },
            "question": "mount rushmore was named after a person with what profession?",
            "rate_limited": false,
            "answers": [
                "prospector",
                "lawyer",
                "architect"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "architect": 0.37522045388866043,
                "prospector": 0.07955922452893267,
                "lawyer": 0.567433070603956
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.507298739536441,
                    1.8586811903527372,
                    1.634020070110822
                ],
                "result_count_important_words": [
                    631.0,
                    15100.0,
                    11500.0
                ],
                "wikipedia_search": [
                    1.0625,
                    2.574876237623762,
                    0.3626237623762376
                ],
                "answer_relation_to_question": [
                    1.1106719367588933,
                    0.8893280632411067,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.3125,
                    0.6607142857142857,
                    1.0267857142857144
                ],
                "word_count_appended": [
                    219.0,
                    250.0,
                    286.0
                ],
                "question_related_to_answer": [
                    0.0,
                    1.3571428571428572,
                    0.6428571428571428
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_noun_chunks": [
                    2590000.0,
                    12200000.0,
                    8260000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    3.0
                ],
                "word_count_raw": [
                    1.0,
                    5.0,
                    0.0
                ],
                "result_count_bing": [
                    804000.0,
                    620000.0,
                    1540000.0
                ],
                "result_count": [
                    49300.0,
                    102000.0,
                    120000.0
                ],
                "word_count_appended_bing": [
                    30.0,
                    35.0,
                    29.0
                ]
            },
            "integer_answers": {
                "architect": 7,
                "prospector": 0,
                "lawyer": 7
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "What is the total cost of all the vowels on \u201cWheel of Fortune\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "$2,500"
            ],
            "lines": [
                [
                    0.1534090909090909,
                    0.39743589743589747,
                    0,
                    0,
                    0.30128205128205127,
                    0.3333333333333333,
                    0.2607385079125848,
                    0.09523809523809523,
                    0.0,
                    0.3783783783783784,
                    0.3157894736842105,
                    0.32892150076149307,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.45454545454545453,
                    0.24358974358974358,
                    0,
                    0,
                    0.4230769230769231,
                    0.3333333333333333,
                    0.6322532027128862,
                    0.865032403493942,
                    1.0,
                    0.35135135135135137,
                    0.2631578947368421,
                    0.4455757559710964,
                    0,
                    1.0,
                    1.0
                ],
                [
                    0.39204545454545453,
                    0.358974358974359,
                    0,
                    0,
                    0.27564102564102566,
                    0.3333333333333333,
                    0.10700828937452901,
                    0.039729501267962805,
                    0.0,
                    0.2702702702702703,
                    0.42105263157894735,
                    0.22550274326741054,
                    0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "$2,500": 0.546537823891961,
                "$1,250": 0.23313875717592136,
                "$2,900": 0.22032341893211752
            },
            "question": "what is the total cost of all the vowels on \u201cwheel of fortune\u201d?",
            "rate_limited": false,
            "answers": [
                "$1,250",
                "$2,500",
                "$2,900"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "$2,500": 0.7093346127901192,
                "$1,250": 0.47464536983674344,
                "$2,900": 0.18154477244389403
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6446075038074652,
                    2.227878779855482,
                    1.1275137163370528
                ],
                "result_count_important_words": [
                    3460.0,
                    8390.0,
                    1420.0
                ],
                "wikipedia_search": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    6.0,
                    5.0,
                    8.0
                ],
                "answer_relation_to_question_bing": [
                    0.7948717948717949,
                    0.48717948717948717,
                    0.717948717948718
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    3380.0,
                    30700.0,
                    1410.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count_bing": [
                    367000.0,
                    367000.0,
                    367000.0
                ],
                "result_count": [
                    47.0,
                    66.0,
                    43.0
                ],
                "answer_relation_to_question": [
                    0.6136363636363636,
                    1.8181818181818181,
                    1.5681818181818181
                ],
                "word_count_appended": [
                    14.0,
                    13.0,
                    10.0
                ]
            },
            "integer_answers": {
                "$2,500": 7,
                "$1,250": 3,
                "$2,900": 1
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which brand mascot was NOT a real person?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "little debbie",
                "sara lee",
                "betty crocker"
            ],
            "lines": [
                [
                    0.3798288225769142,
                    0.25161290322580643,
                    0.5,
                    0,
                    0.2628055878928987,
                    0.26746411483253585,
                    0.41776649746192895,
                    0.47435574714452033,
                    0.3032407407407408,
                    0.27923976608187134,
                    0.31746031746031744,
                    0.33311365676938215,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.3033731535639933,
                    0.3193548387096774,
                    0.5,
                    0,
                    0.36161233993015135,
                    0.35933014354066983,
                    0.41725888324873095,
                    0.4850382303892263,
                    0.4207175925925926,
                    0.35526315789473684,
                    0.42063492063492064,
                    0.3623835753110531,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.3167980238590925,
                    0.4290322580645161,
                    0.0,
                    0,
                    0.37558207217694994,
                    0.37320574162679426,
                    0.1649746192893401,
                    0.040606022466253444,
                    0.2760416666666667,
                    0.3654970760233918,
                    0.2619047619047619,
                    0.30450276791956477,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "betty crocker": 0.552593075385026,
                "sara lee": 0.1838512560283458,
                "little debbie": 0.26355566858662827
            },
            "question": "which brand mascot was not a real person?",
            "rate_limited": false,
            "answers": [
                "little debbie",
                "sara lee",
                "betty crocker"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "betty crocker": 0.0,
                "sara lee": 0.0,
                "little debbie": 0.0
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3350907458449428,
                    1.1009313975115753,
                    1.5639778566434819
                ],
                "result_count_important_words": [
                    162000.0,
                    163000.0,
                    660000.0
                ],
                "wikipedia_search": [
                    1.574074074074074,
                    0.6342592592592593,
                    1.7916666666666667
                ],
                "word_count_appended_bing": [
                    23.0,
                    10.0,
                    30.0
                ],
                "answer_relation_to_question_bing": [
                    1.4903225806451612,
                    1.0838709677419356,
                    0.4258064516129032
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    1.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    163000.0,
                    95100.0,
                    2920000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    2.0
                ],
                "result_count_bing": [
                    972000.0,
                    588000.0,
                    530000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_count_appended": [
                    151.0,
                    99.0,
                    92.0
                ],
                "answer_relation_to_question": [
                    0.721027064538515,
                    1.1797610786160404,
                    1.0992118568454448
                ],
                "result_count": [
                    163000.0,
                    95100.0,
                    85500.0
                ]
            },
            "integer_answers": {
                "betty crocker": 8,
                "sara lee": 1,
                "little debbie": 4
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What iconic painting once hung in Napoleon's bedroom?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "mona lisa"
            ],
            "question": "what iconic painting once hung in napoleon's bedroom?",
            "answers": [
                "the starry night",
                "mona lisa",
                "the birth of venus"
            ],
            "integer_answers": {
                "the birth of venus": 2,
                "mona lisa": 7,
                "the starry night": 5
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.386688318629934,
                    1.9813390594373312,
                    1.6319726219327346
                ],
                "result_count_important_words": [
                    2980000.0,
                    148000.0,
                    227000.0
                ],
                "wikipedia_search": [
                    2.1902173913043477,
                    0.6177536231884058,
                    0.19202898550724637
                ],
                "word_count_appended_bing": [
                    4.0,
                    38.0,
                    11.0
                ],
                "answer_relation_to_question_bing": [
                    0.425,
                    0.325,
                    1.25
                ],
                "word_count_appended": [
                    45.0,
                    216.0,
                    75.0
                ],
                "question_related_to_answer": [
                    0.15384615384615385,
                    1.8461538461538463,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    5870000.0,
                    296000.0,
                    3110000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    23.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    48.0,
                    1.0
                ],
                "result_count_bing": [
                    114000.0,
                    46600.0,
                    82000.0
                ],
                "result_count": [
                    6050000.0,
                    190000.0,
                    2140000.0
                ],
                "answer_relation_to_question": [
                    1.6016260162601625,
                    0.7615176151761518,
                    2.6368563685636857
                ]
            },
            "negative_question": false,
            "fraction_answers": {
                "the birth of venus": 0.21656576098398345,
                "mona lisa": 0.4591865654438973,
                "the starry night": 0.3242476735721192
            },
            "lines": [
                [
                    0.3203252032520325,
                    0.2125,
                    0.07692307692307693,
                    0.0,
                    0.7219570405727923,
                    0.4699093157460841,
                    0.8882265275707899,
                    0.6328158689090125,
                    0.7300724637681159,
                    0.13392857142857142,
                    0.07547169811320754,
                    0.2773376637259868,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.15230352303523037,
                    0.1625,
                    0.9230769230769231,
                    1.0,
                    0.022673031026252982,
                    0.19208573784006594,
                    0.044113263785394936,
                    0.031910306166451054,
                    0.20591787439613526,
                    0.6428571428571429,
                    0.7169811320754716,
                    0.39626781188746624,
                    0.9583333333333334,
                    0.9795918367346939,
                    1.0
                ],
                [
                    0.5273712737127372,
                    0.625,
                    0.0,
                    0.0,
                    0.2553699284009546,
                    0.33800494641385,
                    0.0676602086438152,
                    0.33527382492453645,
                    0.06400966183574879,
                    0.22321428571428573,
                    0.20754716981132076,
                    0.3263945243865469,
                    0.041666666666666664,
                    0.02040816326530612,
                    1.0
                ]
            ],
            "rate_limited": false,
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "the birth of venus": 0.23776301657930676,
                "mona lisa": 0.9286036508893303,
                "the starry night": 0.12375658456009626
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these is NOT among the four \u201cC\u2019s\u201d of diamond buying?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "color",
                "cut",
                "core"
            ],
            "lines": [
                [
                    0.3347904185572209,
                    0.3536389967338387,
                    0.09834343028843229,
                    0.09952801871049262,
                    0.25784862496957894,
                    0.25249500998003993,
                    0.2713178294573644,
                    0.21817343173431736,
                    0.15657439199123635,
                    0.2746947835738069,
                    0.1958174904942966,
                    0.3263765962188272,
                    0.21457489878542513,
                    0.19636363636363635,
                    -1.0
                ],
                [
                    0.2684989740212824,
                    0.3019350465273926,
                    0.42385339748346573,
                    0.47435568508237497,
                    0.27731808225845705,
                    0.3592814371257485,
                    0.38019732205778717,
                    0.34732472324723246,
                    0.45210846887589246,
                    0.3118756936736959,
                    0.4068441064638783,
                    0.32074049452039927,
                    0.2854251012145749,
                    0.30363636363636365,
                    -1.0
                ],
                [
                    0.3967106074214966,
                    0.34442595673876875,
                    0.477803172228102,
                    0.4261162962071325,
                    0.464833292771964,
                    0.38822355289421157,
                    0.3484848484848485,
                    0.4345018450184502,
                    0.39131713913287125,
                    0.4134295227524972,
                    0.3973384030418251,
                    0.3528829092607735,
                    0.5,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "color": 0.5356374917344982,
                "core": 0.1662760648638655,
                "cut": 0.29808644340163637
            },
            "question": "which of these is not among the four \u201cc\u2019s\u201d of diamond buying?",
            "rate_limited": false,
            "answers": [
                "color",
                "cut",
                "core"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "color": 0.0,
                "core": 0.0,
                "cut": 0.0
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.0417404226870368,
                    1.0755570328776043,
                    0.8827025444353588
                ],
                "result_count_important_words": [
                    6490000.0,
                    3400000.0,
                    4300000.0
                ],
                "wikipedia_search": [
                    1.3737024320350546,
                    0.19156612449643015,
                    0.4347314434685151
                ],
                "word_count_appended_bing": [
                    160.0,
                    49.0,
                    54.0
                ],
                "answer_relation_to_question_bing": [
                    0.29272200653232266,
                    0.39612990694521477,
                    0.31114808652246256
                ],
                "question_related_to_answer": [
                    2.4099394182694063,
                    0.4568796150992056,
                    0.13318096663138815
                ],
                "question_related_to_answer_bing": [
                    2.4028318877370443,
                    0.15386588950575059,
                    0.4433022227572054
                ],
                "result_count_noun_chunks": [
                    611000.0,
                    331000.0,
                    142000.0
                ],
                "word_count_noun_chunks": [
                    141.0,
                    106.0,
                    0.0
                ],
                "word_count_raw": [
                    167.0,
                    108.0,
                    0.0
                ],
                "result_count_bing": [
                    24800000.0,
                    14100000.0,
                    11200000.0
                ],
                "word_count_appended": [
                    406.0,
                    339.0,
                    156.0
                ],
                "answer_relation_to_question": [
                    0.9912574886566745,
                    1.3890061558723052,
                    0.6197363554710202
                ],
                "result_count": [
                    1990000.0,
                    1830000.0,
                    289000.0
                ]
            },
            "integer_answers": {
                "color": 11,
                "core": 0,
                "cut": 3
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What is the correct pronunciation of the performer who sings \u201cSmooth Operator\u201d?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "shah-day"
            ],
            "lines": [
                [
                    0.5,
                    0.3333333333333333,
                    0,
                    0,
                    0.7912087912087912,
                    0.22736093143596378,
                    0.631578947368421,
                    0.43503937007874016,
                    0.0,
                    0.75,
                    0.35714285714285715,
                    0.6881450895778238,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.0,
                    0.3333333333333333,
                    0,
                    0,
                    0.14285714285714285,
                    0.5756791720569211,
                    0.21052631578947367,
                    0.001968503937007874,
                    0.0,
                    0.12777777777777777,
                    0.32142857142857145,
                    0.1725624188105975,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.5,
                    0.3333333333333333,
                    0,
                    0,
                    0.06593406593406594,
                    0.19695989650711512,
                    0.15789473684210525,
                    0.562992125984252,
                    1.0,
                    0.12222222222222222,
                    0.32142857142857145,
                    0.13929249161157853,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "sayd": 0.15717776966590213,
                "shah-day": 0.5594841100121609,
                "say-dee": 0.283338120321937
            },
            "question": "what is the correct pronunciation of the performer who sings \u201csmooth operator\u201d?",
            "rate_limited": false,
            "answers": [
                "shah-day",
                "sayd",
                "say-dee"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sayd": 0.16395200445625363,
                "shah-day": 0.8420550140969041,
                "say-dee": 0.1142818667775396
            },
            "integer_answers": {
                "sayd": 1,
                "shah-day": 9,
                "say-dee": 2
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    4.128870537466944,
                    1.0353745128635852,
                    0.8357549496694713
                ],
                "result_count_important_words": [
                    36.0,
                    12.0,
                    9.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_appended_bing": [
                    30.0,
                    27.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    0.3333333333333333,
                    0.3333333333333333,
                    0.3333333333333333
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    2210.0,
                    10.0,
                    2860.0
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    70300.0,
                    178000.0,
                    60900.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    72.0,
                    13.0,
                    6.0
                ],
                "answer_relation_to_question": [
                    1.0,
                    0.0,
                    1.0
                ],
                "word_count_appended": [
                    135.0,
                    23.0,
                    22.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "In which version of \u201cDragnet\u201d is the line \u201cJust the facts, ma\u2019am\u201d first said?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "'80s movie"
            ],
            "lines": [
                [
                    0.4179414093428345,
                    0.3159420289855073,
                    0,
                    0,
                    0.3170731707317073,
                    0.1810344827586207,
                    0.2222222222222222,
                    0.3982777179763186,
                    0.012077294685990338,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.4160423139743523,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.147458432304038,
                    0.3405797101449275,
                    0,
                    0,
                    0.21951219512195122,
                    0.22413793103448276,
                    0.18518518518518517,
                    0.4117330462863294,
                    0.8140096618357487,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.299809775843622,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.4346001583531275,
                    0.34347826086956523,
                    0,
                    0,
                    0.4634146341463415,
                    0.5948275862068966,
                    0.5925925925925926,
                    0.189989235737352,
                    0.17391304347826086,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.28414791018202556,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "'80s movie": 0.3743630088232829,
                "'50s movie": 0.33090926044229513,
                "'50s tv show": 0.294727730734422
            },
            "question": "in which version of \u201cdragnet\u201d is the line \u201cjust the facts, ma\u2019am\u201d first said?",
            "rate_limited": false,
            "answers": [
                "'50s tv show",
                "'50s movie",
                "'80s movie"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "'80s movie": 0.34000000019868215,
                "'50s movie": 0.29999999900658925,
                "'50s tv show": 0.3194444440305233
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.080211569871762,
                    1.4990488792181103,
                    1.4207395509101282
                ],
                "result_count_important_words": [
                    12.0,
                    10.0,
                    32.0
                ],
                "wikipedia_search": [
                    0.036231884057971016,
                    2.442028985507246,
                    0.5217391304347826
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.6318840579710145,
                    0.681159420289855,
                    0.6869565217391305
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    148000.0,
                    153000.0,
                    70600.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    105000.0,
                    130000.0,
                    345000.0
                ],
                "word_count_appended": [
                    3.0,
                    3.0,
                    3.0
                ],
                "answer_relation_to_question": [
                    2.0897070467141727,
                    0.73729216152019,
                    2.1730007917656375
                ],
                "result_count": [
                    13.0,
                    9.0,
                    19.0
                ]
            },
            "integer_answers": {
                "'80s movie": 5,
                "'50s movie": 2,
                "'50s tv show": 3
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these companies went public first?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "facebook"
            ],
            "lines": [
                [
                    0.55,
                    0.5,
                    1.0,
                    1.0,
                    0.9723128001422897,
                    0.31470588235294117,
                    0.9817279809943052,
                    0.9863447646931772,
                    0.5581804281345565,
                    0.2832498608792432,
                    0.2561576354679803,
                    0.34304329980042697,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.18333333333333335,
                    0.0,
                    0.0,
                    0.0,
                    0.015562933538862869,
                    0.3411764705882353,
                    0.009782342871117632,
                    0.007151599510427416,
                    0.12904106596767148,
                    0.36282693377851977,
                    0.35467980295566504,
                    0.3134721642068133,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.26666666666666666,
                    0.5,
                    0.0,
                    0.0,
                    0.012124266318847454,
                    0.34411764705882353,
                    0.008489676134577089,
                    0.006503635796395402,
                    0.31277850589777195,
                    0.35392320534223703,
                    0.3891625615763547,
                    0.34348453599275963,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "facebook": 0.6961230466046372,
                "ferrari": 0.122644760482189,
                "alibaba": 0.1812321929131738
            },
            "question": "which of these companies went public first?",
            "rate_limited": false,
            "answers": [
                "facebook",
                "ferrari",
                "alibaba"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "facebook": 0.5970400342622877,
                "ferrari": 0.470841708969926,
                "alibaba": 0.49541120101341485
            },
            "integer_answers": {
                "facebook": 10,
                "ferrari": 1,
                "alibaba": 3
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.029129899401281,
                    0.9404164926204399,
                    1.030453607978279
                ],
                "result_count_important_words": [
                    281000000.0,
                    2800000.0,
                    2430000.0
                ],
                "wikipedia_search": [
                    1.6745412844036696,
                    0.3871231979030144,
                    0.9383355176933159
                ],
                "word_count_appended_bing": [
                    52.0,
                    72.0,
                    79.0
                ],
                "answer_relation_to_question_bing": [
                    1.0,
                    0.0,
                    1.0
                ],
                "question_related_to_answer": [
                    1.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    411000000.0,
                    2980000.0,
                    2710000.0
                ],
                "word_count_noun_chunks": [
                    7.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    107000000.0,
                    116000000.0,
                    117000000.0
                ],
                "word_count_raw": [
                    7.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    328000000.0,
                    5250000.0,
                    4090000.0
                ],
                "answer_relation_to_question": [
                    1.1,
                    0.3666666666666667,
                    0.5333333333333333
                ],
                "word_count_appended": [
                    509.0,
                    652.0,
                    636.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "What topic would a herpetologist study?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "crocodile teeth"
            ],
            "lines": [
                [
                    0.21839080459770113,
                    0.6666666666666666,
                    0,
                    0,
                    0.9669656530299715,
                    0.9072431633407243,
                    0.9160908193484699,
                    0.4864161379534732,
                    0.7333333333333334,
                    0.4117647058823529,
                    0.42857142857142855,
                    0.42615615213479896,
                    0,
                    0,
                    1.0
                ],
                [
                    0.10344827586206896,
                    0.0,
                    0,
                    0,
                    0.022861518267337563,
                    0.043237250554323724,
                    0.039289239881539983,
                    0.38718073857166097,
                    0.0,
                    0.29411764705882354,
                    0.2857142857142857,
                    0.26900990400527963,
                    0,
                    0,
                    1.0
                ],
                [
                    0.6781609195402298,
                    0.3333333333333333,
                    0,
                    0,
                    0.010172828702690878,
                    0.04951958610495196,
                    0.04461994076999013,
                    0.1264031234748658,
                    0.26666666666666666,
                    0.29411764705882354,
                    0.2857142857142857,
                    0.3048339438599214,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "venereal disease": 0.616159886485892,
                "mushroom farming": 0.144485885991532,
                "crocodile teeth": 0.2393542275225759
            },
            "question": "what topic would a herpetologist study?",
            "rate_limited": false,
            "answers": [
                "venereal disease",
                "mushroom farming",
                "crocodile teeth"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "venereal disease": 0.31499999945362406,
                "mushroom farming": 0.29999999900658925,
                "crocodile teeth": 0.3194444440305233
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2784684564043969,
                    0.807029712015839,
                    0.9145018315797642
                ],
                "result_count_important_words": [
                    46400.0,
                    1990.0,
                    2260.0
                ],
                "wikipedia_search": [
                    1.4666666666666668,
                    0.0,
                    0.5333333333333333
                ],
                "word_count_appended_bing": [
                    3.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.6666666666666666,
                    0.0,
                    0.3333333333333333
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    299000.0,
                    238000.0,
                    77700.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    491000.0,
                    23400.0,
                    26800.0
                ],
                "word_count_appended": [
                    35.0,
                    25.0,
                    25.0
                ],
                "answer_relation_to_question": [
                    0.43678160919540227,
                    0.20689655172413793,
                    1.3563218390804597
                ],
                "result_count": [
                    8840.0,
                    209.0,
                    93.0
                ]
            },
            "integer_answers": {
                "venereal disease": 9,
                "mushroom farming": 0,
                "crocodile teeth": 1
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "In the U.K., who appoints the Prime Minister?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "the parliament"
            ],
            "lines": [
                [
                    0.08333333333333333,
                    0.0,
                    0.5,
                    0.5,
                    0.4540569586243955,
                    0.44729849424269263,
                    0.3319165378670788,
                    0.9127906976744186,
                    0.0,
                    0.2297872340425532,
                    0.3,
                    0.34755608047467274,
                    0.3333333333333333,
                    0.06666666666666667,
                    0.0
                ],
                [
                    0.9166666666666667,
                    0.325,
                    0.3333333333333333,
                    0.0,
                    0.3197205803331542,
                    0.254207263064659,
                    0.3786707882534776,
                    0.05116279069767442,
                    0.393939393939394,
                    0.23404255319148937,
                    0.3,
                    0.31907152109782755,
                    0.0,
                    0.06666666666666667,
                    0.0
                ],
                [
                    0.0,
                    0.675,
                    0.16666666666666666,
                    0.5,
                    0.2262224610424503,
                    0.29849424269264835,
                    0.2894126738794436,
                    0.03604651162790698,
                    0.6060606060606061,
                    0.5361702127659574,
                    0.4,
                    0.33337239842749977,
                    0.6666666666666666,
                    0.8666666666666667,
                    0.0
                ]
            ],
            "fraction_answers": {
                "the parliament": 0.27803439694602455,
                "the people": 0.3219099525899389,
                "the queen": 0.40005565046403657
            },
            "question": "in the u.k., who appoints the prime minister?",
            "rate_limited": false,
            "answers": [
                "the people",
                "the parliament",
                "the queen"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "the parliament": 0.3194444440305233,
                "the people": 0.31499999945362406,
                "the queen": 0.31499999945362406
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3902243218986907,
                    1.27628608439131,
                    1.3334895937099989
                ],
                "result_count_important_words": [
                    859000.0,
                    980000.0,
                    749000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.7878787878787878,
                    1.212121212121212
                ],
                "word_count_appended_bing": [
                    6.0,
                    6.0,
                    8.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.65,
                    1.35
                ],
                "question_related_to_answer": [
                    1.5,
                    1.0,
                    0.5
                ],
                "question_related_to_answer_bing": [
                    1.0,
                    0.0,
                    1.0
                ],
                "result_count_noun_chunks": [
                    47100000.0,
                    2640000.0,
                    1860000.0
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    4.0
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    13.0
                ],
                "result_count_bing": [
                    5050000.0,
                    2870000.0,
                    3370000.0
                ],
                "word_count_appended": [
                    54.0,
                    55.0,
                    126.0
                ],
                "answer_relation_to_question": [
                    0.16666666666666666,
                    1.8333333333333335,
                    0.0
                ],
                "result_count": [
                    1690000.0,
                    1190000.0,
                    842000.0
                ]
            },
            "integer_answers": {
                "the parliament": 2,
                "the people": 6,
                "the queen": 6
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which writer has stated that his/her trademark series of books would never be adapted for film?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "jeff kinney"
            ],
            "lines": [
                [
                    0.2810185062130374,
                    0.2378844169246646,
                    0.0,
                    0,
                    0.05370462456489311,
                    0.23942366646229307,
                    0.31203007518796994,
                    0.09674898405751797,
                    0.19386621622785488,
                    0.2972027972027972,
                    0.16666666666666669,
                    0.32037285418143135,
                    0.25,
                    0,
                    -1.0
                ],
                [
                    0.3502880806755902,
                    0.37907120743034056,
                    0.5,
                    0,
                    0.47345847836897065,
                    0.4678111587982833,
                    0.34536340852130326,
                    0.458893404188809,
                    0.3061337837721452,
                    0.32867132867132864,
                    0.36904761904761907,
                    0.3434002094341234,
                    0.5,
                    0,
                    -1.0
                ],
                [
                    0.36869341311137244,
                    0.38304437564499483,
                    0.5,
                    0,
                    0.47283689706613624,
                    0.29276517473942365,
                    0.3426065162907268,
                    0.444357611753673,
                    0.5,
                    0.3741258741258741,
                    0.4642857142857143,
                    0.33622693638444523,
                    0.25,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "jeff kinney": 0.2118429144329399,
                "sue grafton": 0.19631022018191446,
                "james patterson": 0.5918468653851457
            },
            "question": "which writer has stated that his/her trademark series of books would never be adapted for film?",
            "rate_limited": false,
            "answers": [
                "james patterson",
                "sue grafton",
                "jeff kinney"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "jeff kinney": 0.2374910837227541,
                "sue grafton": 0.22610830839745638,
                "james patterson": 0.17335312190188346
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.5147800414599613,
                    2.192397067922272,
                    2.292822890617767
                ],
                "result_count_important_words": [
                    75000.0,
                    61700.0,
                    62800.0
                ],
                "wikipedia_search": [
                    3.6736054052657416,
                    2.3263945947342584,
                    0.0
                ],
                "word_count_appended_bing": [
                    28.0,
                    11.0,
                    3.0
                ],
                "answer_relation_to_question_bing": [
                    2.621155830753354,
                    1.2092879256965945,
                    1.1695562435500515
                ],
                "question_related_to_answer": [
                    1.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    25800.0,
                    2630.0,
                    3560.0
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    42500.0,
                    5250.0,
                    33800.0
                ],
                "word_count_appended": [
                    58.0,
                    49.0,
                    36.0
                ],
                "answer_relation_to_question": [
                    2.1898149378696257,
                    1.4971191932440981,
                    1.3130658688862757
                ],
                "result_count": [
                    7180.0,
                    427.0,
                    437.0
                ]
            },
            "integer_answers": {
                "jeff kinney": 0,
                "sue grafton": 0,
                "james patterson": 12
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "In Harry Potter's Quidditch, what ALWAYS happens when one team catches the snitch?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "the game ends"
            ],
            "lines": [
                [
                    0.25728155339805825,
                    0.6187845303867404,
                    0,
                    0,
                    0.19444444444444445,
                    0.22088655146506386,
                    0.024731182795698924,
                    0.01224105461393597,
                    0.0625,
                    0.09433962264150944,
                    0.3333333333333333,
                    0.16188830411412294,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.5787590241473737,
                    0.37569060773480667,
                    0,
                    0,
                    0.046296296296296294,
                    0.2201352366641623,
                    0.0064516129032258064,
                    0.003766478342749529,
                    0.9375,
                    0.09433962264150944,
                    0.3333333333333333,
                    0.09902940447502051,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.1639594224545681,
                    0.005524861878453039,
                    0,
                    0,
                    0.7592592592592593,
                    0.5589782118707739,
                    0.9688172043010753,
                    0.9839924670433146,
                    0.0,
                    0.8113207547169812,
                    0.3333333333333333,
                    0.7390822914108566,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "the game ends": 0.6103556505223847,
                "that team loses": 0.22460846804487314,
                "that team wins": 0.16503588143274228
            },
            "question": "in harry potter's quidditch, what always happens when one team catches the snitch?",
            "rate_limited": false,
            "answers": [
                "that team wins",
                "that team loses",
                "the game ends"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "the game ends": 0.3952380970830009,
                "that team loses": 0.3194444440305233,
                "that team wins": 0.3194444440305233
            },
            "integer_answers": {
                "the game ends": 8,
                "that team loses": 2,
                "that team wins": 2
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1332181287988607,
                    0.6932058313251437,
                    5.1735760398759965
                ],
                "result_count_important_words": [
                    23.0,
                    6.0,
                    901.0
                ],
                "wikipedia_search": [
                    0.0625,
                    0.9375,
                    0.0
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.6187845303867403,
                    0.3756906077348066,
                    0.0055248618784530384
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    26.0,
                    8.0,
                    2090.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    5.0
                ],
                "result_count_bing": [
                    29400.0,
                    29300.0,
                    74400.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    8.0
                ],
                "result_count": [
                    21.0,
                    5.0,
                    82.0
                ],
                "answer_relation_to_question": [
                    0.5145631067961165,
                    1.1575180482947474,
                    0.3279188449091362
                ],
                "word_count_appended": [
                    5.0,
                    5.0,
                    43.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Laurie Metcalf, Amy Morton and Tracy Letts are members of a theatre company from what city?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "new york",
                "los angeles",
                "chicago"
            ],
            "lines": [
                [
                    0.4240707253365481,
                    0.36594202898550726,
                    0.19498739495798317,
                    0.34375,
                    0.38724373576309795,
                    0.3783251231527094,
                    0.5607278882776132,
                    0.6023564064801178,
                    0.2384799271652288,
                    0.1348973607038123,
                    0.057692307692307696,
                    0.34529045661879926,
                    0.0743801652892562,
                    0.061946902654867256,
                    1.0
                ],
                [
                    0.20146674703636727,
                    0.3347826086956522,
                    0.20491876750700283,
                    0.15,
                    0.25683371298405466,
                    0.4266009852216749,
                    0.1917054591620821,
                    0.16936671575846834,
                    0.2840352524392133,
                    0.06744868035190615,
                    0.09615384615384616,
                    0.3129283553239001,
                    0.008264462809917356,
                    0.008849557522123894,
                    1.0
                ],
                [
                    0.3744625276270846,
                    0.29927536231884055,
                    0.6000938375350141,
                    0.50625,
                    0.3559225512528474,
                    0.19507389162561575,
                    0.2475666525603047,
                    0.22827687776141384,
                    0.47748482039555795,
                    0.7976539589442815,
                    0.8461538461538461,
                    0.3417811880573007,
                    0.9173553719008265,
                    0.9292035398230089,
                    1.0
                ]
            ],
            "fraction_answers": {
                "new york": 0.29786360164841774,
                "los angeles": 0.19381108221187213,
                "chicago": 0.5083253161397102
            },
            "question": "laurie metcalf, amy morton and tracy letts are members of a theatre company from what city?",
            "rate_limited": false,
            "answers": [
                "new york",
                "los angeles",
                "chicago"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "new york": 0.0,
                "los angeles": 0.0,
                "chicago": 0.0
            },
            "integer_answers": {
                "new york": 6,
                "los angeles": 1,
                "chicago": 7
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.4529045661879927,
                    3.1292835532390013,
                    3.417811880573007
                ],
                "result_count_important_words": [
                    2650.0,
                    906.0,
                    1170.0
                ],
                "wikipedia_search": [
                    0.9539197086609152,
                    1.1361410097568532,
                    1.9099392815822318
                ],
                "word_count_appended_bing": [
                    3.0,
                    5.0,
                    44.0
                ],
                "answer_relation_to_question_bing": [
                    1.463768115942029,
                    1.3391304347826087,
                    1.1971014492753622
                ],
                "question_related_to_answer": [
                    0.9749369747899159,
                    1.024593837535014,
                    3.0004691876750704
                ],
                "question_related_to_answer_bing": [
                    1.375,
                    0.6,
                    2.025
                ],
                "result_count_noun_chunks": [
                    4090.0,
                    1150.0,
                    1550.0
                ],
                "word_count_noun_chunks": [
                    9.0,
                    1.0,
                    111.0
                ],
                "result_count_bing": [
                    76800.0,
                    86600.0,
                    39600.0
                ],
                "word_count_raw": [
                    7.0,
                    1.0,
                    105.0
                ],
                "result_count": [
                    1360.0,
                    902.0,
                    1250.0
                ],
                "answer_relation_to_question": [
                    2.5444243520192886,
                    1.2088004822182037,
                    2.2467751657625077
                ],
                "word_count_appended": [
                    46.0,
                    23.0,
                    272.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which artist painted the ceiling of one of France's most iconic opera houses?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "marc chagall"
            ],
            "lines": [
                [
                    0.5692901234567902,
                    0.456140350877193,
                    0.5,
                    0,
                    0.5781659388646289,
                    0.2571245597182197,
                    0.009790459604585999,
                    0.9413173652694611,
                    0.8431401931401932,
                    0.6748768472906403,
                    0.7755102040816326,
                    0.3885394556681672,
                    0.9523809523809523,
                    1.0,
                    -1.0
                ],
                [
                    0.14622464726631393,
                    0.3969298245614035,
                    0.5,
                    0,
                    0.2480349344978166,
                    0.40025616394492475,
                    0.6307422664299988,
                    0.034491017964071856,
                    0.07062937062937064,
                    0.18226600985221675,
                    0.14285714285714285,
                    0.30566917092690254,
                    0.047619047619047616,
                    0.0,
                    -1.0
                ],
                [
                    0.28448522927689596,
                    0.1469298245614035,
                    0.0,
                    0,
                    0.1737991266375546,
                    0.34261927633685557,
                    0.3594672739654152,
                    0.024191616766467066,
                    0.08623043623043623,
                    0.14285714285714285,
                    0.08163265306122448,
                    0.3057913734049302,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "marc chagall": 0.6112520346424973,
                "edgar degas": 0.23890150742686228,
                "auguste renoir": 0.14984645793064044
            },
            "question": "which artist painted the ceiling of one of france's most iconic opera houses?",
            "rate_limited": false,
            "answers": [
                "marc chagall",
                "edgar degas",
                "auguste renoir"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "marc chagall": 0.8893012113213029,
                "edgar degas": 0.1669355041940768,
                "auguste renoir": 0.0966681836728696
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.719776189677171,
                    2.139684196488318,
                    2.1405396138345116
                ],
                "result_count_important_words": [
                    87700.0,
                    5650000.0,
                    3220000.0
                ],
                "wikipedia_search": [
                    5.058841158841159,
                    0.4237762237762238,
                    0.5173826173826174
                ],
                "word_count_appended_bing": [
                    38.0,
                    7.0,
                    4.0
                ],
                "answer_relation_to_question_bing": [
                    1.368421052631579,
                    1.1907894736842106,
                    0.4407894736842105
                ],
                "question_related_to_answer": [
                    1.0,
                    1.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    3930000.0,
                    144000.0,
                    101000.0
                ],
                "word_count_noun_chunks": [
                    20.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    29.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    80300.0,
                    125000.0,
                    107000.0
                ],
                "word_count_appended": [
                    137.0,
                    37.0,
                    29.0
                ],
                "answer_relation_to_question": [
                    3.415740740740741,
                    0.8773478835978836,
                    1.7069113756613756
                ],
                "result_count": [
                    331000.0,
                    142000.0,
                    99500.0
                ]
            },
            "integer_answers": {
                "marc chagall": 11,
                "edgar degas": 2,
                "auguste renoir": 0
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these two U.S. cities are in the same time zone?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "el paso / pierre",
                "bismarck / cheyenne",
                "pensacola / sioux falls"
            ],
            "lines": [
                [
                    0.2903698979591837,
                    0.2594086021505377,
                    0,
                    0,
                    0.9999356477058408,
                    0.3333333333333333,
                    0.0011718214343593005,
                    0.9998671272920542,
                    0.014492753623188406,
                    0.46153846153846156,
                    0.3333333333333333,
                    0.7480352720923842,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.4238732993197279,
                    0.49731182795698925,
                    0,
                    0,
                    6.435229415928678e-05,
                    0.3333333333333333,
                    0.00028672226585387136,
                    0.00013287270794578793,
                    0.7671497584541062,
                    0.3076923076923077,
                    0.3333333333333333,
                    0.16373851470834228,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.28575680272108844,
                    0.24327956989247315,
                    0,
                    0,
                    0.0,
                    0.3333333333333333,
                    0.9985414562997869,
                    0.0,
                    0.2183574879227053,
                    0.23076923076923078,
                    0.3333333333333333,
                    0.08822621319927354,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "pensacola / sioux falls": 0.2731597427471225,
                "el paso / pierre": 0.4441486250462677,
                "bismarck / cheyenne": 0.28269163220660987
            },
            "question": "which of these two u.s. cities are in the same time zone?",
            "rate_limited": false,
            "answers": [
                "el paso / pierre",
                "bismarck / cheyenne",
                "pensacola / sioux falls"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "pensacola / sioux falls": 0.0,
                "el paso / pierre": 0.0,
                "bismarck / cheyenne": 0.0
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.9921410883695367,
                    0.6549540588333691,
                    0.35290485279709416
                ],
                "result_count_important_words": [
                    94.0,
                    23.0,
                    80100.0
                ],
                "wikipedia_search": [
                    0.043478260869565216,
                    2.3014492753623186,
                    0.6550724637681159
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.0376344086021505,
                    1.9892473118279568,
                    0.9731182795698925
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    90300.0,
                    12.0,
                    0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    12500000.0,
                    12500000.0,
                    12500000.0
                ],
                "word_count_appended": [
                    6.0,
                    4.0,
                    3.0
                ],
                "answer_relation_to_question": [
                    1.1614795918367349,
                    1.6954931972789116,
                    1.1430272108843538
                ],
                "result_count": [
                    202000.0,
                    13.0,
                    0
                ]
            },
            "integer_answers": {
                "pensacola / sioux falls": 1,
                "el paso / pierre": 6,
                "bismarck / cheyenne": 3
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Until it was banned, lithium was a key ingredient in which of these brands?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "cracker jack",
                "7up",
                "good and plenty"
            ],
            "lines": [
                [
                    0.0661764705882353,
                    0.09855072463768116,
                    0,
                    0,
                    0.1543866693435053,
                    0.26851851851851855,
                    0.5601719197707736,
                    0.5986622073578596,
                    0.13513513513513514,
                    0.05454545454545454,
                    0.21428571428571427,
                    0.3534187673397574,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.8575851393188855,
                    0.5256038647342995,
                    0,
                    0,
                    0.8452118048584621,
                    0.27116402116402116,
                    0.4383954154727794,
                    0.3979933110367893,
                    0.8648648648648649,
                    0.9212121212121213,
                    0.7380952380952381,
                    0.5394817953191229,
                    1.0,
                    1.0,
                    -1.0
                ],
                [
                    0.07623839009287925,
                    0.3758454106280193,
                    0,
                    0,
                    0.0004015257980325236,
                    0.4603174603174603,
                    0.0014326647564469914,
                    0.0033444816053511705,
                    0.0,
                    0.024242424242424242,
                    0.047619047619047616,
                    0.10709943734111964,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "cracker jack": 0.20865429846021957,
                "7up": 0.6999672980063819,
                "good and plenty": 0.09137840353339843
            },
            "question": "until it was banned, lithium was a key ingredient in which of these brands?",
            "rate_limited": false,
            "answers": [
                "cracker jack",
                "7up",
                "good and plenty"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "cracker jack": 0.0,
                "7up": 0.0,
                "good and plenty": 0.0
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7670938366987872,
                    2.6974089765956144,
                    0.5354971867055982
                ],
                "result_count_important_words": [
                    782.0,
                    612.0,
                    2.0
                ],
                "wikipedia_search": [
                    0.40540540540540543,
                    2.5945945945945947,
                    0.0
                ],
                "word_count_appended_bing": [
                    9.0,
                    31.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.2956521739130435,
                    1.5768115942028986,
                    1.1275362318840578
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    1790.0,
                    1190.0,
                    10.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    11.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    9.0,
                    0.0
                ],
                "result_count_bing": [
                    20300.0,
                    20500.0,
                    34800.0
                ],
                "word_count_appended": [
                    9.0,
                    152.0,
                    4.0
                ],
                "answer_relation_to_question": [
                    0.2647058823529412,
                    3.430340557275542,
                    0.304953560371517
                ],
                "result_count": [
                    769.0,
                    4210.0,
                    2.0
                ]
            },
            "integer_answers": {
                "cracker jack": 2,
                "7up": 9,
                "good and plenty": 1
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Aside from blood cells, what would you also find inside your blood vessels?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "plasma"
            ],
            "lines": [
                [
                    0.37961969480209823,
                    0.4494318181818182,
                    0.6666666666666666,
                    0.75,
                    0.5472030339970202,
                    0.3870967741935484,
                    0.5343845816907578,
                    0.5250911554679908,
                    0.08711650922177237,
                    0.37073863636363635,
                    0.3148148148148148,
                    0.38794416823215455,
                    0.5862068965517241,
                    0.75,
                    1.0
                ],
                [
                    0.47917660149419805,
                    0.3255681818181818,
                    0.3333333333333333,
                    0.25,
                    0.42259244209670865,
                    0.34535104364326374,
                    0.4178712220762155,
                    0.47249289031924474,
                    0.8813045434098065,
                    0.4460227272727273,
                    0.4351851851851852,
                    0.3502675844442184,
                    0.41379310344827586,
                    0.25,
                    1.0
                ],
                [
                    0.1412037037037037,
                    0.225,
                    0.0,
                    0.0,
                    0.030204523906271163,
                    0.2675521821631879,
                    0.04774419623302672,
                    0.00241595421276444,
                    0.031578947368421054,
                    0.18323863636363635,
                    0.25,
                    0.261788247323627,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "marrow": 0.41592563275295413,
                "plasma": 0.4811653392988573,
                "plastids": 0.10290902794818844
            },
            "question": "aside from blood cells, what would you also find inside your blood vessels?",
            "rate_limited": false,
            "answers": [
                "plasma",
                "marrow",
                "plastids"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "marrow": 0.34000000019868215,
                "plasma": 0.354545455177625,
                "plastids": 0.29999999900658925
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.3276650093929274,
                    2.1016055066653103,
                    1.5707294839417618
                ],
                "result_count_important_words": [
                    122000.0,
                    95400.0,
                    10900.0
                ],
                "wikipedia_search": [
                    0.4355825461088619,
                    4.406522717049032,
                    0.15789473684210525
                ],
                "word_count_appended_bing": [
                    34.0,
                    47.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    1.7977272727272728,
                    1.3022727272727272,
                    0.9
                ],
                "question_related_to_answer": [
                    1.3333333333333333,
                    0.6666666666666666,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.75,
                    0.25,
                    0.0
                ],
                "result_count_noun_chunks": [
                    5890000.0,
                    5300000.0,
                    27100.0
                ],
                "word_count_noun_chunks": [
                    17.0,
                    12.0,
                    0.0
                ],
                "result_count_bing": [
                    4080000.0,
                    3640000.0,
                    2820000.0
                ],
                "word_count_raw": [
                    12.0,
                    4.0,
                    0.0
                ],
                "word_count_appended": [
                    261.0,
                    314.0,
                    129.0
                ],
                "answer_relation_to_question": [
                    1.518478779208393,
                    1.9167064059767922,
                    0.5648148148148148
                ],
                "result_count": [
                    404000.0,
                    312000.0,
                    22300.0
                ]
            },
            "integer_answers": {
                "marrow": 4,
                "plasma": 10,
                "plastids": 0
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these Kentucky Derby winners was named for its trainer?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "clyde van dusen"
            ],
            "lines": [
                [
                    0.822745915884602,
                    0.6926765475152572,
                    0,
                    0,
                    0.008573312816003518,
                    0.3529062870699881,
                    0.3683623440296765,
                    0.01032681326691775,
                    0.7333333333333334,
                    0.5596330275229358,
                    0.45454545454545453,
                    0.3168115514609991,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.06222222222222222,
                    0.0,
                    0,
                    0,
                    0.8342492855572653,
                    0.5763147489126137,
                    0.6303665464733197,
                    0.9816547199611226,
                    0.0,
                    0.3577981651376147,
                    0.30303030303030304,
                    0.38693708992445364,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.11503186189317575,
                    0.3073234524847428,
                    0,
                    0,
                    0.15717740162673116,
                    0.07077896401739818,
                    0.0012711094970038134,
                    0.008018466771959665,
                    0.26666666666666666,
                    0.08256880733944955,
                    0.24242424242424243,
                    0.2962513586145473,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "lieut. gibson": 0.15475123313359168,
                "paul jones": 0.4132573081218915,
                "clyde van dusen": 0.4319914587445168
            },
            "question": "which of these kentucky derby winners was named for its trainer?",
            "rate_limited": false,
            "answers": [
                "clyde van dusen",
                "paul jones",
                "lieut. gibson"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "lieut. gibson": 0.06728286666201438,
                "paul jones": 0.2286184806168431,
                "clyde van dusen": 0.3511333110423034
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.5840577573049954,
                    1.934685449622268,
                    1.4812567930727365
                ],
                "result_count_important_words": [
                    28400.0,
                    48600.0,
                    98.0
                ],
                "wikipedia_search": [
                    2.2,
                    0.0,
                    0.8
                ],
                "word_count_appended_bing": [
                    15.0,
                    10.0,
                    8.0
                ],
                "answer_relation_to_question_bing": [
                    2.770706190061029,
                    0.0,
                    1.2292938099389712
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    85.0,
                    8080.0,
                    66.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    357000.0,
                    583000.0,
                    71600.0
                ],
                "word_count_appended": [
                    61.0,
                    39.0,
                    9.0
                ],
                "answer_relation_to_question": [
                    4.11372957942301,
                    0.3111111111111111,
                    0.5751593094658788
                ],
                "result_count": [
                    78.0,
                    7590.0,
                    1430.0
                ]
            },
            "integer_answers": {
                "lieut. gibson": 0,
                "paul jones": 5,
                "clyde van dusen": 5
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these Hebrew texts does NOT form a significant part of the Christian Old Testament?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "torah",
                "ketuvim",
                "talmud"
            ],
            "lines": [
                [
                    0.37051673412339026,
                    0.2868150747265449,
                    0.02542372881355931,
                    0.0,
                    0.24946075991372157,
                    0.29431438127090304,
                    0.1482641607935265,
                    0.23176499875529,
                    0.3473424212550529,
                    0.2920560747663552,
                    0.2660550458715596,
                    0.3279523570480406,
                    0.03125,
                    0.023809523809523836,
                    -1.0
                ],
                [
                    0.35685999594176104,
                    0.3330118124122622,
                    0.4830508474576271,
                    0.5,
                    0.4795088767214203,
                    0.36789297658862874,
                    0.47637692508483426,
                    0.4810804082648743,
                    0.4468275234427879,
                    0.3726635514018692,
                    0.3669724770642202,
                    0.3405277015229985,
                    0.5,
                    0.47619047619047616,
                    -1.0
                ],
                [
                    0.2726232699348487,
                    0.380173112861193,
                    0.4915254237288136,
                    0.5,
                    0.27103036336485814,
                    0.3377926421404682,
                    0.37535891412163924,
                    0.2871545929798357,
                    0.2058300553021592,
                    0.3352803738317757,
                    0.3669724770642202,
                    0.3315199414289609,
                    0.46875,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "talmud": 0.2679984047487468,
                "ketuvim": 0.14557663255803432,
                "torah": 0.5864249626932189
            },
            "question": "which of these hebrew texts does not form a significant part of the christian old testament?",
            "rate_limited": false,
            "answers": [
                "torah",
                "ketuvim",
                "talmud"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "talmud": 0.0,
                "ketuvim": 0.0,
                "torah": 0.0
            },
            "integer_answers": {
                "talmud": 2,
                "ketuvim": 0,
                "torah": 12
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.408667001327432,
                    2.232612178678021,
                    2.3587208199945473
                ],
                "result_count_important_words": [
                    539000.0,
                    36200.0,
                    191000.0
                ],
                "wikipedia_search": [
                    2.1372061024292597,
                    0.7444146718009689,
                    4.118379225769771
                ],
                "answer_relation_to_question": [
                    1.8127657222725362,
                    2.0039600568153455,
                    3.1832742209121183
                ],
                "word_count_appended_bing": [
                    51.0,
                    29.0,
                    29.0
                ],
                "answer_relation_to_question_bing": [
                    2.5582191032814614,
                    2.003858251052854,
                    1.4379226456656844
                ],
                "question_related_to_answer": [
                    1.8983050847457628,
                    0.06779661016949153,
                    0.03389830508474576
                ],
                "question_related_to_answer_bing": [
                    2.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    431000.0,
                    30400.0,
                    342000.0
                ],
                "word_count_noun_chunks": [
                    15.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    2460000.0,
                    1580000.0,
                    1940000.0
                ],
                "word_count_raw": [
                    20.0,
                    1.0,
                    0.0
                ],
                "result_count": [
                    302000.0,
                    24700.0,
                    276000.0
                ],
                "word_count_appended": [
                    356.0,
                    218.0,
                    282.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "How many of the three Baltic countries border Russia?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "three",
                "two",
                "none"
            ],
            "lines": [
                [
                    0.37267080745341613,
                    0.34463276836158196,
                    0.3557483731019523,
                    0.23741007194244604,
                    0.4027047332832457,
                    0.332044839582528,
                    0.33158813263525305,
                    0.40786240786240785,
                    0.02857142857142857,
                    0.4777542372881356,
                    0.3611111111111111,
                    0.3511337699974082,
                    0.6838046272493573,
                    0.6587301587301587,
                    5.0
                ],
                [
                    0.3105590062111801,
                    0.3276836158192091,
                    0.01968896508291932,
                    0.09435074890906947,
                    0.4380165289256198,
                    0.3598763045999227,
                    0.5002908667830134,
                    0.45503685503685504,
                    0.2357142857142857,
                    0.2288135593220339,
                    0.2,
                    0.33655881616139366,
                    0.012853470437017995,
                    0.03439153439153439,
                    5.0
                ],
                [
                    0.3167701863354037,
                    0.3276836158192091,
                    0.6245626618151284,
                    0.6682391791484845,
                    0.1592787377911345,
                    0.3080788558175493,
                    0.16812100058173357,
                    0.1371007371007371,
                    0.7357142857142858,
                    0.2934322033898305,
                    0.4388888888888889,
                    0.3123074138411981,
                    0.3033419023136247,
                    0.30687830687830686,
                    5.0
                ]
            ],
            "fraction_answers": {
                "none": 0.36431414110253685,
                "three": 0.3818405333693164,
                "two": 0.25384532552814676
            },
            "question": "how many of the three baltic countries border russia?",
            "rate_limited": false,
            "answers": [
                "three",
                "two",
                "none"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "none": 0.0,
                "three": 0.0,
                "two": 0.0
            },
            "integer_answers": {
                "none": 4,
                "three": 6,
                "two": 4
            },
            "categorical_data": {
                "question_type": 5
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.4045350799896328,
                    1.3462352646455746,
                    1.2492296553647924
                ],
                "result_count_important_words": [
                    1140000.0,
                    1720000.0,
                    578000.0
                ],
                "wikipedia_search": [
                    0.05714285714285714,
                    0.4714285714285714,
                    1.4714285714285715
                ],
                "answer_relation_to_question": [
                    0.37267080745341613,
                    0.3105590062111801,
                    0.3167701863354037
                ],
                "word_count_appended_bing": [
                    65.0,
                    36.0,
                    79.0
                ],
                "answer_relation_to_question_bing": [
                    0.6892655367231638,
                    0.655367231638418,
                    0.655367231638418
                ],
                "question_related_to_answer": [
                    0.7114967462039046,
                    0.03937793016583864,
                    1.2491253236302569
                ],
                "question_related_to_answer_bing": [
                    0.4748201438848921,
                    0.18870149781813894,
                    1.336478358296969
                ],
                "result_count_noun_chunks": [
                    8300000.0,
                    9260000.0,
                    2790000.0
                ],
                "word_count_noun_chunks": [
                    266.0,
                    5.0,
                    118.0
                ],
                "result_count_bing": [
                    859000.0,
                    931000.0,
                    797000.0
                ],
                "word_count_raw": [
                    249.0,
                    13.0,
                    116.0
                ],
                "result_count": [
                    5360000.0,
                    5830000.0,
                    2120000.0
                ],
                "word_count_appended": [
                    451.0,
                    216.0,
                    277.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these athletes has a notably obscured glabella?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "michael phelps"
            ],
            "lines": [
                [
                    0.8823529411764706,
                    0.0,
                    1.0,
                    0,
                    0.59375,
                    0.7323026851098454,
                    0.5652173913043478,
                    0.5909090909090909,
                    0.45,
                    0.46153846153846156,
                    0.375,
                    0.3942060721812434,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.0,
                    0.5,
                    0.0,
                    0,
                    0.25,
                    0.24410089503661514,
                    0.30434782608695654,
                    0.3181818181818182,
                    0.0,
                    0.3076923076923077,
                    0.275,
                    0.31194020021074814,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.11764705882352941,
                    0.5,
                    0.0,
                    0,
                    0.15625,
                    0.023596419853539462,
                    0.13043478260869565,
                    0.09090909090909091,
                    0.55,
                    0.23076923076923078,
                    0.35,
                    0.29385372760800843,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "anthony davis": 0.22213275550655406,
                "michael phelps": 0.5495706038381327,
                "michael strahan": 0.22829664065531324
            },
            "question": "which of these athletes has a notably obscured glabella?",
            "rate_limited": false,
            "answers": [
                "michael phelps",
                "michael strahan",
                "anthony davis"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "anthony davis": 0.29999999900658925,
                "michael phelps": 0.34000000019868215,
                "michael strahan": 0.31499999945362406
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.5768242887249737,
                    1.2477608008429926,
                    1.1754149104320337
                ],
                "result_count_important_words": [
                    13.0,
                    7.0,
                    3.0
                ],
                "wikipedia_search": [
                    0.9,
                    0.0,
                    1.1
                ],
                "word_count_appended_bing": [
                    15.0,
                    11.0,
                    14.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    1.0,
                    1.0
                ],
                "question_related_to_answer": [
                    1.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    13.0,
                    7.0,
                    2.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    360000.0,
                    120000.0,
                    11600.0
                ],
                "word_count_appended": [
                    6.0,
                    4.0,
                    3.0
                ],
                "answer_relation_to_question": [
                    0.8823529411764706,
                    0.0,
                    0.11764705882352941
                ],
                "result_count": [
                    19.0,
                    8.0,
                    5.0
                ]
            },
            "integer_answers": {
                "anthony davis": 1,
                "michael phelps": 9,
                "michael strahan": 1
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Tom from MySpace shares his name with a key character in what film franchise?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "the matrix"
            ],
            "lines": [
                [
                    0.3196026054691712,
                    0.3894414936069562,
                    1.0,
                    0,
                    0.8821305553221876,
                    0.4305912596401028,
                    0.1396353964647864,
                    0.8066046688176125,
                    0.23463153875525009,
                    0.3384615384615385,
                    0.29523809523809524,
                    0.39476962099438195,
                    0.5,
                    0.125,
                    1.0
                ],
                [
                    0.3730057826268746,
                    0.25641965385896676,
                    0.0,
                    0,
                    0.017474586238763336,
                    0.27924164524421596,
                    0.029201529340056518,
                    0.11691022964509394,
                    0.2946366297569047,
                    0.10256410256410256,
                    0.3333333333333333,
                    0.25133786967738597,
                    0.5,
                    0.0,
                    1.0
                ],
                [
                    0.30739161190395425,
                    0.3541388525340771,
                    0.0,
                    0,
                    0.10039485843904898,
                    0.29016709511568123,
                    0.8311630741951571,
                    0.0764851015372936,
                    0.4707318314878452,
                    0.558974358974359,
                    0.37142857142857144,
                    0.353892509328232,
                    0.0,
                    0.875,
                    1.0
                ]
            ],
            "fraction_answers": {
                "the godfather": 0.19647118171428446,
                "the matrix": 0.35305906653417074,
                "harry potter": 0.45046975175154486
            },
            "question": "tom from myspace shares his name with a key character in what film franchise?",
            "rate_limited": false,
            "answers": [
                "harry potter",
                "the godfather",
                "the matrix"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "the godfather": 0.0815502691177463,
                "the matrix": 0.6870942134282793,
                "harry potter": 0.3783384593337785
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.763387346960674,
                    1.759365087741702,
                    2.477247565297624
                ],
                "result_count_important_words": [
                    252000.0,
                    52700.0,
                    1500000.0
                ],
                "wikipedia_search": [
                    1.1731576937762505,
                    1.4731831487845235,
                    2.353659157439226
                ],
                "word_count_appended_bing": [
                    31.0,
                    35.0,
                    39.0
                ],
                "answer_relation_to_question_bing": [
                    2.336648961641737,
                    1.5385179231538006,
                    2.1248331152044626
                ],
                "question_related_to_answer": [
                    2.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    425000.0,
                    61600.0,
                    40300.0
                ],
                "word_count_noun_chunks": [
                    1.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    7.0
                ],
                "result_count_bing": [
                    134000.0,
                    86900.0,
                    90300.0
                ],
                "word_count_appended": [
                    66.0,
                    20.0,
                    109.0
                ],
                "answer_relation_to_question": [
                    1.598013027345856,
                    1.8650289131343731,
                    1.5369580595197712
                ],
                "result_count": [
                    2100000.0,
                    41600.0,
                    239000.0
                ]
            },
            "integer_answers": {
                "the godfather": 1,
                "the matrix": 5,
                "harry potter": 7
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "The \u201cCC:\u201d feature in email stands for what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "carbon copy"
            ],
            "lines": [
                [
                    0.1402116402116402,
                    0.17115768463073855,
                    0.0,
                    0.0,
                    0.003931207930784695,
                    0.3154875717017208,
                    0.00011863575856227418,
                    0.376220562894888,
                    0.1873479318734793,
                    0.088,
                    0.07142857142857142,
                    0.17221964448852842,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.4038800705467372,
                    0.44910179640718567,
                    0.0,
                    0.0,
                    0.05192775693251833,
                    0.3403441682600382,
                    0.0010729557575852738,
                    0.511200459506031,
                    0.25669099756691,
                    0.22,
                    0.07142857142857142,
                    0.30688938592425297,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.4559082892416226,
                    0.3797405189620759,
                    1.0,
                    1.0,
                    0.944141035136697,
                    0.3441682600382409,
                    0.9988084084838524,
                    0.11257897759908099,
                    0.5559610705596106,
                    0.692,
                    0.8571428571428571,
                    0.5208909695872186,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "carbon copy": 0.7043814561965183,
                "copy chain": 0.10900881792277957,
                "copy contacts": 0.18660972588070218
            },
            "question": "the \u201ccc:\u201d feature in email stands for what?",
            "rate_limited": false,
            "answers": [
                "copy chain",
                "copy contacts",
                "carbon copy"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "carbon copy": 0.3952380970830009,
                "copy chain": 0.29999999900658925,
                "copy contacts": 0.3194444440305233
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.6888785779541137,
                    1.2275575436970119,
                    2.0835638783488744
                ],
                "result_count_important_words": [
                    272.0,
                    2460.0,
                    2290000.0
                ],
                "wikipedia_search": [
                    0.5620437956204379,
                    0.7700729927007299,
                    1.667883211678832
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    24.0
                ],
                "answer_relation_to_question_bing": [
                    0.5134730538922156,
                    1.347305389221557,
                    1.1392215568862276
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    1.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_noun_chunks": [
                    1310000.0,
                    1780000.0,
                    392000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    31.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    40.0
                ],
                "result_count_bing": [
                    16500000.0,
                    17800000.0,
                    18000000.0
                ],
                "word_count_appended": [
                    22.0,
                    55.0,
                    173.0
                ],
                "answer_relation_to_question": [
                    0.4206349206349206,
                    1.2116402116402116,
                    1.3677248677248677
                ],
                "result_count": [
                    483.0,
                    6380.0,
                    116000.0
                ]
            },
            "integer_answers": {
                "carbon copy": 12,
                "copy chain": 0,
                "copy contacts": 2
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these foods is cultivated in a paddy?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "rice"
            ],
            "lines": [
                [
                    0.5384615384615384,
                    0.0,
                    0.5833333333333334,
                    1.0,
                    0.5433012400653956,
                    0.4621692135669027,
                    0.7809681726990235,
                    0.3688078644688028,
                    1.0,
                    0.6416184971098265,
                    0.6937269372693727,
                    0.4719687599043825,
                    0.9316770186335404,
                    1.0,
                    -1.0
                ],
                [
                    0.038461538461538464,
                    0.0,
                    0.4166666666666667,
                    0.0,
                    0.45668799889554995,
                    0.060752888557584796,
                    0.21897486852014758,
                    0.6311812445607028,
                    0.0,
                    0.31213872832369943,
                    0.1881918819188192,
                    0.4370488333381273,
                    0.06832298136645963,
                    0.0,
                    -1.0
                ],
                [
                    0.4230769230769231,
                    1.0,
                    0.0,
                    0.0,
                    1.0761039054435373e-05,
                    0.47707789787551247,
                    5.6958780828940126e-05,
                    1.0890970494380753e-05,
                    0.0,
                    0.046242774566473986,
                    0.11808118081180811,
                    0.09098240675749025,
                    0.0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "cake": 0.20203054504352114,
                "rice": 0.6440023268222942,
                "dunkaroos": 0.15396712813418464
            },
            "question": "which of these foods is cultivated in a paddy?",
            "rate_limited": false,
            "answers": [
                "rice",
                "cake",
                "dunkaroos"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "cake": 0.30197287065573825,
                "rice": 0.7316138245910202,
                "dunkaroos": 0.10778609231521957
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.4159062797131474,
                    1.3111465000143818,
                    0.27294722027247076
                ],
                "result_count_important_words": [
                    617000.0,
                    173000.0,
                    45.0
                ],
                "wikipedia_search": [
                    3.0,
                    0.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    188.0,
                    51.0,
                    32.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "question_related_to_answer": [
                    1.1666666666666667,
                    0.8333333333333334,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    1490000.0,
                    2550000.0,
                    44.0
                ],
                "word_count_noun_chunks": [
                    150.0,
                    11.0,
                    0.0
                ],
                "result_count_bing": [
                    1240000.0,
                    163000.0,
                    1280000.0
                ],
                "word_count_raw": [
                    292.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    666.0,
                    324.0,
                    48.0
                ],
                "answer_relation_to_question": [
                    1.0769230769230769,
                    0.07692307692307693,
                    0.8461538461538461
                ],
                "result_count": [
                    2070000.0,
                    1740000.0,
                    41.0
                ]
            },
            "integer_answers": {
                "cake": 1,
                "rice": 11,
                "dunkaroos": 2
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "J.K. Rowling\u2019s first book published in England was titled \u201cHarry Potter and the\u201d what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "philosopher's stone"
            ],
            "lines": [
                [
                    0.2189656843118155,
                    0.29996713938443753,
                    0,
                    0,
                    0.6588485534595402,
                    0.4078740157480315,
                    0.467889417678199,
                    0.17270681641265415,
                    0.0700354609929078,
                    0.4830508474576271,
                    0.5909090909090909,
                    0.46248646309587255,
                    0.6216216216216216,
                    0.6,
                    1.0
                ],
                [
                    0.6831874720791786,
                    0.6848813454640474,
                    0,
                    0,
                    0.34112970788260455,
                    0.2677165354330709,
                    0.5320571092454949,
                    0.8272406206432202,
                    0.874113475177305,
                    0.4872881355932203,
                    0.36363636363636365,
                    0.467552866813177,
                    0.3783783783783784,
                    0.4,
                    1.0
                ],
                [
                    0.09784684360900592,
                    0.015151515151515154,
                    0,
                    0,
                    2.1738657855264015e-05,
                    0.32440944881889766,
                    5.347307630607989e-05,
                    5.256294412559039e-05,
                    0.05585106382978724,
                    0.029661016949152543,
                    0.045454545454545456,
                    0.06996067009095049,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "magician's stone": 0.053205239881845126,
                "philosopher's stone": 0.42119625925598314,
                "sorcerer's stone": 0.5255985008621719
            },
            "question": "j.k. rowling\u2019s first book published in england was titled \u201charry potter and the\u201d what?",
            "rate_limited": false,
            "answers": [
                "philosopher's stone",
                "sorcerer's stone",
                "magician's stone"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "magician's stone": 0.0708078320896507,
                "philosopher's stone": 0.7285985988586332,
                "sorcerer's stone": 0.5101621251024736
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.6998917047669804,
                    3.740422934505416,
                    0.5596853607276039
                ],
                "result_count_important_words": [
                    175000.0,
                    199000.0,
                    20.0
                ],
                "wikipedia_search": [
                    0.42021276595744683,
                    5.24468085106383,
                    0.3351063829787234
                ],
                "word_count_appended_bing": [
                    26.0,
                    16.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.799802836306625,
                    4.109288072784284,
                    0.09090909090909091
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    138000.0,
                    661000.0,
                    42.0
                ],
                "word_count_noun_chunks": [
                    23.0,
                    14.0,
                    0.0
                ],
                "word_count_raw": [
                    48.0,
                    32.0,
                    0.0
                ],
                "result_count_bing": [
                    259000.0,
                    170000.0,
                    206000.0
                ],
                "word_count_appended": [
                    114.0,
                    115.0,
                    7.0
                ],
                "answer_relation_to_question": [
                    1.751725474494524,
                    5.465499776633429,
                    0.7827747488720473
                ],
                "result_count": [
                    394000.0,
                    204000.0,
                    13.0
                ]
            },
            "integer_answers": {
                "magician's stone": 0,
                "philosopher's stone": 5,
                "sorcerer's stone": 7
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which former NFL star does NOT have a football video game named after him?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "emmitt smith",
                "kurt warner"
            ],
            "lines": [
                [
                    0.31674254274224767,
                    0.31416320320190655,
                    0.5,
                    0,
                    0.4017790956263899,
                    0.4681242337556191,
                    0.392692750287687,
                    0.401468788249694,
                    0.414966373785995,
                    0.3495145631067962,
                    0.4666666666666667,
                    0.33407476243612466,
                    0.5,
                    0,
                    -1.0
                ],
                [
                    0.36262478457157843,
                    0.3892332389894771,
                    0.5,
                    0,
                    0.20681986656782803,
                    0.46505925623212097,
                    0.21461449942462602,
                    0.2083843329253366,
                    0.2899205822012069,
                    0.3300970873786408,
                    0.2583333333333333,
                    0.326747586972758,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.3206326726861739,
                    0.2966035578086164,
                    0.0,
                    0,
                    0.39140103780578206,
                    0.06681651001225991,
                    0.392692750287687,
                    0.3901468788249694,
                    0.2951130440127982,
                    0.32038834951456313,
                    0.275,
                    0.3391776505911173,
                    0.5,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "kurt warner": 0.40200459140933886,
                "brett favre": 0.4080275719005157,
                "emmitt smith": 0.18996783669014558
            },
            "question": "which former nfl star does not have a football video game named after him?",
            "rate_limited": false,
            "answers": [
                "emmitt smith",
                "brett favre",
                "kurt warner"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "kurt warner": 0.34000000019868215,
                "brett favre": 0.3194444440305233,
                "emmitt smith": 0.34000000019868215
            },
            "integer_answers": {
                "kurt warner": 4,
                "brett favre": 7,
                "emmitt smith": 1
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9911028507665038,
                    2.0790289563269035,
                    1.929868192906592
                ],
                "result_count_important_words": [
                    37300.0,
                    99200.0,
                    37300.0
                ],
                "wikipedia_search": [
                    1.020403514568061,
                    2.5209530135855176,
                    2.458643471846422
                ],
                "answer_relation_to_question": [
                    2.1990894870930275,
                    1.648502585141059,
                    2.1524079277659136
                ],
                "word_count_appended_bing": [
                    4.0,
                    29.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    2.2300415615771216,
                    1.3292011321262753,
                    2.4407573062966033
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    1.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    32200.0,
                    95300.0,
                    35900.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count_bing": [
                    1560000.0,
                    1710000.0,
                    21200000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    26500.0,
                    79100.0,
                    29300.0
                ],
                "word_count_appended": [
                    124.0,
                    140.0,
                    148.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "How do you spell the last name of Duke University\u2019s men's basketball coach?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "krzyzewski"
            ],
            "lines": [
                [
                    0.34444444444444444,
                    0.4815367797508967,
                    0.0,
                    0.0,
                    0.0,
                    0.304635761589404,
                    0.0,
                    0.0,
                    0.0,
                    0.05,
                    0.17532467532467533,
                    0.06210608209448193,
                    0.0,
                    0.0,
                    5.0
                ],
                [
                    0.3111111111111111,
                    0.42789868803286546,
                    1.0,
                    1.0,
                    1.0,
                    0.39072847682119205,
                    1.0,
                    1.0,
                    1.0,
                    0.9,
                    0.6493506493506493,
                    0.8757878358110363,
                    1.0,
                    1.0,
                    5.0
                ],
                [
                    0.34444444444444444,
                    0.09056453221623778,
                    0.0,
                    0.0,
                    0.0,
                    0.304635761589404,
                    0.0,
                    0.0,
                    0.0,
                    0.05,
                    0.17532467532467533,
                    0.06210608209448193,
                    0.0,
                    0.0,
                    5.0
                ]
            ],
            "fraction_answers": {
                "khzyrweski": 0.07336253540494597,
                "crzyzewski": 0.10128912451456444,
                "krzyzewski": 0.8253483400804896
            },
            "question": "how do you spell the last name of duke university\u2019s men's basketball coach?",
            "rate_limited": false,
            "answers": [
                "crzyzewski",
                "krzyzewski",
                "khzyrweski"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "khzyrweski": 0.3194444440305233,
                "crzyzewski": 0.3194444440305233,
                "krzyzewski": 0.3952380970830009
            },
            "integer_answers": {
                "khzyrweski": 0,
                "crzyzewski": 2,
                "krzyzewski": 12
            },
            "categorical_data": {
                "question_type": 5
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.3726364925668915,
                    5.254727014866217,
                    0.3726364925668915
                ],
                "result_count_important_words": [
                    0,
                    50500.0,
                    0
                ],
                "wikipedia_search": [
                    0.0,
                    5.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    27.0,
                    100.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    2.8892206785053802,
                    2.5673921281971928,
                    0.5433871932974267
                ],
                "question_related_to_answer": [
                    0.0,
                    1.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    0,
                    44300.0,
                    0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    126.0,
                    0.0
                ],
                "result_count_bing": [
                    322000.0,
                    413000.0,
                    322000.0
                ],
                "word_count_raw": [
                    0.0,
                    68.0,
                    0.0
                ],
                "result_count": [
                    0,
                    53600.0,
                    0
                ],
                "answer_relation_to_question": [
                    2.0666666666666664,
                    1.8666666666666665,
                    2.0666666666666664
                ],
                "word_count_appended": [
                    16.0,
                    288.0,
                    16.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these consists of frozen water?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "snowflake",
                "garden rake",
                "drake"
            ],
            "lines": [
                [
                    0.625,
                    0.0,
                    0,
                    0,
                    0.5880993645291739,
                    0.5985130111524164,
                    0.44489092996555685,
                    0.47943870590528165,
                    0.6818181818181819,
                    0.5073446327683616,
                    0.6767676767676768,
                    0.39853832402219086,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.125,
                    1.0,
                    0,
                    0,
                    0.013287117273252455,
                    0.30390334572490707,
                    0.013346727898966704,
                    0.014487104528032223,
                    0.0,
                    0.04858757062146893,
                    0.04040404040404041,
                    0.2575663249053273,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.25,
                    0.0,
                    0,
                    0,
                    0.3986135181975737,
                    0.09758364312267657,
                    0.5417623421354765,
                    0.5060741895666862,
                    0.3181818181818182,
                    0.4440677966101695,
                    0.2828282828282828,
                    0.3438953510724818,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "garden rake": 0.18165822313559948,
                "snowflake": 0.5000410826928838,
                "drake": 0.3183006941715165
            },
            "question": "which of these consists of frozen water?",
            "rate_limited": false,
            "answers": [
                "snowflake",
                "garden rake",
                "drake"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "garden rake": 0.0,
                "snowflake": 0.0,
                "drake": 0.0
            },
            "integer_answers": {
                "garden rake": 1,
                "snowflake": 7,
                "drake": 2
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1956149720665725,
                    0.772698974715982,
                    1.0316860532174454
                ],
                "result_count_important_words": [
                    620000.0,
                    18600.0,
                    755000.0
                ],
                "wikipedia_search": [
                    1.3636363636363638,
                    0.0,
                    0.6363636363636364
                ],
                "word_count_appended_bing": [
                    67.0,
                    4.0,
                    28.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    1.0,
                    0.0
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    738000.0,
                    22300.0,
                    779000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    6440000.0,
                    3270000.0,
                    1050000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    5090000.0,
                    115000.0,
                    3450000.0
                ],
                "answer_relation_to_question": [
                    1.25,
                    0.25,
                    0.5
                ],
                "word_count_appended": [
                    449.0,
                    43.0,
                    393.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which country did NOT have a native player selected in the first round of the 2016 NBA draft?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "haiti",
                "bahamas",
                "brazil"
            ],
            "lines": [
                [
                    0.3871983808179661,
                    0.30582408401957273,
                    0.5,
                    0,
                    0.49014860577725833,
                    0.33322346736980885,
                    0.4882821822347382,
                    0.37847222222222227,
                    0.3753654432396138,
                    0.3416370106761566,
                    0.3279569892473118,
                    0.34398364979481877,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.2261809120564956,
                    0.3552482396467359,
                    0.25,
                    0,
                    0.05585239605944231,
                    0.3335530652603823,
                    0.04417450241038612,
                    0.33767361111111116,
                    0.2638855120163557,
                    0.3309608540925267,
                    0.33870967741935487,
                    0.3288022947371156,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.3866207071255383,
                    0.33892767633369136,
                    0.25,
                    0,
                    0.45399899816329936,
                    0.33322346736980885,
                    0.46754331535487564,
                    0.2838541666666667,
                    0.36074904474403063,
                    0.3274021352313167,
                    0.33333333333333337,
                    0.32721405546806565,
                    0.5,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "bahamas": 0.40537829772155287,
                "brazil": 0.25186663080144206,
                "haiti": 0.342755071477005
            },
            "question": "which country did not have a native player selected in the first round of the 2016 nba draft?",
            "rate_limited": false,
            "answers": [
                "haiti",
                "bahamas",
                "brazil"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "bahamas": 0.0,
                "brazil": 0.0,
                "haiti": 0.0
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.4962616032828997,
                    2.7391632842061506,
                    2.7645751125109497
                ],
                "result_count_important_words": [
                    98200.0,
                    3820000.0,
                    272000.0
                ],
                "wikipedia_search": [
                    1.9941529081661797,
                    3.77783180773831,
                    2.2280152840955103
                ],
                "answer_relation_to_question": [
                    1.128016191820339,
                    2.738190879435044,
                    1.133792928744617
                ],
                "answer_relation_to_question_bing": [
                    2.7184628237259814,
                    2.0265246449456975,
                    2.2550125313283207
                ],
                "word_count_appended": [
                    178.0,
                    190.0,
                    194.0
                ],
                "question_related_to_answer": [
                    0.0,
                    0.5,
                    0.5
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    140000.0,
                    187000.0,
                    249000.0
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    506000.0,
                    505000.0,
                    506000.0
                ],
                "result_count": [
                    118000.0,
                    5320000.0,
                    551000.0
                ],
                "word_count_appended_bing": [
                    32.0,
                    30.0,
                    31.0
                ]
            },
            "integer_answers": {
                "bahamas": 5,
                "brazil": 3,
                "haiti": 5
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What word describes joining a cause just to feel good about it?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "slacktivism"
            ],
            "lines": [
                [
                    0.27112365029606406,
                    0.09027777777777778,
                    0,
                    0,
                    0.002719286013108353,
                    0.3017319963536919,
                    0.0030607439962329307,
                    0.001966783216783217,
                    0.08739837398373984,
                    0.08634538152610442,
                    0.216,
                    0.19249250095048046,
                    0,
                    0,
                    1.0
                ],
                [
                    0.10034900731452456,
                    0.4708333333333334,
                    0,
                    0,
                    0.000209175847162181,
                    0.39653600729261623,
                    0.00023544184586407157,
                    7.284382284382284e-05,
                    0.2516759378120097,
                    0.04417670682730924,
                    0.224,
                    0.08662022032625762,
                    0,
                    0,
                    1.0
                ],
                [
                    0.6285273423894113,
                    0.4388888888888889,
                    0,
                    0,
                    0.9970715381397295,
                    0.3017319963536919,
                    0.996703814157903,
                    0.997960372960373,
                    0.6609256882042505,
                    0.8694779116465864,
                    0.56,
                    0.720887278723262,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "slacktivism": 0.7172174831464095,
                "gung-faux": 0.15747086744219213,
                "joinerism": 0.12531164941139827
            },
            "question": "what word describes joining a cause just to feel good about it?",
            "rate_limited": false,
            "answers": [
                "joinerism",
                "gung-faux",
                "slacktivism"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "slacktivism": 0.34226657738468486,
                "gung-faux": 0.10528428107279986,
                "joinerism": 0.0625801511786846
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1549550057028828,
                    0.5197213219575457,
                    4.325323672339572
                ],
                "result_count_important_words": [
                    39.0,
                    3.0,
                    12700.0
                ],
                "wikipedia_search": [
                    0.524390243902439,
                    1.5100556268720582,
                    3.9655541292255028
                ],
                "word_count_appended_bing": [
                    27.0,
                    28.0,
                    70.0
                ],
                "answer_relation_to_question_bing": [
                    0.5416666666666666,
                    2.825,
                    2.6333333333333333
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    81.0,
                    3.0,
                    41100.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    331000.0,
                    435000.0,
                    331000.0
                ],
                "word_count_appended": [
                    43.0,
                    22.0,
                    433.0
                ],
                "answer_relation_to_question": [
                    1.3556182514803203,
                    0.5017450365726228,
                    3.1426367119470564
                ],
                "result_count": [
                    39.0,
                    3.0,
                    14300.0
                ]
            },
            "integer_answers": {
                "slacktivism": 8,
                "gung-faux": 2,
                "joinerism": 0
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "One of Apple\u2019s biggest flops was a product named after a man who did what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "invented the transistor"
            ],
            "lines": [
                [
                    0.32575757575757575,
                    0.5777777777777778,
                    0,
                    0,
                    0.40425531914893614,
                    0.17420212765957446,
                    0.2846153846153846,
                    0.2484472049689441,
                    0.3467086834733893,
                    0.3181818181818182,
                    0.3333333333333333,
                    0.29051374224268794,
                    0,
                    0,
                    1.0
                ],
                [
                    0.5055361305361306,
                    0.1911111111111111,
                    0,
                    0,
                    0.3404255319148936,
                    0.6090425531914894,
                    0.4846153846153846,
                    0.42857142857142855,
                    0.2574229691876751,
                    0.4090909090909091,
                    0.3333333333333333,
                    0.4397996235231823,
                    0,
                    0,
                    1.0
                ],
                [
                    0.1687062937062937,
                    0.23111111111111113,
                    0,
                    0,
                    0.2553191489361702,
                    0.21675531914893617,
                    0.23076923076923078,
                    0.32298136645962733,
                    0.39586834733893556,
                    0.2727272727272727,
                    0.3333333333333333,
                    0.2696866342341298,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "invented the transistor": 0.39989489750755375,
                "developed calculus": 0.26972580577650407,
                "discovered saturn": 0.3303792967159422
            },
            "question": "one of apple\u2019s biggest flops was a product named after a man who did what?",
            "rate_limited": false,
            "answers": [
                "discovered saturn",
                "invented the transistor",
                "developed calculus"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "invented the transistor": 0.4226305847204297,
                "developed calculus": -0.018258700558718213,
                "discovered saturn": 0.153223120145668
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7430824534561276,
                    2.638797741139094,
                    1.6181198054047787
                ],
                "result_count_important_words": [
                    37.0,
                    63.0,
                    30.0
                ],
                "wikipedia_search": [
                    1.040126050420168,
                    0.7722689075630252,
                    1.1876050420168067
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    1.7333333333333334,
                    0.5733333333333333,
                    0.6933333333333334
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    40.0,
                    69.0,
                    52.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    131000.0,
                    458000.0,
                    163000.0
                ],
                "word_count_appended": [
                    7.0,
                    9.0,
                    6.0
                ],
                "answer_relation_to_question": [
                    1.303030303030303,
                    2.0221445221445222,
                    0.6748251748251748
                ],
                "result_count": [
                    76.0,
                    64.0,
                    48.0
                ]
            },
            "integer_answers": {
                "invented the transistor": 6,
                "developed calculus": 1,
                "discovered saturn": 3
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What dish is made with ham, poached eggs and Hollandaise sauce?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "eggs benedict"
            ],
            "lines": [
                [
                    0.0507936507936508,
                    0.057142857142857155,
                    0.0,
                    0.0,
                    0.005749329244921426,
                    0.2550218340611354,
                    0.0057774044946679905,
                    0.0020842698925170626,
                    0.0,
                    0.18120805369127516,
                    0.07317073170731707,
                    0.2748753315994897,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.8982027907720603,
                    0.8523809523809525,
                    1.0,
                    1.0,
                    0.9939951450108598,
                    0.4200873362445415,
                    0.9900486116413428,
                    0.9787894887408558,
                    0.8927489177489177,
                    0.6946308724832215,
                    0.8536585365853658,
                    0.46031400938730754,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.051003558434288915,
                    0.0904761904761905,
                    0.0,
                    0.0,
                    0.00025552574421873004,
                    0.32489082969432315,
                    0.004173983863989208,
                    0.01912624136662716,
                    0.10725108225108224,
                    0.12416107382550336,
                    0.07317073170731707,
                    0.2648106590132027,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "eggs benedict": 0.8596326186425305,
                "benedict cumberbatch": 0.07566570545548165,
                "pope benedict": 0.06470167590198798
            },
            "question": "what dish is made with ham, poached eggs and hollandaise sauce?",
            "rate_limited": false,
            "answers": [
                "pope benedict",
                "eggs benedict",
                "benedict cumberbatch"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "eggs benedict": 0.3952380970830009,
                "benedict cumberbatch": 0.29999999900658925,
                "pope benedict": 0.29999999900658925
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.6492519895969382,
                    2.761884056323846,
                    1.5888639540792164
                ],
                "result_count_important_words": [
                    2270.0,
                    389000.0,
                    1640.0
                ],
                "wikipedia_search": [
                    0.0,
                    5.356493506493507,
                    0.6435064935064935
                ],
                "word_count_appended_bing": [
                    3.0,
                    35.0,
                    3.0
                ],
                "answer_relation_to_question_bing": [
                    0.34285714285714286,
                    5.114285714285714,
                    0.5428571428571429
                ],
                "question_related_to_answer": [
                    0.0,
                    1.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    1020.0,
                    479000.0,
                    9360.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    83.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    140.0,
                    0.0
                ],
                "result_count_bing": [
                    584000.0,
                    962000.0,
                    744000.0
                ],
                "word_count_appended": [
                    54.0,
                    207.0,
                    37.0
                ],
                "answer_relation_to_question": [
                    0.3047619047619048,
                    5.389216744632362,
                    0.3060213506057335
                ],
                "result_count": [
                    2250.0,
                    389000.0,
                    100.0
                ]
            },
            "integer_answers": {
                "eggs benedict": 14,
                "benedict cumberbatch": 0,
                "pope benedict": 0
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Though perhaps more famous as butter, which of these is a location in Florida?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "tillamook"
            ],
            "lines": [
                [
                    0.2079231175115504,
                    0.12379045712379046,
                    0.5,
                    0,
                    0.9516994633273703,
                    0.4852944495858068,
                    0.5286504658694896,
                    0.24325853290590232,
                    0.19230769230769232,
                    0.6330275229357798,
                    0.48717948717948717,
                    0.3897498066100443,
                    0.0,
                    0,
                    -1.0
                ],
                [
                    0.3250421476648263,
                    0.29996663329996665,
                    0.5,
                    0,
                    0.024686940966010734,
                    0.5078662844502629,
                    0.4713053305887314,
                    0.14388082217612672,
                    0.5,
                    0.3669724770642202,
                    0.5128205128205128,
                    0.301383162258646,
                    1.0,
                    0,
                    -1.0
                ],
                [
                    0.4670347348236233,
                    0.5762429095762429,
                    0.0,
                    0,
                    0.023613595706618962,
                    0.006839265963930208,
                    4.420354177891778e-05,
                    0.612860644917971,
                    0.3076923076923077,
                    0.0,
                    0.0,
                    0.30886703113130975,
                    0.0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "tillamook": 0.39524008294640944,
                "kerrygold": 0.41282702594077536,
                "land o\u2019 lakes": 0.19193289111281525
            },
            "question": "though perhaps more famous as butter, which of these is a location in florida?",
            "rate_limited": false,
            "answers": [
                "tillamook",
                "kerrygold",
                "land o\u2019 lakes"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "tillamook": 0.464268179208271,
                "kerrygold": 0.4377076900462353,
                "land o\u2019 lakes": 0.2095814265099074
            },
            "integer_answers": {
                "tillamook": 5,
                "kerrygold": 4,
                "land o\u2019 lakes": 3
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.5589992264401773,
                    1.205532649034584,
                    1.235468124525239
                ],
                "result_count_important_words": [
                    885000.0,
                    789000.0,
                    74.0
                ],
                "wikipedia_search": [
                    0.38461538461538464,
                    1.0,
                    0.6153846153846154
                ],
                "word_count_appended_bing": [
                    38.0,
                    40.0,
                    0.0
                ],
                "answer_relation_to_question_bing": [
                    0.3713713713713714,
                    0.8998998998998999,
                    1.7287287287287287
                ],
                "question_related_to_answer": [
                    0.5,
                    0.5,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    129000.0,
                    76300.0,
                    325000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    7.0,
                    0.0
                ],
                "result_count_bing": [
                    2150000.0,
                    2250000.0,
                    30300.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    2660.0,
                    69.0,
                    66.0
                ],
                "answer_relation_to_question": [
                    0.8316924700462016,
                    1.3001685906593052,
                    1.8681389392944932
                ],
                "word_count_appended": [
                    207.0,
                    120.0,
                    0.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these sharks is NOT a Lamniforme?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "goblin shark",
                "hammerhead shark"
            ],
            "lines": [
                [
                    0.3473451327433628,
                    0.3446601941747573,
                    0.4021739130434783,
                    0.0,
                    0.47118719928387603,
                    0.4876368217969919,
                    0.48247589872961527,
                    0.4460111122759199,
                    0.4612794612794613,
                    0.2926208651399491,
                    0.34057971014492755,
                    0.32647369444670593,
                    0,
                    0.5,
                    -1.0
                ],
                [
                    0.2558997050147493,
                    0.3932038834951456,
                    0.18115942028985507,
                    0.5,
                    0.05242251314758867,
                    0.1279234585400425,
                    0.08509775655464458,
                    0.0984904077995597,
                    0.14814814814814814,
                    0.3040712468193384,
                    0.3188405797101449,
                    0.3375772861316906,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.3967551622418879,
                    0.2621359223300971,
                    0.4166666666666667,
                    0.5,
                    0.4763902875685353,
                    0.3844397196629656,
                    0.43242634471574015,
                    0.4554984799245204,
                    0.39057239057239057,
                    0.4033078880407125,
                    0.34057971014492755,
                    0.3359490194216035,
                    0,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "goblin shark": 0.2457778456832238,
                "great white shark": 0.5687947068229373,
                "hammerhead shark": 0.1854274474938389
            },
            "question": "which of these sharks is not a lamniforme?",
            "rate_limited": false,
            "answers": [
                "goblin shark",
                "great white shark",
                "hammerhead shark"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "goblin shark": 0.354545455177625,
                "great white shark": 0.3194444440305233,
                "hammerhead shark": 0.354545455177625
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.6941052222131763,
                    0.6496908554732377,
                    0.656203922313586
                ],
                "result_count_important_words": [
                    38900.0,
                    921000.0,
                    150000.0
                ],
                "wikipedia_search": [
                    0.07744107744107744,
                    0.7037037037037037,
                    0.21885521885521886
                ],
                "word_count_appended_bing": [
                    22.0,
                    25.0,
                    22.0
                ],
                "answer_relation_to_question_bing": [
                    0.3106796116504854,
                    0.21359223300970873,
                    0.47572815533980584
                ],
                "question_related_to_answer": [
                    0.391304347826087,
                    1.2753623188405796,
                    0.3333333333333333
                ],
                "question_related_to_answer_bing": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    103000.0,
                    766000.0,
                    84900.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    62800.0,
                    1890000.0,
                    587000.0
                ],
                "word_count_raw": [
                    0.0,
                    2.0,
                    0.0
                ],
                "word_count_appended": [
                    163.0,
                    154.0,
                    76.0
                ],
                "answer_relation_to_question": [
                    0.6106194690265487,
                    0.9764011799410028,
                    0.41297935103244837
                ],
                "result_count": [
                    103000.0,
                    1600000.0,
                    84400.0
                ]
            },
            "integer_answers": {
                "goblin shark": 3,
                "great white shark": 9,
                "hammerhead shark": 1
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which Oscar-winning actress has NOT won the award for playing a real person?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "emma thompson",
                "susan sarandon",
                "hilary swank"
            ],
            "lines": [
                [
                    0.3553046646704743,
                    0.30081502480510275,
                    0.25,
                    0,
                    0.3656387665198238,
                    0.45627839065541853,
                    0.3445378151260504,
                    0.3428030303030303,
                    0.15571548907652633,
                    0.3181818181818182,
                    0.36000000000000004,
                    0.3616979926390609,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.3376995502223803,
                    0.39480020924032266,
                    0.5,
                    0,
                    0.3303964757709251,
                    0.05548345230369889,
                    0.3319327731092437,
                    0.3352272727272727,
                    0.3973421303873431,
                    0.3446969696969697,
                    0.31000000000000005,
                    0.329408261496122,
                    0.5,
                    0.0,
                    -1.0
                ],
                [
                    0.3069957851071454,
                    0.30438476595457464,
                    0.25,
                    0,
                    0.30396475770925113,
                    0.4882381570408825,
                    0.32352941176470584,
                    0.32196969696969696,
                    0.44694238053613056,
                    0.33712121212121215,
                    0.33,
                    0.3088937458648171,
                    0.0,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "hilary swank": 0.35045539798947445,
                "emma thompson": 0.2906195396957992,
                "susan sarandon": 0.3589250623147264
            },
            "question": "which oscar-winning actress has not won the award for playing a real person?",
            "rate_limited": false,
            "answers": [
                "emma thompson",
                "susan sarandon",
                "hilary swank"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "hilary swank": 0.0,
                "emma thompson": 0.0,
                "susan sarandon": 0.0
            },
            "integer_answers": {
                "hilary swank": 6,
                "emma thompson": 4,
                "susan sarandon": 3
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.212832117775026,
                    2.7294678160620482,
                    3.057700066162926
                ],
                "result_count_important_words": [
                    74.0,
                    80.0,
                    84.0
                ],
                "wikipedia_search": [
                    5.508552174775579,
                    1.6425259138025097,
                    0.8489219114219113
                ],
                "answer_relation_to_question": [
                    2.315125365272411,
                    2.5968071964419157,
                    3.0880674382856736
                ],
                "word_count_appended_bing": [
                    28.0,
                    38.0,
                    34.0
                ],
                "answer_relation_to_question_bing": [
                    2.7885896527285614,
                    1.472797070635483,
                    2.7386132766359554
                ],
                "question_related_to_answer": [
                    0.5,
                    0.0,
                    0.5
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    83.0,
                    87.0,
                    94.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count_bing": [
                    539000.0,
                    5480000.0,
                    145000.0
                ],
                "result_count": [
                    61.0,
                    77.0,
                    89.0
                ],
                "word_count_appended": [
                    96.0,
                    82.0,
                    86.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "According to Alexa, which of these is NOT one the top five most popular sports sites?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "sports illustrated"
            ],
            "lines": [
                [
                    0.37269975786924936,
                    0.2297734627831715,
                    0.375,
                    0,
                    0.33682830930537355,
                    0.3329200247985121,
                    0.4687819856704197,
                    0.4657657657657658,
                    0.29144171779141104,
                    0.41860465116279066,
                    0.46875,
                    0.3377066223248705,
                    0.375,
                    0,
                    -1.0
                ],
                [
                    0.2819270449521002,
                    0.2977346278317152,
                    0.125,
                    0,
                    0.27719528178243774,
                    0.3326100433973962,
                    0.07569554294221642,
                    0.17477477477477477,
                    0.37903374233128834,
                    0.3851744186046512,
                    0.453125,
                    0.3239598743172041,
                    0.125,
                    0,
                    -1.0
                ],
                [
                    0.3453731971786504,
                    0.47249190938511326,
                    0.5,
                    0,
                    0.38597640891218876,
                    0.3344699318040918,
                    0.4555224713873639,
                    0.35945945945945945,
                    0.3295245398773006,
                    0.19622093023255816,
                    0.078125,
                    0.33833350335792545,
                    0.5,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "deadspin": 0.2840837747342247,
                "sports illustrated": 0.4614616081777027,
                "yahoo sports": 0.25445461708807265
            },
            "question": "according to alexa, which of these is not one the top five most popular sports sites?",
            "rate_limited": false,
            "answers": [
                "yahoo sports",
                "sports illustrated",
                "deadspin"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "deadspin": 0.3473720330909726,
                "sports illustrated": 0.41711354587026356,
                "yahoo sports": 0.16144794815874156
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.622933776751295,
                    1.7604012568279592,
                    1.6166649664207458
                ],
                "result_count_important_words": [
                    67100.0,
                    912000.0,
                    95600.0
                ],
                "wikipedia_search": [
                    2.0855828220858896,
                    1.2096625766871165,
                    1.7047546012269938
                ],
                "word_count_appended_bing": [
                    2.0,
                    3.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    0.540453074433657,
                    0.4045307443365696,
                    0.05501618122977346
                ],
                "question_related_to_answer": [
                    0.25,
                    0.75,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    38000.0,
                    361000.0,
                    156000.0
                ],
                "word_count_noun_chunks": [
                    1.0,
                    3.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    53900000.0,
                    54000000.0,
                    53400000.0
                ],
                "word_count_appended": [
                    56.0,
                    79.0,
                    209.0
                ],
                "answer_relation_to_question": [
                    1.018401937046005,
                    1.7445836403831982,
                    1.2370144225707969
                ],
                "result_count": [
                    249000.0,
                    340000.0,
                    174000.0
                ]
            },
            "integer_answers": {
                "deadspin": 2,
                "sports illustrated": 8,
                "yahoo sports": 2
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "The best-selling book \u201cThe Chocolate War\u201d is about what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "the rise of hershey's"
            ],
            "lines": [
                [
                    0.22427055702917773,
                    0.10500575373993097,
                    0,
                    0,
                    0.07142857142857142,
                    0.733464309102816,
                    1.1542767878785546e-05,
                    1.0862469191321756e-06,
                    0.061224489795918366,
                    0.38461538461538464,
                    0.15384615384615385,
                    0.2927518991045213,
                    0,
                    0,
                    1.0
                ],
                [
                    0.12756557618626582,
                    0.3777330264672037,
                    0,
                    0,
                    0.9285714285714286,
                    0.14931237721021612,
                    0.14582363420199074,
                    0.01694545193846194,
                    0.2571428571428571,
                    0.38461538461538464,
                    0.6923076923076923,
                    0.5137468348257979,
                    0,
                    0,
                    1.0
                ],
                [
                    0.6481638667845564,
                    0.5172612197928654,
                    0,
                    0,
                    0.0,
                    0.11722331368696791,
                    0.8541648230301305,
                    0.983053461814619,
                    0.6816326530612244,
                    0.23076923076923078,
                    0.15384615384615385,
                    0.19350126606968082,
                    0,
                    0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "high school conformity": 0.20266197476772718,
                "the rise of hershey's": 0.43796159888554287,
                "sugar addiction": 0.35937642634672995
            },
            "question": "the best-selling book \u201cthe chocolate war\u201d is about what?",
            "rate_limited": false,
            "answers": [
                "high school conformity",
                "sugar addiction",
                "the rise of hershey's"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "high school conformity": 0.29999999900658925,
                "the rise of hershey's": 0.3194444440305233,
                "sugar addiction": 0.29999999900658925
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.4637594955226065,
                    2.5687341741289895,
                    0.9675063303484042
                ],
                "result_count_important_words": [
                    3.0,
                    37900.0,
                    222000.0
                ],
                "wikipedia_search": [
                    0.30612244897959184,
                    1.2857142857142856,
                    3.4081632653061225
                ],
                "word_count_appended_bing": [
                    2.0,
                    9.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.42002301495972383,
                    1.5109321058688145,
                    2.0690448791714613
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    2.0,
                    31200.0,
                    1810000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    2240000.0,
                    456000.0,
                    358000.0
                ],
                "word_count_appended": [
                    5.0,
                    5.0,
                    3.0
                ],
                "answer_relation_to_question": [
                    1.1213527851458887,
                    0.6378278809313291,
                    3.240819333922782
                ],
                "result_count": [
                    2.0,
                    26.0,
                    0
                ]
            },
            "integer_answers": {
                "high school conformity": 2,
                "the rise of hershey's": 5,
                "sugar addiction": 3
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "The \u201cAmerican Craftsman\u201d style of house was an architectural reaction to what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "industrial revolution"
            ],
            "lines": [
                [
                    0.14941372665923564,
                    0.18517316017316016,
                    0.16666666666666666,
                    0,
                    0.0009607589996096916,
                    0.40625,
                    0.27876106194690264,
                    0.017743979721166033,
                    0.4568860134502426,
                    0.22727272727272727,
                    0.25,
                    0.2900549158428909,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.2911043713438923,
                    0.633982683982684,
                    0.8333333333333334,
                    0,
                    0.9982886480319453,
                    0.3549107142857143,
                    0.4336283185840708,
                    0.9664131812420785,
                    0.36962458500042567,
                    0.6181818181818182,
                    0.625,
                    0.493895587404564,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.5594819019968721,
                    0.18084415584415586,
                    0.0,
                    0,
                    0.0007505929684450717,
                    0.23883928571428573,
                    0.28761061946902655,
                    0.015842839036755388,
                    0.17348940154933173,
                    0.15454545454545454,
                    0.125,
                    0.21604949675254506,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "world war i": 0.18686023159481552,
                "industrial revolution": 0.662951018568502,
                "the great depression": 0.15018874983668248
            },
            "question": "the \u201camerican craftsman\u201d style of house was an architectural reaction to what?",
            "rate_limited": false,
            "answers": [
                "world war i",
                "industrial revolution",
                "the great depression"
            ],
            "ml_answers": {
                "world war i": 0.05879150653791562,
                "industrial revolution": 0.8730864853095323,
                "the great depression": 0.19867426551145378
            },
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "integer_answers": {
                "world war i": 2,
                "industrial revolution": 10,
                "the great depression": 1
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7403294950573454,
                    2.963373524427384,
                    1.2962969805152704
                ],
                "result_count_important_words": [
                    6300000.0,
                    9800000.0,
                    6500000.0
                ],
                "wikipedia_search": [
                    2.284430067251213,
                    1.8481229250021283,
                    0.8674470077466587
                ],
                "word_count_appended_bing": [
                    4.0,
                    10.0,
                    2.0
                ],
                "answer_relation_to_question_bing": [
                    0.7406926406926406,
                    2.535930735930736,
                    0.7233766233766235
                ],
                "question_related_to_answer": [
                    0.16666666666666666,
                    0.8333333333333334,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    56.0,
                    3050.0,
                    50.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    11.0,
                    0.0
                ],
                "result_count_bing": [
                    182000.0,
                    159000.0,
                    107000.0
                ],
                "word_count_raw": [
                    0.0,
                    12.0,
                    0.0
                ],
                "result_count": [
                    64.0,
                    66500.0,
                    50.0
                ],
                "answer_relation_to_question": [
                    0.7470686332961782,
                    1.4555218567194614,
                    2.7974095099843606
                ],
                "word_count_appended": [
                    25.0,
                    68.0,
                    17.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "One symptom of argyria is turning roughly the same skin color as which cartoon character?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "papa smurf",
                "the grinch",
                "garfield"
            ],
            "lines": [
                [
                    0.4520805884936319,
                    0.25529371584699456,
                    0.5,
                    0,
                    0.01639344262295082,
                    0.14711262085184218,
                    0.03036576949620428,
                    0.030032467532467532,
                    0.0347008547008547,
                    0.09375,
                    0.17142857142857143,
                    0.25340093988544543,
                    0,
                    1.0,
                    -1.0
                ],
                [
                    0.13180171277997363,
                    0.24043715846994537,
                    0.0,
                    0,
                    0.00819672131147541,
                    0.02978834596289522,
                    0.01725327812284334,
                    0.012175324675324676,
                    0.06256410256410257,
                    0.09375,
                    0.05714285714285714,
                    0.21086893988736866,
                    0,
                    0.0,
                    -1.0
                ],
                [
                    0.41611769872639437,
                    0.5042691256830601,
                    0.5,
                    0,
                    0.9754098360655737,
                    0.8230990331852626,
                    0.9523809523809523,
                    0.9577922077922078,
                    0.9027350427350427,
                    0.8125,
                    0.7714285714285715,
                    0.5357301202271859,
                    0,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "garfield": 0.6792885490186876,
                "the grinch": 0.07199820340973216,
                "papa smurf": 0.24871324757158023
            },
            "question": "one symptom of argyria is turning roughly the same skin color as which cartoon character?",
            "rate_limited": false,
            "answers": [
                "papa smurf",
                "the grinch",
                "garfield"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "garfield": 0.0,
                "the grinch": 0.0,
                "papa smurf": 0.0
            },
            "integer_answers": {
                "garfield": 9,
                "the grinch": 0,
                "papa smurf": 3
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.0272075190835634,
                    1.6869515190989492,
                    4.285840961817487
                ],
                "result_count_important_words": [
                    44.0,
                    25.0,
                    1380.0
                ],
                "wikipedia_search": [
                    0.1735042735042735,
                    0.3128205128205128,
                    4.513675213675214
                ],
                "word_count_appended_bing": [
                    6.0,
                    2.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    1.021174863387978,
                    0.9617486338797814,
                    2.01707650273224
                ],
                "question_related_to_answer": [
                    1.0,
                    0.0,
                    1.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    37.0,
                    15.0,
                    1180.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    5630.0,
                    1140.0,
                    31500.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    20.0,
                    10.0,
                    1190.0
                ],
                "answer_relation_to_question": [
                    2.7124835309617916,
                    0.7908102766798418,
                    2.496706192358366
                ],
                "word_count_appended": [
                    6.0,
                    6.0,
                    52.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which three-letter-titled movie grossed the most worldwide?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ray"
            ],
            "lines": [
                [
                    0.6027777777777777,
                    0.3418803418803419,
                    0.2222222222222222,
                    0.0,
                    0.6121449559255632,
                    0.07754010695187166,
                    0.3033419023136247,
                    0.6405744051809095,
                    0.0,
                    0.3635220125786163,
                    0.4583333333333333,
                    0.34916641693412914,
                    0.2222222222222222,
                    0.14285714285714285,
                    -1.0
                ],
                [
                    0.044444444444444446,
                    0.5213675213675214,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.08863858961802155,
                    0.44786096256684493,
                    0.2917737789203085,
                    0.2196255103477404,
                    0.0,
                    0.3522012578616352,
                    0.26666666666666666,
                    0.32032513386139294,
                    0.3333333333333333,
                    0.7142857142857143,
                    -1.0
                ],
                [
                    0.3527777777777778,
                    0.13675213675213677,
                    0.4444444444444444,
                    0.6666666666666666,
                    0.29921645445641526,
                    0.47459893048128343,
                    0.40488431876606684,
                    0.13980008447135014,
                    1.0,
                    0.2842767295597484,
                    0.275,
                    0.3305084492044779,
                    0.4444444444444444,
                    0.14285714285714285,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "big": 0.3854448271344254,
                "saw": 0.3097559171555539,
                "ray": 0.3047992557100207
            },
            "question": "which three-letter-titled movie grossed the most worldwide?",
            "rate_limited": false,
            "answers": [
                "saw",
                "ray",
                "big"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "big": 0.18737494940353583,
                "saw": 0.16444558524912584,
                "ray": 0.38913208094671076
            },
            "integer_answers": {
                "big": 6,
                "saw": 6,
                "ray": 2
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.7458320846706457,
                    1.6016256693069646,
                    1.6525422460223895
                ],
                "result_count_important_words": [
                    944000.0,
                    908000.0,
                    1260000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_appended_bing": [
                    55.0,
                    32.0,
                    33.0
                ],
                "answer_relation_to_question_bing": [
                    1.0256410256410255,
                    1.564102564102564,
                    0.41025641025641024
                ],
                "question_related_to_answer": [
                    0.2222222222222222,
                    0.3333333333333333,
                    0.4444444444444444
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.3333333333333333,
                    0.6666666666666666
                ],
                "result_count_noun_chunks": [
                    4550000.0,
                    1560000.0,
                    993000.0
                ],
                "word_count_noun_chunks": [
                    2.0,
                    3.0,
                    4.0
                ],
                "result_count_bing": [
                    8120000.0,
                    46900000.0,
                    49700000.0
                ],
                "word_count_raw": [
                    1.0,
                    5.0,
                    1.0
                ],
                "result_count": [
                    12500000.0,
                    1810000.0,
                    6110000.0
                ],
                "answer_relation_to_question": [
                    1.8083333333333333,
                    0.13333333333333333,
                    1.0583333333333333
                ],
                "word_count_appended": [
                    289.0,
                    280.0,
                    226.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Talking is discouraged on what Amtrak car?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "quiet car"
            ],
            "lines": [
                [
                    0.13212025316455697,
                    0.10784313725490197,
                    0,
                    0,
                    0.9887211172175522,
                    0.20865613056502427,
                    0.9902403611036016,
                    0.6668817898246746,
                    0.20454545454545456,
                    0.08870967741935484,
                    0.13333333333333333,
                    0.30474681743431625,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.07518598711969797,
                    0.06372549019607843,
                    0,
                    0,
                    0.010894587404812273,
                    0.7437248218159281,
                    0.009446771543043561,
                    0.2667527159298699,
                    0.20454545454545456,
                    0.0,
                    0.0,
                    0.3496430497742442,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.792693759715745,
                    0.8284313725490196,
                    0,
                    0,
                    0.0003842953776355014,
                    0.047619047619047616,
                    0.0003128673533548189,
                    0.06636549424545553,
                    0.5909090909090909,
                    0.9112903225806451,
                    0.8666666666666667,
                    0.34561013279143954,
                    0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "sports argument car": 0.3477998247147973,
                "quiet car": 0.4954802772552819,
                "meet & greet car": 0.15671989802992084
            },
            "question": "talking is discouraged on what amtrak car?",
            "rate_limited": false,
            "answers": [
                "sports argument car",
                "meet & greet car",
                "quiet car"
            ],
            "ml_answers": {
                "sports argument car": 0.06654845735495415,
                "quiet car": 0.6820485739681009,
                "meet & greet car": 0.09861671802946721
            },
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "integer_answers": {
                "sports argument car": 3,
                "quiet car": 6,
                "meet & greet car": 2
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.218987269737265,
                    1.3985721990969768,
                    1.3824405311657582
                ],
                "result_count_important_words": [
                    3260000.0,
                    31100.0,
                    1030.0
                ],
                "wikipedia_search": [
                    0.4090909090909091,
                    0.4090909090909091,
                    1.1818181818181819
                ],
                "word_count_appended_bing": [
                    2.0,
                    0.0,
                    13.0
                ],
                "answer_relation_to_question_bing": [
                    0.21568627450980393,
                    0.12745098039215685,
                    1.656862745098039
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    620000.0,
                    248000.0,
                    61700.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    4040000.0,
                    14400000.0,
                    922000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    3.0
                ],
                "result_count": [
                    2650000.0,
                    29200.0,
                    1030.0
                ],
                "answer_relation_to_question": [
                    0.3963607594936709,
                    0.22555796135909392,
                    2.378081279147235
                ],
                "word_count_appended": [
                    11.0,
                    0.0,
                    113.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "The creator of Wonder Woman also created an early version of what device?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "lie detector"
            ],
            "lines": [
                [
                    0.23047619047619045,
                    0.5050505050505051,
                    0,
                    0,
                    0.3082706766917293,
                    0.10822861775435752,
                    0.9947396816833018,
                    0.9946314869532648,
                    0.03469387755102041,
                    0.14728682170542637,
                    0.05555555555555555,
                    0.250390021250332,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.4542857142857143,
                    0.32323232323232326,
                    0,
                    0,
                    0.15789473684210525,
                    0.4458856911228212,
                    0.002394119233881845,
                    0.002455366432227725,
                    0.3508967223252938,
                    0.32558139534883723,
                    0.5,
                    0.2042749703172686,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.3152380952380952,
                    0.1717171717171717,
                    0,
                    0,
                    0.5338345864661654,
                    0.4458856911228212,
                    0.0028661990828162936,
                    0.0029131466145074702,
                    0.6144094001236857,
                    0.5271317829457365,
                    0.4444444444444444,
                    0.5453350084323993,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "lie detector": 0.466981293848987,
                "hearing aid": 0.23057508659503942,
                "magic marker": 0.3024436195559736
            },
            "question": "the creator of wonder woman also created an early version of what device?",
            "rate_limited": false,
            "answers": [
                "magic marker",
                "hearing aid",
                "lie detector"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "lie detector": 0.8407307550286011,
                "hearing aid": 0.4382141899240651,
                "magic marker": 0.11104974967537298
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.752730148752324,
                    1.4299247922208802,
                    3.8173450590267954
                ],
                "result_count_important_words": [
                    29500.0,
                    71.0,
                    85.0
                ],
                "wikipedia_search": [
                    0.24285714285714285,
                    2.4562770562770564,
                    4.3008658008658
                ],
                "word_count_appended_bing": [
                    2.0,
                    18.0,
                    16.0
                ],
                "answer_relation_to_question_bing": [
                    1.5151515151515151,
                    0.9696969696969697,
                    0.5151515151515151
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    23900.0,
                    59.0,
                    70.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    11.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    25.0
                ],
                "result_count_bing": [
                    267000.0,
                    1100000.0,
                    1100000.0
                ],
                "word_count_appended": [
                    19.0,
                    42.0,
                    68.0
                ],
                "answer_relation_to_question": [
                    1.1523809523809523,
                    2.2714285714285714,
                    1.576190476190476
                ],
                "result_count": [
                    41.0,
                    21.0,
                    71.0
                ]
            },
            "integer_answers": {
                "lie detector": 6,
                "hearing aid": 3,
                "magic marker": 3
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these verbs has two meanings that are opposites of each other?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "cleave"
            ],
            "lines": [
                [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.1702127659574468,
                    0.21795487277964473,
                    0.3160100586756077,
                    0.3491573961890076,
                    0,
                    0.31666666666666665,
                    0.29347826086956524,
                    0.3295442722356468,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.16666666666666669,
                    0.0,
                    0.0,
                    0.0,
                    0.3735224586288416,
                    0.6841094575132021,
                    0.20955574182732606,
                    0.047251899988985574,
                    0,
                    0.2777777777777778,
                    0.25,
                    0.29317139773310363,
                    0.3333333333333333,
                    0.0,
                    -1.0
                ],
                [
                    0.8333333333333334,
                    1.0,
                    1.0,
                    1.0,
                    0.4562647754137116,
                    0.09793566970715314,
                    0.4744341994970662,
                    0.6035907038220069,
                    0,
                    0.40555555555555556,
                    0.45652173913043476,
                    0.37728433003124956,
                    0.6666666666666666,
                    1.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "cleave": 0.6439682287043984,
                "branch": 0.15330956102873736,
                "jut": 0.20272221026686438
            },
            "question": "which of these verbs has two meanings that are opposites of each other?",
            "rate_limited": false,
            "answers": [
                "branch",
                "jut",
                "cleave"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "cleave": 0.354545455177625,
                "branch": 0.31499999945362406,
                "jut": 0.29999999900658925
            },
            "integer_answers": {
                "cleave": 12,
                "branch": 0,
                "jut": 1
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.9886328167069405,
                    0.8795141931993109,
                    1.1318529900937486
                ],
                "result_count_important_words": [
                    377000.0,
                    250000.0,
                    566000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    27.0,
                    23.0,
                    42.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    1.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_noun_chunks": [
                    3170000.0,
                    429000.0,
                    5480000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    2.0
                ],
                "result_count_bing": [
                    9080000.0,
                    28500000.0,
                    4080000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    2.0
                ],
                "result_count": [
                    2880000.0,
                    6320000.0,
                    7720000.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    0.3333333333333333,
                    1.6666666666666665
                ],
                "word_count_appended": [
                    228.0,
                    200.0,
                    292.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these is NOT a real animal?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "jackalope",
                "wholphin"
            ],
            "lines": [
                [
                    0.39601449275362316,
                    0.32333333333333336,
                    0.5,
                    0.0,
                    0.2763358778625954,
                    0.33240997229916897,
                    0.38793538616860174,
                    0.3877396569122099,
                    0.16379310344827586,
                    0.3161225849433711,
                    0.32899628252788105,
                    0.35089668782107786,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0.26859903381642514,
                    0.30666666666666664,
                    0.0,
                    0.5,
                    0.27251908396946567,
                    0.33379501385041555,
                    0.3695103483089349,
                    0.36957618567103934,
                    0.39655172413793105,
                    0.29047301798800795,
                    0.3215613382899628,
                    0.32678061464084107,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.33538647342995165,
                    0.37,
                    0.5,
                    0.5,
                    0.45114503816793894,
                    0.33379501385041555,
                    0.2425542655224634,
                    0.24268415741675076,
                    0.4396551724137931,
                    0.3934043970686209,
                    0.34944237918215615,
                    0.32232269753808107,
                    0.5,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "liger": 0.4634238532371871,
                "wholphin": 0.21708720077283264,
                "jackalope": 0.3194889459899803
            },
            "question": "which of these is not a real animal?",
            "rate_limited": false,
            "answers": [
                "jackalope",
                "liger",
                "wholphin"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "liger": 0.3194444440305233,
                "wholphin": 0.354545455177625,
                "jackalope": 0.354545455177625
            },
            "integer_answers": {
                "liger": 8,
                "wholphin": 3,
                "jackalope": 3
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.5964132487156885,
                    0.6928775414366357,
                    0.7107092098476758
                ],
                "result_count_important_words": [
                    444000.0,
                    517000.0,
                    1020000.0
                ],
                "wikipedia_search": [
                    1.3448275862068966,
                    0.41379310344827586,
                    0.2413793103448276
                ],
                "word_count_appended_bing": [
                    92.0,
                    96.0,
                    81.0
                ],
                "answer_relation_to_question_bing": [
                    0.7066666666666667,
                    0.7733333333333333,
                    0.52
                ],
                "question_related_to_answer": [
                    0.0,
                    1.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    445000.0,
                    517000.0,
                    1020000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    9.0,
                    0.0
                ],
                "result_count_bing": [
                    121000000.0,
                    120000000.0,
                    120000000.0
                ],
                "word_count_raw": [
                    0.0,
                    7.0,
                    0.0
                ],
                "word_count_appended": [
                    552.0,
                    629.0,
                    320.0
                ],
                "answer_relation_to_question": [
                    0.41594202898550725,
                    0.9256038647342995,
                    0.6584541062801932
                ],
                "result_count": [
                    879000.0,
                    894000.0,
                    192000.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which Hawaiian island has active volcanoes?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "big island",
                "maui",
                "oahu"
            ],
            "lines": [
                [
                    0.6842969769693907,
                    0.6446786556603774,
                    0.6,
                    0.1875,
                    0.36961451247165533,
                    0.09718309859154929,
                    0.38183161004431315,
                    0.36981132075471695,
                    0.0,
                    0.1728247914183552,
                    0.13675213675213677,
                    0.36510498039149597,
                    0.047619047619047616,
                    0.05,
                    -1.0
                ],
                [
                    0.16271946444360239,
                    0.16241647012578614,
                    0.24,
                    0.4375,
                    0.34467120181405897,
                    0.7887323943661971,
                    0.33677991137370755,
                    0.3441509433962264,
                    0.3795045045045045,
                    0.4445768772348033,
                    0.42735042735042733,
                    0.32227948850469523,
                    0.5476190476190477,
                    0.575,
                    -1.0
                ],
                [
                    0.15298355858700685,
                    0.19290487421383645,
                    0.16,
                    0.375,
                    0.2857142857142857,
                    0.11408450704225352,
                    0.2813884785819793,
                    0.2860377358490566,
                    0.6204954954954954,
                    0.38259833134684146,
                    0.4358974358974359,
                    0.31261553110380885,
                    0.40476190476190477,
                    0.375,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "maui": 0.3938071950523612,
                "oahu": 0.3128201527567075,
                "big island": 0.29337265219093134
            },
            "question": "which hawaiian island has active volcanoes?",
            "rate_limited": false,
            "answers": [
                "big island",
                "maui",
                "oahu"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "maui": 0.0,
                "oahu": 0.0,
                "big island": 0.0
            },
            "integer_answers": {
                "maui": 5,
                "oahu": 2,
                "big island": 7
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.4604199215659839,
                    1.289117954018781,
                    1.2504621244152354
                ],
                "result_count_important_words": [
                    517000.0,
                    456000.0,
                    381000.0
                ],
                "wikipedia_search": [
                    0.0,
                    1.1385135135135136,
                    1.8614864864864864
                ],
                "word_count_appended_bing": [
                    16.0,
                    50.0,
                    51.0
                ],
                "answer_relation_to_question_bing": [
                    2.5787146226415096,
                    0.6496658805031446,
                    0.7716194968553458
                ],
                "question_related_to_answer": [
                    1.2,
                    0.48,
                    0.32
                ],
                "question_related_to_answer_bing": [
                    0.1875,
                    0.4375,
                    0.375
                ],
                "result_count_noun_chunks": [
                    490000.0,
                    456000.0,
                    379000.0
                ],
                "word_count_noun_chunks": [
                    2.0,
                    23.0,
                    17.0
                ],
                "result_count_bing": [
                    138000.0,
                    1120000.0,
                    162000.0
                ],
                "word_count_raw": [
                    2.0,
                    23.0,
                    15.0
                ],
                "result_count": [
                    489000.0,
                    456000.0,
                    378000.0
                ],
                "answer_relation_to_question": [
                    2.737187907877563,
                    0.6508778577744095,
                    0.6119342343480274
                ],
                "word_count_appended": [
                    145.0,
                    373.0,
                    321.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Pixies, Bon Iver, Iron & Wine and Bauhaus were all once signed to which record label?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "subpop",
                "4ad",
                "geffen"
            ],
            "lines": [
                [
                    0.3111097856728633,
                    0.2599097331240188,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.3325062034739454,
                    0.010835913312693499,
                    0.5755064456721916,
                    0.3812222222222222,
                    0.06666666666666667,
                    0.25,
                    0.3333333333333333,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.5145404868185102,
                    0.4640077184720042,
                    1.0,
                    1.0,
                    0.3333333333333333,
                    0.3349875930521092,
                    0.15944272445820434,
                    0.2532228360957643,
                    0.6154444444444445,
                    0.8666666666666667,
                    0.5,
                    0.3333333333333333,
                    0.9655172413793104,
                    1.0,
                    -1.0
                ],
                [
                    0.17434972750862668,
                    0.276082548403977,
                    0.0,
                    0.0,
                    0.3333333333333333,
                    0.3325062034739454,
                    0.8297213622291022,
                    0.1712707182320442,
                    0.0033333333333333335,
                    0.06666666666666667,
                    0.25,
                    0.3333333333333333,
                    0.034482758620689655,
                    0.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "4ad": 0.5957497412895486,
                "geffen": 0.20036285608107512,
                "subpop": 0.20388740262937627
            },
            "question": "pixies, bon iver, iron & wine and bauhaus were all once signed to which record label?",
            "rate_limited": false,
            "answers": [
                "subpop",
                "4ad",
                "geffen"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "4ad": 0.0,
                "geffen": 0.0,
                "subpop": 0.0
            },
            "integer_answers": {
                "4ad": 10,
                "geffen": 1,
                "subpop": 3
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.6666666666666665,
                    2.6666666666666665,
                    2.6666666666666665
                ],
                "result_count_important_words": [
                    35.0,
                    515.0,
                    2680.0
                ],
                "wikipedia_search": [
                    2.287333333333333,
                    3.6926666666666668,
                    0.02
                ],
                "word_count_appended_bing": [
                    8.0,
                    16.0,
                    8.0
                ],
                "answer_relation_to_question_bing": [
                    1.8193681318681318,
                    3.2480540293040296,
                    1.932577838827839
                ],
                "question_related_to_answer": [
                    0.0,
                    1.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    125000.0,
                    55000.0,
                    37200.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    84.0,
                    3.0
                ],
                "result_count_bing": [
                    1340000.0,
                    1350000.0,
                    1340000.0
                ],
                "word_count_raw": [
                    0.0,
                    12.0,
                    0.0
                ],
                "result_count": [
                    31.0,
                    31.0,
                    31.0
                ],
                "answer_relation_to_question": [
                    2.488878285382906,
                    4.1163238945480805,
                    1.3947978200690132
                ],
                "word_count_appended": [
                    1.0,
                    13.0,
                    1.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these would an oologist study?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "human liver"
            ],
            "lines": [
                [
                    0.0,
                    0,
                    0,
                    0,
                    0.0484196368527236,
                    0.07602663706992231,
                    0.495242214532872,
                    0.05367057371992597,
                    0.0,
                    0.125,
                    0.34782608695652173,
                    0.20983483483483484,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.9230769230769231,
                    0,
                    0,
                    0,
                    0.05043712172158709,
                    0.17110617832038474,
                    0.018166089965397925,
                    0.05181986428130783,
                    0.08333333333333333,
                    0.21875,
                    0.2608695652173913,
                    0.3019269269269269,
                    0,
                    0,
                    -1.0
                ],
                [
                    0.07692307692307693,
                    0,
                    0,
                    0,
                    0.9011432414256894,
                    0.7528671846096929,
                    0.4865916955017301,
                    0.8945095619987662,
                    0.9166666666666666,
                    0.65625,
                    0.391304347826087,
                    0.4882382382382382,
                    0,
                    0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "ice cave": 0.1506688871074223,
                "human liver": 0.23105400031591694,
                "ostrich egg": 0.6182771125766607
            },
            "question": "which of these would an oologist study?",
            "rate_limited": false,
            "answers": [
                "ice cave",
                "human liver",
                "ostrich egg"
            ],
            "ml_answers": {
                "ice cave": 0.05273345734652927,
                "human liver": 0.24676766866121166,
                "ostrich egg": 0.1871833550942093
            },
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "integer_answers": {
                "ice cave": 1,
                "human liver": 1,
                "ostrich egg": 7
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.4196696696696697,
                    0.6038538538538538,
                    0.9764764764764764
                ],
                "result_count_important_words": [
                    2290.0,
                    84.0,
                    2250.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.08333333333333333,
                    0.9166666666666666
                ],
                "word_count_appended_bing": [
                    8.0,
                    6.0,
                    9.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    87.0,
                    84.0,
                    1450.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    82200.0,
                    185000.0,
                    814000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    72.0,
                    75.0,
                    1340.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    0.9230769230769231,
                    0.07692307692307693
                ],
                "word_count_appended": [
                    12.0,
                    21.0,
                    63.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What book that heavily influenced \u201cThe Matrix\u201d makes a cameo in the movie?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "neuromancer",
                "simulacra & simulation",
                "gravity's rainbow"
            ],
            "lines": [
                [
                    0.638993966166527,
                    0.6924242424242424,
                    0,
                    0,
                    0.5916499218575575,
                    0.22341302555647155,
                    0.1937984496124031,
                    0.12135517911691197,
                    0.8431818181818183,
                    0.9621451104100947,
                    0.9523809523809523,
                    0.3442268106016644,
                    0,
                    0.9473684210526315,
                    1.0
                ],
                [
                    0.16572985689744174,
                    0.08181818181818183,
                    0,
                    0,
                    0.36447867827640096,
                    0.2225886232481451,
                    0.7648578811369509,
                    0.8497639544570953,
                    0.030303030303030304,
                    0.0,
                    0.0,
                    0.36279654181599835,
                    0,
                    0.05263157894736842,
                    1.0
                ],
                [
                    0.19527617693603122,
                    0.22575757575757574,
                    0,
                    0,
                    0.04387139986604153,
                    0.5539983511953833,
                    0.041343669250646,
                    0.02888086642599278,
                    0.1265151515151515,
                    0.03785488958990536,
                    0.047619047619047616,
                    0.29297664758233716,
                    0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "simulacra & simulation": 0.26317893880914667,
                "neuromancer": 0.5919034452146614,
                "gravity's rainbow": 0.144917615976192
            },
            "question": "what book that heavily influenced \u201cthe matrix\u201d makes a cameo in the movie?",
            "rate_limited": false,
            "answers": [
                "neuromancer",
                "simulacra & simulation",
                "gravity's rainbow"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "simulacra & simulation": 0.0,
                "neuromancer": 0.0,
                "gravity's rainbow": 0.0
            },
            "integer_answers": {
                "simulacra & simulation": 3,
                "neuromancer": 7,
                "gravity's rainbow": 1
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.4095876742116507,
                    2.5395757927119886,
                    2.0508365330763603
                ],
                "result_count_important_words": [
                    375.0,
                    1480.0,
                    80.0
                ],
                "wikipedia_search": [
                    5.0590909090909095,
                    0.18181818181818182,
                    0.759090909090909
                ],
                "word_count_appended_bing": [
                    60.0,
                    0.0,
                    3.0
                ],
                "answer_relation_to_question_bing": [
                    2.077272727272727,
                    0.24545454545454548,
                    0.6772727272727272
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    4370.0,
                    30600.0,
                    1040.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    27100.0,
                    27000.0,
                    67200.0
                ],
                "word_count_raw": [
                    18.0,
                    1.0,
                    0.0
                ],
                "result_count": [
                    10600.0,
                    6530.0,
                    786.0
                ],
                "answer_relation_to_question": [
                    3.194969830832635,
                    0.8286492844872088,
                    0.9763808846801562
                ],
                "word_count_appended": [
                    305.0,
                    0.0,
                    12.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Who holds the record as the youngest solo artist with a Billboard #1 hit?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "justin bieber",
                "michael jackson",
                "stevie wonder"
            ],
            "lines": [
                [
                    0.3602218272828265,
                    0.2626579915226303,
                    0.3076923076923077,
                    0,
                    0.33226397800183316,
                    0.32671081677704195,
                    0.6165670367207515,
                    0.33492252681764006,
                    0.33041159359030725,
                    0.43478260869565216,
                    0.0,
                    0.33276831987815103,
                    0.3684210526315789,
                    0.43478260869565216,
                    0.0
                ],
                [
                    0.3132038792451629,
                    0.3284452014629028,
                    0.07692307692307693,
                    0,
                    0.3340971585701192,
                    0.34613686534216337,
                    0.2527754056362084,
                    0.33293603496225666,
                    0.5173005833887676,
                    0.13043478260869565,
                    0.0,
                    0.333589590585914,
                    0.21052631578947367,
                    0.13043478260869565,
                    0.0
                ],
                [
                    0.32657429347201067,
                    0.40889680701446685,
                    0.6153846153846154,
                    0,
                    0.33363886342804766,
                    0.3271523178807947,
                    0.13065755764304013,
                    0.3321414382201033,
                    0.15228782302092517,
                    0.43478260869565216,
                    1.0,
                    0.33364208953593494,
                    0.42105263157894735,
                    0.43478260869565216,
                    0.0
                ]
            ],
            "fraction_answers": {
                "justin bieber": 0.34170789756202863,
                "stevie wonder": 0.4039225888130916,
                "michael jackson": 0.2543695136248797
            },
            "question": "who holds the record as the youngest solo artist with a billboard #1 hit?",
            "rate_limited": false,
            "answers": [
                "justin bieber",
                "michael jackson",
                "stevie wonder"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "justin bieber": 0.3194444440305233,
                "stevie wonder": 0.3194444440305233,
                "michael jackson": 0.3194444440305233
            },
            "integer_answers": {
                "justin bieber": 5,
                "stevie wonder": 5,
                "michael jackson": 3
            },
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.6621465590252082,
                    2.668716724687312,
                    2.6691367162874795
                ],
                "result_count_important_words": [
                    72200.0,
                    29600.0,
                    15300.0
                ],
                "wikipedia_search": [
                    2.643292748722458,
                    4.138404667110141,
                    1.2183025841674013
                ],
                "word_count_appended_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "answer_relation_to_question_bing": [
                    1.3132899576131516,
                    1.642226007314514,
                    2.044484035072334
                ],
                "question_related_to_answer": [
                    0.3076923076923077,
                    0.07692307692307693,
                    0.6153846153846154
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    843000.0,
                    838000.0,
                    836000.0
                ],
                "word_count_noun_chunks": [
                    7.0,
                    4.0,
                    8.0
                ],
                "result_count_bing": [
                    7400000.0,
                    7840000.0,
                    7410000.0
                ],
                "word_count_raw": [
                    10.0,
                    3.0,
                    10.0
                ],
                "result_count": [
                    725000.0,
                    729000.0,
                    728000.0
                ],
                "answer_relation_to_question": [
                    2.5215527909797855,
                    2.1924271547161402,
                    2.2860200543040747
                ],
                "word_count_appended": [
                    10.0,
                    3.0,
                    10.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of these states does NOT touch the Mason-Dixon Line?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "west virginia"
            ],
            "lines": [
                [
                    0.4219079641350211,
                    0.30920890937019974,
                    0.44375000000000003,
                    0.4375,
                    0.12987012987012986,
                    0.2621060722521138,
                    0.36382232612507304,
                    0.3520518358531317,
                    0.3337639783124365,
                    0.4439799331103679,
                    0.4418604651162791,
                    0.3408839373271146,
                    0.3793103448275862,
                    0.3666666666666667,
                    -1.0
                ],
                [
                    0.169160952883263,
                    0.3695622119815668,
                    0.17500000000000002,
                    0.0625,
                    0.412987012987013,
                    0.3747117601844735,
                    0.3275862068965517,
                    0.3747300215982722,
                    0.3209575849994352,
                    0.2399665551839465,
                    0.23255813953488375,
                    0.3244312424633553,
                    0.15517241379310343,
                    0.2,
                    -1.0
                ],
                [
                    0.4089310829817159,
                    0.32122887864823346,
                    0.38125000000000003,
                    0.5,
                    0.45714285714285713,
                    0.36318216756341276,
                    0.30859146697837525,
                    0.2732181425485961,
                    0.3452784366881283,
                    0.31605351170568563,
                    0.32558139534883723,
                    0.3346848202095301,
                    0.46551724137931033,
                    0.43333333333333335,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "tennessee": 0.2522866664959978,
                "delaware": 0.46581084249916227,
                "west virginia": 0.28190249100483994
            },
            "question": "which of these states does not touch the mason-dixon line?",
            "rate_limited": false,
            "answers": [
                "west virginia",
                "delaware",
                "tennessee"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "tennessee": 0.16370927156903978,
                "delaware": 0.06610246191497414,
                "west virginia": 0.20948952275540586
            },
            "integer_answers": {
                "tennessee": 2,
                "delaware": 9,
                "west virginia": 3
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.591160626728854,
                    1.755687575366447,
                    1.6531517979046988
                ],
                "result_count_important_words": [
                    932000.0,
                    1180000.0,
                    1310000.0
                ],
                "wikipedia_search": [
                    0.6649440867502541,
                    0.7161696600022591,
                    0.6188862532474868
                ],
                "word_count_appended_bing": [
                    10.0,
                    46.0,
                    30.0
                ],
                "answer_relation_to_question_bing": [
                    1.1447465437788018,
                    0.7826267281105991,
                    1.072626728110599
                ],
                "question_related_to_answer": [
                    0.225,
                    1.3,
                    0.475
                ],
                "question_related_to_answer_bing": [
                    0.125,
                    0.875,
                    0.0
                ],
                "result_count_noun_chunks": [
                    137000.0,
                    116000.0,
                    210000.0
                ],
                "word_count_noun_chunks": [
                    7.0,
                    20.0,
                    2.0
                ],
                "result_count_bing": [
                    619000.0,
                    326000.0,
                    356000.0
                ],
                "word_count_raw": [
                    4.0,
                    9.0,
                    2.0
                ],
                "result_count": [
                    1140000.0,
                    268000.0,
                    132000.0
                ],
                "answer_relation_to_question": [
                    0.6247362869198312,
                    2.646712376933896,
                    0.7285513361462729
                ],
                "word_count_appended": [
                    67.0,
                    311.0,
                    220.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "One of Tupac Shakur\u2019s biggest posthumous hits samples what singer?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "bruce hornsby"
            ],
            "lines": [
                [
                    0.26023721176782405,
                    0.5467836257309941,
                    0,
                    0,
                    0.03159041394335512,
                    0.5680888369374635,
                    0.03033625730994152,
                    0.09421565176581016,
                    0.36768018018018017,
                    0.15625,
                    0.125,
                    0.26983986302323865,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.08476897252407456,
                    0.3654970760233918,
                    0,
                    0,
                    0.7770515613652869,
                    0.15254237288135594,
                    0.7785087719298246,
                    0.44937228675349056,
                    0.4671546546546546,
                    0.203125,
                    0.125,
                    0.3052735818609703,
                    0,
                    0.0,
                    1.0
                ],
                [
                    0.6549938157081013,
                    0.08771929824561403,
                    0,
                    0,
                    0.19135802469135801,
                    0.2793687901811806,
                    0.1911549707602339,
                    0.4564120614806993,
                    0.16516516516516516,
                    0.640625,
                    0.75,
                    0.42488655511579104,
                    0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "john mellencamp": 0.3371176616357317,
                "bruce hornsby": 0.44015306194074033,
                "christopher cross": 0.2227292764235279
            },
            "question": "one of tupac shakur\u2019s biggest posthumous hits samples what singer?",
            "rate_limited": false,
            "answers": [
                "christopher cross",
                "john mellencamp",
                "bruce hornsby"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "john mellencamp": 0.31499999945362406,
                "bruce hornsby": 0.354545455177625,
                "christopher cross": 0.3194444440305233
            },
            "integer_answers": {
                "john mellencamp": 3,
                "bruce hornsby": 6,
                "christopher cross": 2
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.8888790411626704,
                    2.1369150730267923,
                    2.9742058858105374
                ],
                "result_count_important_words": [
                    83.0,
                    2130.0,
                    523.0
                ],
                "wikipedia_search": [
                    1.1030405405405406,
                    1.4014639639639639,
                    0.4954954954954955
                ],
                "word_count_appended_bing": [
                    2.0,
                    2.0,
                    12.0
                ],
                "answer_relation_to_question_bing": [
                    1.6403508771929824,
                    1.0964912280701755,
                    0.2631578947368421
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    80300.0,
                    383000.0,
                    389000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    7.0
                ],
                "result_count_bing": [
                    97200.0,
                    26100.0,
                    47800.0
                ],
                "result_count": [
                    87.0,
                    2140.0,
                    527.0
                ],
                "answer_relation_to_question": [
                    1.8216604823747682,
                    0.5933828076685219,
                    4.5849567099567095
                ],
                "word_count_appended": [
                    10.0,
                    13.0,
                    41.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Who is NOT considered an official member of the Eagles?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "j.d. souther"
            ],
            "lines": [
                [
                    0.3624978441904011,
                    0.4125180375180375,
                    0.5,
                    0.5,
                    0.4997003634301621,
                    0.24143401015228427,
                    0.4998634176013878,
                    0.3872017353579176,
                    0.33333333333333337,
                    0.4323943661971831,
                    0.4166666666666667,
                    0.35177122260257315,
                    0,
                    0.5,
                    0.0
                ],
                [
                    0.31140669656055975,
                    0.3042929292929293,
                    0.16666666666666669,
                    0.3,
                    0.49969391963296134,
                    0.2771256345177665,
                    0.21838680698513363,
                    0.28199566160520606,
                    0.2691658223573117,
                    0.2605633802816901,
                    0.27450980392156865,
                    0.3202284702111234,
                    0,
                    0.25,
                    0.0
                ],
                [
                    0.32609545924903915,
                    0.2831890331890332,
                    0.33333333333333337,
                    0.2,
                    0.0006057169368765547,
                    0.4814403553299492,
                    0.28174977541347856,
                    0.33080260303687636,
                    0.39750084430935495,
                    0.3070422535211268,
                    0.3088235294117647,
                    0.32800030718630346,
                    0,
                    0.25,
                    0.0
                ]
            ],
            "fraction_answers": {
                "j.d. souther": 0.1634798466077005,
                "randy meisner": 0.4255329550718589,
                "bernie leadon": 0.4109871983204405
            },
            "question": "who is not considered an official member of the eagles?",
            "rate_limited": false,
            "answers": [
                "j.d. souther",
                "randy meisner",
                "bernie leadon"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "j.d. souther": 0.7567038904590648,
                "randy meisner": 0.07867378819179413,
                "bernie leadon": 0.11149801494357803
            },
            "negative_question": true,
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1858302191794148,
                    1.438172238311013,
                    1.3759975425095723
                ],
                "result_count_important_words": [
                    97.0,
                    200000.0,
                    155000.0
                ],
                "wikipedia_search": [
                    1.0,
                    1.3850050658561297,
                    0.6149949341438703
                ],
                "word_count_appended_bing": [
                    17.0,
                    46.0,
                    39.0
                ],
                "answer_relation_to_question_bing": [
                    0.5248917748917749,
                    1.1742424242424243,
                    1.3008658008658007
                ],
                "question_related_to_answer": [
                    0.0,
                    0.6666666666666666,
                    0.3333333333333333
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.4,
                    0.6
                ],
                "result_count_noun_chunks": [
                    104000.0,
                    201000.0,
                    156000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    2.0,
                    2.0
                ],
                "result_count_bing": [
                    3260000.0,
                    2810000.0,
                    234000.0
                ],
                "word_count_appended": [
                    48.0,
                    170.0,
                    137.0
                ],
                "answer_relation_to_question": [
                    1.100017246476791,
                    1.5087464275155218,
                    1.391236326007687
                ],
                "result_count": [
                    93.0,
                    95.0,
                    155000.0
                ]
            },
            "integer_answers": {
                "j.d. souther": 1,
                "randy meisner": 9,
                "bernie leadon": 3
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "The lyrics to \u201cThe Star-Spangled Banner\u201d were written during what conflict?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "the civil war",
                "american revolution",
                "the war of 1812"
            ],
            "lines": [
                [
                    0.16746031746031745,
                    0.4484848484848485,
                    0.0,
                    0.0,
                    0.43418141592920356,
                    0.3648780487804878,
                    0.4610655737704918,
                    0.5488308115543329,
                    0.19607344632768361,
                    0.2956989247311828,
                    0.25,
                    0.3233393071713582,
                    0.1,
                    0.038461538461538464,
                    1.0
                ],
                [
                    0.0947089947089947,
                    0.16464646464646462,
                    0.0,
                    0.0,
                    0.32079646017699115,
                    0.47804878048780486,
                    0.32581967213114754,
                    0.3170563961485557,
                    0.0927401129943503,
                    0.23118279569892472,
                    0.16666666666666666,
                    0.3069016548206847,
                    0.0,
                    0.07692307692307693,
                    1.0
                ],
                [
                    0.7378306878306878,
                    0.38686868686868686,
                    1.0,
                    1.0,
                    0.24502212389380532,
                    0.15707317073170732,
                    0.21311475409836064,
                    0.13411279229711143,
                    0.7111864406779661,
                    0.4731182795698925,
                    0.5833333333333334,
                    0.3697590380079571,
                    0.9,
                    0.8846153846153846,
                    1.0
                ]
            ],
            "fraction_answers": {
                "american revolution": 0.18396364824311867,
                "the war of 1812": 0.556859620851778,
                "the civil war": 0.25917673090510324
            },
            "question": "the lyrics to \u201cthe star-spangled banner\u201d were written during what conflict?",
            "rate_limited": false,
            "answers": [
                "the civil war",
                "american revolution",
                "the war of 1812"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "american revolution": 0.0,
                "the war of 1812": 0.0,
                "the civil war": 0.0
            },
            "integer_answers": {
                "american revolution": 1,
                "the war of 1812": 9,
                "the civil war": 4
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.9400358430281492,
                    1.8414099289241084,
                    2.2185542280477426
                ],
                "result_count_important_words": [
                    225000.0,
                    159000.0,
                    104000.0
                ],
                "wikipedia_search": [
                    1.1764406779661016,
                    0.5564406779661017,
                    4.267118644067796
                ],
                "word_count_appended_bing": [
                    3.0,
                    2.0,
                    7.0
                ],
                "answer_relation_to_question_bing": [
                    1.3454545454545455,
                    0.4939393939393939,
                    1.1606060606060606
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    1.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_noun_chunks": [
                    798000.0,
                    461000.0,
                    195000.0
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    18.0
                ],
                "word_count_raw": [
                    1.0,
                    2.0,
                    23.0
                ],
                "result_count_bing": [
                    374000.0,
                    490000.0,
                    161000.0
                ],
                "result_count": [
                    157000.0,
                    116000.0,
                    88600.0
                ],
                "answer_relation_to_question": [
                    0.8373015873015872,
                    0.4735449735449735,
                    3.689153439153439
                ],
                "word_count_appended": [
                    55.0,
                    43.0,
                    88.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Where do marsupials keep their undeveloped young?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "underwater"
            ],
            "lines": [
                [
                    0.984375,
                    1.0,
                    0,
                    0,
                    0.003919440424505548,
                    0.7293703558858037,
                    0.005663355305914029,
                    0.05510171952350132,
                    0,
                    0.13636363636363635,
                    0.275,
                    0.3235490877357495,
                    0,
                    0.0,
                    3.0
                ],
                [
                    0.015625,
                    0.0,
                    0,
                    0,
                    0.0011456825856246986,
                    0.022291748142354323,
                    0.0009284189026088571,
                    0.00029737435933318174,
                    0,
                    0.030303030303030304,
                    0.05,
                    0.10697086317467669,
                    0,
                    0.0,
                    3.0
                ],
                [
                    0.0,
                    0.0,
                    0,
                    0,
                    0.9949348769898697,
                    0.248337895971842,
                    0.9934082257914771,
                    0.9446009061171655,
                    0,
                    0.8333333333333334,
                    0.675,
                    0.5694800490895738,
                    0,
                    1.0,
                    3.0
                ]
            ],
            "fraction_answers": {
                "in their pouches": 0.35133425952391106,
                "underwater": 0.6259095287293261,
                "in a paper bag": 0.02275621174676281
            },
            "question": "where do marsupials keep their undeveloped young?",
            "rate_limited": false,
            "answers": [
                "in their pouches",
                "in a paper bag",
                "underwater"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "in their pouches": 0.3053373886511991,
                "underwater": 0.631059125602763,
                "in a paper bag": 0.035774280971791736
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 3
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.9706472632072485,
                    0.32091258952403007,
                    1.7084401472687214
                ],
                "result_count_important_words": [
                    61.0,
                    10.0,
                    10700.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    11.0,
                    2.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    3.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    9450.0,
                    51.0,
                    162000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    373000.0,
                    11400.0,
                    127000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_appended": [
                    27.0,
                    6.0,
                    165.0
                ],
                "answer_relation_to_question": [
                    1.96875,
                    0.03125,
                    0.0
                ],
                "result_count": [
                    65.0,
                    19.0,
                    16500.0
                ]
            },
            "integer_answers": {
                "in their pouches": 3,
                "underwater": 7,
                "in a paper bag": 0
            }
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Catching a catfish with your bare hands is called what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "noodling"
            ],
            "lines": [
                [
                    0.031451612903225803,
                    0.0,
                    0.0,
                    0,
                    0.07497750674797561,
                    0.3356953055801594,
                    0.018880528795443358,
                    0.017625149647497007,
                    0.06666666666666667,
                    0.1678082191780822,
                    0.2087912087912088,
                    0.27746520724509005,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.1303030303030303,
                    0.0,
                    0.0,
                    0,
                    0.00029991002699190244,
                    0.337466784765279,
                    0.0001757963574994726,
                    0.0002438699951226001,
                    0.06666666666666667,
                    0.039383561643835614,
                    0.2032967032967033,
                    0.0551751227581459,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.8382453567937439,
                    1.0,
                    1.0,
                    0,
                    0.9247225832250325,
                    0.32683790965456155,
                    0.9809436748470571,
                    0.9821309803573804,
                    0.8666666666666666,
                    0.7928082191780822,
                    0.5879120879120879,
                    0.6673596699967641,
                    1.0,
                    1.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "noodling": 0.8436636268177982,
                "whiskering": 0.06407780352409806,
                "strumming": 0.09225856965810376
            },
            "question": "catching a catfish with your bare hands is called what?",
            "rate_limited": false,
            "answers": [
                "strumming",
                "whiskering",
                "noodling"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "noodling": 0.3952380970830009,
                "whiskering": 0.29999999900658925,
                "strumming": 0.29999999900658925
            },
            "integer_answers": {
                "noodling": 12,
                "whiskering": 1,
                "strumming": 0
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.3873260362254503,
                    0.2758756137907295,
                    3.3367983499838205
                ],
                "result_count_important_words": [
                    537.0,
                    5.0,
                    27900.0
                ],
                "wikipedia_search": [
                    0.3333333333333333,
                    0.3333333333333333,
                    4.333333333333333
                ],
                "word_count_appended_bing": [
                    38.0,
                    37.0,
                    107.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    4.0
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    1.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    795.0,
                    11.0,
                    44300.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    139.0
                ],
                "result_count_bing": [
                    3790000.0,
                    3810000.0,
                    3690000.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    212.0
                ],
                "result_count": [
                    1500.0,
                    6.0,
                    18500.0
                ],
                "answer_relation_to_question": [
                    0.15725806451612903,
                    0.6515151515151515,
                    4.19122678396872
                ],
                "word_count_appended": [
                    98.0,
                    23.0,
                    463.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which verb describes the sound minerals make when they are heated?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "decrepitate"
            ],
            "lines": [
                [
                    0.8788888888888888,
                    0.875,
                    0,
                    0,
                    1.0,
                    0.33620689655172414,
                    1.0,
                    1.0,
                    1.0,
                    0.7377049180327869,
                    0.509090909090909,
                    0.7383038961355131,
                    1.0,
                    1.0,
                    2.0
                ],
                [
                    0.005,
                    0.0,
                    0,
                    0,
                    0.0,
                    0.33405172413793105,
                    0.0,
                    0.0,
                    0.0,
                    0.13114754098360656,
                    0.24545454545454545,
                    0.13084805193224341,
                    0.0,
                    0.0,
                    2.0
                ],
                [
                    0.11611111111111111,
                    0.125,
                    0,
                    0,
                    0.0,
                    0.3297413793103448,
                    0.0,
                    0.0,
                    0.0,
                    0.13114754098360656,
                    0.24545454545454545,
                    0.13084805193224341,
                    0.0,
                    0.0,
                    2.0
                ]
            ],
            "fraction_answers": {
                "frangelle": 0.07054182187569388,
                "decrepitate": 0.8395996257249851,
                "recleft": 0.08985855239932096
            },
            "question": "which verb describes the sound minerals make when they are heated?",
            "rate_limited": false,
            "answers": [
                "decrepitate",
                "frangelle",
                "recleft"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "frangelle": 0.29999999900658925,
                "decrepitate": 0.3952380970830009,
                "recleft": 0.29999999900658925
            },
            "integer_answers": {
                "frangelle": 0,
                "decrepitate": 12,
                "recleft": 0
            },
            "categorical_data": {
                "question_type": 2
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.6915194806775653,
                    0.6542402596612171,
                    0.6542402596612171
                ],
                "result_count_important_words": [
                    4370.0,
                    0,
                    0
                ],
                "wikipedia_search": [
                    2.0,
                    0.0,
                    0.0
                ],
                "word_count_appended_bing": [
                    56.0,
                    27.0,
                    27.0
                ],
                "answer_relation_to_question_bing": [
                    3.5,
                    0.0,
                    0.5
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    4580.0,
                    0,
                    0
                ],
                "word_count_noun_chunks": [
                    10.0,
                    0.0,
                    0.0
                ],
                "result_count_bing": [
                    156000.0,
                    155000.0,
                    153000.0
                ],
                "word_count_raw": [
                    6.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    4370.0,
                    0,
                    0
                ],
                "answer_relation_to_question": [
                    3.5155555555555553,
                    0.02,
                    0.46444444444444444
                ],
                "word_count_appended": [
                    90.0,
                    16.0,
                    16.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Before a performance, an orchestra usually tunes to what instrument?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "oboe"
            ],
            "lines": [
                [
                    0.3908323657162517,
                    0.32923789173789175,
                    0.9166666666666667,
                    0,
                    0.38003220611916266,
                    0.36524453694068676,
                    0.403068340306834,
                    0.40749260598093984,
                    0.23219795418965544,
                    0.4712103407755582,
                    0.6016949152542372,
                    0.3473591429711546,
                    0.9583333333333334,
                    0.8947368421052632,
                    1.0
                ],
                [
                    0.27928597448526565,
                    0.40028490028490027,
                    0.08333333333333333,
                    0,
                    0.2882447665056361,
                    0.3215400624349636,
                    0.19595536959553697,
                    0.269142293789024,
                    0.41594850516012344,
                    0.1045828437132785,
                    0.11016949152542373,
                    0.33245503440495233,
                    0.0,
                    0.07894736842105263,
                    1.0
                ],
                [
                    0.32988165979848266,
                    0.270477207977208,
                    0.0,
                    0,
                    0.33172302737520126,
                    0.31321540062434966,
                    0.40097629009762903,
                    0.3233651002300362,
                    0.3518535406502211,
                    0.42420681551116335,
                    0.288135593220339,
                    0.32018582262389306,
                    0.041666666666666664,
                    0.02631578947368421,
                    1.0
                ]
            ],
            "fraction_answers": {
                "french horn": 0.2215299956656531,
                "bassoon": 0.26323099340375955,
                "oboe": 0.5152390109305873
            },
            "question": "before a performance, an orchestra usually tunes to what instrument?",
            "rate_limited": false,
            "answers": [
                "oboe",
                "french horn",
                "bassoon"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "french horn": 0.31337831476324385,
                "bassoon": 0.06575845231244892,
                "oboe": 0.7007279455720743
            },
            "integer_answers": {
                "french horn": 2,
                "bassoon": 0,
                "oboe": 11
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.736795714855773,
                    1.6622751720247617,
                    1.6009291131194652
                ],
                "result_count_important_words": [
                    578000.0,
                    281000.0,
                    575000.0
                ],
                "wikipedia_search": [
                    0.9287918167586218,
                    1.6637940206404938,
                    1.4074141626008845
                ],
                "word_count_appended_bing": [
                    71.0,
                    13.0,
                    34.0
                ],
                "answer_relation_to_question_bing": [
                    1.316951566951567,
                    1.601139601139601,
                    1.081908831908832
                ],
                "question_related_to_answer": [
                    1.8333333333333335,
                    0.16666666666666666,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    1240000.0,
                    819000.0,
                    984000.0
                ],
                "word_count_noun_chunks": [
                    23.0,
                    0.0,
                    1.0
                ],
                "result_count_bing": [
                    702000.0,
                    618000.0,
                    602000.0
                ],
                "word_count_raw": [
                    34.0,
                    3.0,
                    1.0
                ],
                "word_count_appended": [
                    401.0,
                    89.0,
                    361.0
                ],
                "answer_relation_to_question": [
                    1.5633294628650067,
                    1.1171438979410626,
                    1.3195266391939307
                ],
                "result_count": [
                    1180000.0,
                    895000.0,
                    1030000.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these celebrities has NOT been a ProActiv spokesperson?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "katy perry",
                "lindsay lohan",
                "selena gomez"
            ],
            "lines": [
                [
                    0.36,
                    0.5,
                    0.0,
                    0.09999999999999998,
                    0.21406491499227207,
                    0.3902439024390244,
                    0.2547584187408492,
                    0.21391917896087237,
                    0.4431818181818182,
                    0.2922077922077922,
                    0.4,
                    0.2923751337757684,
                    0.12962962962962965,
                    0.05555555555555558,
                    -1.0
                ],
                [
                    0.36,
                    0.5,
                    0.5,
                    0.5,
                    0.3486604842864503,
                    0.3390971969421187,
                    0.32430453879941434,
                    0.34862091084028224,
                    0.23958333333333331,
                    0.33116883116883117,
                    0.39166666666666666,
                    0.3479336440987335,
                    0.3888888888888889,
                    0.4722222222222222,
                    -1.0
                ],
                [
                    0.28,
                    0.0,
                    0.5,
                    0.4,
                    0.4372746007212777,
                    0.27065890061885695,
                    0.42093704245973645,
                    0.43745991019884545,
                    0.3172348484848485,
                    0.37662337662337664,
                    0.20833333333333331,
                    0.3596912221254981,
                    0.4814814814814815,
                    0.4722222222222222,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "selena gomez": 0.29115472310436047,
                "katy perry": 0.47915195078805967,
                "lindsay lohan": 0.2296933261075798
            },
            "question": "which of these celebrities has not been a proactiv spokesperson?",
            "rate_limited": false,
            "answers": [
                "katy perry",
                "lindsay lohan",
                "selena gomez"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "selena gomez": 0.0,
                "katy perry": 0.0,
                "lindsay lohan": 0.0
            },
            "integer_answers": {
                "selena gomez": 4,
                "katy perry": 9,
                "lindsay lohan": 1
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2457491973453894,
                    0.9123981354075987,
                    0.8418526672470117
                ],
                "result_count_important_words": [
                    335000.0,
                    240000.0,
                    108000.0
                ],
                "wikipedia_search": [
                    0.34090909090909094,
                    1.5625,
                    1.0965909090909092
                ],
                "word_count_appended_bing": [
                    12.0,
                    13.0,
                    35.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    1.0
                ],
                "question_related_to_answer": [
                    2.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.8,
                    0.0,
                    0.2
                ],
                "result_count_noun_chunks": [
                    446000.0,
                    236000.0,
                    97500.0
                ],
                "word_count_noun_chunks": [
                    20.0,
                    6.0,
                    1.0
                ],
                "word_count_raw": [
                    16.0,
                    1.0,
                    1.0
                ],
                "result_count_bing": [
                    603000.0,
                    884000.0,
                    1260000.0
                ],
                "result_count": [
                    444000.0,
                    235000.0,
                    97400.0
                ],
                "answer_relation_to_question": [
                    0.28,
                    0.28,
                    0.44
                ],
                "word_count_appended": [
                    96.0,
                    78.0,
                    57.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "In the original NES \u201cMike Tyson\u2019s Punch-Out!!\u201d, who does Little Mac face right before the final opponent?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "mr. dream",
                "super macho man",
                "mr. sandman"
            ],
            "lines": [
                [
                    0.3621677018112806,
                    0.19658480152551297,
                    1.0,
                    0,
                    0.1641110650642354,
                    0.2657952069716776,
                    0.4381720430107527,
                    0.30379241516966066,
                    0.0104775828460039,
                    0.37373737373737376,
                    0.07407407407407407,
                    0.3213361164488029,
                    0.6129032258064516,
                    0.35,
                    0.0
                ],
                [
                    0.49161645910846036,
                    0.4329124103827661,
                    0.0,
                    0,
                    0.2018234562784915,
                    0.2657952069716776,
                    0.05053763440860215,
                    0.3469061876247505,
                    0.91973858256753,
                    0.31313131313131315,
                    0.4444444444444444,
                    0.35251120184297025,
                    0.22580645161290322,
                    0.3,
                    0.0
                ],
                [
                    0.14621583908025895,
                    0.3705027880917209,
                    0.0,
                    0,
                    0.6340654786572731,
                    0.4684095860566449,
                    0.5112903225806451,
                    0.34930139720558884,
                    0.06978383458646617,
                    0.31313131313131315,
                    0.48148148148148145,
                    0.3261526817082268,
                    0.16129032258064516,
                    0.35,
                    0.0
                ]
            ],
            "fraction_answers": {
                "super macho man": 0.3342479498749161,
                "mr. dream": 0.3440885851127558,
                "mr. sandman": 0.321663465012328
            },
            "question": "in the original nes \u201cmike tyson\u2019s punch-out!!\u201d, who does little mac face right before the final opponent?",
            "rate_limited": false,
            "answers": [
                "mr. dream",
                "super macho man",
                "mr. sandman"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "super macho man": 0.0,
                "mr. dream": 0.0,
                "mr. sandman": 0.0
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 0
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    3.534697280936832,
                    3.877623220272673,
                    3.5876794987904947
                ],
                "result_count_important_words": [
                    815.0,
                    94.0,
                    951.0
                ],
                "wikipedia_search": [
                    0.09429824561403508,
                    8.277647243107769,
                    0.6280545112781954
                ],
                "word_count_appended_bing": [
                    2.0,
                    12.0,
                    13.0
                ],
                "answer_relation_to_question_bing": [
                    2.1624328167806426,
                    4.762036514210427,
                    4.07553066900893
                ],
                "question_related_to_answer": [
                    1.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    7610.0,
                    8690.0,
                    8750.0
                ],
                "word_count_noun_chunks": [
                    19.0,
                    7.0,
                    5.0
                ],
                "word_count_raw": [
                    7.0,
                    6.0,
                    7.0
                ],
                "result_count_bing": [
                    122000.0,
                    122000.0,
                    215000.0
                ],
                "result_count": [
                    792.0,
                    974.0,
                    3060.0
                ],
                "answer_relation_to_question": [
                    3.983844719924087,
                    5.407781050193064,
                    1.6083742298828485
                ],
                "word_count_appended": [
                    37.0,
                    31.0,
                    31.0
                ]
            },
            "integer_answers": {
                "super macho man": 4,
                "mr. dream": 4,
                "mr. sandman": 5
            }
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Which of these do NOT have flippers?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "new yorkers"
            ],
            "lines": [
                [
                    0,
                    0,
                    0.5,
                    0,
                    0.25016719212763927,
                    0.41024935826916026,
                    0.28170377541142305,
                    0.28170377541142305,
                    0.5,
                    0.4422336328626444,
                    0.45180722891566266,
                    0.37404580152671757,
                    0.5,
                    0.5,
                    -1.0
                ],
                [
                    0,
                    0,
                    0.5,
                    0,
                    0.25494411006018913,
                    0.1452145214521452,
                    0.31994191674733785,
                    0.31994191674733785,
                    0.4,
                    0.11424903722721436,
                    0.1566265060240964,
                    0.33129770992366414,
                    0.5,
                    0.0,
                    -1.0
                ],
                [
                    0,
                    0,
                    0.0,
                    0,
                    0.4948886978121716,
                    0.44453612027869455,
                    0.3983543078412391,
                    0.3983543078412391,
                    0.09999999999999998,
                    0.4435173299101412,
                    0.39156626506024095,
                    0.29465648854961835,
                    0.0,
                    0.5,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "new yorkers": 0.1832889519046054,
                "pinball machines": 0.3698411786739373,
                "dolphins": 0.44686986942145734
            },
            "question": "which of these do not have flippers?",
            "rate_limited": false,
            "answers": [
                "new yorkers",
                "dolphins",
                "pinball machines"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "new yorkers": 0.6583886222902257,
                "pinball machines": 0.3029786308528064,
                "dolphins": 0.27430523198202605
            },
            "integer_answers": {
                "new yorkers": 3,
                "pinball machines": 4,
                "dolphins": 4
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.25190839694656486,
                    0.3374045801526718,
                    0.41068702290076337
                ],
                "result_count_important_words": [
                    451000.0,
                    372000.0,
                    210000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.2,
                    0.8
                ],
                "word_count_appended_bing": [
                    8.0,
                    57.0,
                    18.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    1.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    451000.0,
                    372000.0,
                    210000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    4.0
                ],
                "result_count_bing": [
                    9790000.0,
                    38700000.0,
                    6050000.0
                ],
                "word_count_raw": [
                    0.0,
                    17.0,
                    0.0
                ],
                "result_count": [
                    5230000.0,
                    5130000.0,
                    107000.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    90.0,
                    601.0,
                    88.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "In \u201cPeanuts,\u201d what breed of dog is Snoopy?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "beagle"
            ],
            "lines": [
                [
                    0.31436212049708984,
                    0.3568181818181818,
                    0.0,
                    0,
                    0.11120806172902872,
                    0.5735140771637122,
                    0.15621436716077536,
                    2.3657836138902657e-05,
                    0.5347605109704354,
                    0.09758897818599312,
                    0.09,
                    0.3457735480057595,
                    0.0,
                    0.00819672131147541,
                    1.0
                ],
                [
                    0.11063394683026584,
                    0.18611111111111112,
                    0.0,
                    0,
                    7.794069283572953e-05,
                    0.20437956204379562,
                    0.28848346636259975,
                    0.640492636931267,
                    0.007907335556916934,
                    0.1859931113662457,
                    0.3,
                    0.25130100273886286,
                    0.0,
                    0.00819672131147541,
                    1.0
                ],
                [
                    0.5750039326726443,
                    0.45707070707070707,
                    1.0,
                    0,
                    0.8887139975781355,
                    0.22210636079249219,
                    0.5553021664766249,
                    0.35948370523259404,
                    0.45733215347264766,
                    0.7164179104477612,
                    0.61,
                    0.40292544925537765,
                    1.0,
                    0.9836065573770492,
                    1.0
                ]
            ],
            "fraction_answers": {
                "beagle": 0.6329202261827719,
                "pitbull": 0.16796744884195203,
                "border collie": 0.19911232497527617
            },
            "question": "in \u201cpeanuts,\u201d what breed of dog is snoopy?",
            "rate_limited": false,
            "answers": [
                "border collie",
                "pitbull",
                "beagle"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "beagle": 0.8467551897091281,
                "pitbull": 0.024453956217415765,
                "border collie": 0.36678082360971453
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.383094192023038,
                    1.0052040109554514,
                    1.6117017970215106
                ],
                "result_count_important_words": [
                    137000.0,
                    253000.0,
                    487000.0
                ],
                "wikipedia_search": [
                    1.0695210219408708,
                    0.01581467111383387,
                    0.9146643069452953
                ],
                "answer_relation_to_question": [
                    0.9430863614912695,
                    0.3319018404907975,
                    1.725011798017933
                ],
                "answer_relation_to_question_bing": [
                    0.7136363636363636,
                    0.37222222222222223,
                    0.9141414141414141
                ],
                "word_count_appended": [
                    85.0,
                    162.0,
                    624.0
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    2.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    41.0,
                    1110000.0,
                    623000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    120.0
                ],
                "result_count_bing": [
                    1650000.0,
                    588000.0,
                    639000.0
                ],
                "result_count": [
                    117000.0,
                    82.0,
                    935000.0
                ],
                "word_count_appended_bing": [
                    9.0,
                    30.0,
                    61.0
                ]
            },
            "integer_answers": {
                "beagle": 10,
                "pitbull": 1,
                "border collie": 2
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "Which of the following is a dice-based game?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "farkle"
            ],
            "lines": [
                [
                    0.3809040079531883,
                    0.46533923303834807,
                    0.0,
                    0,
                    0.017411256579835336,
                    0.02414486921529175,
                    0.012445887445887446,
                    0.0181460120973414,
                    0.0,
                    0.16608695652173913,
                    0.23809523809523808,
                    0.28648220173373445,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.055605596589203145,
                    0.0,
                    0.0,
                    0,
                    0.650560129572142,
                    0.6522468142186452,
                    0.7498453927025356,
                    0.6330004220002813,
                    0.0,
                    0.27043478260869563,
                    0.19047619047619047,
                    0.3154669671266756,
                    0.0,
                    0.0,
                    -1.0
                ],
                [
                    0.5634903954576086,
                    0.534660766961652,
                    1.0,
                    0,
                    0.3320286138480227,
                    0.32360831656606304,
                    0.237708719851577,
                    0.34885356590237726,
                    1.0,
                    0.5634782608695652,
                    0.5714285714285714,
                    0.39805083113958994,
                    1.0,
                    1.0,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "sparkle": 0.27058740733033604,
                "quirkle": 0.12377351251389263,
                "farkle": 0.6056390801557713
            },
            "question": "which of the following is a dice-based game?",
            "rate_limited": false,
            "answers": [
                "quirkle",
                "sparkle",
                "farkle"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "sparkle": 0.13263782723987846,
                "quirkle": 0.23551666352134928,
                "farkle": 0.898081880234184
            },
            "integer_answers": {
                "sparkle": 4,
                "quirkle": 0,
                "farkle": 9
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1459288069349378,
                    1.2618678685067024,
                    1.5922033245583598
                ],
                "result_count_important_words": [
                    64400.0,
                    3880000.0,
                    1230000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    4.0
                ],
                "word_count_appended_bing": [
                    35.0,
                    28.0,
                    84.0
                ],
                "answer_relation_to_question_bing": [
                    1.3960176991150441,
                    0.0,
                    1.6039823008849559
                ],
                "question_related_to_answer": [
                    0.0,
                    0.0,
                    1.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    64500.0,
                    2250000.0,
                    1240000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    8.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    5.0
                ],
                "result_count_bing": [
                    144000.0,
                    3890000.0,
                    1930000.0
                ],
                "result_count": [
                    64500.0,
                    2410000.0,
                    1230000.0
                ],
                "answer_relation_to_question": [
                    1.142712023859565,
                    0.16681678976760944,
                    1.6904711863728257
                ],
                "word_count_appended": [
                    191.0,
                    311.0,
                    648.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "What was the first popular home video game?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "tekken 2",
                "pong",
                "half-life 3"
            ],
            "lines": [
                [
                    0.41071648853399145,
                    0.21735836627140975,
                    0.0,
                    0.0,
                    0.025607714071251227,
                    0.3448844884488449,
                    0.03076923076923077,
                    0.023464326592903473,
                    0.09539914521327161,
                    0.15568022440392706,
                    0.19852941176470587,
                    0.2870363353579324,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.352829705983317,
                    0.21791831357048747,
                    1.0,
                    1.0,
                    0.9498238928344592,
                    0.334983498349835,
                    0.9230769230769231,
                    0.9462037390309043,
                    0.7075650244795225,
                    0.7012622720897616,
                    0.6617647058823529,
                    0.45450451932528696,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.23645380548269151,
                    0.5647233201581028,
                    0.0,
                    0.0,
                    0.024568393094289508,
                    0.3201320132013201,
                    0.046153846153846156,
                    0.030331934376192294,
                    0.19703583030720578,
                    0.14305750350631136,
                    0.13970588235294118,
                    0.25845914531678066,
                    0.0,
                    0.0,
                    1.0
                ]
            ],
            "fraction_answers": {
                "tekken 2": 0.1278175522448192,
                "pong": 0.7321380424730607,
                "half-life 3": 0.1400444052821201
            },
            "question": "what was the first popular home video game?",
            "rate_limited": false,
            "answers": [
                "tekken 2",
                "pong",
                "half-life 3"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "tekken 2": 0.0,
                "pong": 0.0,
                "half-life 3": 0.0
            },
            "integer_answers": {
                "tekken 2": 2,
                "pong": 11,
                "half-life 3": 1
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.1481453414317295,
                    1.8180180773011478,
                    1.0338365812671226
                ],
                "result_count_important_words": [
                    140000.0,
                    4200000.0,
                    210000.0
                ],
                "wikipedia_search": [
                    0.38159658085308645,
                    2.83026009791809,
                    0.7881433212288231
                ],
                "word_count_appended_bing": [
                    27.0,
                    90.0,
                    19.0
                ],
                "answer_relation_to_question_bing": [
                    0.869433465085639,
                    0.8716732542819499,
                    2.258893280632411
                ],
                "question_related_to_answer": [
                    0.0,
                    1.0,
                    0.0
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    123000.0,
                    4960000.0,
                    159000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    34.0,
                    0.0
                ],
                "result_count_bing": [
                    20900000.0,
                    20300000.0,
                    19400000.0
                ],
                "word_count_raw": [
                    0.0,
                    28.0,
                    0.0
                ],
                "result_count": [
                    88700.0,
                    3290000.0,
                    85100.0
                ],
                "answer_relation_to_question": [
                    1.6428659541359658,
                    1.411318823933268,
                    0.9458152219307661
                ],
                "word_count_appended": [
                    111.0,
                    500.0,
                    102.0
                ]
            },
            "negative_question": false
        },
        "right_answer": [
            0,
            1,
            0
        ]
    },
    "Featuring 20 scoops of ice cream, the Vermonster is found on what chain's menu?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "ben & jerry's"
            ],
            "lines": [
                [
                    0.6429691177567454,
                    0.5259478502855987,
                    0.375,
                    0.5,
                    0.02065767284991568,
                    0.33266932270916333,
                    0.014089347079037801,
                    0.041327913279132794,
                    0.15921843066037847,
                    0.7272727272727273,
                    0.7857142857142857,
                    0.283505428086033,
                    0.02,
                    0.04878048780487805,
                    1.0
                ],
                [
                    0.23062803823192968,
                    0.34620112534019826,
                    0.041666666666666664,
                    0.0,
                    0.01812816188870152,
                    0.3346613545816733,
                    0.013402061855670102,
                    0.03726287262872629,
                    0.36529698691253404,
                    0.2727272727272727,
                    0.10714285714285714,
                    0.23820889132873285,
                    0.02,
                    0.024390243902439025,
                    1.0
                ],
                [
                    0.12640284401132493,
                    0.12785102437420323,
                    0.5833333333333334,
                    0.5,
                    0.9612141652613828,
                    0.33266932270916333,
                    0.9725085910652921,
                    0.9214092140921409,
                    0.47548458242708747,
                    0.0,
                    0.10714285714285714,
                    0.47828568058523413,
                    0.96,
                    0.926829268292683,
                    1.0
                ]
            ],
            "fraction_answers": {
                "ben & jerry's": 0.5337950630924787,
                "dairy queen": 0.14640832380052868,
                "baskin-robbins": 0.3197966131069926
            },
            "question": "featuring 20 scoops of ice cream, the vermonster is found on what chain's menu?",
            "rate_limited": false,
            "answers": [
                "baskin-robbins",
                "dairy queen",
                "ben & jerry's"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "ben & jerry's": 0.3952380970830009,
                "dairy queen": 0.29999999900658925,
                "baskin-robbins": 0.3194444440305233
            },
            "negative_question": false,
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    2.5515488527742973,
                    2.1438800219585956,
                    4.304571125267107
                ],
                "result_count_important_words": [
                    41.0,
                    39.0,
                    2830.0
                ],
                "wikipedia_search": [
                    1.2737474452830277,
                    2.9223758953002723,
                    3.8038766594166997
                ],
                "word_count_appended_bing": [
                    44.0,
                    6.0,
                    6.0
                ],
                "answer_relation_to_question_bing": [
                    3.1556871017135912,
                    2.0772067520411888,
                    0.7671061462452191
                ],
                "question_related_to_answer": [
                    1.5,
                    0.16666666666666666,
                    2.3333333333333335
                ],
                "question_related_to_answer_bing": [
                    1.0,
                    0.0,
                    1.0
                ],
                "result_count_noun_chunks": [
                    61.0,
                    55.0,
                    1360.0
                ],
                "word_count_noun_chunks": [
                    1.0,
                    1.0,
                    48.0
                ],
                "word_count_raw": [
                    2.0,
                    1.0,
                    38.0
                ],
                "result_count_bing": [
                    1670000.0,
                    1680000.0,
                    1670000.0
                ],
                "word_count_appended": [
                    72.0,
                    27.0,
                    0.0
                ],
                "answer_relation_to_question": [
                    5.143752942053963,
                    1.8450243058554374,
                    1.0112227520905994
                ],
                "result_count": [
                    49.0,
                    43.0,
                    2280.0
                ]
            },
            "integer_answers": {
                "ben & jerry's": 8,
                "dairy queen": 1,
                "baskin-robbins": 5
            }
        },
        "right_answer": [
            0,
            0,
            1
        ]
    },
    "The U.S. has never had a Miss America from what state?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "new mexico"
            ],
            "lines": [
                [
                    0.35319498069498073,
                    0.2928503787878788,
                    0.3,
                    0.5,
                    0.26397800183318054,
                    0.45176604633497913,
                    0.17993809791783905,
                    0.33808553971486766,
                    0.318804923840329,
                    0.4364754098360656,
                    0.45161290322580644,
                    0.33856217236564246,
                    0.5,
                    0.5,
                    1.0
                ],
                [
                    0.332138100833753,
                    0.3928233225108225,
                    0.4,
                    0.0,
                    0.31759853345554534,
                    0.319787314849981,
                    0.38111986494091166,
                    0.3604887983706721,
                    0.2949478392633389,
                    0.3777322404371585,
                    0.43010752688172044,
                    0.3260980534384422,
                    0.0,
                    0.0,
                    1.0
                ],
                [
                    0.3146669184712663,
                    0.3143262987012987,
                    0.3,
                    0.5,
                    0.41842346471127406,
                    0.2284466388150399,
                    0.4389420371412493,
                    0.3014256619144603,
                    0.3862472368963321,
                    0.18579234972677594,
                    0.11827956989247312,
                    0.3353397741959153,
                    0.5,
                    0.5,
                    1.0
                ]
            ],
            "fraction_answers": {
                "nebraska": 0.3083014356477021,
                "new mexico": 0.2535330779212044,
                "north dakota": 0.43816548643109343
            },
            "question": "the u.s. has never had a miss america from what state?",
            "rate_limited": false,
            "answers": [
                "new mexico",
                "north dakota",
                "nebraska"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "nebraska": 0.30943798091539726,
                "new mexico": 0.4271469415986549,
                "north dakota": 0.21394232936027544
            },
            "integer_answers": {
                "nebraska": 5,
                "new mexico": 4,
                "north dakota": 5
            },
            "categorical_data": {
                "question_type": 1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    1.2915026210748604,
                    1.3912155724924622,
                    1.3172818064326774
                ],
                "result_count_important_words": [
                    4550000.0,
                    1690000.0,
                    868000.0
                ],
                "wikipedia_search": [
                    1.0871704569580258,
                    1.2303129644199668,
                    0.6825165786220073
                ],
                "word_count_appended_bing": [
                    9.0,
                    13.0,
                    71.0
                ],
                "answer_relation_to_question_bing": [
                    1.6571969696969697,
                    0.8574134199134199,
                    1.4853896103896103
                ],
                "question_related_to_answer": [
                    0.4,
                    0.2,
                    0.4
                ],
                "question_related_to_answer_bing": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    1590000.0,
                    1370000.0,
                    1950000.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    39.0,
                    0.0
                ],
                "result_count_bing": [
                    25400000.0,
                    94900000.0,
                    143000000.0
                ],
                "word_count_raw": [
                    0.0,
                    16.0,
                    0.0
                ],
                "result_count": [
                    5150000.0,
                    3980000.0,
                    1780000.0
                ],
                "answer_relation_to_question": [
                    1.1744401544401544,
                    1.342895193329976,
                    1.4826646522298694
                ],
                "word_count_appended": [
                    93.0,
                    179.0,
                    460.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            1,
            0,
            0
        ]
    },
    "Which of these do adverbs NOT typically modify?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "pronoun"
            ],
            "lines": [
                [
                    0.5,
                    0.5,
                    0.4985754985754986,
                    0.49074074074074076,
                    0.3187785388127854,
                    0.23880597014925375,
                    0.3654929577464789,
                    0.30383734249713634,
                    0.5,
                    0.41725352112676056,
                    0.4251207729468599,
                    0.35690122236628685,
                    0.4985754985754986,
                    0.49595323741007197,
                    -1.0
                ],
                [
                    0.1431497235828249,
                    0.11683006535947715,
                    0.043209876543209846,
                    0.04938271604938271,
                    0.3216324200913242,
                    0.4161258922777417,
                    0.38028169014084506,
                    0.36254295532646047,
                    0.1739750445632799,
                    0.2162977867203219,
                    0.2173913043478261,
                    0.31470655938094005,
                    0.0432098765432099,
                    0.10611510791366907,
                    -1.0
                ],
                [
                    0.35685027641717515,
                    0.3831699346405229,
                    0.45821462488129155,
                    0.45987654320987653,
                    0.3595890410958904,
                    0.34506813757300453,
                    0.25422535211267605,
                    0.33361970217640324,
                    0.32602495543672017,
                    0.36644869215291753,
                    0.35748792270531404,
                    0.3283922182527731,
                    0.45821462488129155,
                    0.397931654676259,
                    -1.0
                ]
            ],
            "fraction_answers": {
                "adjective": 0.2592694742554121,
                "pronoun": 0.15570924272180406,
                "adverb": 0.5850212830227839
            },
            "question": "which of these do adverbs not typically modify?",
            "rate_limited": false,
            "answers": [
                "pronoun",
                "adverb",
                "adjective"
            ],
            "columns_in_order": [
                "answer_relation_to_question",
                "answer_relation_to_question_bing",
                "question_related_to_answer",
                "question_related_to_answer_bing",
                "result_count",
                "result_count_bing",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_appended_bing",
                "word_count_appended_relation_to_question",
                "word_count_noun_chunks",
                "word_count_raw",
                "question_type"
            ],
            "ml_answers": {
                "adjective": 0.21218292011174883,
                "pronoun": 0.8600960889164171,
                "adverb": 0.05093861204938606
            },
            "integer_answers": {
                "adjective": 1,
                "pronoun": 3,
                "adverb": 10
            },
            "categorical_data": {
                "question_type": -1
            },
            "data": {
                "word_count_appended_relation_to_question": [
                    0.8585926658022787,
                    1.11176064371436,
                    1.0296466904833617
                ],
                "result_count_important_words": [
                    191000.0,
                    170000.0,
                    349000.0
                ],
                "wikipedia_search": [
                    0.0,
                    1.9561497326203208,
                    1.0438502673796792
                ],
                "word_count_appended_bing": [
                    31.0,
                    117.0,
                    59.0
                ],
                "answer_relation_to_question_bing": [
                    0.0,
                    2.299019607843137,
                    0.7009803921568627
                ],
                "question_related_to_answer": [
                    0.002849002849002849,
                    0.9135802469135802,
                    0.0835707502374169
                ],
                "question_related_to_answer_bing": [
                    0.018518518518518517,
                    0.9012345679012346,
                    0.08024691358024691
                ],
                "result_count_noun_chunks": [
                    685000.0,
                    480000.0,
                    581000.0
                ],
                "word_count_noun_chunks": [
                    3.0,
                    962.0,
                    88.0
                ],
                "word_count_raw": [
                    9.0,
                    876.0,
                    227.0
                ],
                "result_count_bing": [
                    16100000.0,
                    5170000.0,
                    9550000.0
                ],
                "result_count": [
                    1270000.0,
                    1250000.0,
                    984000.0
                ],
                "answer_relation_to_question": [
                    0.0,
                    2.1411016585030507,
                    0.8588983414969493
                ],
                "word_count_appended": [
                    329.0,
                    1128.0,
                    531.0
                ]
            },
            "negative_question": true
        },
        "right_answer": [
            1,
            0,
            0
        ]
    }
}