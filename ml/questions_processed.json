{
    "Which of these versions of the Old Testament typically contains the most books?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.5806451612903226,
                    0.5911708253358925,
                    0.31477927063339733,
                    0.2826025048456836,
                    0.5398126463700235,
                    0.6868131868131868,
                    0.6846153846153846,
                    0.07997432605905005
                ],
                [
                    0.36484983314794217,
                    0.3585412667946257,
                    0.472168905950096,
                    0.226874977812804,
                    0.3992974238875878,
                    0.3131868131868132,
                    0.3,
                    0.43029525032092425
                ],
                [
                    0.054505005561735265,
                    0.050287907869481764,
                    0.21305182341650672,
                    0.4905225173415125,
                    0.06088992974238876,
                    0.0,
                    0.015384615384615385,
                    0.48973042362002567
                ]
            ],
            "fraction_answers": {
                "catholic": 0.4700516632453676,
                "protestant": 0.3581518088875992,
                "eastern orthodox": 0.17179652786703326
            },
            "question": "Which of these versions of the Old Testament typically contains the most books?",
            "rate_limited": false,
            "answers": [
                "catholic",
                "protestant",
                "eastern orthodox"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "catholic": 0.4775552895315408,
                "protestant": 0.27990882442907905,
                "eastern orthodox": 0.3251901384625996
            },
            "integer_answers": {
                "catholic": 5,
                "protestant": 1,
                "eastern orthodox": 2
            },
            "data": {
                "result_count_important_words": [
                    1540000.0,
                    934000.0,
                    131000.0
                ],
                "wikipedia_search": [
                    1.413012524228418,
                    1.13437488906402,
                    2.4526125867075623
                ],
                "result_count_noun_chunks": [
                    3280000.0,
                    4920000.0,
                    2220000.0
                ],
                "word_relation_to_question": [
                    0.3998716302952503,
                    2.1514762516046213,
                    2.4486521181001284
                ],
                "word_count_noun_chunks": [
                    125.0,
                    57.0,
                    0.0
                ],
                "word_count_raw": [
                    89.0,
                    39.0,
                    2.0
                ],
                "word_count_appended": [
                    461.0,
                    341.0,
                    52.0
                ],
                "result_count": [
                    2610000.0,
                    1640000.0,
                    245000.0
                ]
            },
            "z-best_answer_by_ml": [
                "catholic"
            ]
        },
        "lines": [
            [
                0,
                0.5806451612903226,
                0.5911708253358925,
                0.31477927063339733,
                0.2826025048456836,
                0.5398126463700235,
                0.6868131868131868,
                0.6846153846153846,
                0.07997432605905005
            ],
            [
                0,
                0.36484983314794217,
                0.3585412667946257,
                0.472168905950096,
                0.226874977812804,
                0.3992974238875878,
                0.3131868131868132,
                0.3,
                0.43029525032092425
            ],
            [
                1,
                0.054505005561735265,
                0.050287907869481764,
                0.21305182341650672,
                0.4905225173415125,
                0.06088992974238876,
                0.0,
                0.015384615384615385,
                0.48973042362002567
            ]
        ]
    },
    "The material that forms images in an Etch A Sketch is also the main component in which item?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.05357142857142857,
                    0.04838709677419355,
                    0.039473684210526314,
                    0.5265340673417274,
                    0.21739130434782608,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.31296922417855
                ],
                [
                    0.2857142857142857,
                    0.3225806451612903,
                    0.5131578947368421,
                    0.22878380055409095,
                    0.17391304347826086,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.5222175363803926
                ],
                [
                    0.6607142857142857,
                    0.6290322580645161,
                    0.4473684210526316,
                    0.24468213210418163,
                    0.6086956521739131,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.16481323944105739
                ]
            ],
            "fraction_answers": {
                "zinc supplement tablets": 0.2331241840113648,
                "soda cans": 0.4277465819021565,
                "u.s. nickels": 0.33912923408647866
            },
            "question": "The material that forms images in an Etch A Sketch is also the main component in which item?",
            "rate_limited": false,
            "answers": [
                "zinc supplement tablets",
                "u.s. nickels",
                "soda cans"
            ],
            "ml_answers": {
                "zinc supplement tablets": 0.15295199783083352,
                "soda cans": 0.2750482514970484,
                "u.s. nickels": 0.31547219440875873
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "zinc supplement tablets": 3,
                "soda cans": 3,
                "u.s. nickels": 2
            },
            "data": {
                "result_count_important_words": [
                    3.0,
                    20.0,
                    39.0
                ],
                "wikipedia_search": [
                    4.212272538733819,
                    1.8302704044327276,
                    1.957457056833453
                ],
                "result_count_noun_chunks": [
                    3.0,
                    39.0,
                    34.0
                ],
                "word_relation_to_question": [
                    2.5037537934284,
                    4.177740291043141,
                    1.318505915528459
                ],
                "word_count_noun_chunks": [
                    1.0,
                    1.0,
                    1.0
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    1.0
                ],
                "word_count_appended": [
                    5.0,
                    4.0,
                    14.0
                ],
                "result_count": [
                    3.0,
                    16.0,
                    37.0
                ]
            },
            "z-best_answer_by_ml": [
                "u.s. nickels"
            ]
        },
        "lines": [
            [
                0,
                0.05357142857142857,
                0.04838709677419355,
                0.039473684210526314,
                0.5265340673417274,
                0.21739130434782608,
                0.3333333333333333,
                0.3333333333333333,
                0.31296922417855
            ],
            [
                0,
                0.2857142857142857,
                0.3225806451612903,
                0.5131578947368421,
                0.22878380055409095,
                0.17391304347826086,
                0.3333333333333333,
                0.3333333333333333,
                0.5222175363803926
            ],
            [
                1,
                0.6607142857142857,
                0.6290322580645161,
                0.4473684210526316,
                0.24468213210418163,
                0.6086956521739131,
                0.3333333333333333,
                0.3333333333333333,
                0.16481323944105739
            ]
        ]
    },
    "Which of these actresses is NOT mentioned in Madonna\u2019s song \u201cVogue\u201d?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.031273374000631626,
                    0.46446721652920103,
                    0.425818373812038,
                    0.4688183807439825,
                    0.3220338983050848,
                    0.5,
                    0.25,
                    0.32328869047619047
                ],
                [
                    0.4999750677326596,
                    0.11426310089144381,
                    0.10929250263991552,
                    0.18347500420804574,
                    0.3728813559322034,
                    0.5,
                    0.5,
                    0.29947916666666663
                ],
                [
                    0.4687515582667088,
                    0.42126968257935515,
                    0.46488912354804646,
                    0.34770661504797173,
                    0.30508474576271183,
                    0.0,
                    0.25,
                    0.37723214285714285
                ]
            ],
            "fraction_answers": {
                "jean harlow": 0.3035750165332179,
                "audrey hepburn": 0.3551584504822663,
                "rita hayworth": 0.34126653298451576
            },
            "question": "Which of these actresses is NOT mentioned in Madonna\u2019s song \u201cVogue\u201d?",
            "rate_limited": false,
            "answers": [
                "jean harlow",
                "audrey hepburn",
                "rita hayworth"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "jean harlow": 0.38314476400506575,
                "audrey hepburn": 0.23015305739368508,
                "rita hayworth": 0.33136519303549705
            },
            "integer_answers": {
                "jean harlow": 2,
                "audrey hepburn": 4,
                "rita hayworth": 2
            },
            "data": {
                "result_count_important_words": [
                    85300.0,
                    926000.0,
                    189000.0
                ],
                "wikipedia_search": [
                    0.24945295404814005,
                    2.532199966335634,
                    1.2183470796162261
                ],
                "result_count_noun_chunks": [
                    281000.0,
                    1480000.0,
                    133000.0
                ],
                "word_relation_to_question": [
                    1.413690476190476,
                    1.6041666666666665,
                    0.9821428571428572
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    1.0
                ],
                "word_count_appended": [
                    42.0,
                    30.0,
                    46.0
                ],
                "result_count": [
                    1410000.0,
                    75.0,
                    94000.0
                ]
            },
            "z-best_answer_by_ml": [
                "jean harlow"
            ]
        },
        "lines": [
            [
                0,
                0.031273374000631626,
                0.46446721652920103,
                0.425818373812038,
                0.4688183807439825,
                0.3220338983050848,
                0.5,
                0.25,
                0.32328869047619047
            ],
            [
                1,
                0.4999750677326596,
                0.11426310089144381,
                0.10929250263991552,
                0.18347500420804574,
                0.3728813559322034,
                0.5,
                0.5,
                0.29947916666666663
            ],
            [
                0,
                0.4687515582667088,
                0.42126968257935515,
                0.46488912354804646,
                0.34770661504797173,
                0.30508474576271183,
                0.0,
                0.25,
                0.37723214285714285
            ]
        ]
    },
    "Wrestling legend Ric Flair entered the ring to the same music used in what classic film?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.6382813108118627,
                    0.6381915343508515,
                    0.3369707893344827,
                    0.732760827637417,
                    0.17647058823529413,
                    0,
                    0,
                    0.2268023147444017
                ],
                [
                    0.35922783529228614,
                    0.3586943985237015,
                    0.6621867836921811,
                    0.0445646010268299,
                    0.6078431372549019,
                    0,
                    0,
                    0.5089541679946823
                ],
                [
                    0.0024908538958511714,
                    0.003114067125446926,
                    0.0008424269733362067,
                    0.22267457133575316,
                    0.21568627450980393,
                    0,
                    0,
                    0.264243517260916
                ]
            ],
            "fraction_answers": {
                "star wars: episode iv": 0.4582462275190517,
                "back to the future": 0.1181752851835179,
                "2001: a space odyssey": 0.4235784872974305
            },
            "question": "Wrestling legend Ric Flair entered the ring to the same music used in what classic film?",
            "rate_limited": false,
            "answers": [
                "star wars: episode iv",
                "2001: a space odyssey",
                "back to the future"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "star wars: episode iv": 0.4022840099612982,
                "back to the future": 0.15562150519059464,
                "2001: a space odyssey": 0.3137208899131361
            },
            "z-best_answer_by_ml": [
                "star wars: episode iv"
            ],
            "data": {
                "result_count_important_words": [
                    16600.0,
                    9330.0,
                    81.0
                ],
                "wikipedia_search": [
                    5.129325793461918,
                    0.31195220718780925,
                    1.5587219993502717
                ],
                "result_count_noun_chunks": [
                    17200.0,
                    33800.0,
                    43.0
                ],
                "word_relation_to_question": [
                    1.3608138884664103,
                    3.0537250079680938,
                    1.585461103565496
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    16400.0,
                    9230.0,
                    64.0
                ],
                "word_count_appended": [
                    9.0,
                    31.0,
                    11.0
                ]
            },
            "integer_answers": {
                "star wars: episode iv": 3,
                "back to the future": 0,
                "2001: a space odyssey": 3
            }
        },
        "lines": [
            [
                0,
                0.6382813108118627,
                0.6381915343508515,
                0.3369707893344827,
                0.732760827637417,
                0.17647058823529413,
                0,
                0,
                0.2268023147444017
            ],
            [
                1,
                0.35922783529228614,
                0.3586943985237015,
                0.6621867836921811,
                0.0445646010268299,
                0.6078431372549019,
                0,
                0,
                0.5089541679946823
            ],
            [
                0,
                0.0024908538958511714,
                0.003114067125446926,
                0.0008424269733362067,
                0.22267457133575316,
                0.21568627450980393,
                0,
                0,
                0.264243517260916
            ]
        ]
    },
    "The man famously known as the Science Guy holds a patent for which of these items?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.11428571428571428,
                    0.10344827586206896,
                    0.007138423339540658,
                    0.23822222222222225,
                    0.14,
                    0,
                    0,
                    0.3065058479532164
                ],
                [
                    0.5619047619047619,
                    0.603448275862069,
                    0.9745499689633768,
                    0.7137777777777778,
                    0.62,
                    0,
                    0,
                    0.32876461988304095
                ],
                [
                    0.3238095238095238,
                    0.29310344827586204,
                    0.01831160769708256,
                    0.048,
                    0.24,
                    0,
                    0,
                    0.3647295321637427
                ]
            ],
            "fraction_answers": {
                "pulse rate monitor": 0.1516000806104604,
                "mechanical pencil": 0.6337409007318376,
                "ballet shoe": 0.21465901865770187
            },
            "question": "The man famously known as the Science Guy holds a patent for which of these items?",
            "rate_limited": false,
            "answers": [
                "pulse rate monitor",
                "mechanical pencil",
                "ballet shoe"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "pulse rate monitor": 0.32559523786462485,
                "mechanical pencil": 0.34393939425547915,
                "ballet shoe": 0.32559523786462485
            },
            "integer_answers": {
                "pulse rate monitor": 0,
                "mechanical pencil": 5,
                "ballet shoe": 1
            },
            "data": {
                "result_count_important_words": [
                    6.0,
                    35.0,
                    17.0
                ],
                "wikipedia_search": [
                    1.1911111111111112,
                    3.568888888888889,
                    0.24
                ],
                "result_count_noun_chunks": [
                    23.0,
                    3140.0,
                    59.0
                ],
                "word_relation_to_question": [
                    1.8390350877192982,
                    1.9725877192982457,
                    2.1883771929824563
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    7.0,
                    31.0,
                    12.0
                ],
                "result_count": [
                    12.0,
                    59.0,
                    34.0
                ]
            },
            "z-best_answer_by_ml": [
                "mechanical pencil"
            ]
        },
        "lines": [
            [
                0,
                0.11428571428571428,
                0.10344827586206896,
                0.007138423339540658,
                0.23822222222222225,
                0.14,
                0,
                0,
                0.3065058479532164
            ],
            [
                0,
                0.5619047619047619,
                0.603448275862069,
                0.9745499689633768,
                0.7137777777777778,
                0.62,
                0,
                0,
                0.32876461988304095
            ],
            [
                1,
                0.3238095238095238,
                0.29310344827586204,
                0.01831160769708256,
                0.048,
                0.24,
                0,
                0,
                0.3647295321637427
            ]
        ]
    },
    "What advertising mascot wears epaulettes?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.07692307692307693,
                    0.07692307692307693,
                    0.08035714285714286,
                    0.08333333333333333,
                    0.2765957446808511,
                    0,
                    0,
                    0.2519349845201238
                ],
                [
                    0.5961538461538461,
                    0.5961538461538461,
                    0.625,
                    0.21794871794871795,
                    0.6382978723404256,
                    0,
                    0,
                    0.39744582043343657
                ],
                [
                    0.3269230769230769,
                    0.3269230769230769,
                    0.29464285714285715,
                    0.6987179487179488,
                    0.0851063829787234,
                    0,
                    0,
                    0.3506191950464396
                ]
            ],
            "fraction_answers": {
                "sun-maid raisin girl": 0.14101122653960083,
                "mr. peanut": 0.5118333505050454,
                "cap'n crunch": 0.3471554229553538
            },
            "question": "What advertising mascot wears epaulettes?",
            "rate_limited": false,
            "answers": [
                "sun-maid raisin girl",
                "mr. peanut",
                "cap'n crunch"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "sun-maid raisin girl": 0.31157143216045696,
                "mr. peanut": 0.3739747133846042,
                "cap'n crunch": 0.28638353297773866
            },
            "integer_answers": {
                "sun-maid raisin girl": 0,
                "mr. peanut": 5,
                "cap'n crunch": 1
            },
            "data": {
                "result_count_important_words": [
                    8.0,
                    62.0,
                    34.0
                ],
                "wikipedia_search": [
                    0.25,
                    0.6538461538461539,
                    2.0961538461538463
                ],
                "result_count_noun_chunks": [
                    9.0,
                    70.0,
                    33.0
                ],
                "word_relation_to_question": [
                    1.0077399380804952,
                    1.5897832817337463,
                    1.4024767801857585
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    13.0,
                    30.0,
                    4.0
                ],
                "result_count": [
                    8.0,
                    62.0,
                    34.0
                ]
            },
            "z-best_answer_by_ml": [
                "mr. peanut"
            ]
        },
        "lines": [
            [
                0,
                0.07692307692307693,
                0.07692307692307693,
                0.08035714285714286,
                0.08333333333333333,
                0.2765957446808511,
                0,
                0,
                0.2519349845201238
            ],
            [
                0,
                0.5961538461538461,
                0.5961538461538461,
                0.625,
                0.21794871794871795,
                0.6382978723404256,
                0,
                0,
                0.39744582043343657
            ],
            [
                1,
                0.3269230769230769,
                0.3269230769230769,
                0.29464285714285715,
                0.6987179487179488,
                0.0851063829787234,
                0,
                0,
                0.3506191950464396
            ]
        ]
    },
    "Which alum from \u201cThe Hills\u201d founded a wildly popular millennial skincare line?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.12903225806451613,
                    0.16083916083916083,
                    0.21379310344827587,
                    0.3932291666666667,
                    0.16666666666666666,
                    0,
                    0,
                    0.571637243854597
                ],
                [
                    0.5645161290322581,
                    0.46153846153846156,
                    0.41379310344827586,
                    0.5166495901639344,
                    0.5333333333333333,
                    0,
                    0,
                    0.167357182694606
                ],
                [
                    0.3064516129032258,
                    0.3776223776223776,
                    0.3724137931034483,
                    0.0901212431693989,
                    0.3,
                    0,
                    0,
                    0.26100557345079695
                ]
            ],
            "fraction_answers": {
                "emily weiss": 0.2725329332566472,
                "whitney port": 0.2846024333748746,
                "lauren conrad": 0.4428646333684782
            },
            "question": "Which alum from \u201cThe Hills\u201d founded a wildly popular millennial skincare line?",
            "rate_limited": false,
            "answers": [
                "emily weiss",
                "lauren conrad",
                "whitney port"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "emily weiss": 0.31157143216045696,
                "whitney port": 0.28638353297773866,
                "lauren conrad": 0.35275388472807867
            },
            "integer_answers": {
                "emily weiss": 1,
                "whitney port": 0,
                "lauren conrad": 5
            },
            "data": {
                "result_count_important_words": [
                    23.0,
                    66.0,
                    54.0
                ],
                "wikipedia_search": [
                    1.5729166666666667,
                    2.0665983606557377,
                    0.3604849726775956
                ],
                "result_count_noun_chunks": [
                    31.0,
                    60.0,
                    54.0
                ],
                "word_relation_to_question": [
                    4.001460706982179,
                    1.171500278862242,
                    1.8270390141555786
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    5.0,
                    16.0,
                    9.0
                ],
                "result_count": [
                    8.0,
                    35.0,
                    19.0
                ]
            },
            "z-best_answer_by_ml": [
                "lauren conrad"
            ]
        },
        "lines": [
            [
                1,
                0.12903225806451613,
                0.16083916083916083,
                0.21379310344827587,
                0.3932291666666667,
                0.16666666666666666,
                0,
                0,
                0.571637243854597
            ],
            [
                0,
                0.5645161290322581,
                0.46153846153846156,
                0.41379310344827586,
                0.5166495901639344,
                0.5333333333333333,
                0,
                0,
                0.167357182694606
            ],
            [
                0,
                0.3064516129032258,
                0.3776223776223776,
                0.3724137931034483,
                0.0901212431693989,
                0.3,
                0,
                0,
                0.26100557345079695
            ]
        ]
    },
    "Which color is NOT represented in the original electronic Simon game?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.3375886524822695,
                    0.33810143042912877,
                    0.36744712990936557,
                    0.4636430376008925,
                    0.3205499276410999,
                    0.25,
                    0.25000000000000006,
                    0.25531274281274285
                ],
                [
                    0.32340425531914896,
                    0.3224967490247074,
                    0.2620845921450151,
                    0.3968557101297111,
                    0.3437047756874096,
                    0.5,
                    0.45833333333333337,
                    0.4062645687645688
                ],
                [
                    0.33900709219858155,
                    0.33940182054616386,
                    0.3704682779456193,
                    0.1395012522693965,
                    0.3357452966714906,
                    0.25,
                    0.2916666666666667,
                    0.3384226884226884
                ]
            ],
            "fraction_answers": {
                "blue": 0.3543392697811253,
                "orange": 0.24671400389902648,
                "green": 0.3989467263198483
            },
            "question": "Which color is NOT represented in the original electronic Simon game?",
            "rate_limited": false,
            "answers": [
                "blue",
                "orange",
                "green"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "blue": 0.26748429307020793,
                "orange": 0.32418132981866316,
                "green": 0.3121287071095203
            },
            "integer_answers": {
                "blue": 4,
                "orange": 3,
                "green": 1
            },
            "data": {
                "result_count_important_words": [
                    2490000.0,
                    2730000.0,
                    2470000.0
                ],
                "wikipedia_search": [
                    0.3635696239910755,
                    1.0314428987028894,
                    3.604987477306035
                ],
                "result_count_noun_chunks": [
                    3510000.0,
                    6300000.0,
                    3430000.0
                ],
                "word_relation_to_question": [
                    2.936247086247086,
                    1.1248251748251747,
                    1.938927738927739
                ],
                "word_count_noun_chunks": [
                    5.0,
                    0.0,
                    5.0
                ],
                "word_count_raw": [
                    6.0,
                    1.0,
                    5.0
                ],
                "word_count_appended": [
                    248.0,
                    216.0,
                    227.0
                ],
                "result_count": [
                    2290000.0,
                    2490000.0,
                    2270000.0
                ]
            },
            "z-best_answer_by_ml": [
                "orange"
            ]
        },
        "lines": [
            [
                0,
                0.3375886524822695,
                0.33810143042912877,
                0.36744712990936557,
                0.4636430376008925,
                0.3205499276410999,
                0.25,
                0.25000000000000006,
                0.25531274281274285
            ],
            [
                1,
                0.32340425531914896,
                0.3224967490247074,
                0.2620845921450151,
                0.3968557101297111,
                0.3437047756874096,
                0.5,
                0.45833333333333337,
                0.4062645687645688
            ],
            [
                0,
                0.33900709219858155,
                0.33940182054616386,
                0.3704682779456193,
                0.1395012522693965,
                0.3357452966714906,
                0.25,
                0.2916666666666667,
                0.3384226884226884
            ]
        ]
    },
    "Which person is most likely to use a Reuleaux triangle at work?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.06451612903225806,
                    0.06802721088435375,
                    0.09770114942528736,
                    0.02611754966887417,
                    0.05970149253731343,
                    0,
                    0,
                    0.3235713727329581
                ],
                [
                    0.3419354838709677,
                    0.3333333333333333,
                    0.3563218390804598,
                    0.15378565970453387,
                    0.07462686567164178,
                    0,
                    0,
                    0.22611788617886178
                ],
                [
                    0.5935483870967742,
                    0.5986394557823129,
                    0.5459770114942529,
                    0.820096790626592,
                    0.8656716417910447,
                    0,
                    0,
                    0.4503107410881801
                ]
            ],
            "fraction_answers": {
                "banksy": 0.6457073379798596,
                "adam levine": 0.24768684463996637,
                "greta gerwig": 0.10660581738017412
            },
            "question": "Which person is most likely to use a Reuleaux triangle at work?",
            "rate_limited": false,
            "answers": [
                "greta gerwig",
                "adam levine",
                "banksy"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "banksy": 0.3739747133846042,
                "adam levine": 0.26516270432121314,
                "greta gerwig": 0.31157143216045696
            },
            "integer_answers": {
                "banksy": 6,
                "adam levine": 0,
                "greta gerwig": 0
            },
            "data": {
                "result_count_important_words": [
                    10.0,
                    49.0,
                    88.0
                ],
                "wikipedia_search": [
                    0.07835264900662252,
                    0.46135697911360163,
                    2.460290371879776
                ],
                "result_count_noun_chunks": [
                    17.0,
                    62.0,
                    95.0
                ],
                "word_relation_to_question": [
                    1.2942854909318324,
                    0.9044715447154471,
                    1.8012429643527204
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    10.0,
                    53.0,
                    92.0
                ],
                "word_count_appended": [
                    4.0,
                    5.0,
                    58.0
                ]
            },
            "z-best_answer_by_ml": [
                "banksy"
            ]
        },
        "lines": [
            [
                0,
                0.06451612903225806,
                0.06802721088435375,
                0.09770114942528736,
                0.02611754966887417,
                0.05970149253731343,
                0,
                0,
                0.3235713727329581
            ],
            [
                1,
                0.3419354838709677,
                0.3333333333333333,
                0.3563218390804598,
                0.15378565970453387,
                0.07462686567164178,
                0,
                0,
                0.22611788617886178
            ],
            [
                0,
                0.5935483870967742,
                0.5986394557823129,
                0.5459770114942529,
                0.820096790626592,
                0.8656716417910447,
                0,
                0,
                0.4503107410881801
            ]
        ]
    },
    "Which of these songs was written by the man nicknamed \u201cSlowhand\u201d?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.999274112201514,
                    0.9992964952673318,
                    0.8333333333333334,
                    0.2256383712905452,
                    0.7285714285714285,
                    0,
                    1.0,
                    0.3954534466086476
                ],
                [
                    0.0004053659134402368,
                    0.0003997185981069327,
                    0.08484848484848485,
                    0.6922015182884748,
                    0.15714285714285714,
                    0,
                    0.0,
                    0.3915380232076254
                ],
                [
                    0.00032052188504576865,
                    0.0003037861345612689,
                    0.08181818181818182,
                    0.08216011042097998,
                    0.11428571428571428,
                    0,
                    0.0,
                    0.213008530183727
                ]
            ],
            "fraction_answers": {
                "lay down sally": 0.7402238838961144,
                "lover lay down": 0.07027097781831573,
                "lay lady lay": 0.18950513828556992
            },
            "question": "Which of these songs was written by the man nicknamed \u201cSlowhand\u201d?",
            "rate_limited": false,
            "answers": [
                "lay down sally",
                "lay lady lay",
                "lover lay down"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "lay down sally": 0.3635484425931417,
                "lover lay down": 0.29035060350393144,
                "lay lady lay": 0.31157143216045696
            },
            "z-best_answer_by_ml": [
                "lay down sally"
            ],
            "data": {
                "result_count_important_words": [
                    125000.0,
                    50.0,
                    38.0
                ],
                "wikipedia_search": [
                    0.6769151138716356,
                    2.0766045548654244,
                    0.24648033126293994
                ],
                "result_count_noun_chunks": [
                    1100000.0,
                    112000.0,
                    108000.0
                ],
                "word_relation_to_question": [
                    1.5818137864345905,
                    1.5661520928305015,
                    0.852034120734908
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    3.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    106000.0,
                    43.0,
                    34.0
                ],
                "word_count_appended": [
                    51.0,
                    11.0,
                    8.0
                ]
            },
            "integer_answers": {
                "lay down sally": 6,
                "lover lay down": 0,
                "lay lady lay": 1
            }
        },
        "lines": [
            [
                1,
                0.999274112201514,
                0.9992964952673318,
                0.8333333333333334,
                0.2256383712905452,
                0.7285714285714285,
                0,
                1.0,
                0.3954534466086476
            ],
            [
                0,
                0.0004053659134402368,
                0.0003997185981069327,
                0.08484848484848485,
                0.6922015182884748,
                0.15714285714285714,
                0,
                0.0,
                0.3915380232076254
            ],
            [
                0,
                0.00032052188504576865,
                0.0003037861345612689,
                0.08181818181818182,
                0.08216011042097998,
                0.11428571428571428,
                0,
                0.0,
                0.213008530183727
            ]
        ]
    },
    "Which of these quantities is the largest?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.9668604117661541,
                    0.9413040606772662,
                    0.9663256299170259,
                    0,
                    0.9787798408488063,
                    0,
                    0,
                    1.0
                ],
                [
                    1.566671963509972e-05,
                    2.085607224543426e-05,
                    8.832008445478193e-06,
                    0,
                    0.010610079575596816,
                    0,
                    0,
                    0.0
                ],
                [
                    0.033123921514210834,
                    0.05867508325048838,
                    0.03366553807452864,
                    0,
                    0.010610079575596816,
                    0,
                    0,
                    0.0
                ]
            ],
            "fraction_answers": {
                "two half-dozens": 0.0021310868751845662,
                "baker's dozen": 0.02721492448296494,
                "dozen": 0.9706539886418506
            },
            "question": "Which of these quantities is the largest?",
            "rate_limited": false,
            "answers": [
                "dozen",
                "two half-dozens",
                "baker's dozen"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "two half-dozens": 0.13115916706799896,
                "baker's dozen": 0.12865795091138382,
                "dozen": 0.6429398941929336
            },
            "z-best_answer_by_ml": [
                "dozen"
            ],
            "data": {
                "result_count_important_words": [
                    677000.0,
                    15.0,
                    42200.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    1860000.0,
                    17.0,
                    64800.0
                ],
                "word_relation_to_question": [
                    2.0,
                    0.0,
                    0.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    369.0,
                    4.0,
                    4.0
                ],
                "result_count": [
                    864000.0,
                    14.0,
                    29600.0
                ]
            },
            "integer_answers": {
                "two half-dozens": 0,
                "baker's dozen": 0,
                "dozen": 5
            }
        },
        "lines": [
            [
                0,
                0.9668604117661541,
                0.9413040606772662,
                0.9663256299170259,
                0,
                0.9787798408488063,
                0,
                0,
                1.0
            ],
            [
                0,
                1.566671963509972e-05,
                2.085607224543426e-05,
                8.832008445478193e-06,
                0,
                0.010610079575596816,
                0,
                0,
                0.0
            ],
            [
                1,
                0.033123921514210834,
                0.05867508325048838,
                0.03366553807452864,
                0,
                0.010610079575596816,
                0,
                0,
                0.0
            ]
        ]
    },
    "The actor who played Don Draper provides the voice for what car company\u2019s ads?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.1782650142993327,
                    0.164827703276184,
                    0.1767881241565452,
                    0.5622393219582915,
                    0.335042735042735,
                    0.68,
                    0.6666666666666666,
                    0.3702022280264299
                ],
                [
                    0.4232602478551001,
                    0.42777117637073503,
                    0.4241372662425294,
                    0.3708803808408609,
                    0.31794871794871793,
                    0.12,
                    0.25,
                    0.319589008197601
                ],
                [
                    0.3984747378455672,
                    0.40740112035308096,
                    0.3990746096009254,
                    0.06688029720084755,
                    0.347008547008547,
                    0.2,
                    0.08333333333333333,
                    0.3102087637759691
                ]
            ],
            "fraction_answers": {
                "jaguar": 0.27654767613978387,
                "bmw": 0.33169834968194306,
                "mercedes-benz": 0.3917539741782731
            },
            "question": "The actor who played Don Draper provides the voice for what car company\u2019s ads?",
            "rate_limited": false,
            "answers": [
                "mercedes-benz",
                "bmw",
                "jaguar"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "jaguar": 0.27595726218627614,
                "bmw": 0.27595726218627614,
                "mercedes-benz": 0.3796181564740296
            },
            "z-best_answer_by_ml": [
                "mercedes-benz"
            ],
            "data": {
                "result_count_important_words": [
                    971000.0,
                    2520000.0,
                    2400000.0
                ],
                "wikipedia_search": [
                    3.9356752537080406,
                    2.5961626658860264,
                    0.4681620804059328
                ],
                "result_count_noun_chunks": [
                    917000.0,
                    2200000.0,
                    2070000.0
                ],
                "word_relation_to_question": [
                    2.2212133681585793,
                    1.917534049185606,
                    1.8612525826558148
                ],
                "word_count_noun_chunks": [
                    17.0,
                    3.0,
                    5.0
                ],
                "word_count_raw": [
                    8.0,
                    3.0,
                    1.0
                ],
                "result_count": [
                    935000.0,
                    2220000.0,
                    2090000.0
                ],
                "word_count_appended": [
                    196.0,
                    186.0,
                    203.0
                ]
            },
            "integer_answers": {
                "jaguar": 1,
                "bmw": 3,
                "mercedes-benz": 4
            }
        },
        "lines": [
            [
                1,
                0.1782650142993327,
                0.164827703276184,
                0.1767881241565452,
                0.5622393219582915,
                0.335042735042735,
                0.68,
                0.6666666666666666,
                0.3702022280264299
            ],
            [
                0,
                0.4232602478551001,
                0.42777117637073503,
                0.4241372662425294,
                0.3708803808408609,
                0.31794871794871793,
                0.12,
                0.25,
                0.319589008197601
            ],
            [
                0,
                0.3984747378455672,
                0.40740112035308096,
                0.3990746096009254,
                0.06688029720084755,
                0.347008547008547,
                0.2,
                0.08333333333333333,
                0.3102087637759691
            ]
        ]
    },
    "Which of these substances expands when it freezes?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.20040531411844179,
                    0.5971735867933967,
                    0.1791675796909037,
                    0.0,
                    0.2867383512544803,
                    0.0,
                    0,
                    0.25
                ],
                [
                    0.7633415897320424,
                    0.39394697348674335,
                    0.8159214240600147,
                    1.0,
                    0.3548387096774194,
                    1.0,
                    0,
                    0.0
                ],
                [
                    0.03625309614951588,
                    0.00887943971985993,
                    0.004910996249081605,
                    0.0,
                    0.35842293906810035,
                    0.0,
                    0,
                    0.75
                ]
            ],
            "fraction_answers": {
                "sodium chloride": 0.21621211883674607,
                "carbon dioxide": 0.6182926709937456,
                "dihydrogen monoxide": 0.16549521016950824
            },
            "question": "Which of these substances expands when it freezes?",
            "rate_limited": false,
            "answers": [
                "sodium chloride",
                "carbon dioxide",
                "dihydrogen monoxide"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "sodium chloride": 0.38225360497332217,
                "carbon dioxide": 0.5112864536532432,
                "dihydrogen monoxide": 0.20825414493833785
            },
            "z-best_answer_by_ml": [
                "carbon dioxide"
            ],
            "data": {
                "result_count_important_words": [
                    1910000.0,
                    1260000.0,
                    28400.0
                ],
                "wikipedia_search": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    1390000.0,
                    6330000.0,
                    38100.0
                ],
                "word_relation_to_question": [
                    0.25,
                    0.0,
                    0.75
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    89000.0,
                    339000.0,
                    16100.0
                ],
                "word_count_appended": [
                    80.0,
                    99.0,
                    100.0
                ]
            },
            "integer_answers": {
                "sodium chloride": 1,
                "carbon dioxide": 4,
                "dihydrogen monoxide": 2
            }
        },
        "lines": [
            [
                0,
                0.20040531411844179,
                0.5971735867933967,
                0.1791675796909037,
                0.0,
                0.2867383512544803,
                0.0,
                0,
                0.25
            ],
            [
                0,
                0.7633415897320424,
                0.39394697348674335,
                0.8159214240600147,
                1.0,
                0.3548387096774194,
                1.0,
                0,
                0.0
            ],
            [
                1,
                0.03625309614951588,
                0.00887943971985993,
                0.004910996249081605,
                0.0,
                0.35842293906810035,
                0.0,
                0,
                0.75
            ]
        ]
    },
    "Which U.S. president's wife was NOT born in North America?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.2829232995658466,
                    0.3324538258575198,
                    0.2651023464802796,
                    0.35336066652605147,
                    0.31678486997635935,
                    0.125,
                    0.0,
                    0.28965034003276047
                ],
                [
                    0.3836830680173661,
                    0.3232189973614776,
                    0.36095856215676486,
                    0.3798067381558843,
                    0.36879432624113473,
                    0.375,
                    0.5,
                    0.34399178235638134
                ],
                [
                    0.33339363241678727,
                    0.34432717678100266,
                    0.37393909136295556,
                    0.26683259531806425,
                    0.3144208037825059,
                    0.5,
                    0.5,
                    0.36635787761085825
                ]
            ],
            "fraction_answers": {
                "martin van buren": 0.25018220568195654,
                "john quincy adams": 0.5086811628902957,
                "rutherford b. hayes": 0.24113663142774774
            },
            "question": "Which U.S. president's wife was NOT born in North America?",
            "rate_limited": false,
            "answers": [
                "john quincy adams",
                "rutherford b. hayes",
                "martin van buren"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "martin van buren": 0.35443025729131133,
                "john quincy adams": 0.3049087145126065,
                "rutherford b. hayes": 0.3635484425931417
            },
            "integer_answers": {
                "martin van buren": 2,
                "john quincy adams": 5,
                "rutherford b. hayes": 1
            },
            "data": {
                "result_count_important_words": [
                    127000.0,
                    134000.0,
                    118000.0
                ],
                "wikipedia_search": [
                    1.759672001687382,
                    1.4423191421293886,
                    2.7980088561832295
                ],
                "result_count_noun_chunks": [
                    94100.0,
                    55700.0,
                    50500.0
                ],
                "word_relation_to_question": [
                    2.9448952395413537,
                    2.1841150470106614,
                    1.8709897134479845
                ],
                "word_count_noun_chunks": [
                    3.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    6.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    120000.0,
                    64300.0,
                    92100.0
                ],
                "word_count_appended": [
                    155.0,
                    111.0,
                    157.0
                ]
            },
            "z-best_answer_by_ml": [
                "rutherford b. hayes"
            ]
        },
        "lines": [
            [
                1,
                0.2829232995658466,
                0.3324538258575198,
                0.2651023464802796,
                0.35336066652605147,
                0.31678486997635935,
                0.125,
                0.0,
                0.28965034003276047
            ],
            [
                0,
                0.3836830680173661,
                0.3232189973614776,
                0.36095856215676486,
                0.3798067381558843,
                0.36879432624113473,
                0.375,
                0.5,
                0.34399178235638134
            ],
            [
                0,
                0.33339363241678727,
                0.34432717678100266,
                0.37393909136295556,
                0.26683259531806425,
                0.3144208037825059,
                0.5,
                0.5,
                0.36635787761085825
            ]
        ]
    },
    "Who was the president of the Screen Actors Guild before its merger with AFTRA?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.0028834890217162767,
                    0.24198132358911897,
                    0.25132219166490377,
                    0.3248780487804878,
                    0.33053221288515405,
                    0.1896551724137931,
                    0.22448979591836735,
                    0.4144305388609112
                ],
                [
                    0.0029135253656924877,
                    0.047503045066991476,
                    0.0040194626613073835,
                    0.22235772357723577,
                    0.21568627450980393,
                    0.08620689655172414,
                    0.04081632653061224,
                    0.09222648208671777
                ],
                [
                    0.9942029856125912,
                    0.7105156313438896,
                    0.7446583456737889,
                    0.45276422764227636,
                    0.453781512605042,
                    0.7241379310344828,
                    0.7346938775510204,
                    0.49334297905237107
                ]
            ],
            "fraction_answers": {
                "gabrielle carteris": 0.2475215966418066,
                "ken howard": 0.6635121863144329,
                "melissa gilbert": 0.08896621704376065
            },
            "question": "Who was the president of the Screen Actors Guild before its merger with AFTRA?",
            "rate_limited": false,
            "answers": [
                "gabrielle carteris",
                "melissa gilbert",
                "ken howard"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "gabrielle carteris": 0.20214649654355132,
                "ken howard": 0.5305668581411278,
                "melissa gilbert": 0.16840516855240167
            },
            "integer_answers": {
                "gabrielle carteris": 0,
                "ken howard": 8,
                "melissa gilbert": 0
            },
            "data": {
                "result_count_important_words": [
                    5960.0,
                    1170.0,
                    17500.0
                ],
                "wikipedia_search": [
                    2.274146341463415,
                    1.5565040650406503,
                    3.1693495934959346
                ],
                "result_count_noun_chunks": [
                    5940.0,
                    95.0,
                    17600.0
                ],
                "word_relation_to_question": [
                    2.9010137720263782,
                    0.6455853746070244,
                    3.4534008533665976
                ],
                "word_count_noun_chunks": [
                    11.0,
                    5.0,
                    42.0
                ],
                "word_count_raw": [
                    11.0,
                    2.0,
                    36.0
                ],
                "word_count_appended": [
                    118.0,
                    77.0,
                    162.0
                ],
                "result_count": [
                    96.0,
                    97.0,
                    33100.0
                ]
            },
            "z-best_answer_by_ml": [
                "ken howard"
            ]
        },
        "lines": [
            [
                0,
                0.0028834890217162767,
                0.24198132358911897,
                0.25132219166490377,
                0.3248780487804878,
                0.33053221288515405,
                0.1896551724137931,
                0.22448979591836735,
                0.4144305388609112
            ],
            [
                0,
                0.0029135253656924877,
                0.047503045066991476,
                0.0040194626613073835,
                0.22235772357723577,
                0.21568627450980393,
                0.08620689655172414,
                0.04081632653061224,
                0.09222648208671777
            ],
            [
                1,
                0.9942029856125912,
                0.7105156313438896,
                0.7446583456737889,
                0.45276422764227636,
                0.453781512605042,
                0.7241379310344828,
                0.7346938775510204,
                0.49334297905237107
            ]
        ]
    },
    "Jean Valjean, the protagonist of \u201cLes Mis\u00e9rables,\u201d is identified by what prisoner number?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.5069444444444444,
                    0.004549090181963607,
                    0.0018657071601271727,
                    0.09663968604365954,
                    0.26141078838174275,
                    0.0,
                    0.0,
                    0.01887178554375753
                ],
                [
                    0.3055555555555556,
                    0.9948010397920416,
                    0.9975821958231005,
                    0.8004883269783932,
                    0.5643153526970954,
                    1.0,
                    1.0,
                    0.8877166655298121
                ],
                [
                    0.1875,
                    0.000649870025994801,
                    0.0005520970167723266,
                    0.10287198697794724,
                    0.17427385892116182,
                    0.0,
                    0.0,
                    0.09341154892643035
                ]
            ],
            "fraction_answers": {
                "y2k": 0.11128518771946187,
                "24601": 0.8188073920469998,
                "867-5309": 0.06990742023353831
            },
            "question": "Jean Valjean, the protagonist of \u201cLes Mis\u00e9rables,\u201d is identified by what prisoner number?",
            "rate_limited": false,
            "answers": [
                "y2k",
                "24601",
                "867-5309"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "y2k": 0.28368788585608096,
                "24601": 0.4420214376981768,
                "867-5309": 0.29035060350393144
            },
            "integer_answers": {
                "y2k": 1,
                "24601": 7,
                "867-5309": 0
            },
            "data": {
                "result_count_important_words": [
                    91.0,
                    19900.0,
                    13.0
                ],
                "wikipedia_search": [
                    0.5798381162619572,
                    4.802929961870359,
                    0.6172319218676834
                ],
                "result_count_noun_chunks": [
                    98.0,
                    52400.0,
                    29.0
                ],
                "word_relation_to_question": [
                    0.1321024988063027,
                    6.214016658708685,
                    0.6538808424850124
                ],
                "word_count_noun_chunks": [
                    0.0,
                    33.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    11.0,
                    0.0
                ],
                "word_count_appended": [
                    63.0,
                    136.0,
                    42.0
                ],
                "result_count": [
                    73.0,
                    44.0,
                    27.0
                ]
            },
            "z-best_answer_by_ml": [
                "24601"
            ]
        },
        "lines": [
            [
                0,
                0.5069444444444444,
                0.004549090181963607,
                0.0018657071601271727,
                0.09663968604365954,
                0.26141078838174275,
                0.0,
                0.0,
                0.01887178554375753
            ],
            [
                1,
                0.3055555555555556,
                0.9948010397920416,
                0.9975821958231005,
                0.8004883269783932,
                0.5643153526970954,
                1.0,
                1.0,
                0.8877166655298121
            ],
            [
                0,
                0.1875,
                0.000649870025994801,
                0.0005520970167723266,
                0.10287198697794724,
                0.17427385892116182,
                0.0,
                0.0,
                0.09341154892643035
            ]
        ]
    },
    "What is the grammatically correct way to announce people have arrived?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.04090893595100019,
                    0.05122376184315808,
                    0.8732274641628994,
                    0.0,
                    0.14285714285714285,
                    0,
                    0,
                    0.3722685761726857
                ],
                [
                    0.9583297032965784,
                    0.945957061775109,
                    0.08886828175109153,
                    0.5,
                    0.23809523809523808,
                    0,
                    0,
                    0.40568908260689085
                ],
                [
                    0.0007613607524213923,
                    0.0028191763817329335,
                    0.03790425408600904,
                    0.5,
                    0.6190476190476191,
                    0,
                    0,
                    0.2220423412204234
                ]
            ],
            "fraction_answers": {
                "they're here": 0.24674764683114772,
                "there here": 0.5228232279208179,
                "their here": 0.2304291252480343
            },
            "question": "What is the grammatically correct way to announce people have arrived?",
            "rate_limited": false,
            "answers": [
                "they're here",
                "there here",
                "their here"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "they're here": 0.16256017083297328,
                "there here": 0.3353246342726473,
                "their here": 0.16945784707725287
            },
            "integer_answers": {
                "they're here": 1,
                "there here": 4,
                "their here": 1
            },
            "data": {
                "result_count_important_words": [
                    137000.0,
                    2530000.0,
                    7540.0
                ],
                "wikipedia_search": [
                    0.0,
                    2.5,
                    2.5
                ],
                "result_count_noun_chunks": [
                    2260000.0,
                    230000.0,
                    98100.0
                ],
                "word_relation_to_question": [
                    1.8613428808634285,
                    2.0284454130344542,
                    1.110211706102117
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    3.0,
                    5.0,
                    13.0
                ],
                "result_count": [
                    108000.0,
                    2530000.0,
                    2010.0
                ]
            },
            "z-best_answer_by_ml": [
                "there here"
            ]
        },
        "lines": [
            [
                1,
                0.04090893595100019,
                0.05122376184315808,
                0.8732274641628994,
                0.0,
                0.14285714285714285,
                0,
                0,
                0.3722685761726857
            ],
            [
                0,
                0.9583297032965784,
                0.945957061775109,
                0.08886828175109153,
                0.5,
                0.23809523809523808,
                0,
                0,
                0.40568908260689085
            ],
            [
                0,
                0.0007613607524213923,
                0.0028191763817329335,
                0.03790425408600904,
                0.5,
                0.6190476190476191,
                0,
                0,
                0.2220423412204234
            ]
        ]
    },
    "Basketball is NOT a major theme of which of these 90s movies?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.4714285714285714,
                    0.4998314227916386,
                    0.47893962982183014,
                    0.3041666666666667,
                    0.4952076677316294,
                    0,
                    0,
                    0.24155405405405406
                ],
                [
                    0.44656084656084655,
                    0.4637559002022927,
                    0.45783601453035805,
                    0.2773809523809524,
                    0.3993610223642172,
                    0,
                    0,
                    0.29405405405405405
                ],
                [
                    0.08201058201058203,
                    0.03641267700606877,
                    0.06322435564781181,
                    0.41845238095238096,
                    0.10543130990415334,
                    0,
                    0,
                    0.4643918918918919
                ]
            ],
            "fraction_answers": {
                "white men can't jump": 0.16962399583520327,
                "point break": 0.22035040330242636,
                "eddie": 0.6100256008623705
            },
            "question": "Basketball is NOT a major theme of which of these 90s movies?",
            "rate_limited": false,
            "answers": [
                "white men can't jump",
                "point break",
                "eddie"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "white men can't jump": 0.5639744487311814,
                "point break": 0.3828332959591441,
                "eddie": 0.16140616253978374
            },
            "z-best_answer_by_ml": [
                "white men can't jump"
            ],
            "data": {
                "result_count_important_words": [
                    100.0,
                    21500.0,
                    275000.0
                ],
                "wikipedia_search": [
                    1.9583333333333333,
                    2.2261904761904763,
                    0.8154761904761905
                ],
                "result_count_noun_chunks": [
                    9740.0,
                    19500.0,
                    202000.0
                ],
                "word_relation_to_question": [
                    2.5844594594594597,
                    2.0594594594594597,
                    0.3560810810810811
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    10800.0,
                    20200.0,
                    158000.0
                ],
                "word_count_appended": [
                    3.0,
                    63.0,
                    247.0
                ]
            },
            "integer_answers": {
                "white men can't jump": 1,
                "point break": 1,
                "eddie": 4
            }
        },
        "lines": [
            [
                0,
                0.4714285714285714,
                0.4998314227916386,
                0.47893962982183014,
                0.3041666666666667,
                0.4952076677316294,
                0,
                0,
                0.24155405405405406
            ],
            [
                1,
                0.44656084656084655,
                0.4637559002022927,
                0.45783601453035805,
                0.2773809523809524,
                0.3993610223642172,
                0,
                0,
                0.29405405405405405
            ],
            [
                0,
                0.08201058201058203,
                0.03641267700606877,
                0.06322435564781181,
                0.41845238095238096,
                0.10543130990415334,
                0,
                0,
                0.4643918918918919
            ]
        ]
    },
    "What soda is named for a medical condition?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.04626828038325769,
                    0.0564042303172738,
                    0.07021413473026376,
                    0.11149825783972127,
                    0.27853403141361255,
                    0,
                    0.0,
                    0.3956293349653203
                ],
                [
                    0.7438224911749874,
                    0.7755581668625147,
                    0.8620531201176362,
                    0.7746806039488967,
                    0.45654450261780105,
                    0,
                    1.0,
                    0.46460628314973484
                ],
                [
                    0.2099092284417549,
                    0.16803760282021152,
                    0.0677327451521,
                    0.11382113821138214,
                    0.2649214659685864,
                    0,
                    0.0,
                    0.13976438188494494
                ]
            ],
            "fraction_answers": {
                "fanta": 0.13774093749699715,
                "pepsi": 0.7253235954102245,
                "faygo": 0.13693546709277846
            },
            "question": "What soda is named for a medical condition?",
            "rate_limited": false,
            "answers": [
                "faygo",
                "pepsi",
                "fanta"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "fanta": 0.15307699897600052,
                "pepsi": 0.5558606324637917,
                "faygo": 0.16272895241890437
            },
            "z-best_answer_by_ml": [
                "pepsi"
            ],
            "data": {
                "result_count_important_words": [
                    14400.0,
                    198000.0,
                    42900.0
                ],
                "wikipedia_search": [
                    0.33449477351916374,
                    2.3240418118466897,
                    0.34146341463414637
                ],
                "result_count_noun_chunks": [
                    76400.0,
                    938000.0,
                    73700.0
                ],
                "word_relation_to_question": [
                    1.582517339861281,
                    1.8584251325989392,
                    0.5590575275397797
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    4.0,
                    0.0
                ],
                "result_count": [
                    7340.0,
                    118000.0,
                    33300.0
                ],
                "word_count_appended": [
                    266.0,
                    436.0,
                    253.0
                ]
            },
            "integer_answers": {
                "fanta": 0,
                "pepsi": 7,
                "faygo": 0
            }
        },
        "lines": [
            [
                0,
                0.04626828038325769,
                0.0564042303172738,
                0.07021413473026376,
                0.11149825783972127,
                0.27853403141361255,
                0,
                0.0,
                0.3956293349653203
            ],
            [
                1,
                0.7438224911749874,
                0.7755581668625147,
                0.8620531201176362,
                0.7746806039488967,
                0.45654450261780105,
                0,
                1.0,
                0.46460628314973484
            ],
            [
                0,
                0.2099092284417549,
                0.16803760282021152,
                0.0677327451521,
                0.11382113821138214,
                0.2649214659685864,
                0,
                0.0,
                0.13976438188494494
            ]
        ]
    },
    "Who was NOT a wife of Henry VIII?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.2440941970030916,
                    0.24604831783463865,
                    0.2448513058303689,
                    0.34954407294832823,
                    0.26322418136020154,
                    0.29032258064516125,
                    0.3,
                    0.2953499879732752
                ],
                [
                    0.4999778116355783,
                    0.49996737426305515,
                    0.4999734527370228,
                    0.378972800249396,
                    0.4672544080604534,
                    0.5,
                    0.5,
                    0.36638078227059756
                ],
                [
                    0.25592799136133015,
                    0.2539843079023062,
                    0.2551752414326083,
                    0.27148312680227576,
                    0.2695214105793451,
                    0.2096774193548387,
                    0.2,
                    0.33826922975612733
                ]
            ],
            "fraction_answers": {
                "catherine of york": 0.07186834269597422,
                "catherine parr": 0.44164133910123365,
                "catherine howard": 0.48649031820279215
            },
            "question": "Who was NOT a wife of Henry VIII?",
            "rate_limited": false,
            "answers": [
                "catherine parr",
                "catherine of york",
                "catherine howard"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "catherine of york": 0.4420214376981768,
                "catherine parr": 0.30114516136899444,
                "catherine howard": 0.30114516136899444
            },
            "z-best_answer_by_ml": [
                "catherine of york"
            ],
            "data": {
                "result_count_important_words": [
                    288000.0,
                    37.0,
                    279000.0
                ],
                "wikipedia_search": [
                    1.5045592705167175,
                    1.21027199750604,
                    2.2851687319772425
                ],
                "result_count_noun_chunks": [
                    346000.0,
                    36.0,
                    332000.0
                ],
                "word_relation_to_question": [
                    2.046500120267248,
                    1.3361921772940248,
                    1.617307702438727
                ],
                "word_count_noun_chunks": [
                    26.0,
                    0.0,
                    36.0
                ],
                "word_count_raw": [
                    24.0,
                    0.0,
                    36.0
                ],
                "result_count": [
                    346000.0,
                    30.0,
                    330000.0
                ],
                "word_count_appended": [
                    188.0,
                    26.0,
                    183.0
                ]
            },
            "integer_answers": {
                "catherine of york": 0,
                "catherine parr": 5,
                "catherine howard": 3
            }
        },
        "lines": [
            [
                0,
                0.2440941970030916,
                0.24604831783463865,
                0.2448513058303689,
                0.34954407294832823,
                0.26322418136020154,
                0.29032258064516125,
                0.3,
                0.2953499879732752
            ],
            [
                1,
                0.4999778116355783,
                0.49996737426305515,
                0.4999734527370228,
                0.378972800249396,
                0.4672544080604534,
                0.5,
                0.5,
                0.36638078227059756
            ],
            [
                0,
                0.25592799136133015,
                0.2539843079023062,
                0.2551752414326083,
                0.27148312680227576,
                0.2695214105793451,
                0.2096774193548387,
                0.2,
                0.33826922975612733
            ]
        ]
    },
    "Which of these is NOT a name of one of the Florida Keys?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "pigeon key"
            ],
            "question": "Which of these is NOT a name of one of the Florida Keys?",
            "answers": [
                "fat deer key",
                "turtle key",
                "pigeon key"
            ],
            "integer_answers": {
                "pigeon key": 3,
                "turtle key": 2,
                "fat deer key": 2
            },
            "data": {
                "result_count_important_words": [
                    20.0,
                    36500.0,
                    92.0
                ],
                "wikipedia_search": [
                    0.5119047619047619,
                    0.25,
                    1.2380952380952381
                ],
                "result_count_noun_chunks": [
                    24.0,
                    43800.0,
                    5890000.0
                ],
                "word_relation_to_question": [
                    1.3984493770092272,
                    0.3846388337011713,
                    1.2169117892896013
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    18.0,
                    34300.0,
                    54.0
                ],
                "word_count_appended": [
                    19.0,
                    35.0,
                    70.0
                ]
            },
            "negative_question": true,
            "fraction_answers": {
                "pigeon key": 0.3694142054335063,
                "turtle key": 0.36252836950943745,
                "fat deer key": 0.2680574250570562
            },
            "lines": [
                [
                    0.49973815896660073,
                    0.4997268655085764,
                    0.49999797769532767,
                    0.37202380952380953,
                    0.42338709677419356,
                    0,
                    0.0,
                    0.26692510383179546
                ],
                [
                    0.0010473641335971284,
                    0.0015295531519720074,
                    0.4963092939729928,
                    0.4375,
                    0.3588709677419355,
                    0,
                    0.5,
                    0.43589352771647144
                ],
                [
                    0.49921447689980214,
                    0.49874358133945157,
                    0.003692728331679551,
                    0.19047619047619047,
                    0.21774193548387094,
                    0,
                    0.5,
                    0.29718136845173315
                ]
            ],
            "rate_limited": false,
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "pigeon key": 0.4138428763538842,
                "turtle key": 0.2806351618771721,
                "fat deer key": 0.3788852969376022
            }
        },
        "lines": [
            [
                0,
                0.49973815896660073,
                0.4997268655085764,
                0.49999797769532767,
                0.37202380952380953,
                0.42338709677419356,
                0,
                0.0,
                0.26692510383179546
            ],
            [
                1,
                0.0010473641335971284,
                0.0015295531519720074,
                0.4963092939729928,
                0.4375,
                0.3588709677419355,
                0,
                0.5,
                0.43589352771647144
            ],
            [
                0,
                0.49921447689980214,
                0.49874358133945157,
                0.003692728331679551,
                0.19047619047619047,
                0.21774193548387094,
                0,
                0.5,
                0.29718136845173315
            ]
        ]
    },
    "Lonnie Lynn's only Academy Award win was in what category?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.4044943820224719,
                    0.1865604057236008,
                    0.416809019473864,
                    0.24921574580044525,
                    0.29310344827586204,
                    0.0,
                    0.0,
                    0.3688883366560489
                ],
                [
                    0.3402889245585875,
                    0.15051621083137112,
                    0.3484796720191322,
                    0.261917411166045,
                    0.20689655172413793,
                    0.041666666666666664,
                    0.0,
                    0.2991255279032798
                ],
                [
                    0.2552166934189406,
                    0.6629233834450281,
                    0.23471130850700375,
                    0.4888668430335097,
                    0.5,
                    0.9583333333333334,
                    1.0,
                    0.3319861354406714
                ]
            ],
            "fraction_answers": {
                "best adapted screenplay": 0.23988391724403663,
                "best original song": 0.5540047121473108,
                "best cinematography": 0.20611137060865253
            },
            "question": "Lonnie Lynn's only Academy Award win was in what category?",
            "rate_limited": false,
            "answers": [
                "best adapted screenplay",
                "best cinematography",
                "best original song"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "best adapted screenplay": 0.1440935326406532,
                "best original song": 0.7224139102363077,
                "best cinematography": 0.18982177642744905
            },
            "z-best_answer_by_ml": [
                "best original song"
            ],
            "data": {
                "result_count_important_words": [
                    10300.0,
                    8310.0,
                    36600.0
                ],
                "wikipedia_search": [
                    0.996862983201781,
                    1.04766964466418,
                    1.9554673721340388
                ],
                "result_count_noun_chunks": [
                    12200.0,
                    10200.0,
                    6870.0
                ],
                "word_relation_to_question": [
                    1.4755533466241955,
                    1.196502111613119,
                    1.3279445417626854
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    23.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    19.0
                ],
                "result_count": [
                    12600.0,
                    10600.0,
                    7950.0
                ],
                "word_count_appended": [
                    51.0,
                    36.0,
                    87.0
                ]
            },
            "integer_answers": {
                "best adapted screenplay": 3,
                "best original song": 5,
                "best cinematography": 0
            }
        },
        "lines": [
            [
                0,
                0.4044943820224719,
                0.1865604057236008,
                0.416809019473864,
                0.24921574580044525,
                0.29310344827586204,
                0.0,
                0.0,
                0.3688883366560489
            ],
            [
                0,
                0.3402889245585875,
                0.15051621083137112,
                0.3484796720191322,
                0.261917411166045,
                0.20689655172413793,
                0.041666666666666664,
                0.0,
                0.2991255279032798
            ],
            [
                1,
                0.2552166934189406,
                0.6629233834450281,
                0.23471130850700375,
                0.4888668430335097,
                0.5,
                0.9583333333333334,
                1.0,
                0.3319861354406714
            ]
        ]
    },
    "Which of these products was featured on \u201cShark Tank\u201d?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.0037989255564082887,
                    0.003958601612057042,
                    0.7279562542720438,
                    0.03125,
                    0.140625,
                    0.0,
                    0.0,
                    0.008928571428571428
                ],
                [
                    0.9938603223330775,
                    0.99203510278056,
                    0.11380724538619276,
                    0.84375,
                    0.73046875,
                    1.0,
                    1.0,
                    0.8160714285714286
                ],
                [
                    0.002340752110514198,
                    0.00400629560738303,
                    0.1582365003417635,
                    0.125,
                    0.12890625,
                    0.0,
                    0.0,
                    0.175
                ]
            ],
            "fraction_answers": {
                "scrub daddy": 0.8112491061339073,
                "sticky buddy": 0.07418622475745759,
                "instant pot": 0.11456466910863507
            },
            "question": "Which of these products was featured on \u201cShark Tank\u201d?",
            "rate_limited": false,
            "answers": [
                "instant pot",
                "scrub daddy",
                "sticky buddy"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "scrub daddy": 0.6447987814361417,
                "sticky buddy": 0.1592824991515911,
                "instant pot": 0.2750845947247941
            },
            "z-best_answer_by_ml": [
                "scrub daddy"
            ],
            "data": {
                "result_count_important_words": [
                    83.0,
                    20800.0,
                    84.0
                ],
                "wikipedia_search": [
                    0.125,
                    3.375,
                    0.5
                ],
                "result_count_noun_chunks": [
                    213000.0,
                    33300.0,
                    46300.0
                ],
                "word_relation_to_question": [
                    0.03571428571428571,
                    3.2642857142857142,
                    0.7
                ],
                "word_count_noun_chunks": [
                    0.0,
                    6.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    4.0,
                    0.0
                ],
                "result_count": [
                    99.0,
                    25900.0,
                    61.0
                ],
                "word_count_appended": [
                    36.0,
                    187.0,
                    33.0
                ]
            },
            "integer_answers": {
                "scrub daddy": 7,
                "sticky buddy": 0,
                "instant pot": 1
            }
        },
        "lines": [
            [
                0,
                0.0037989255564082887,
                0.003958601612057042,
                0.7279562542720438,
                0.03125,
                0.140625,
                0.0,
                0.0,
                0.008928571428571428
            ],
            [
                1,
                0.9938603223330775,
                0.99203510278056,
                0.11380724538619276,
                0.84375,
                0.73046875,
                1.0,
                1.0,
                0.8160714285714286
            ],
            [
                0,
                0.002340752110514198,
                0.00400629560738303,
                0.1582365003417635,
                0.125,
                0.12890625,
                0.0,
                0.0,
                0.175
            ]
        ]
    },
    "What are the first words spoken by God in the King James Bible?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.9984547723760846,
                    0.9993452565560494,
                    0.9983090530697191,
                    0.5000000000000001,
                    0.8189655172413793,
                    1.0,
                    1.0,
                    0.2707271335629544
                ],
                [
                    0.00035659099013431595,
                    0.00013784072504221373,
                    0.00042273673257023933,
                    0.33333333333333337,
                    0.04310344827586207,
                    0.0,
                    0.0,
                    0.5141465748182166
                ],
                [
                    0.001188636633781053,
                    0.0005169027189083015,
                    0.001268210197710718,
                    0.16666666666666669,
                    0.13793103448275862,
                    0.0,
                    0.0,
                    0.21512629161882887
                ]
            ],
            "fraction_answers": {
                "hello, my children": 0.11143756560939484,
                "let there be light": 0.8232252166007733,
                "this is my gift": 0.06533721778983179
            },
            "question": "What are the first words spoken by God in the King James Bible?",
            "rate_limited": false,
            "answers": [
                "let there be light",
                "hello, my children",
                "this is my gift"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "hello, my children": 0.31157143216045696,
                "let there be light": 0.4420214376981768,
                "this is my gift": 0.29035060350393144
            },
            "z-best_answer_by_ml": [
                "let there be light"
            ],
            "data": {
                "result_count_important_words": [
                    87000.0,
                    12.0,
                    45.0
                ],
                "wikipedia_search": [
                    1.0,
                    0.6666666666666666,
                    0.3333333333333333
                ],
                "result_count_noun_chunks": [
                    30700.0,
                    13.0,
                    39.0
                ],
                "word_relation_to_question": [
                    1.353635667814772,
                    2.570732874091083,
                    1.0756314580941444
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    6.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    33600.0,
                    12.0,
                    40.0
                ],
                "word_count_appended": [
                    95.0,
                    5.0,
                    16.0
                ]
            },
            "integer_answers": {
                "hello, my children": 1,
                "let there be light": 7,
                "this is my gift": 0
            }
        },
        "lines": [
            [
                1,
                0.9984547723760846,
                0.9993452565560494,
                0.9983090530697191,
                0.5000000000000001,
                0.8189655172413793,
                1.0,
                1.0,
                0.2707271335629544
            ],
            [
                0,
                0.00035659099013431595,
                0.00013784072504221373,
                0.00042273673257023933,
                0.33333333333333337,
                0.04310344827586207,
                0.0,
                0.0,
                0.5141465748182166
            ],
            [
                0,
                0.001188636633781053,
                0.0005169027189083015,
                0.001268210197710718,
                0.16666666666666669,
                0.13793103448275862,
                0.0,
                0.0,
                0.21512629161882887
            ]
        ]
    },
    "The inventor of the Erector Set made another toy that contained what?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.14705882352941177,
                    0.2358490566037736,
                    0.25,
                    0.37416666666666665,
                    0.13043478260869568,
                    0.0,
                    0,
                    0.3179352952273156
                ],
                [
                    0.44117647058823534,
                    0.3679245283018868,
                    0.32894736842105265,
                    0.43666666666666665,
                    0.43478260869565216,
                    0.5,
                    0,
                    0.3765445199019053
                ],
                [
                    0.411764705882353,
                    0.39622641509433965,
                    0.42105263157894735,
                    0.18916666666666665,
                    0.43478260869565216,
                    0.5,
                    0,
                    0.3055201848707791
                ]
            ],
            "fraction_answers": {
                "uranium ore": 0.5841586786754676,
                "asbestos powder": 0.2404247963460749,
                "live ants": 0.17541652497845747
            },
            "question": "The inventor of the Erector Set made another toy that contained what?",
            "rate_limited": false,
            "answers": [
                "uranium ore",
                "live ants",
                "asbestos powder"
            ],
            "ml_answers": {
                "uranium ore": 0.182784743197819,
                "asbestos powder": 0.33811359121267043,
                "live ants": 0.448004765392564
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "uranium ore": 5,
                "asbestos powder": 2,
                "live ants": 0
            },
            "data": {
                "result_count_important_words": [
                    28.0,
                    14.0,
                    11.0
                ],
                "wikipedia_search": [
                    0.5033333333333334,
                    0.25333333333333335,
                    1.2433333333333334
                ],
                "result_count_noun_chunks": [
                    38.0,
                    26.0,
                    12.0
                ],
                "word_relation_to_question": [
                    1.456517638181475,
                    0.9876438407847576,
                    1.5558385210337673
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    17.0,
                    3.0,
                    3.0
                ],
                "result_count": [
                    12.0,
                    2.0,
                    3.0
                ]
            },
            "z-best_answer_by_ml": [
                "live ants"
            ]
        },
        "lines": [
            [
                1,
                0.14705882352941177,
                0.2358490566037736,
                0.25,
                0.37416666666666665,
                0.13043478260869568,
                0.0,
                0,
                0.3179352952273156
            ],
            [
                0,
                0.44117647058823534,
                0.3679245283018868,
                0.32894736842105265,
                0.43666666666666665,
                0.43478260869565216,
                0.5,
                0,
                0.3765445199019053
            ],
            [
                0,
                0.411764705882353,
                0.39622641509433965,
                0.42105263157894735,
                0.18916666666666665,
                0.43478260869565216,
                0.5,
                0,
                0.3055201848707791
            ]
        ]
    },
    "Which of these creatures is most likely to bark?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    3.285166330148213e-06,
                    3.4959005687830225e-06,
                    0.08528065526379186,
                    0.038461538461538464,
                    0.19182083739045763,
                    0.15,
                    0.06439393939393939,
                    0.0
                ],
                [
                    0.014446815789205996,
                    6.341401031745948e-06,
                    0.033004095398699106,
                    0.2692307692307692,
                    0.15579357351509251,
                    0.05,
                    0.0,
                    0.6666666666666666
                ],
                [
                    0.9855498990444639,
                    0.9999901626983995,
                    0.881715249337509,
                    0.6923076923076923,
                    0.6523855890944499,
                    0.8,
                    0.9356060606060606,
                    0.3333333333333333
                ]
            ],
            "fraction_answers": {
                "blue whale": 0.14864353275018316,
                "mime": 0.06624546894707828,
                "dog": 0.7851109983027386
            },
            "question": "Which of these creatures is most likely to bark?",
            "rate_limited": false,
            "answers": [
                "mime",
                "blue whale",
                "dog"
            ],
            "ml_answers": {
                "blue whale": 0.3282429070115309,
                "mime": 0.15887898414880341,
                "dog": 0.7224139102363077
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "blue whale": 1,
                "mime": 0,
                "dog": 7
            },
            "data": {
                "result_count_important_words": [
                    43.0,
                    78.0,
                    12300000.0
                ],
                "wikipedia_search": [
                    0.07692307692307693,
                    0.5384615384615384,
                    1.3846153846153846
                ],
                "result_count_noun_chunks": [
                    3540000.0,
                    1370000.0,
                    36600000.0
                ],
                "word_relation_to_question": [
                    0.0,
                    1.3333333333333333,
                    0.6666666666666666
                ],
                "word_count_noun_chunks": [
                    3.0,
                    1.0,
                    16.0
                ],
                "word_count_raw": [
                    17.0,
                    0.0,
                    247.0
                ],
                "result_count": [
                    83.0,
                    365000.0,
                    24900000.0
                ],
                "word_count_appended": [
                    197.0,
                    160.0,
                    670.0
                ]
            },
            "z-best_answer_by_ml": [
                "dog"
            ]
        },
        "lines": [
            [
                0,
                3.285166330148213e-06,
                3.4959005687830225e-06,
                0.08528065526379186,
                0.038461538461538464,
                0.19182083739045763,
                0.15,
                0.06439393939393939,
                0.0
            ],
            [
                0,
                0.014446815789205996,
                6.341401031745948e-06,
                0.033004095398699106,
                0.2692307692307692,
                0.15579357351509251,
                0.05,
                0.0,
                0.6666666666666666
            ],
            [
                1,
                0.9855498990444639,
                0.9999901626983995,
                0.881715249337509,
                0.6923076923076923,
                0.6523855890944499,
                0.8,
                0.9356060606060606,
                0.3333333333333333
            ]
        ]
    },
    "Which of these phrases, written backwards, is a hip hop group?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "drummers ear"
            ],
            "question": "Which of these phrases, written backwards, is a hip hop group?",
            "answers": [
                "drummers ear",
                "beat chefs",
                "blues rhythm"
            ],
            "integer_answers": {
                "beat chefs": 0,
                "drummers ear": 2,
                "blues rhythm": 4
            },
            "data": {
                "result_count_important_words": [
                    35.0,
                    11.0,
                    50600.0
                ],
                "wikipedia_search": [
                    0.14285714285714285,
                    0.0,
                    3.857142857142857
                ],
                "result_count_noun_chunks": [
                    50.0,
                    50.0,
                    38700.0
                ],
                "word_relation_to_question": [
                    3.208909370199693,
                    1.312596006144393,
                    0.47849462365591394
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    10.0,
                    10.0,
                    7.0
                ],
                "result_count": [
                    36.0,
                    11.0,
                    49500.0
                ]
            },
            "negative_question": false,
            "fraction_answers": {
                "beat chefs": 0.1057695727786581,
                "drummers ear": 0.17509547401949974,
                "blues rhythm": 0.7191349532018422
            },
            "lines": [
                [
                    0.0007265828405352494,
                    0.0006910713580539431,
                    0.001288659793814433,
                    0.03571428571428571,
                    0.37037037037037035,
                    0,
                    0,
                    0.6417818740399386
                ],
                [
                    0.00022201142349688175,
                    0.0002171938553883821,
                    0.001288659793814433,
                    0.0,
                    0.37037037037037035,
                    0,
                    0,
                    0.2625192012288786
                ],
                [
                    0.9990514057359678,
                    0.9990917347865577,
                    0.9974226804123711,
                    0.9642857142857143,
                    0.25925925925925924,
                    0,
                    0,
                    0.09569892473118279
                ]
            ],
            "rate_limited": false,
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "beat chefs": 0.2869022426115935,
                "drummers ear": 0.3700884917405783,
                "blues rhythm": 0.2878486344402781
            }
        },
        "lines": [
            [
                1,
                0.0007265828405352494,
                0.0006910713580539431,
                0.001288659793814433,
                0.03571428571428571,
                0.37037037037037035,
                0,
                0,
                0.6417818740399386
            ],
            [
                0,
                0.00022201142349688175,
                0.0002171938553883821,
                0.001288659793814433,
                0.0,
                0.37037037037037035,
                0,
                0,
                0.2625192012288786
            ],
            [
                0,
                0.9990514057359678,
                0.9990917347865577,
                0.9974226804123711,
                0.9642857142857143,
                0.25925925925925924,
                0,
                0,
                0.09569892473118279
            ]
        ]
    },
    "Which of these is a French territory?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.9967879741182523,
                    0.999508582035879,
                    0.9998480957482336,
                    0.48484848484848486,
                    0.6169354838709677,
                    1.0,
                    0,
                    0.7006004366812227
                ],
                [
                    0.0032074860553310096,
                    0.0004891212209962812,
                    0.0001510039146185326,
                    0.18181818181818182,
                    0.3467741935483871,
                    0.0,
                    0,
                    0.19186681222707425
                ],
                [
                    4.539826416776198e-06,
                    2.29674312467819e-06,
                    9.00337147830163e-07,
                    0.3333333333333333,
                    0.036290322580645164,
                    0.0,
                    0,
                    0.10753275109170306
                ]
            ],
            "fraction_answers": {
                "french guiana": 0.8283612939004342,
                "french stewart": 0.10347239982636988,
                "french cyprus": 0.06816630627319584
            },
            "question": "Which of these is a French territory?",
            "rate_limited": false,
            "answers": [
                "french guiana",
                "french stewart",
                "french cyprus"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "french guiana": 0.6429398941929336,
                "french stewart": 0.2231334168032289,
                "french cyprus": 0.14603843893289
            },
            "z-best_answer_by_ml": [
                "french guiana"
            ],
            "data": {
                "result_count_important_words": [
                    23500000.0,
                    11500.0,
                    54.0
                ],
                "wikipedia_search": [
                    0.9696969696969697,
                    0.36363636363636365,
                    0.6666666666666666
                ],
                "result_count_noun_chunks": [
                    63300000.0,
                    9560.0,
                    57.0
                ],
                "word_relation_to_question": [
                    1.4012008733624455,
                    0.3837336244541485,
                    0.21506550218340612
                ],
                "word_count_noun_chunks": [
                    5.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    10100000.0,
                    32500.0,
                    46.0
                ],
                "word_count_appended": [
                    153.0,
                    86.0,
                    9.0
                ]
            },
            "integer_answers": {
                "french guiana": 7,
                "french stewart": 0,
                "french cyprus": 0
            }
        },
        "lines": [
            [
                1,
                0.9967879741182523,
                0.999508582035879,
                0.9998480957482336,
                0.48484848484848486,
                0.6169354838709677,
                1.0,
                0,
                0.7006004366812227
            ],
            [
                0,
                0.0032074860553310096,
                0.0004891212209962812,
                0.0001510039146185326,
                0.18181818181818182,
                0.3467741935483871,
                0.0,
                0,
                0.19186681222707425
            ],
            [
                0,
                4.539826416776198e-06,
                2.29674312467819e-06,
                9.00337147830163e-07,
                0.3333333333333333,
                0.036290322580645164,
                0.0,
                0,
                0.10753275109170306
            ]
        ]
    },
    "Aside from blood cells, what would you also find inside your blood vessels?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.2727272727272727,
                    0.46085116388431696,
                    0.8099032066899322,
                    0.08711650922177237,
                    0.3728813559322034,
                    0.5769230769230769,
                    0.8,
                    0.3467081500554304
                ],
                [
                    0.22077922077922077,
                    0.4537973195391488,
                    0.1823928359781392,
                    0.8813045434098065,
                    0.3432203389830508,
                    0.4230769230769231,
                    0.2,
                    0.3501579468106665
                ],
                [
                    0.5064935064935064,
                    0.08535151657653421,
                    0.0077039573319286235,
                    0.031578947368421054,
                    0.2838983050847458,
                    0.0,
                    0.0,
                    0.30313390313390315
                ]
            ],
            "fraction_answers": {
                "marrow": 0.38184114107211947,
                "plasma": 0.4658888419292506,
                "plastids": 0.1522700169986299
            },
            "question": "Aside from blood cells, what would you also find inside your blood vessels?",
            "rate_limited": false,
            "answers": [
                "plasma",
                "marrow",
                "plastids"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "marrow": 0.30114516136899444,
                "plasma": 0.4420214376981768,
                "plastids": 0.3049087145126065
            },
            "integer_answers": {
                "marrow": 2,
                "plasma": 5,
                "plastids": 1
            },
            "data": {
                "result_count_important_words": [
                    196000.0,
                    193000.0,
                    36300.0
                ],
                "wikipedia_search": [
                    0.4355825461088619,
                    4.406522717049032,
                    0.15789473684210525
                ],
                "result_count_noun_chunks": [
                    1230000.0,
                    277000.0,
                    11700.0
                ],
                "word_relation_to_question": [
                    1.7335407502771518,
                    1.7507897340533323,
                    1.5156695156695157
                ],
                "word_count_noun_chunks": [
                    15.0,
                    11.0,
                    0.0
                ],
                "word_count_raw": [
                    12.0,
                    3.0,
                    0.0
                ],
                "word_count_appended": [
                    88.0,
                    81.0,
                    67.0
                ],
                "result_count": [
                    21.0,
                    17.0,
                    39.0
                ]
            },
            "z-best_answer_by_ml": [
                "plasma"
            ]
        },
        "lines": [
            [
                1,
                0.2727272727272727,
                0.46085116388431696,
                0.8099032066899322,
                0.08711650922177237,
                0.3728813559322034,
                0.5769230769230769,
                0.8,
                0.3467081500554304
            ],
            [
                0,
                0.22077922077922077,
                0.4537973195391488,
                0.1823928359781392,
                0.8813045434098065,
                0.3432203389830508,
                0.4230769230769231,
                0.2,
                0.3501579468106665
            ],
            [
                0,
                0.5064935064935064,
                0.08535151657653421,
                0.0077039573319286235,
                0.031578947368421054,
                0.2838983050847458,
                0.0,
                0.0,
                0.30313390313390315
            ]
        ]
    },
    "What term describes a person from the state between New York and Rhode Island?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.11021722869311677,
                    0.0021153566586265314,
                    0.9390284319731649,
                    0.23612410656270302,
                    0.2692307692307692,
                    0,
                    0,
                    0.32962315833353883
                ],
                [
                    0.0013281023402079168,
                    0.001026526513162384,
                    0.0007130590232095691,
                    0.05494476933073424,
                    0.297008547008547,
                    0,
                    0,
                    0.45063002032591604
                ],
                [
                    0.8884546689666754,
                    0.9968581168282111,
                    0.06025850900362555,
                    0.7089311241065627,
                    0.4337606837606838,
                    0,
                    0,
                    0.21974682134054516
                ]
            ],
            "fraction_answers": {
                "hoosier": 0.5513349873343839,
                "nutmegger": 0.13427517075696288,
                "cheesehead": 0.3143898419086532
            },
            "question": "What term describes a person from the state between New York and Rhode Island?",
            "rate_limited": false,
            "answers": [
                "cheesehead",
                "nutmegger",
                "hoosier"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "hoosier": 0.48137956702894213,
                "nutmegger": 0.15321542323277937,
                "cheesehead": 0.30270860491242296
            },
            "integer_answers": {
                "hoosier": 4,
                "nutmegger": 1,
                "cheesehead": 1
            },
            "data": {
                "result_count_important_words": [
                    7130.0,
                    3460.0,
                    3360000.0
                ],
                "wikipedia_search": [
                    1.4167446393762182,
                    0.32966861598440544,
                    4.253586744639376
                ],
                "result_count_noun_chunks": [
                    3740000.0,
                    2840.0,
                    240000.0
                ],
                "word_relation_to_question": [
                    2.3073621083347717,
                    3.1544101422814124,
                    1.5382277493838161
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    126.0,
                    139.0,
                    203.0
                ],
                "result_count": [
                    72200.0,
                    870.0,
                    582000.0
                ]
            },
            "z-best_answer_by_ml": [
                "hoosier"
            ]
        },
        "lines": [
            [
                0,
                0.11021722869311677,
                0.0021153566586265314,
                0.9390284319731649,
                0.23612410656270302,
                0.2692307692307692,
                0,
                0,
                0.32962315833353883
            ],
            [
                1,
                0.0013281023402079168,
                0.001026526513162384,
                0.0007130590232095691,
                0.05494476933073424,
                0.297008547008547,
                0,
                0,
                0.45063002032591604
            ],
            [
                0,
                0.8884546689666754,
                0.9968581168282111,
                0.06025850900362555,
                0.7089311241065627,
                0.4337606837606838,
                0,
                0,
                0.21974682134054516
            ]
        ]
    },
    "Where in the home does the Maillard reaction typically occur?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.78333880576012,
                    0.7832653252661695,
                    0.24043062200956938,
                    0.7149588595484118,
                    0.4796084828711256,
                    1.0,
                    1.0,
                    0.416243654822335
                ],
                [
                    0.04169989211501478,
                    0.04178978471929084,
                    0.32595693779904306,
                    0.1256816877152698,
                    0.2463295269168026,
                    0.0,
                    0.0,
                    0.35046531302876477
                ],
                [
                    0.17496130212486513,
                    0.17494489001453967,
                    0.43361244019138756,
                    0.1593594527363184,
                    0.2740619902120718,
                    0.0,
                    0.0,
                    0.23329103214890015
                ]
            ],
            "fraction_answers": {
                "bathroom": 0.18127888842851034,
                "kitchen": 0.6772307187847164,
                "bedroom": 0.14149039278677322
            },
            "question": "Where in the home does the Maillard reaction typically occur?",
            "rate_limited": false,
            "answers": [
                "kitchen",
                "bedroom",
                "bathroom"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "bathroom": 0.16651342199966526,
                "kitchen": 0.7224139102363077,
                "bedroom": 0.15044677076605945
            },
            "integer_answers": {
                "bathroom": 1,
                "kitchen": 7,
                "bedroom": 0
            },
            "data": {
                "result_count_important_words": [
                    167000.0,
                    8910.0,
                    37300.0
                ],
                "wikipedia_search": [
                    2.859835438193647,
                    0.5027267508610792,
                    0.6374378109452736
                ],
                "result_count_noun_chunks": [
                    80400.0,
                    109000.0,
                    145000.0
                ],
                "word_relation_to_question": [
                    1.248730964467005,
                    1.0513959390862944,
                    0.6998730964467005
                ],
                "word_count_noun_chunks": [
                    5.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    25.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    294.0,
                    151.0,
                    168.0
                ],
                "result_count": [
                    167000.0,
                    8890.0,
                    37300.0
                ]
            },
            "z-best_answer_by_ml": [
                "kitchen"
            ]
        },
        "lines": [
            [
                1,
                0.78333880576012,
                0.7832653252661695,
                0.24043062200956938,
                0.7149588595484118,
                0.4796084828711256,
                1.0,
                1.0,
                0.416243654822335
            ],
            [
                0,
                0.04169989211501478,
                0.04178978471929084,
                0.32595693779904306,
                0.1256816877152698,
                0.2463295269168026,
                0.0,
                0.0,
                0.35046531302876477
            ],
            [
                0,
                0.17496130212486513,
                0.17494489001453967,
                0.43361244019138756,
                0.1593594527363184,
                0.2740619902120718,
                0.0,
                0.0,
                0.23329103214890015
            ]
        ]
    },
    "Who is the director of \u201cTyler Perry\u2019s Madea\u2019s Family Reunion\u201d?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.9997701677775224,
                    0.2823741007194245,
                    0.2345132743362832,
                    0.5559806205654982,
                    0.8598726114649682,
                    0.9956331877729258,
                    0.9959677419354839,
                    0.7911041339612768
                ],
                [
                    0.00013023825940396843,
                    0.3920863309352518,
                    0.37168141592920356,
                    0.15290426771153237,
                    0.06687898089171974,
                    0.0,
                    0.0,
                    0.1379591836734694
                ],
                [
                    9.959396307362292e-05,
                    0.3255395683453237,
                    0.3938053097345133,
                    0.2911151117229694,
                    0.0732484076433121,
                    0.004366812227074236,
                    0.004032258064516129,
                    0.07093668236525379
                ]
            ],
            "fraction_answers": {
                "tyler perry": 0.7144019798166729,
                "abraham lincoln": 0.14539296800825452,
                "george lucas": 0.14020505217507262
            },
            "question": "Who is the director of \u201cTyler Perry\u2019s Madea\u2019s Family Reunion\u201d?",
            "rate_limited": false,
            "answers": [
                "tyler perry",
                "george lucas",
                "abraham lincoln"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "tyler perry": 0.34393939425547915,
                "abraham lincoln": 0.32559523786462485,
                "george lucas": 0.32559523786462485
            },
            "integer_answers": {
                "tyler perry": 6,
                "abraham lincoln": 1,
                "george lucas": 1
            },
            "data": {
                "result_count_important_words": [
                    157000.0,
                    218000.0,
                    181000.0
                ],
                "wikipedia_search": [
                    3.8918643439584875,
                    1.0703298739807265,
                    2.0378057820607856
                ],
                "result_count_noun_chunks": [
                    106000.0,
                    168000.0,
                    178000.0
                ],
                "word_relation_to_question": [
                    5.537728937728938,
                    0.9657142857142857,
                    0.49655677655677655
                ],
                "word_count_noun_chunks": [
                    228.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    247.0,
                    0.0,
                    1.0
                ],
                "result_count": [
                    261000.0,
                    34.0,
                    26.0
                ],
                "word_count_appended": [
                    270.0,
                    21.0,
                    23.0
                ]
            },
            "z-best_answer_by_ml": [
                "tyler perry"
            ]
        },
        "lines": [
            [
                1,
                0.9997701677775224,
                0.2823741007194245,
                0.2345132743362832,
                0.5559806205654982,
                0.8598726114649682,
                0.9956331877729258,
                0.9959677419354839,
                0.7911041339612768
            ],
            [
                0,
                0.00013023825940396843,
                0.3920863309352518,
                0.37168141592920356,
                0.15290426771153237,
                0.06687898089171974,
                0.0,
                0.0,
                0.1379591836734694
            ],
            [
                0,
                9.959396307362292e-05,
                0.3255395683453237,
                0.3938053097345133,
                0.2911151117229694,
                0.0732484076433121,
                0.004366812227074236,
                0.004032258064516129,
                0.07093668236525379
            ]
        ]
    },
    "The first person to lead an expedition to the South Pole came from what country?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    3.5358958058952887e-05,
                    0.27601224521538714,
                    0.4229095226576741,
                    0.383931694648721,
                    0.3208137715179969,
                    0.875,
                    0.36363636363636365,
                    0.14544305252269854
                ],
                [
                    2.851528875722007e-05,
                    2.120995736798569e-05,
                    3.6362314097669174e-05,
                    0.38405329402673144,
                    0.29890453834115804,
                    0.0,
                    0.09090909090909091,
                    0.3824656169788913
                ],
                [
                    0.9999361257531838,
                    0.7239665448272449,
                    0.5770541150282282,
                    0.2320150113245475,
                    0.38028169014084506,
                    0.125,
                    0.5454545454545454,
                    0.4720913304984101
                ]
            ],
            "fraction_answers": {
                "canada": 0.5069749203783757,
                "iceland": 0.14455232847701183,
                "norway": 0.3484727511446125
            },
            "question": "The first person to lead an expedition to the South Pole came from what country?",
            "rate_limited": false,
            "answers": [
                "norway",
                "iceland",
                "canada"
            ],
            "ml_answers": {
                "canada": 0.3881850913886827,
                "iceland": 0.20749499302639285,
                "norway": 0.30480895820581455
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "canada": 6,
                "iceland": 1,
                "norway": 1
            },
            "data": {
                "result_count_important_words": [
                    976000.0,
                    75.0,
                    2560000.0
                ],
                "wikipedia_search": [
                    2.6875218625410473,
                    2.6883730581871204,
                    1.6241050792718326
                ],
                "result_count_noun_chunks": [
                    1070000.0,
                    92.0,
                    1460000.0
                ],
                "word_relation_to_question": [
                    1.01810136765889,
                    2.6772593188522396,
                    3.304639313488871
                ],
                "word_count_noun_chunks": [
                    7.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    4.0,
                    1.0,
                    6.0
                ],
                "word_count_appended": [
                    205.0,
                    191.0,
                    243.0
                ],
                "result_count": [
                    93.0,
                    75.0,
                    2630000.0
                ]
            },
            "z-best_answer_by_ml": [
                "canada"
            ]
        },
        "lines": [
            [
                1,
                3.5358958058952887e-05,
                0.27601224521538714,
                0.4229095226576741,
                0.383931694648721,
                0.3208137715179969,
                0.875,
                0.36363636363636365,
                0.14544305252269854
            ],
            [
                0,
                2.851528875722007e-05,
                2.120995736798569e-05,
                3.6362314097669174e-05,
                0.38405329402673144,
                0.29890453834115804,
                0.0,
                0.09090909090909091,
                0.3824656169788913
            ],
            [
                0,
                0.9999361257531838,
                0.7239665448272449,
                0.5770541150282282,
                0.2320150113245475,
                0.38028169014084506,
                0.125,
                0.5454545454545454,
                0.4720913304984101
            ]
        ]
    },
    "The \u201cAmerican Craftsman\u201d style of house was an architectural reaction to what?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "industrial revolution"
            ],
            "question": "The \u201cAmerican Craftsman\u201d style of house was an architectural reaction to what?",
            "answers": [
                "world war i",
                "industrial revolution",
                "the great depression"
            ],
            "integer_answers": {
                "world war i": 2,
                "industrial revolution": 5,
                "the great depression": 1
            },
            "data": {
                "result_count_important_words": [
                    85200.0,
                    44700.0,
                    37400.0
                ],
                "wikipedia_search": [
                    2.286645006839945,
                    1.8446648426812584,
                    0.868690150478796
                ],
                "result_count_noun_chunks": [
                    8060.0,
                    17700.0,
                    4720.0
                ],
                "word_relation_to_question": [
                    0.8093820347106238,
                    1.9765812214073053,
                    2.2140367438820707
                ],
                "word_count_noun_chunks": [
                    0.0,
                    12.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    9.0,
                    1.0
                ],
                "result_count": [
                    67.0,
                    65700.0,
                    59.0
                ],
                "word_count_appended": [
                    34.0,
                    72.0,
                    30.0
                ]
            },
            "negative_question": false,
            "fraction_answers": {
                "world war i": 0.20549046656623232,
                "industrial revolution": 0.6299550249771215,
                "the great depression": 0.16455450845664618
            },
            "lines": [
                [
                    0.0010178348980645945,
                    0.5092647937836222,
                    0.26443569553805774,
                    0.45732900136798904,
                    0.25,
                    0.0,
                    0.0,
                    0.16187640694212474
                ],
                [
                    0.9980858627290129,
                    0.26718469814704127,
                    0.5807086614173228,
                    0.3689329685362517,
                    0.5294117647058824,
                    1.0,
                    0.9,
                    0.39531624428146106
                ],
                [
                    0.0008963023729225534,
                    0.22355050806933652,
                    0.15485564304461943,
                    0.17373803009575922,
                    0.22058823529411764,
                    0.0,
                    0.1,
                    0.44280734877641414
                ]
            ],
            "rate_limited": false,
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "world war i": 0.20720128377883534,
                "industrial revolution": 0.6737695082091776,
                "the great depression": 0.20201830501168508
            }
        },
        "lines": [
            [
                0,
                0.0010178348980645945,
                0.5092647937836222,
                0.26443569553805774,
                0.45732900136798904,
                0.25,
                0.0,
                0.0,
                0.16187640694212474
            ],
            [
                1,
                0.9980858627290129,
                0.26718469814704127,
                0.5807086614173228,
                0.3689329685362517,
                0.5294117647058824,
                1.0,
                0.9,
                0.39531624428146106
            ],
            [
                0,
                0.0008963023729225534,
                0.22355050806933652,
                0.15485564304461943,
                0.17373803009575922,
                0.22058823529411764,
                0.0,
                0.1,
                0.44280734877641414
            ]
        ]
    },
    "Which of these modes of transportation has only one wheel?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.3048327137546468,
                    0.02545324640528713,
                    0.31115107913669066,
                    0.09574468085106383,
                    0.10982658959537572,
                    0.0,
                    0.0,
                    0.07547169811320754
                ],
                [
                    0.4646840148698885,
                    0.9645440743056176,
                    0.4676258992805755,
                    0.5425531914893617,
                    0.4069364161849711,
                    0.6511627906976745,
                    0.6226415094339622,
                    0.6514198589670288
                ],
                [
                    0.23048327137546468,
                    0.010002679289095293,
                    0.22122302158273383,
                    0.3617021276595745,
                    0.48323699421965316,
                    0.3488372093023256,
                    0.37735849056603776,
                    0.27310844291976366
                ]
            ],
            "fraction_answers": {
                "unicycle": 0.2882440296143311,
                "bus": 0.596445969403635,
                "monster truck": 0.11531000098203395
            },
            "question": "Which of these modes of transportation has only one wheel?",
            "rate_limited": false,
            "answers": [
                "monster truck",
                "bus",
                "unicycle"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "unicycle": 0.3635484425931417,
                "bus": 0.4420214376981768,
                "monster truck": 0.26516270432121314
            },
            "integer_answers": {
                "unicycle": 1,
                "bus": 7,
                "monster truck": 0
            },
            "data": {
                "result_count_important_words": [
                    1140000.0,
                    43200000.0,
                    448000.0
                ],
                "wikipedia_search": [
                    0.19148936170212766,
                    1.0851063829787233,
                    0.723404255319149
                ],
                "result_count_noun_chunks": [
                    1730000.0,
                    2600000.0,
                    1230000.0
                ],
                "word_relation_to_question": [
                    0.22641509433962265,
                    1.9542595769010864,
                    0.819325328759291
                ],
                "word_count_noun_chunks": [
                    0.0,
                    28.0,
                    15.0
                ],
                "word_count_raw": [
                    0.0,
                    33.0,
                    20.0
                ],
                "word_count_appended": [
                    95.0,
                    352.0,
                    418.0
                ],
                "result_count": [
                    1640000.0,
                    2500000.0,
                    1240000.0
                ]
            },
            "z-best_answer_by_ml": [
                "bus"
            ]
        },
        "lines": [
            [
                0,
                0.3048327137546468,
                0.02545324640528713,
                0.31115107913669066,
                0.09574468085106383,
                0.10982658959537572,
                0.0,
                0.0,
                0.07547169811320754
            ],
            [
                0,
                0.4646840148698885,
                0.9645440743056176,
                0.4676258992805755,
                0.5425531914893617,
                0.4069364161849711,
                0.6511627906976745,
                0.6226415094339622,
                0.6514198589670288
            ],
            [
                1,
                0.23048327137546468,
                0.010002679289095293,
                0.22122302158273383,
                0.3617021276595745,
                0.48323699421965316,
                0.3488372093023256,
                0.37735849056603776,
                0.27310844291976366
            ]
        ]
    },
    "Who defeated Napoleon at the Battle of Waterloo?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.008465987002921362,
                    0.00027221995372260786,
                    0.00017381411370603292,
                    0.06636363636363636,
                    0.14601769911504425,
                    0.0,
                    0.0,
                    0.04905660377358491
                ],
                [
                    0.9777618792106362,
                    0.9837039236794238,
                    0.8888221723603956,
                    0.9245454545454544,
                    0.4247787610619469,
                    1.0,
                    1.0,
                    0.8462548507961996
                ],
                [
                    0.013772133786442497,
                    0.01602385636685351,
                    0.11100401352589831,
                    0.009090909090909089,
                    0.42920353982300885,
                    0.0,
                    0.0,
                    0.10468854543021547
                ]
            ],
            "fraction_answers": {
                "jack skellington": 0.03379374504032694,
                "the duke of wellington": 0.8807333802067571,
                "beef wellington": 0.08547287475291596
            },
            "question": "Who defeated Napoleon at the Battle of Waterloo?",
            "rate_limited": false,
            "answers": [
                "jack skellington",
                "the duke of wellington",
                "beef wellington"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "jack skellington": 0.1510474224312484,
                "the duke of wellington": 0.7224139102363077,
                "beef wellington": 0.1784984883502141
            },
            "integer_answers": {
                "jack skellington": 0,
                "the duke of wellington": 7,
                "beef wellington": 1
            },
            "data": {
                "result_count_important_words": [
                    44.0,
                    159000.0,
                    2590.0
                ],
                "wikipedia_search": [
                    0.33181818181818185,
                    4.622727272727273,
                    0.045454545454545456
                ],
                "result_count_noun_chunks": [
                    44.0,
                    225000.0,
                    28100.0
                ],
                "word_relation_to_question": [
                    0.24528301886792453,
                    4.231274253980998,
                    0.5234427271510773
                ],
                "word_count_noun_chunks": [
                    0.0,
                    14.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    14.0,
                    0.0
                ],
                "result_count": [
                    1420.0,
                    164000.0,
                    2310.0
                ],
                "word_count_appended": [
                    33.0,
                    96.0,
                    97.0
                ]
            },
            "z-best_answer_by_ml": [
                "the duke of wellington"
            ]
        },
        "lines": [
            [
                0,
                0.008465987002921362,
                0.00027221995372260786,
                0.00017381411370603292,
                0.06636363636363636,
                0.14601769911504425,
                0.0,
                0.0,
                0.04905660377358491
            ],
            [
                1,
                0.9777618792106362,
                0.9837039236794238,
                0.8888221723603956,
                0.9245454545454544,
                0.4247787610619469,
                1.0,
                1.0,
                0.8462548507961996
            ],
            [
                0,
                0.013772133786442497,
                0.01602385636685351,
                0.11100401352589831,
                0.009090909090909089,
                0.42920353982300885,
                0.0,
                0.0,
                0.10468854543021547
            ]
        ]
    },
    "Made famous in a documentary, where is the \u201cGrey Gardens\u201d home located?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.17486338797814208,
                    9.938958231528033e-05,
                    1.974386475325264e-05,
                    0.34523809523809523,
                    0.14285714285714285,
                    0.0,
                    0,
                    0.18962895377128952
                ],
                [
                    0.453551912568306,
                    0.8841531593463479,
                    0.9999312149227951,
                    0.38499999999999995,
                    0.6571428571428571,
                    1.0,
                    0,
                    0.7155312246553122
                ],
                [
                    0.37158469945355194,
                    0.11574745107133687,
                    4.904121245162753e-05,
                    0.26976190476190476,
                    0.2,
                    0.0,
                    0,
                    0.09483982157339821
                ]
            ],
            "fraction_answers": {
                "east hampton, ny": 0.7279014812336598,
                "savannah, ga": 0.15028327401037764,
                "cape cod, ma": 0.1218152447559626
            },
            "question": "Made famous in a documentary, where is the \u201cGrey Gardens\u201d home located?",
            "rate_limited": false,
            "answers": [
                "cape cod, ma",
                "east hampton, ny",
                "savannah, ga"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "east hampton, ny": 0.4420214376981768,
                "savannah, ga": 0.28368788585608096,
                "cape cod, ma": 0.29035060350393144
            },
            "z-best_answer_by_ml": [
                "east hampton, ny"
            ],
            "data": {
                "result_count_important_words": [
                    48.0,
                    427000.0,
                    55900.0
                ],
                "wikipedia_search": [
                    1.726190476190476,
                    1.9249999999999998,
                    1.3488095238095237
                ],
                "result_count_noun_chunks": [
                    31.0,
                    1570000.0,
                    77.0
                ],
                "word_relation_to_question": [
                    1.1377737226277371,
                    4.293187347931873,
                    0.5690389294403893
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    32.0,
                    83.0,
                    68.0
                ],
                "word_count_appended": [
                    5.0,
                    23.0,
                    7.0
                ]
            },
            "integer_answers": {
                "east hampton, ny": 7,
                "savannah, ga": 0,
                "cape cod, ma": 0
            }
        },
        "lines": [
            [
                0,
                0.17486338797814208,
                9.938958231528033e-05,
                1.974386475325264e-05,
                0.34523809523809523,
                0.14285714285714285,
                0.0,
                0,
                0.18962895377128952
            ],
            [
                1,
                0.453551912568306,
                0.8841531593463479,
                0.9999312149227951,
                0.38499999999999995,
                0.6571428571428571,
                1.0,
                0,
                0.7155312246553122
            ],
            [
                0,
                0.37158469945355194,
                0.11574745107133687,
                4.904121245162753e-05,
                0.26976190476190476,
                0.2,
                0.0,
                0,
                0.09483982157339821
            ]
        ]
    },
    "Which of these is NOT a suit in a traditional Tarot deck?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.4194895591647332,
                    0.47838983050847456,
                    0.4246031746031746,
                    0.2818010255628205,
                    0.39775051124744376,
                    0.5,
                    0.5,
                    0.4809090909090909
                ],
                [
                    0.27262180974477956,
                    0.1172316384180791,
                    0.28350970017636684,
                    0.28815042858867557,
                    0.2939672801635992,
                    0.1643835616438356,
                    0.2578947368421053,
                    0.12956709956709955
                ],
                [
                    0.30788863109048725,
                    0.40437853107344635,
                    0.29188712522045857,
                    0.43004854584850394,
                    0.30828220858895705,
                    0.3356164383561644,
                    0.2421052631578947,
                    0.38952380952380955
                ]
            ],
            "fraction_answers": {
                "gloves": 0.12926420200106561,
                "cups": 0.32256736178506956,
                "swords": 0.5481684362138648
            },
            "question": "Which of these is NOT a suit in a traditional Tarot deck?",
            "rate_limited": false,
            "answers": [
                "gloves",
                "swords",
                "cups"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "gloves": 0.5396700767640724,
                "cups": 0.2698103492460082,
                "swords": 0.14313877367219574
            },
            "integer_answers": {
                "gloves": 1,
                "cups": 1,
                "swords": 6
            },
            "data": {
                "result_count_important_words": [
                    153000.0,
                    2710000.0,
                    677000.0
                ],
                "wikipedia_search": [
                    1.309193846623077,
                    1.2710974284679464,
                    0.41970872490897654
                ],
                "result_count_noun_chunks": [
                    171000.0,
                    491000.0,
                    472000.0
                ],
                "word_relation_to_question": [
                    0.19090909090909092,
                    3.7043290043290042,
                    1.1047619047619046
                ],
                "word_count_noun_chunks": [
                    0.0,
                    49.0,
                    24.0
                ],
                "word_count_raw": [
                    0.0,
                    46.0,
                    49.0
                ],
                "word_count_appended": [
                    200.0,
                    403.0,
                    375.0
                ],
                "result_count": [
                    347000.0,
                    980000.0,
                    828000.0
                ]
            },
            "z-best_answer_by_ml": [
                "gloves"
            ]
        },
        "lines": [
            [
                1,
                0.4194895591647332,
                0.47838983050847456,
                0.4246031746031746,
                0.2818010255628205,
                0.39775051124744376,
                0.5,
                0.5,
                0.4809090909090909
            ],
            [
                0,
                0.27262180974477956,
                0.1172316384180791,
                0.28350970017636684,
                0.28815042858867557,
                0.2939672801635992,
                0.1643835616438356,
                0.2578947368421053,
                0.12956709956709955
            ],
            [
                0,
                0.30788863109048725,
                0.40437853107344635,
                0.29188712522045857,
                0.43004854584850394,
                0.30828220858895705,
                0.3356164383561644,
                0.2421052631578947,
                0.38952380952380955
            ]
        ]
    },
    "How does the second verse of \u201cThe Star-Spangled Banner\u201d begin?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.0033731984053971173,
                    0.009715168911459484,
                    0.002188183807439825,
                    0.6444444444444445,
                    0.036585365853658534,
                    0.0,
                    0.0,
                    0.21705931495405176
                ],
                [
                    0.6593069610548912,
                    0.9494369618017222,
                    0.8606856309263311,
                    0.07222222222222222,
                    0.5121951219512195,
                    0.6666666666666666,
                    1.0,
                    0.4456850459482038
                ],
                [
                    0.33731984053971176,
                    0.04084786928681828,
                    0.13712618526622902,
                    0.2833333333333333,
                    0.45121951219512196,
                    0.3333333333333333,
                    0.0,
                    0.33725563909774436
                ]
            ],
            "fraction_answers": {
                "on the shore": 0.6457748263214071,
                "travels far": 0.1141707095470564,
                "where the foe": 0.2400544641315365
            },
            "question": "How does the second verse of \u201cThe Star-Spangled Banner\u201d begin?",
            "rate_limited": false,
            "answers": [
                "travels far",
                "on the shore",
                "where the foe"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "on the shore": 0.6429398941929336,
                "travels far": 0.16943492430828147,
                "where the foe": 0.23565193591040015
            },
            "integer_answers": {
                "on the shore": 7,
                "travels far": 1,
                "where the foe": 0
            },
            "data": {
                "result_count_important_words": [
                    2640.0,
                    258000.0,
                    11100.0
                ],
                "wikipedia_search": [
                    3.2222222222222223,
                    0.3611111111111111,
                    1.4166666666666665
                ],
                "result_count_noun_chunks": [
                    30.0,
                    11800.0,
                    1880.0
                ],
                "word_relation_to_question": [
                    1.3023558897243106,
                    2.6741102756892228,
                    2.023533834586466
                ],
                "word_count_noun_chunks": [
                    0.0,
                    6.0,
                    3.0
                ],
                "word_count_raw": [
                    0.0,
                    2.0,
                    0.0
                ],
                "result_count": [
                    11.0,
                    2150.0,
                    1100.0
                ],
                "word_count_appended": [
                    3.0,
                    42.0,
                    37.0
                ]
            },
            "z-best_answer_by_ml": [
                "on the shore"
            ]
        },
        "lines": [
            [
                0,
                0.0033731984053971173,
                0.009715168911459484,
                0.002188183807439825,
                0.6444444444444445,
                0.036585365853658534,
                0.0,
                0.0,
                0.21705931495405176
            ],
            [
                1,
                0.6593069610548912,
                0.9494369618017222,
                0.8606856309263311,
                0.07222222222222222,
                0.5121951219512195,
                0.6666666666666666,
                1.0,
                0.4456850459482038
            ],
            [
                0,
                0.33731984053971176,
                0.04084786928681828,
                0.13712618526622902,
                0.2833333333333333,
                0.45121951219512196,
                0.3333333333333333,
                0.0,
                0.33725563909774436
            ]
        ]
    },
    "Which of these substances is both artificially made and found in nature?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.01717699775952203,
                    0.22241086587436332,
                    0.21497559455810572,
                    0.2833333333333333,
                    0.29122468659594986,
                    0,
                    0.5,
                    0.6956521739130436
                ],
                [
                    0.13144137415982077,
                    0.5628183361629882,
                    0.6895835496936338,
                    0.09999999999999999,
                    0.29122468659594986,
                    0,
                    0.5,
                    0.08695652173913045
                ],
                [
                    0.8513816280806572,
                    0.21477079796264856,
                    0.09544085574826046,
                    0.6166666666666667,
                    0.4175506268081003,
                    0,
                    0.0,
                    0.2173913043478261
                ]
            ],
            "fraction_answers": {
                "latex": 0.3447431256591656,
                "teflon": 0.317824807433474,
                "nylon": 0.3374320669073604
            },
            "question": "Which of these substances is both artificially made and found in nature?",
            "rate_limited": false,
            "answers": [
                "teflon",
                "nylon",
                "latex"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "latex": 0.35275388472807867,
                "teflon": 0.30114516136899444,
                "nylon": 0.2613991511776011
            },
            "z-best_answer_by_ml": [
                "latex"
            ],
            "data": {
                "result_count_important_words": [
                    262000.0,
                    663000.0,
                    253000.0
                ],
                "wikipedia_search": [
                    0.85,
                    0.3,
                    1.85
                ],
                "result_count_noun_chunks": [
                    2070000.0,
                    6640000.0,
                    919000.0
                ],
                "word_relation_to_question": [
                    0.6956521739130435,
                    0.08695652173913043,
                    0.21739130434782608
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    0.0
                ],
                "result_count": [
                    460000.0,
                    3520000.0,
                    22800000.0
                ],
                "word_count_appended": [
                    302.0,
                    302.0,
                    433.0
                ]
            },
            "integer_answers": {
                "latex": 3,
                "teflon": 2,
                "nylon": 2
            }
        },
        "lines": [
            [
                0,
                0.01717699775952203,
                0.22241086587436332,
                0.21497559455810572,
                0.2833333333333333,
                0.29122468659594986,
                0,
                0.5,
                0.6956521739130436
            ],
            [
                0,
                0.13144137415982077,
                0.5628183361629882,
                0.6895835496936338,
                0.09999999999999999,
                0.29122468659594986,
                0,
                0.5,
                0.08695652173913045
            ],
            [
                1,
                0.8513816280806572,
                0.21477079796264856,
                0.09544085574826046,
                0.6166666666666667,
                0.4175506268081003,
                0,
                0.0,
                0.2173913043478261
            ]
        ]
    },
    "Every U.S. state that starts with which of these letters has a Democratic governor?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.2399232245681382,
                    0.4091999435586285,
                    0.44761904761904764,
                    0.18421052631578946,
                    0.27235181644359463,
                    0.23736522911051214,
                    0.302307206068268,
                    0.25
                ],
                [
                    0.29366602687140114,
                    0.46564131508395656,
                    0.14421768707482993,
                    0.5,
                    0.45277246653919695,
                    0.45266172506738545,
                    0.43086283185840707,
                    0.16666666666666666
                ],
                [
                    0.46641074856046066,
                    0.12515874135741498,
                    0.40816326530612246,
                    0.3157894736842105,
                    0.2748757170172084,
                    0.30997304582210244,
                    0.2668299620733249,
                    0.5833333333333334
                ]
            ],
            "fraction_answers": {
                "c": 0.3633110898952305,
                "w": 0.2928721242104973,
                "v": 0.3438167858942722
            },
            "question": "Every U.S. state that starts with which of these letters has a Democratic governor?",
            "rate_limited": false,
            "answers": [
                "w",
                "c",
                "v"
            ],
            "ml_answers": {
                "c": 0.347947869716185,
                "w": 0.2205462035354216,
                "v": 0.3416987332347784
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "c": 5,
                "w": 1,
                "v": 2
            },
            "data": {
                "result_count_important_words": [
                    2900000.0,
                    3300000.0,
                    887000.0
                ],
                "wikipedia_search": [
                    0.3684210526315789,
                    1.0,
                    0.631578947368421
                ],
                "result_count_noun_chunks": [
                    3290000.0,
                    1060000.0,
                    3000000.0
                ],
                "word_relation_to_question": [
                    0.75,
                    0.5,
                    1.75
                ],
                "word_count_noun_chunks": [
                    2818.0,
                    5374.0,
                    3680.0
                ],
                "word_count_raw": [
                    3826.0,
                    5453.0,
                    3377.0
                ],
                "result_count": [
                    1250000.0,
                    1530000.0,
                    2430000.0
                ],
                "word_count_appended": [
                    3561.0,
                    5920.0,
                    3594.0
                ]
            },
            "z-best_answer_by_ml": [
                "c"
            ]
        },
        "lines": [
            [
                0,
                0.2399232245681382,
                0.4091999435586285,
                0.44761904761904764,
                0.18421052631578946,
                0.27235181644359463,
                0.23736522911051214,
                0.302307206068268,
                0.25
            ],
            [
                1,
                0.29366602687140114,
                0.46564131508395656,
                0.14421768707482993,
                0.5,
                0.45277246653919695,
                0.45266172506738545,
                0.43086283185840707,
                0.16666666666666666
            ],
            [
                0,
                0.46641074856046066,
                0.12515874135741498,
                0.40816326530612246,
                0.3157894736842105,
                0.2748757170172084,
                0.30997304582210244,
                0.2668299620733249,
                0.5833333333333334
            ]
        ]
    },
    "What does a rattlesnake typically do when it feels threatened?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.5142857142857142,
                    0.48148148148148145,
                    0.9711191335740073,
                    0.4333333333333334,
                    0.7857142857142857,
                    0,
                    0,
                    0.5903963414634147
                ],
                [
                    0.4666666666666667,
                    0.5,
                    0.02815884476534296,
                    0.2,
                    0.14285714285714285,
                    0,
                    0,
                    0.22987804878048781
                ],
                [
                    0.01904761904761905,
                    0.018518518518518517,
                    0.0007220216606498195,
                    0.3666666666666667,
                    0.07142857142857142,
                    0,
                    0,
                    0.17972560975609758
                ]
            ],
            "fraction_answers": {
                "sends an angry email": 0.10935150117968719,
                "eats its feelings": 0.2612601171782734,
                "rattles its tail": 0.6293883816420396
            },
            "question": "What does a rattlesnake typically do when it feels threatened?",
            "rate_limited": false,
            "answers": [
                "rattles its tail",
                "eats its feelings",
                "sends an angry email"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "sends an angry email": 0.29035060350393144,
                "eats its feelings": 0.28368788585608096,
                "rattles its tail": 0.3739747133846042
            },
            "z-best_answer_by_ml": [
                "rattles its tail"
            ],
            "data": {
                "result_count_important_words": [
                    52.0,
                    54.0,
                    2.0
                ],
                "wikipedia_search": [
                    2.166666666666667,
                    1.0,
                    1.8333333333333335
                ],
                "result_count_noun_chunks": [
                    2690.0,
                    78.0,
                    2.0
                ],
                "word_relation_to_question": [
                    2.3615853658536583,
                    0.9195121951219511,
                    0.7189024390243902
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    54.0,
                    49.0,
                    2.0
                ],
                "word_count_appended": [
                    33.0,
                    6.0,
                    3.0
                ]
            },
            "integer_answers": {
                "sends an angry email": 0,
                "eats its feelings": 1,
                "rattles its tail": 5
            }
        },
        "lines": [
            [
                0,
                0.5142857142857142,
                0.48148148148148145,
                0.9711191335740073,
                0.4333333333333334,
                0.7857142857142857,
                0,
                0,
                0.5903963414634147
            ],
            [
                1,
                0.4666666666666667,
                0.5,
                0.02815884476534296,
                0.2,
                0.14285714285714285,
                0,
                0,
                0.22987804878048781
            ],
            [
                0,
                0.01904761904761905,
                0.018518518518518517,
                0.0007220216606498195,
                0.3666666666666667,
                0.07142857142857142,
                0,
                0,
                0.17972560975609758
            ]
        ]
    },
    "Which of these was a name the ancient Greeks gave the planet Venus?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.00023317548373137667,
                    0.0014548937746812337,
                    0.00036000624916507985,
                    0.14285714285714288,
                    0.14790996784565916,
                    0.0,
                    0.0,
                    0.0
                ],
                [
                    0.9568383990455752,
                    0.7590750128771654,
                    0.9396389522233216,
                    0.634920634920635,
                    0.5627009646302251,
                    1.0,
                    1.0,
                    0.045454545454545456
                ],
                [
                    0.04292842547069338,
                    0.23947009334815336,
                    0.06000104152751331,
                    0.22222222222222224,
                    0.28938906752411575,
                    0.0,
                    0.0,
                    0.9545454545454546
                ]
            ],
            "fraction_answers": {
                "antimony": 0.2260695380797691,
                "flourine": 0.036601898276297465,
                "phosphorus": 0.7373285636439335
            },
            "question": "Which of these was a name the ancient Greeks gave the planet Venus?",
            "rate_limited": false,
            "answers": [
                "flourine",
                "phosphorus",
                "antimony"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "antimony": 0.35610014144698815,
                "flourine": 0.1510474224312484,
                "phosphorus": 0.7224139102363077
            },
            "integer_answers": {
                "antimony": 1,
                "flourine": 0,
                "phosphorus": 7
            },
            "data": {
                "result_count_important_words": [
                    644.0,
                    336000.0,
                    106000.0
                ],
                "wikipedia_search": [
                    0.42857142857142855,
                    1.9047619047619047,
                    0.6666666666666666
                ],
                "result_count_noun_chunks": [
                    636.0,
                    1660000.0,
                    106000.0
                ],
                "word_relation_to_question": [
                    0.0,
                    0.09090909090909091,
                    1.9090909090909092
                ],
                "word_count_noun_chunks": [
                    0.0,
                    6.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    4.0,
                    0.0
                ],
                "result_count": [
                    541.0,
                    2220000.0,
                    99600.0
                ],
                "word_count_appended": [
                    92.0,
                    350.0,
                    180.0
                ]
            },
            "z-best_answer_by_ml": [
                "phosphorus"
            ]
        },
        "lines": [
            [
                0,
                0.00023317548373137667,
                0.0014548937746812337,
                0.00036000624916507985,
                0.14285714285714288,
                0.14790996784565916,
                0.0,
                0.0,
                0.0
            ],
            [
                1,
                0.9568383990455752,
                0.7590750128771654,
                0.9396389522233216,
                0.634920634920635,
                0.5627009646302251,
                1.0,
                1.0,
                0.045454545454545456
            ],
            [
                0,
                0.04292842547069338,
                0.23947009334815336,
                0.06000104152751331,
                0.22222222222222224,
                0.28938906752411575,
                0.0,
                0.0,
                0.9545454545454546
            ]
        ]
    },
    "Which of these film composers most recently won an Oscar?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.31886625332152346,
                    0.2912,
                    0.3132183908045977,
                    0.6345354645354646,
                    0.36523929471032746,
                    0.23076923076923078,
                    0.0,
                    0.24067094258944852
                ],
                [
                    0.46944198405668736,
                    0.5216,
                    0.47413793103448276,
                    0.23106617520410624,
                    0.37279596977329976,
                    0.46153846153846156,
                    1.0,
                    0.28321320162576524
                ],
                [
                    0.21169176262178918,
                    0.1872,
                    0.21264367816091953,
                    0.13439836026042923,
                    0.2619647355163728,
                    0.3076923076923077,
                    0.0,
                    0.4761158557847862
                ]
            ],
            "fraction_answers": {
                "ennio morricone": 0.2993124470913241,
                "danny elfman": 0.2239633375045756,
                "hans zimmer": 0.47672421540410037
            },
            "question": "Which of these film composers most recently won an Oscar?",
            "rate_limited": false,
            "answers": [
                "ennio morricone",
                "hans zimmer",
                "danny elfman"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "ennio morricone": 0.21869226664059727,
                "danny elfman": 0.2530541254849675,
                "hans zimmer": 0.5537851881178307
            },
            "integer_answers": {
                "ennio morricone": 1,
                "danny elfman": 1,
                "hans zimmer": 6
            },
            "data": {
                "result_count_important_words": [
                    182000.0,
                    326000.0,
                    117000.0
                ],
                "wikipedia_search": [
                    3.172677322677323,
                    1.1553308760205312,
                    0.6719918013021462
                ],
                "result_count_noun_chunks": [
                    109000.0,
                    165000.0,
                    74000.0
                ],
                "word_relation_to_question": [
                    1.2033547129472426,
                    1.4160660081288263,
                    2.380579278923931
                ],
                "word_count_noun_chunks": [
                    3.0,
                    6.0,
                    4.0
                ],
                "word_count_raw": [
                    0.0,
                    4.0,
                    0.0
                ],
                "word_count_appended": [
                    145.0,
                    148.0,
                    104.0
                ],
                "result_count": [
                    108000.0,
                    159000.0,
                    71700.0
                ]
            },
            "z-best_answer_by_ml": [
                "hans zimmer"
            ]
        },
        "lines": [
            [
                1,
                0.31886625332152346,
                0.2912,
                0.3132183908045977,
                0.6345354645354646,
                0.36523929471032746,
                0.23076923076923078,
                0.0,
                0.24067094258944852
            ],
            [
                0,
                0.46944198405668736,
                0.5216,
                0.47413793103448276,
                0.23106617520410624,
                0.37279596977329976,
                0.46153846153846156,
                1.0,
                0.28321320162576524
            ],
            [
                0,
                0.21169176262178918,
                0.1872,
                0.21264367816091953,
                0.13439836026042923,
                0.2619647355163728,
                0.3076923076923077,
                0.0,
                0.4761158557847862
            ]
        ]
    },
    "Which of these figure skating jumps was invented the most recently?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.26847353618000513,
                    0.2203742203742204,
                    0.022664848356798495,
                    0.013447286636385369,
                    0.3323943661971831,
                    0.19117647058823528,
                    0.19672131147540983,
                    0.06776256414223461
                ],
                [
                    0.12042955765788801,
                    0.3783783783783784,
                    0.011197971688147053,
                    0.9759144154912742,
                    0.29295774647887324,
                    0.29411764705882354,
                    0.11475409836065574,
                    0.8128361826667698
                ],
                [
                    0.6110969061621069,
                    0.40124740124740127,
                    0.9661371799550544,
                    0.010638297872340425,
                    0.37464788732394366,
                    0.5147058823529411,
                    0.6885245901639344,
                    0.1194012531909956
                ]
            ],
            "fraction_answers": {
                "salchow": 0.37507324972260125,
                "lutz": 0.16412682549380903,
                "axel": 0.4607999247835897
            },
            "question": "Which of these figure skating jumps was invented the most recently?",
            "rate_limited": false,
            "answers": [
                "lutz",
                "salchow",
                "axel"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "salchow": 0.30114516136899444,
                "lutz": 0.27326161506461843,
                "axel": 0.4420214376981768
            },
            "z-best_answer_by_ml": [
                "axel"
            ],
            "data": {
                "result_count_important_words": [
                    42400.0,
                    72800.0,
                    77200.0
                ],
                "wikipedia_search": [
                    0.053789146545541476,
                    3.903657661965097,
                    0.0425531914893617
                ],
                "result_count_noun_chunks": [
                    118000.0,
                    58300.0,
                    5030000.0
                ],
                "word_relation_to_question": [
                    0.338812820711173,
                    4.064180913333849,
                    0.597006265954978
                ],
                "word_count_noun_chunks": [
                    13.0,
                    20.0,
                    35.0
                ],
                "word_count_raw": [
                    12.0,
                    7.0,
                    42.0
                ],
                "word_count_appended": [
                    236.0,
                    208.0,
                    266.0
                ],
                "result_count": [
                    105000.0,
                    47100.0,
                    239000.0
                ]
            },
            "integer_answers": {
                "salchow": 2,
                "lutz": 0,
                "axel": 6
            }
        },
        "lines": [
            [
                1,
                0.26847353618000513,
                0.2203742203742204,
                0.022664848356798495,
                0.013447286636385369,
                0.3323943661971831,
                0.19117647058823528,
                0.19672131147540983,
                0.06776256414223461
            ],
            [
                0,
                0.12042955765788801,
                0.3783783783783784,
                0.011197971688147053,
                0.9759144154912742,
                0.29295774647887324,
                0.29411764705882354,
                0.11475409836065574,
                0.8128361826667698
            ],
            [
                0,
                0.6110969061621069,
                0.40124740124740127,
                0.9661371799550544,
                0.010638297872340425,
                0.37464788732394366,
                0.5147058823529411,
                0.6885245901639344,
                0.1194012531909956
            ]
        ]
    },
    "Romaine, Iceberg and Butterhead are all varieties of what?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.9986563532701456,
                    0.9980254891401903,
                    0.9993703389194092,
                    0.9895833333333334,
                    0.9793253536452666,
                    1.0,
                    1.0,
                    1.0
                ],
                [
                    0.0011741777729358076,
                    0.001705259378926584,
                    0.0005430104731700835,
                    0.0,
                    0.006528835690968444,
                    0.0,
                    0.0,
                    0.0
                ],
                [
                    0.00016946895691857016,
                    0.00026925148088314486,
                    8.665060742075803e-05,
                    0.010416666666666666,
                    0.014145810663764961,
                    0.0,
                    0.0,
                    0.0
                ]
            ],
            "fraction_answers": {
                "lettuce": 0.9956201085385431,
                "race cars": 0.0031359810469567626,
                "disney dwarfs": 0.0012439104145001148
            },
            "question": "Romaine, Iceberg and Butterhead are all varieties of what?",
            "rate_limited": false,
            "answers": [
                "lettuce",
                "disney dwarfs",
                "race cars"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "lettuce": 0.34393939425547915,
                "race cars": 0.32559523786462485,
                "disney dwarfs": 0.32559523786462485
            },
            "integer_answers": {
                "lettuce": 8,
                "race cars": 0,
                "disney dwarfs": 0
            },
            "data": {
                "result_count_important_words": [
                    55600.0,
                    95.0,
                    15.0
                ],
                "wikipedia_search": [
                    3.9583333333333335,
                    0.0,
                    0.041666666666666664
                ],
                "result_count_noun_chunks": [
                    173000.0,
                    94.0,
                    15.0
                ],
                "word_relation_to_question": [
                    4.0,
                    0.0,
                    0.0
                ],
                "word_count_noun_chunks": [
                    688.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    669.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    900.0,
                    6.0,
                    13.0
                ],
                "result_count": [
                    82500.0,
                    97.0,
                    14.0
                ]
            },
            "z-best_answer_by_ml": [
                "lettuce"
            ]
        },
        "lines": [
            [
                1,
                0.9986563532701456,
                0.9980254891401903,
                0.9993703389194092,
                0.9895833333333334,
                0.9793253536452666,
                1.0,
                1.0,
                1.0
            ],
            [
                0,
                0.0011741777729358076,
                0.001705259378926584,
                0.0005430104731700835,
                0.0,
                0.006528835690968444,
                0.0,
                0.0,
                0.0
            ],
            [
                0,
                0.00016946895691857016,
                0.00026925148088314486,
                8.665060742075803e-05,
                0.010416666666666666,
                0.014145810663764961,
                0.0,
                0.0,
                0.0
            ]
        ]
    },
    "Rejected in the late 1700s, what was the name of the proposed 14th U.S. colony?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.7151062607696727,
                    0.7155172413793104,
                    0.8842544316996872,
                    0.008620689655172414,
                    0.5699658703071673,
                    0.0,
                    0.0,
                    0.35090909090909095
                ],
                [
                    0.2713957495692131,
                    0.27155172413793105,
                    0.08070907194994786,
                    0.24233716475095785,
                    0.378839590443686,
                    1.0,
                    1.0,
                    0.6005194805194806
                ],
                [
                    0.013497989661114302,
                    0.01293103448275862,
                    0.035036496350364967,
                    0.7490421455938697,
                    0.051194539249146756,
                    0.0,
                    0.0,
                    0.04857142857142858
                ]
            ],
            "fraction_answers": {
                "roanoke": 0.40554669809001265,
                "new albion": 0.11378420423858537,
                "vandalia": 0.48066909767140203
            },
            "question": "Rejected in the late 1700s, what was the name of the proposed 14th U.S. colony?",
            "rate_limited": false,
            "answers": [
                "roanoke",
                "vandalia",
                "new albion"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "roanoke": 0.3739747133846042,
                "new albion": 0.29035060350393144,
                "vandalia": 0.4420214376981768
            },
            "z-best_answer_by_ml": [
                "vandalia"
            ],
            "data": {
                "result_count_important_words": [
                    4980.0,
                    1890.0,
                    90.0
                ],
                "wikipedia_search": [
                    0.017241379310344827,
                    0.4846743295019157,
                    1.4980842911877394
                ],
                "result_count_noun_chunks": [
                    42400.0,
                    3870.0,
                    1680.0
                ],
                "word_relation_to_question": [
                    1.7545454545454544,
                    3.0025974025974023,
                    0.24285714285714285
                ],
                "word_count_noun_chunks": [
                    0.0,
                    27.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    7.0,
                    0.0
                ],
                "word_count_appended": [
                    167.0,
                    111.0,
                    15.0
                ],
                "result_count": [
                    4980.0,
                    1890.0,
                    94.0
                ]
            },
            "integer_answers": {
                "roanoke": 4,
                "new albion": 1,
                "vandalia": 3
            }
        },
        "lines": [
            [
                0,
                0.7151062607696727,
                0.7155172413793104,
                0.8842544316996872,
                0.008620689655172414,
                0.5699658703071673,
                0.0,
                0.0,
                0.35090909090909095
            ],
            [
                1,
                0.2713957495692131,
                0.27155172413793105,
                0.08070907194994786,
                0.24233716475095785,
                0.378839590443686,
                1.0,
                1.0,
                0.6005194805194806
            ],
            [
                0,
                0.013497989661114302,
                0.01293103448275862,
                0.035036496350364967,
                0.7490421455938697,
                0.051194539249146756,
                0.0,
                0.0,
                0.04857142857142858
            ]
        ]
    },
    "Which of these are you most likely to find in a toolbox?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.9464057847724373,
                    0.746822033898305,
                    0.9727100783572008,
                    0.05555555555555555,
                    0.8152709359605911,
                    1.0,
                    1.0,
                    0.39215686274509803
                ],
                [
                    0.029987239472564865,
                    0.10911016949152542,
                    0.015941637395298566,
                    0.8888888888888888,
                    0.1206896551724138,
                    0.0,
                    0.0,
                    0.1568627450980392
                ],
                [
                    0.023606975754997872,
                    0.1440677966101695,
                    0.011348284247500676,
                    0.05555555555555555,
                    0.06403940886699508,
                    0.0,
                    0.0,
                    0.45098039215686275
                ]
            ],
            "fraction_answers": {
                "mc hammer": 0.16518504193984132,
                "hammer": 0.7411151564111484,
                "hammerhead shark": 0.09369980164901016
            },
            "question": "Which of these are you most likely to find in a toolbox?",
            "rate_limited": false,
            "answers": [
                "hammer",
                "mc hammer",
                "hammerhead shark"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "mc hammer": 0.29035060350393144,
                "hammer": 0.4420214376981768,
                "hammerhead shark": 0.31157143216045696
            },
            "integer_answers": {
                "mc hammer": 1,
                "hammer": 6,
                "hammerhead shark": 1
            },
            "data": {
                "result_count_important_words": [
                    705000.0,
                    103000.0,
                    136000.0
                ],
                "wikipedia_search": [
                    0.1111111111111111,
                    1.7777777777777777,
                    0.1111111111111111
                ],
                "result_count_noun_chunks": [
                    10800000.0,
                    177000.0,
                    126000.0
                ],
                "word_relation_to_question": [
                    1.1764705882352942,
                    0.47058823529411764,
                    1.3529411764705883
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    4450000.0,
                    141000.0,
                    111000.0
                ],
                "word_count_appended": [
                    331.0,
                    49.0,
                    26.0
                ]
            },
            "z-best_answer_by_ml": [
                "hammer"
            ]
        },
        "lines": [
            [
                1,
                0.9464057847724373,
                0.746822033898305,
                0.9727100783572008,
                0.05555555555555555,
                0.8152709359605911,
                1.0,
                1.0,
                0.39215686274509803
            ],
            [
                0,
                0.029987239472564865,
                0.10911016949152542,
                0.015941637395298566,
                0.8888888888888888,
                0.1206896551724138,
                0.0,
                0.0,
                0.1568627450980392
            ],
            [
                0,
                0.023606975754997872,
                0.1440677966101695,
                0.011348284247500676,
                0.05555555555555555,
                0.06403940886699508,
                0.0,
                0.0,
                0.45098039215686275
            ]
        ]
    },
    "Which of these is usually found on the ocean floor?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.001048498899076156,
                    0.9158979815114073,
                    0.828505214368482,
                    0.28108974358974365,
                    0.390625,
                    0,
                    0,
                    0.05952380952380952
                ],
                [
                    0.0005475494250731037,
                    4.0148952614198674e-05,
                    0.12977983777520277,
                    0.03541666666666667,
                    0.10416666666666667,
                    0,
                    0,
                    0.10119047619047619
                ],
                [
                    0.9984039516758507,
                    0.08406186953597848,
                    0.04171494785631518,
                    0.6834935897435898,
                    0.5052083333333334,
                    0,
                    0,
                    0.8392857142857144
                ]
            ],
            "fraction_answers": {
                "sweet potato": 0.4127817079820864,
                "cherry tomato": 0.06185689094611659,
                "sea cucumber": 0.525361401071797
            },
            "question": "Which of these is usually found on the ocean floor?",
            "rate_limited": false,
            "answers": [
                "sweet potato",
                "cherry tomato",
                "sea cucumber"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "sweet potato": 0.35275388472807867,
                "cherry tomato": 0.29035060350393144,
                "sea cucumber": 0.3739747133846042
            },
            "integer_answers": {
                "sweet potato": 2,
                "cherry tomato": 0,
                "sea cucumber": 4
            },
            "data": {
                "result_count_important_words": [
                    1460000.0,
                    64.0,
                    134000.0
                ],
                "wikipedia_search": [
                    1.1243589743589744,
                    0.14166666666666666,
                    2.7339743589743586
                ],
                "result_count_noun_chunks": [
                    2860000.0,
                    448000.0,
                    144000.0
                ],
                "word_relation_to_question": [
                    0.23809523809523808,
                    0.40476190476190477,
                    3.3571428571428577
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    150.0,
                    40.0,
                    194.0
                ],
                "result_count": [
                    90.0,
                    47.0,
                    85700.0
                ]
            },
            "z-best_answer_by_ml": [
                "sea cucumber"
            ]
        },
        "lines": [
            [
                0,
                0.001048498899076156,
                0.9158979815114073,
                0.828505214368482,
                0.28108974358974365,
                0.390625,
                0,
                0,
                0.05952380952380952
            ],
            [
                0,
                0.0005475494250731037,
                4.0148952614198674e-05,
                0.12977983777520277,
                0.03541666666666667,
                0.10416666666666667,
                0,
                0,
                0.10119047619047619
            ],
            [
                1,
                0.9984039516758507,
                0.08406186953597848,
                0.04171494785631518,
                0.6834935897435898,
                0.5052083333333334,
                0,
                0,
                0.8392857142857144
            ]
        ]
    },
    "The '90s band The Lightning Seeds took their name from which song?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.011342615129977607,
                    0.039243932250742096,
                    0.047376024916974686,
                    0.241156116068292,
                    0.3508771929824561,
                    1.0,
                    1.0,
                    0.14728163992869875
                ],
                [
                    0.9882192581053452,
                    0.9603631918980269,
                    0.9522111938758279,
                    0.4220824843673154,
                    0.2982456140350877,
                    0.0,
                    0.0,
                    0.23398905016552077
                ],
                [
                    0.0004381267646772466,
                    0.000392875851231011,
                    0.00041278120719740325,
                    0.33676139956439266,
                    0.3508771929824561,
                    0.0,
                    0.0,
                    0.6187293099057806
                ]
            ],
            "fraction_answers": {
                "when doves cry": 0.16345146078446687,
                "raspberry beret": 0.35465969015964266,
                "purple rain": 0.48188884905589047
            },
            "question": "The '90s band The Lightning Seeds took their name from which song?",
            "rate_limited": false,
            "answers": [
                "raspberry beret",
                "purple rain",
                "when doves cry"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "when doves cry": 0.34393939425547915,
                "raspberry beret": 0.34393939425547915,
                "purple rain": 0.32559523786462485
            },
            "integer_answers": {
                "when doves cry": 1,
                "raspberry beret": 3,
                "purple rain": 4
            },
            "data": {
                "result_count_important_words": [
                    8990.0,
                    220000.0,
                    90.0
                ],
                "wikipedia_search": [
                    0.964624464273168,
                    1.6883299374692615,
                    1.3470455982575706
                ],
                "result_count_noun_chunks": [
                    10100.0,
                    203000.0,
                    88.0
                ],
                "word_relation_to_question": [
                    0.589126559714795,
                    0.9359562006620831,
                    2.4749172396231223
                ],
                "word_count_noun_chunks": [
                    3.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    3.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    20.0,
                    17.0,
                    20.0
                ],
                "result_count": [
                    2330.0,
                    203000.0,
                    90.0
                ]
            },
            "z-best_answer_by_ml": [
                "raspberry beret",
                "when doves cry"
            ]
        },
        "lines": [
            [
                1,
                0.011342615129977607,
                0.039243932250742096,
                0.047376024916974686,
                0.241156116068292,
                0.3508771929824561,
                1.0,
                1.0,
                0.14728163992869875
            ],
            [
                0,
                0.9882192581053452,
                0.9603631918980269,
                0.9522111938758279,
                0.4220824843673154,
                0.2982456140350877,
                0.0,
                0.0,
                0.23398905016552077
            ],
            [
                0,
                0.0004381267646772466,
                0.000392875851231011,
                0.00041278120719740325,
                0.33676139956439266,
                0.3508771929824561,
                0.0,
                0.0,
                0.6187293099057806
            ]
        ]
    },
    "Which Las Vegas hotel features a replica of the Rialto Bridge?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.02379557768340899,
                    0.023069366543080776,
                    0.22786612864941846,
                    0.1981892595339231,
                    0.5133531157270029,
                    0.03125,
                    0.01282051282051282,
                    0.4217005888782464
                ],
                [
                    0.966466539756919,
                    0.9696739233767468,
                    0.7674657805206108,
                    0.0324301175737756,
                    0.39762611275964393,
                    0.96875,
                    0.9487179487179487,
                    0.20233394741081784
                ],
                [
                    0.009737882559671987,
                    0.0072567100801723865,
                    0.004668090829970726,
                    0.7693806228923012,
                    0.08902077151335312,
                    0.0,
                    0.038461538461538464,
                    0.3759654637109358
                ]
            ],
            "fraction_answers": {
                "caesars palace": 0.16181138500599296,
                "luxor": 0.1815055687294492,
                "the venetian": 0.6566830462645578
            },
            "question": "Which Las Vegas hotel features a replica of the Rialto Bridge?",
            "rate_limited": false,
            "answers": [
                "luxor",
                "the venetian",
                "caesars palace"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "caesars palace": 0.30114516136899444,
                "luxor": 0.3739747133846042,
                "the venetian": 0.4420214376981768
            },
            "integer_answers": {
                "caesars palace": 1,
                "luxor": 2,
                "the venetian": 5
            },
            "data": {
                "result_count_important_words": [
                    7280.0,
                    306000.0,
                    2290.0
                ],
                "wikipedia_search": [
                    1.1891355572035387,
                    0.19458070544265357,
                    4.6162837373538075
                ],
                "result_count_noun_chunks": [
                    144000.0,
                    485000.0,
                    2950.0
                ],
                "word_relation_to_question": [
                    2.108502944391232,
                    1.0116697370540892,
                    1.879827318554679
                ],
                "word_count_noun_chunks": [
                    2.0,
                    62.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    74.0,
                    3.0
                ],
                "word_count_appended": [
                    173.0,
                    134.0,
                    30.0
                ],
                "result_count": [
                    3250.0,
                    132000.0,
                    1330.0
                ]
            },
            "z-best_answer_by_ml": [
                "the venetian"
            ]
        },
        "lines": [
            [
                0,
                0.02379557768340899,
                0.023069366543080776,
                0.22786612864941846,
                0.1981892595339231,
                0.5133531157270029,
                0.03125,
                0.01282051282051282,
                0.4217005888782464
            ],
            [
                1,
                0.966466539756919,
                0.9696739233767468,
                0.7674657805206108,
                0.0324301175737756,
                0.39762611275964393,
                0.96875,
                0.9487179487179487,
                0.20233394741081784
            ],
            [
                0,
                0.009737882559671987,
                0.0072567100801723865,
                0.004668090829970726,
                0.7693806228923012,
                0.08902077151335312,
                0.0,
                0.038461538461538464,
                0.3759654637109358
            ]
        ]
    },
    "What TV series derived from a nearly 20-year-old Michael Crichton screenplay?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.07940033314825097,
                    0.05786718104160926,
                    0.2791220707243117,
                    0.01818181818181818,
                    0.011994949494949494,
                    0.0,
                    0.0,
                    0.30558718460050943
                ],
                [
                    0.19877845641310382,
                    0.05483604298704877,
                    0.7188899375489468,
                    0.3020438754318636,
                    0.07323232323232323,
                    0.0,
                    0.0,
                    0.1794442778738464
                ],
                [
                    0.7218212104386452,
                    0.8872967759713419,
                    0.001987991726741501,
                    0.6797743063863182,
                    0.9147727272727273,
                    1.0,
                    1.0,
                    0.5149685375256441
                ]
            ],
            "fraction_answers": {
                "numb3rs": 0.19090311418589156,
                "the expanse": 0.09401919214893113,
                "er": 0.7150776936651773
            },
            "question": "What TV series derived from a nearly 20-year-old Michael Crichton screenplay?",
            "rate_limited": false,
            "answers": [
                "the expanse",
                "numb3rs",
                "er"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "numb3rs": 0.2322454362527345,
                "the expanse": 0.12052222885353463,
                "er": 0.5442284557106134
            },
            "z-best_answer_by_ml": [
                "er"
            ],
            "data": {
                "result_count_important_words": [
                    4200.0,
                    3980.0,
                    64400.0
                ],
                "wikipedia_search": [
                    0.09090909090909091,
                    1.5102193771593182,
                    3.398871531931591
                ],
                "result_count_noun_chunks": [
                    13900.0,
                    35800.0,
                    99.0
                ],
                "word_relation_to_question": [
                    1.8335231076030567,
                    1.0766656672430783,
                    3.0898112251538645
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    1040.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1172.0
                ],
                "result_count": [
                    14300.0,
                    35800.0,
                    130000.0
                ],
                "word_count_appended": [
                    19.0,
                    116.0,
                    1449.0
                ]
            },
            "integer_answers": {
                "numb3rs": 1,
                "the expanse": 0,
                "er": 7
            }
        },
        "lines": [
            [
                0,
                0.07940033314825097,
                0.05786718104160926,
                0.2791220707243117,
                0.01818181818181818,
                0.011994949494949494,
                0.0,
                0.0,
                0.30558718460050943
            ],
            [
                0,
                0.19877845641310382,
                0.05483604298704877,
                0.7188899375489468,
                0.3020438754318636,
                0.07323232323232323,
                0.0,
                0.0,
                0.1794442778738464
            ],
            [
                1,
                0.7218212104386452,
                0.8872967759713419,
                0.001987991726741501,
                0.6797743063863182,
                0.9147727272727273,
                1.0,
                1.0,
                0.5149685375256441
            ]
        ]
    },
    "What makeup item often contains dried cochineal bugs as an ingredient?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "lipstick"
            ],
            "question": "What makeup item often contains dried cochineal bugs as an ingredient?",
            "answers": [
                "mascara",
                "eyeliner",
                "lipstick"
            ],
            "integer_answers": {
                "lipstick": 3,
                "mascara": 2,
                "eyeliner": 3
            },
            "data": {
                "result_count_important_words": [
                    9060.0,
                    56600.0,
                    9680.0
                ],
                "wikipedia_search": [
                    1.3928571428571428,
                    0.5119047619047619,
                    6.095238095238096
                ],
                "result_count_noun_chunks": [
                    10500.0,
                    55900.0,
                    11100.0
                ],
                "word_relation_to_question": [
                    2.0658437648918486,
                    1.108424282576101,
                    1.8257319525320501
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    57.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    39.0
                ],
                "word_count_appended": [
                    178.0,
                    143.0,
                    16.0
                ],
                "result_count": [
                    6290.0,
                    47100.0,
                    0
                ]
            },
            "negative_question": false,
            "fraction_answers": {
                "lipstick": 0.4307798635759403,
                "mascara": 0.18612710586147635,
                "eyeliner": 0.3830930305625833
            },
            "lines": [
                [
                    0.11781232440531934,
                    0.12025484470400849,
                    0.13548387096774195,
                    0.17410714285714285,
                    0.5281899109792285,
                    0.0,
                    0.0,
                    0.41316875297836975
                ],
                [
                    0.8821876755946807,
                    0.7512609503583754,
                    0.7212903225806452,
                    0.06398809523809523,
                    0.42433234421364985,
                    0.0,
                    0.0,
                    0.2216848565152202
                ],
                [
                    0.0,
                    0.12848420493761614,
                    0.1432258064516129,
                    0.761904761904762,
                    0.04747774480712166,
                    1.0,
                    1.0,
                    0.36514639050641
                ]
            ],
            "rate_limited": false,
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "lipstick": 0.7115347094820504,
                "mascara": 0.20574789035014399,
                "eyeliner": 0.2588143908083011
            }
        },
        "lines": [
            [
                0,
                0.11781232440531934,
                0.12025484470400849,
                0.13548387096774195,
                0.17410714285714285,
                0.5281899109792285,
                0.0,
                0.0,
                0.41316875297836975
            ],
            [
                0,
                0.8821876755946807,
                0.7512609503583754,
                0.7212903225806452,
                0.06398809523809523,
                0.42433234421364985,
                0.0,
                0.0,
                0.2216848565152202
            ],
            [
                1,
                0.0,
                0.12848420493761614,
                0.1432258064516129,
                0.761904761904762,
                0.04747774480712166,
                1.0,
                1.0,
                0.36514639050641
            ]
        ]
    },
    "Which of these video games was NOT produced by FromSoftware?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.11997019374068557,
                    0.12955424384286585,
                    0.12006456419170597,
                    0.352131988967241,
                    0.08514492753623187,
                    0.0,
                    0.0,
                    0.2568846792878483
                ],
                [
                    0.45789865871833085,
                    0.4474862609403623,
                    0.4579091134839831,
                    0.40359642970648607,
                    0.42934782608695654,
                    0.5,
                    0.5,
                    0.3521469965627096
                ],
                [
                    0.4221311475409836,
                    0.42295949521677184,
                    0.4220263223243109,
                    0.24427158132627286,
                    0.4855072463768116,
                    0.5,
                    0.5,
                    0.3909683241494421
                ]
            ],
            "fraction_answers": {
                "demon's souls": 0.15303397076635178,
                "beyond: two souls": 0.11290367862529288,
                "dark souls": 0.7340623506083553
            },
            "question": "Which of these video games was NOT produced by FromSoftware?",
            "rate_limited": false,
            "answers": [
                "dark souls",
                "beyond: two souls",
                "demon's souls"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "demon's souls": 0.4420214376981768,
                "beyond: two souls": 0.4420214376981768,
                "dark souls": 0.31157143216045696
            },
            "z-best_answer_by_ml": [
                "beyond: two souls",
                "demon's souls"
            ],
            "data": {
                "result_count_important_words": [
                    364000.0,
                    51600.0,
                    75700.0
                ],
                "wikipedia_search": [
                    1.1829440882620716,
                    0.7712285623481113,
                    2.0458273493898167
                ],
                "result_count_noun_chunks": [
                    306000.0,
                    33900.0,
                    62800.0
                ],
                "word_relation_to_question": [
                    2.431153207121517,
                    1.4785300343729042,
                    1.0903167585055789
                ],
                "word_count_noun_chunks": [
                    64.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    68.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    306000.0,
                    33900.0,
                    62700.0
                ],
                "word_count_appended": [
                    229.0,
                    39.0,
                    8.0
                ]
            },
            "integer_answers": {
                "demon's souls": 1,
                "beyond: two souls": 0,
                "dark souls": 7
            }
        },
        "lines": [
            [
                0,
                0.11997019374068557,
                0.12955424384286585,
                0.12006456419170597,
                0.352131988967241,
                0.08514492753623187,
                0.0,
                0.0,
                0.2568846792878483
            ],
            [
                1,
                0.45789865871833085,
                0.4474862609403623,
                0.4579091134839831,
                0.40359642970648607,
                0.42934782608695654,
                0.5,
                0.5,
                0.3521469965627096
            ],
            [
                0,
                0.4221311475409836,
                0.42295949521677184,
                0.4220263223243109,
                0.24427158132627286,
                0.4855072463768116,
                0.5,
                0.5,
                0.3909683241494421
            ]
        ]
    },
    "Which of these is NOT the title of a current TV show?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.41692834713103055,
                    0.43018867924528303,
                    0.29446045870409826,
                    0.38209348572157187,
                    0.19383259911894274,
                    0,
                    0,
                    0.3023266206639862
                ],
                [
                    0.22166714244932917,
                    0.1257861635220126,
                    0.23492918912144378,
                    0.2905110587169161,
                    0.4955947136563877,
                    0,
                    0,
                    0.2950804944880039
                ],
                [
                    0.3614045104196403,
                    0.4440251572327044,
                    0.47061035217445796,
                    0.32739545556151195,
                    0.3105726872246696,
                    0,
                    0,
                    0.4025928848480099
                ]
            ],
            "fraction_answers": {
                "chicago med": 0.32672326980502914,
                "chicago police": 0.22779965084633527,
                "chicage fire": 0.44547707934863556
            },
            "question": "Which of these is NOT the title of a current TV show?",
            "rate_limited": false,
            "answers": [
                "chicago med",
                "chicage fire",
                "chicago police"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "chicago med": 0.29031613792954336,
                "chicago police": 0.20386454512494143,
                "chicage fire": 0.22286511799996578
            },
            "z-best_answer_by_ml": [
                "chicago med"
            ],
            "data": {
                "result_count_important_words": [
                    44400.0,
                    238000.0,
                    35600.0
                ],
                "wikipedia_search": [
                    0.9432521142274248,
                    1.675911530264671,
                    1.3808363555079042
                ],
                "result_count_noun_chunks": [
                    328000.0,
                    423000.0,
                    46900.0
                ],
                "word_relation_to_question": [
                    1.5813870346881107,
                    1.639356044095969,
                    0.7792569212159204
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    58200.0,
                    195000.0,
                    97100.0
                ],
                "word_count_appended": [
                    139.0,
                    2.0,
                    86.0
                ]
            },
            "integer_answers": {
                "chicago med": 1,
                "chicago police": 0,
                "chicage fire": 5
            }
        },
        "lines": [
            [
                0,
                0.41692834713103055,
                0.43018867924528303,
                0.29446045870409826,
                0.38209348572157187,
                0.19383259911894274,
                0,
                0,
                0.3023266206639862
            ],
            [
                0,
                0.22166714244932917,
                0.1257861635220126,
                0.23492918912144378,
                0.2905110587169161,
                0.4955947136563877,
                0,
                0,
                0.2950804944880039
            ],
            [
                1,
                0.3614045104196403,
                0.4440251572327044,
                0.47061035217445796,
                0.32739545556151195,
                0.3105726872246696,
                0,
                0,
                0.4025928848480099
            ]
        ]
    },
    "Which of these things is NOT found inside an atom?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.04245686796956949,
                    0.3020372010628875,
                    0.15243554494994035,
                    0.3238856589147287,
                    0.32342807924203276,
                    0.21774193548387094,
                    0.19140625,
                    0.35036848072562354
                ],
                [
                    0.49521960864690207,
                    0.38131089459698847,
                    0.49670591897079425,
                    0.34294320137693635,
                    0.36304909560723514,
                    0.5,
                    0.5,
                    0.3716666666666667
                ],
                [
                    0.46232352338352845,
                    0.316651904340124,
                    0.3508585360792655,
                    0.333171139708335,
                    0.31352282515073215,
                    0.282258064516129,
                    0.30859375,
                    0.27796485260770976
                ]
            ],
            "fraction_answers": {
                "wonton": 0.13727615353361927,
                "neutron": 0.3386638510535441,
                "proton": 0.5240599954128367
            },
            "question": "Which of these things is NOT found inside an atom?",
            "rate_limited": false,
            "answers": [
                "proton",
                "wonton",
                "neutron"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "wonton": 0.4420214376981768,
                "neutron": 0.27595726218627614,
                "proton": 0.30114516136899444
            },
            "z-best_answer_by_ml": [
                "wonton"
            ],
            "data": {
                "result_count_important_words": [
                    447000.0,
                    268000.0,
                    414000.0
                ],
                "wikipedia_search": [
                    1.4089147286821704,
                    1.2564543889845095,
                    1.33463088233332
                ],
                "result_count_noun_chunks": [
                    13400000.0,
                    127000.0,
                    5750000.0
                ],
                "word_relation_to_question": [
                    1.1970521541950114,
                    1.0266666666666666,
                    1.776281179138322
                ],
                "word_count_noun_chunks": [
                    35.0,
                    0.0,
                    27.0
                ],
                "word_count_raw": [
                    79.0,
                    0.0,
                    49.0
                ],
                "result_count": [
                    7590000.0,
                    79300.0,
                    625000.0
                ],
                "word_count_appended": [
                    410.0,
                    318.0,
                    433.0
                ]
            },
            "integer_answers": {
                "wonton": 0,
                "neutron": 2,
                "proton": 6
            }
        },
        "lines": [
            [
                0,
                0.04245686796956949,
                0.3020372010628875,
                0.15243554494994035,
                0.3238856589147287,
                0.32342807924203276,
                0.21774193548387094,
                0.19140625,
                0.35036848072562354
            ],
            [
                1,
                0.49521960864690207,
                0.38131089459698847,
                0.49670591897079425,
                0.34294320137693635,
                0.36304909560723514,
                0.5,
                0.5,
                0.3716666666666667
            ],
            [
                0,
                0.46232352338352845,
                0.316651904340124,
                0.3508585360792655,
                0.333171139708335,
                0.31352282515073215,
                0.282258064516129,
                0.30859375,
                0.27796485260770976
            ]
        ]
    },
    "What actor famously yelled \"Not the bees! Not the bees!\" in a 2006 film?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "nicolas cage"
            ],
            "question": "What actor famously yelled \"Not the bees! Not the bees!\" in a 2006 film?",
            "answers": [
                "nicolas cage",
                "macauley culkin",
                "oprah winfrey"
            ],
            "integer_answers": {
                "macauley culkin": 0,
                "nicolas cage": 5,
                "oprah winfrey": 3
            },
            "data": {
                "result_count_important_words": [
                    8300.0,
                    48.0,
                    15500.0
                ],
                "wikipedia_search": [
                    1.9213859853157478,
                    0.9259916533642583,
                    3.152622361319994
                ],
                "result_count_noun_chunks": [
                    69100.0,
                    30100.0,
                    90800.0
                ],
                "word_relation_to_question": [
                    4.095037685426702,
                    1.7545806596149844,
                    1.150381654958314
                ],
                "word_count_noun_chunks": [
                    24.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    4.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    17.0,
                    1.0,
                    2.0
                ],
                "word_count_appended": [
                    14.0,
                    2.0,
                    5.0
                ]
            },
            "negative_question": false,
            "fraction_answers": {
                "macauley culkin": 0.0888322771802812,
                "nicolas cage": 0.6417031037078497,
                "oprah winfrey": 0.2694646191118692
            },
            "lines": [
                [
                    0.85,
                    0.3480375712848038,
                    0.3636842105263158,
                    0.32023099755262463,
                    0.6666666666666666,
                    1.0,
                    1.0,
                    0.585005383632386
                ],
                [
                    0.05,
                    0.0020127474002012745,
                    0.15842105263157893,
                    0.15433194222737637,
                    0.09523809523809523,
                    0.0,
                    0.0,
                    0.2506543799449978
                ],
                [
                    0.1,
                    0.649949681314995,
                    0.47789473684210526,
                    0.525437060219999,
                    0.23809523809523808,
                    0.0,
                    0.0,
                    0.16434023642261628
                ]
            ],
            "rate_limited": false,
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "macauley culkin": 0.21287143035575118,
                "nicolas cage": 0.7224139102363077,
                "oprah winfrey": 0.20845431372038975
            }
        },
        "lines": [
            [
                1,
                0.85,
                0.3480375712848038,
                0.3636842105263158,
                0.32023099755262463,
                0.6666666666666666,
                1.0,
                1.0,
                0.585005383632386
            ],
            [
                0,
                0.05,
                0.0020127474002012745,
                0.15842105263157893,
                0.15433194222737637,
                0.09523809523809523,
                0.0,
                0.0,
                0.2506543799449978
            ],
            [
                0,
                0.1,
                0.649949681314995,
                0.47789473684210526,
                0.525437060219999,
                0.23809523809523808,
                0.0,
                0.0,
                0.16434023642261628
            ]
        ]
    },
    "Mardi Gras is celebrated right before what other observance?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.5846053913608315,
                    0.5732044198895028,
                    0.24987328940699444,
                    0.4016225749559083,
                    0.47126436781609193,
                    0.9859154929577465,
                    1.0,
                    0.5076976198149257
                ],
                [
                    0.15004871711594672,
                    0.15158839779005526,
                    0.25747592498732896,
                    0.28557319223985894,
                    0.28735632183908044,
                    0.0,
                    0.0,
                    0.31130703006307264
                ],
                [
                    0.2653458915232218,
                    0.275207182320442,
                    0.49265078560567666,
                    0.3128042328042328,
                    0.2413793103448276,
                    0.014084507042253521,
                    0.0,
                    0.18099535012200177
                ]
            ],
            "fraction_answers": {
                "kwanzaa": 0.18041869800441787,
                "ramadan": 0.222808407470332,
                "lent": 0.5967728945252502
            },
            "question": "Mardi Gras is celebrated right before what other observance?",
            "rate_limited": false,
            "answers": [
                "lent",
                "kwanzaa",
                "ramadan"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "kwanzaa": 0.1340888440752111,
                "ramadan": 0.2157396857159027,
                "lent": 0.4416542828654436
            },
            "integer_answers": {
                "kwanzaa": 0,
                "ramadan": 1,
                "lent": 7
            },
            "data": {
                "result_count_important_words": [
                    166000.0,
                    43900.0,
                    79700.0
                ],
                "wikipedia_search": [
                    1.2048677248677249,
                    0.8567195767195768,
                    0.9384126984126984
                ],
                "result_count_noun_chunks": [
                    49300.0,
                    50800.0,
                    97200.0
                ],
                "word_relation_to_question": [
                    2.538488099074628,
                    1.5565351503153628,
                    0.9049767506100087
                ],
                "word_count_noun_chunks": [
                    70.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    114.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    410.0,
                    250.0,
                    210.0
                ],
                "result_count": [
                    180000.0,
                    46200.0,
                    81700.0
                ]
            },
            "z-best_answer_by_ml": [
                "lent"
            ]
        },
        "lines": [
            [
                1,
                0.5846053913608315,
                0.5732044198895028,
                0.24987328940699444,
                0.4016225749559083,
                0.47126436781609193,
                0.9859154929577465,
                1.0,
                0.5076976198149257
            ],
            [
                0,
                0.15004871711594672,
                0.15158839779005526,
                0.25747592498732896,
                0.28557319223985894,
                0.28735632183908044,
                0.0,
                0.0,
                0.31130703006307264
            ],
            [
                0,
                0.2653458915232218,
                0.275207182320442,
                0.49265078560567666,
                0.3128042328042328,
                0.2413793103448276,
                0.014084507042253521,
                0.0,
                0.18099535012200177
            ]
        ]
    },
    "In baking, yeast helps bread rise, but scientifically yeast is what?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.13466941374789287,
                    0.10251798561151079,
                    0.054632899977455476,
                    0.8807241145950824,
                    0.33666191155492153,
                    0.5,
                    0.7272727272727273,
                    0.8666666666666666
                ],
                [
                    0.6817756134107511,
                    0.5080935251798561,
                    0.2660254001653265,
                    0.06468531468531469,
                    0.3166904422253923,
                    0.4,
                    0.18181818181818182,
                    0.13333333333333333
                ],
                [
                    0.18355497284135605,
                    0.3893884892086331,
                    0.679341699857218,
                    0.05459057071960298,
                    0.3466476462196862,
                    0.1,
                    0.09090909090909091,
                    0.0
                ]
            ],
            "fraction_answers": {
                "fungus": 0.4503932149282821,
                "plant": 0.3190527263522695,
                "bacteria": 0.2305540587194484
            },
            "question": "In baking, yeast helps bread rise, but scientifically yeast is what?",
            "rate_limited": false,
            "answers": [
                "fungus",
                "plant",
                "bacteria"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "fungus": 0.4394875441011861,
                "plant": 0.3590901813423653,
                "bacteria": 0.16742284450223888
            },
            "z-best_answer_by_ml": [
                "fungus"
            ],
            "data": {
                "result_count_important_words": [
                    114000.0,
                    565000.0,
                    433000.0
                ],
                "wikipedia_search": [
                    3.5228964583803295,
                    0.25874125874125875,
                    0.21836228287841192
                ],
                "result_count_noun_chunks": [
                    727000.0,
                    3540000.0,
                    9040000.0
                ],
                "word_relation_to_question": [
                    4.333333333333333,
                    0.6666666666666666,
                    0.0
                ],
                "word_count_noun_chunks": [
                    10.0,
                    8.0,
                    2.0
                ],
                "word_count_raw": [
                    8.0,
                    2.0,
                    1.0
                ],
                "result_count": [
                    71900.0,
                    364000.0,
                    98000.0
                ],
                "word_count_appended": [
                    236.0,
                    222.0,
                    243.0
                ]
            },
            "integer_answers": {
                "fungus": 4,
                "plant": 2,
                "bacteria": 2
            }
        },
        "lines": [
            [
                1,
                0.13466941374789287,
                0.10251798561151079,
                0.054632899977455476,
                0.8807241145950824,
                0.33666191155492153,
                0.5,
                0.7272727272727273,
                0.8666666666666666
            ],
            [
                0,
                0.6817756134107511,
                0.5080935251798561,
                0.2660254001653265,
                0.06468531468531469,
                0.3166904422253923,
                0.4,
                0.18181818181818182,
                0.13333333333333333
            ],
            [
                0,
                0.18355497284135605,
                0.3893884892086331,
                0.679341699857218,
                0.05459057071960298,
                0.3466476462196862,
                0.1,
                0.09090909090909091,
                0.0
            ]
        ]
    },
    "Which of these is classified as a neurological condition or disorder?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.0038905622489959837,
                    0.5239179954441914,
                    0.004195029286053507,
                    0.28756674294431733,
                    0.5242214532871973,
                    0.0,
                    0.0,
                    0.17361689245440096
                ],
                [
                    0.033927376171352074,
                    0.18177676537585422,
                    0.022241570365679912,
                    0.20404271548436306,
                    0.2231833910034602,
                    0.0,
                    0.0,
                    0.24195867706047072
                ],
                [
                    0.9621820615796519,
                    0.29430523917995444,
                    0.9735634003482666,
                    0.5083905415713196,
                    0.25259515570934254,
                    1.0,
                    1.0,
                    0.5844244304851283
                ]
            ],
            "fraction_answers": {
                "halitosis": 0.18967608445814457,
                "cystic fibrosis": 0.11339131193264752,
                "multiple sclerosis": 0.696932603609208
            },
            "question": "Which of these is classified as a neurological condition or disorder?",
            "rate_limited": false,
            "answers": [
                "halitosis",
                "cystic fibrosis",
                "multiple sclerosis"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "halitosis": 0.35275388472807867,
                "cystic fibrosis": 0.31157143216045696,
                "multiple sclerosis": 0.35443025729131133
            },
            "z-best_answer_by_ml": [
                "multiple sclerosis"
            ],
            "data": {
                "result_count_important_words": [
                    1150000.0,
                    399000.0,
                    646000.0
                ],
                "wikipedia_search": [
                    0.8627002288329519,
                    0.6121281464530892,
                    1.5251716247139586
                ],
                "result_count_noun_chunks": [
                    106000.0,
                    562000.0,
                    24600000.0
                ],
                "word_relation_to_question": [
                    0.5208506773632029,
                    0.7258760311814122,
                    1.7532732914553848
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    3.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    7.0
                ],
                "result_count": [
                    93000.0,
                    811000.0,
                    23000000.0
                ],
                "word_count_appended": [
                    303.0,
                    129.0,
                    146.0
                ]
            },
            "integer_answers": {
                "halitosis": 2,
                "cystic fibrosis": 0,
                "multiple sclerosis": 6
            }
        },
        "lines": [
            [
                0,
                0.0038905622489959837,
                0.5239179954441914,
                0.004195029286053507,
                0.28756674294431733,
                0.5242214532871973,
                0.0,
                0.0,
                0.17361689245440096
            ],
            [
                0,
                0.033927376171352074,
                0.18177676537585422,
                0.022241570365679912,
                0.20404271548436306,
                0.2231833910034602,
                0.0,
                0.0,
                0.24195867706047072
            ],
            [
                1,
                0.9621820615796519,
                0.29430523917995444,
                0.9735634003482666,
                0.5083905415713196,
                0.25259515570934254,
                1.0,
                1.0,
                0.5844244304851283
            ]
        ]
    },
    "Which of these Uranus moons is NOT named after a Shakespearean character?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.11521252796420584,
                    0.023492996548327427,
                    0.11524775375856239,
                    0.47252959927140253,
                    0.2840599455040872,
                    0.19642857142857145,
                    0.21551724137931033,
                    0.4561671231504058
                ],
                [
                    0.39619686800894854,
                    0.47893758959532945,
                    0.39591673338670935,
                    0.22851548269581057,
                    0.335149863760218,
                    0.3392857142857143,
                    0.3017241379310345,
                    0.24903901822594837
                ],
                [
                    0.4885906040268456,
                    0.4975694138563431,
                    0.4888355128547282,
                    0.2989549180327869,
                    0.3807901907356948,
                    0.4642857142857143,
                    0.4827586206896552,
                    0.2947938586236458
                ]
            ],
            "fraction_answers": {
                "oberon": 0.5303360602487818,
                "trinculo": 0.1508552917236465,
                "umbriel": 0.3188086480275718
            },
            "question": "Which of these Uranus moons is NOT named after a Shakespearean character?",
            "rate_limited": false,
            "answers": [
                "oberon",
                "umbriel",
                "trinculo"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "oberon": 0.30114516136899444,
                "trinculo": 0.3635484425931417,
                "umbriel": 0.27595726218627614
            },
            "integer_answers": {
                "oberon": 6,
                "trinculo": 0,
                "umbriel": 2
            },
            "data": {
                "result_count_important_words": [
                    119000.0,
                    5260.0,
                    607.0
                ],
                "wikipedia_search": [
                    0.2747040072859745,
                    2.7148451730418945,
                    2.010450819672131
                ],
                "result_count_noun_chunks": [
                    17300.0,
                    4680.0,
                    502.0
                ],
                "word_relation_to_question": [
                    0.5259945221951301,
                    3.0115317812886198,
                    2.4624736965162497
                ],
                "word_count_noun_chunks": [
                    34.0,
                    18.0,
                    4.0
                ],
                "word_count_raw": [
                    33.0,
                    23.0,
                    2.0
                ],
                "word_count_appended": [
                    317.0,
                    242.0,
                    175.0
                ],
                "result_count": [
                    17200.0,
                    4640.0,
                    510.0
                ]
            },
            "z-best_answer_by_ml": [
                "trinculo"
            ]
        },
        "lines": [
            [
                0,
                0.11521252796420584,
                0.023492996548327427,
                0.11524775375856239,
                0.47252959927140253,
                0.2840599455040872,
                0.19642857142857145,
                0.21551724137931033,
                0.4561671231504058
            ],
            [
                1,
                0.39619686800894854,
                0.47893758959532945,
                0.39591673338670935,
                0.22851548269581057,
                0.335149863760218,
                0.3392857142857143,
                0.3017241379310345,
                0.24903901822594837
            ],
            [
                0,
                0.4885906040268456,
                0.4975694138563431,
                0.4888355128547282,
                0.2989549180327869,
                0.3807901907356948,
                0.4642857142857143,
                0.4827586206896552,
                0.2947938586236458
            ]
        ]
    },
    "Which of these knots is typically used to add another line to a rope?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.7776735977055871,
                    0.7741661806034255,
                    0.744466800804829,
                    0.5531135737814656,
                    0.6074950690335306,
                    0.8518518518518519,
                    0.8461538461538461,
                    0.5059002774008373
                ],
                [
                    0.20020958579228945,
                    0.20520706294077098,
                    0.21989077321069272,
                    0.39039940475230894,
                    0.14201183431952663,
                    0.14814814814814814,
                    0.15384615384615385,
                    0.37091004677790795
                ],
                [
                    0.022116816502123434,
                    0.020626756455803596,
                    0.035642425984478296,
                    0.05648702146622541,
                    0.2504930966469428,
                    0.0,
                    0.0,
                    0.12318967582125477
                ]
            ],
            "fraction_answers": {
                "grantchester": 0.06356947410960354,
                "rolling hitch": 0.22882787622347484,
                "bowline": 0.7076026496669215
            },
            "question": "Which of these knots is typically used to add another line to a rope?",
            "rate_limited": false,
            "answers": [
                "bowline",
                "rolling hitch",
                "grantchester"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "grantchester": 0.29035060350393144,
                "rolling hitch": 0.30114516136899444,
                "bowline": 0.4420214376981768
            },
            "z-best_answer_by_ml": [
                "bowline"
            ],
            "data": {
                "result_count_important_words": [
                    146000.0,
                    38700.0,
                    3890.0
                ],
                "wikipedia_search": [
                    2.2124542951258626,
                    1.5615976190092358,
                    0.22594808586490164
                ],
                "result_count_noun_chunks": [
                    259000.0,
                    76500.0,
                    12400.0
                ],
                "word_relation_to_question": [
                    2.0236011096033493,
                    1.4836401871116318,
                    0.4927587032850191
                ],
                "word_count_noun_chunks": [
                    23.0,
                    4.0,
                    0.0
                ],
                "word_count_raw": [
                    11.0,
                    2.0,
                    0.0
                ],
                "result_count": [
                    141000.0,
                    36300.0,
                    4010.0
                ],
                "word_count_appended": [
                    308.0,
                    72.0,
                    127.0
                ]
            },
            "integer_answers": {
                "grantchester": 0,
                "rolling hitch": 0,
                "bowline": 8
            }
        },
        "lines": [
            [
                0,
                0.7776735977055871,
                0.7741661806034255,
                0.744466800804829,
                0.5531135737814656,
                0.6074950690335306,
                0.8518518518518519,
                0.8461538461538461,
                0.5059002774008373
            ],
            [
                1,
                0.20020958579228945,
                0.20520706294077098,
                0.21989077321069272,
                0.39039940475230894,
                0.14201183431952663,
                0.14814814814814814,
                0.15384615384615385,
                0.37091004677790795
            ],
            [
                0,
                0.022116816502123434,
                0.020626756455803596,
                0.035642425984478296,
                0.05648702146622541,
                0.2504930966469428,
                0.0,
                0.0,
                0.12318967582125477
            ]
        ]
    },
    "Which of these celebrities is known for having aviophobia?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.43352601156069365,
                    0.4230769230769231,
                    0.44559585492227977,
                    0.8344907407407408,
                    0.5316455696202531,
                    0.125,
                    0.25,
                    0.18487394957983194
                ],
                [
                    0.2658959537572254,
                    0.2905982905982906,
                    0.27461139896373055,
                    0.07423941798941798,
                    0.2911392405063291,
                    0.5,
                    0.5,
                    0.5546218487394957
                ],
                [
                    0.30057803468208094,
                    0.2863247863247863,
                    0.27979274611398963,
                    0.09126984126984126,
                    0.17721518987341772,
                    0.375,
                    0.25,
                    0.2605042016806723
                ]
            ],
            "fraction_answers": {
                "angelina jolie": 0.4035261311875903,
                "john travolta": 0.25258559999309854,
                "john madden": 0.34388826881931117
            },
            "question": "Which of these celebrities is known for having aviophobia?",
            "rate_limited": false,
            "answers": [
                "angelina jolie",
                "john madden",
                "john travolta"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "angelina jolie": 0.34393939425547915,
                "john travolta": 0.32559523786462485,
                "john madden": 0.32559523786462485
            },
            "integer_answers": {
                "angelina jolie": 5,
                "john travolta": 0,
                "john madden": 3
            },
            "data": {
                "result_count_important_words": [
                    99.0,
                    68.0,
                    67.0
                ],
                "wikipedia_search": [
                    2.5034722222222223,
                    0.22271825396825395,
                    0.2738095238095238
                ],
                "result_count_noun_chunks": [
                    86.0,
                    53.0,
                    54.0
                ],
                "word_relation_to_question": [
                    0.5546218487394958,
                    1.6638655462184873,
                    0.7815126050420168
                ],
                "word_count_noun_chunks": [
                    1.0,
                    4.0,
                    3.0
                ],
                "word_count_raw": [
                    1.0,
                    2.0,
                    1.0
                ],
                "word_count_appended": [
                    42.0,
                    23.0,
                    14.0
                ],
                "result_count": [
                    75.0,
                    46.0,
                    52.0
                ]
            },
            "z-best_answer_by_ml": [
                "angelina jolie"
            ]
        },
        "lines": [
            [
                0,
                0.43352601156069365,
                0.4230769230769231,
                0.44559585492227977,
                0.8344907407407408,
                0.5316455696202531,
                0.125,
                0.25,
                0.18487394957983194
            ],
            [
                1,
                0.2658959537572254,
                0.2905982905982906,
                0.27461139896373055,
                0.07423941798941798,
                0.2911392405063291,
                0.5,
                0.5,
                0.5546218487394957
            ],
            [
                0,
                0.30057803468208094,
                0.2863247863247863,
                0.27979274611398963,
                0.09126984126984126,
                0.17721518987341772,
                0.375,
                0.25,
                0.2605042016806723
            ]
        ]
    },
    "Who wrote a #1 hit song for the Monkees?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.3333333333333333,
                    0.29195289499509325,
                    0.3333333333333333,
                    0.49538630269384676,
                    0,
                    0,
                    0,
                    0.22800650693649196
                ],
                [
                    0.3333333333333333,
                    0.5250245338567223,
                    0.3333333333333333,
                    0.32233174465663283,
                    0,
                    0,
                    0,
                    0.26571379305376847
                ],
                [
                    0.3333333333333333,
                    0.1830225711481845,
                    0.3333333333333333,
                    0.1822819526495204,
                    0,
                    0,
                    0,
                    0.5062797000097395
                ]
            ],
            "fraction_answers": {
                "neil diamond": 0.35594734764675806,
                "james taylor": 0.33640247425841974,
                "jackson browne": 0.3076501780948222
            },
            "question": "Who wrote a #1 hit song for the Monkees?",
            "rate_limited": false,
            "answers": [
                "james taylor",
                "neil diamond",
                "jackson browne"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "neil diamond": 0.32559523786462485,
                "james taylor": 0.32559523786462485,
                "jackson browne": 0.32559523786462485
            },
            "integer_answers": {
                "neil diamond": 1,
                "james taylor": 3,
                "jackson browne": 1
            },
            "data": {
                "result_count_important_words": [
                    119000.0,
                    214000.0,
                    74600.0
                ],
                "wikipedia_search": [
                    2.476931513469234,
                    1.611658723283164,
                    0.911409763247602
                ],
                "result_count_noun_chunks": [
                    6980000000.0,
                    6980000000.0,
                    6980000000.0
                ],
                "word_relation_to_question": [
                    1.1400325346824598,
                    1.3285689652688424,
                    2.531398500048698
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    66200000.0,
                    66200000.0,
                    66200000.0
                ]
            },
            "z-best_answer_by_ml": [
                "james taylor",
                "neil diamond",
                "jackson browne"
            ]
        },
        "lines": [
            [
                0,
                0.3333333333333333,
                0.29195289499509325,
                0.3333333333333333,
                0.49538630269384676,
                0,
                0,
                0,
                0.22800650693649196
            ],
            [
                1,
                0.3333333333333333,
                0.5250245338567223,
                0.3333333333333333,
                0.32233174465663283,
                0,
                0,
                0,
                0.26571379305376847
            ],
            [
                0,
                0.3333333333333333,
                0.1830225711481845,
                0.3333333333333333,
                0.1822819526495204,
                0,
                0,
                0,
                0.5062797000097395
            ]
        ]
    },
    "What Japanese word means \u201cempty orchestra\u201d?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.05035335689045936,
                    0.38591549295774646,
                    0.12121212121212122,
                    0.1431818181818182,
                    0.1886993603411514,
                    0.0,
                    0.0,
                    0.34087809036658145
                ],
                [
                    0.053003533568904596,
                    0.2953051643192488,
                    0.39302112029384756,
                    0.5053030303030304,
                    0.21748400852878466,
                    0.01718213058419244,
                    0.015873015873015872,
                    0.2347527706734868
                ],
                [
                    0.8966431095406361,
                    0.3187793427230047,
                    0.4857667584940312,
                    0.3515151515151515,
                    0.593816631130064,
                    0.9828178694158075,
                    0.9841269841269841,
                    0.42436913895993184
                ]
            ],
            "fraction_answers": {
                "sake": 0.15378002999373475,
                "anime": 0.21649059676806387,
                "karaoke": 0.6297293732382014
            },
            "question": "What Japanese word means \u201cempty orchestra\u201d?",
            "rate_limited": false,
            "answers": [
                "sake",
                "anime",
                "karaoke"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "sake": 0.31157143216045696,
                "anime": 0.2613991511776011,
                "karaoke": 0.4420214376981768
            },
            "z-best_answer_by_ml": [
                "karaoke"
            ],
            "data": {
                "result_count_important_words": [
                    822000.0,
                    629000.0,
                    679000.0
                ],
                "wikipedia_search": [
                    0.7159090909090909,
                    2.526515151515152,
                    1.7575757575757576
                ],
                "result_count_noun_chunks": [
                    13200.0,
                    42800.0,
                    52900.0
                ],
                "word_relation_to_question": [
                    1.7043904518329072,
                    1.173763853367434,
                    2.121845694799659
                ],
                "word_count_noun_chunks": [
                    0.0,
                    5.0,
                    286.0
                ],
                "word_count_raw": [
                    0.0,
                    5.0,
                    310.0
                ],
                "result_count": [
                    1140.0,
                    1200.0,
                    20300.0
                ],
                "word_count_appended": [
                    177.0,
                    204.0,
                    557.0
                ]
            },
            "integer_answers": {
                "sake": 1,
                "anime": 1,
                "karaoke": 6
            }
        },
        "lines": [
            [
                0,
                0.05035335689045936,
                0.38591549295774646,
                0.12121212121212122,
                0.1431818181818182,
                0.1886993603411514,
                0.0,
                0.0,
                0.34087809036658145
            ],
            [
                0,
                0.053003533568904596,
                0.2953051643192488,
                0.39302112029384756,
                0.5053030303030304,
                0.21748400852878466,
                0.01718213058419244,
                0.015873015873015872,
                0.2347527706734868
            ],
            [
                1,
                0.8966431095406361,
                0.3187793427230047,
                0.4857667584940312,
                0.3515151515151515,
                0.593816631130064,
                0.9828178694158075,
                0.9841269841269841,
                0.42436913895993184
            ]
        ]
    },
    "What is Telluride, Colorado named after?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.4705406157504617,
                    0.5987902817955617,
                    0.6962060218374324,
                    0.0,
                    0.5213675213675214,
                    0,
                    0,
                    0.6325136612021858
                ],
                [
                    0.5287257456550887,
                    0.40054214795784193,
                    0.3032976728796735,
                    1.0,
                    0.4017094017094017,
                    0,
                    0,
                    0.13524590163934427
                ],
                [
                    0.0007336385944496446,
                    0.0006675702465964032,
                    0.0004963052828940112,
                    0.0,
                    0.07692307692307693,
                    0,
                    0,
                    0.23224043715846993
                ]
            ],
            "fraction_answers": {
                "a european city": 0.05184350470091448,
                "an element": 0.48656968365886044,
                "a governor": 0.4615868116402249
            },
            "question": "What is Telluride, Colorado named after?",
            "rate_limited": false,
            "answers": [
                "an element",
                "a governor",
                "a european city"
            ],
            "ml_answers": {
                "a european city": 0.22380845234698638,
                "an element": 0.40282805637365576,
                "a governor": 0.3000753679432724
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "a european city": 0,
                "an element": 4,
                "a governor": 2
            },
            "data": {
                "result_count_important_words": [
                    29600.0,
                    19800.0,
                    33.0
                ],
                "wikipedia_search": [
                    0.0,
                    1.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    50500.0,
                    22000.0,
                    36.0
                ],
                "word_relation_to_question": [
                    1.2650273224043715,
                    0.27049180327868855,
                    0.46448087431693985
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    61.0,
                    47.0,
                    9.0
                ],
                "result_count": [
                    18600.0,
                    20900.0,
                    29.0
                ]
            },
            "z-best_answer_by_ml": [
                "an element"
            ]
        },
        "lines": [
            [
                1,
                0.4705406157504617,
                0.5987902817955617,
                0.6962060218374324,
                0.0,
                0.5213675213675214,
                0,
                0,
                0.6325136612021858
            ],
            [
                0,
                0.5287257456550887,
                0.40054214795784193,
                0.3032976728796735,
                1.0,
                0.4017094017094017,
                0,
                0,
                0.13524590163934427
            ],
            [
                0,
                0.0007336385944496446,
                0.0006675702465964032,
                0.0004963052828940112,
                0.0,
                0.07692307692307693,
                0,
                0,
                0.23224043715846993
            ]
        ]
    },
    "Which of these is NOT a machine used for printing?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.49166431195252897,
                    0.49267498643516006,
                    0.46876419809177644,
                    0.3976897689768977,
                    0.3376835236541599,
                    0,
                    0,
                    0.44558861746361744
                ],
                [
                    0.10794009607233684,
                    0.10933260987520349,
                    0.101317582916856,
                    0.3335844454010618,
                    0.26835236541598695,
                    0,
                    0,
                    0.3377079002079002
                ],
                [
                    0.4003955919751342,
                    0.39799240368963645,
                    0.42991821899136756,
                    0.2687257856220405,
                    0.3939641109298532,
                    0,
                    0,
                    0.21670348232848236
                ]
            ],
            "fraction_answers": {
                "spirit duplicator": 0.29743346882116195,
                "hydraulophone": 0.12197819780861985,
                "hectograph": 0.5805883333702183
            },
            "question": "Which of these is NOT a machine used for printing?",
            "rate_limited": false,
            "answers": [
                "hydraulophone",
                "hectograph",
                "spirit duplicator"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "spirit duplicator": 0.34393939425547915,
                "hydraulophone": 0.34393939425547915,
                "hectograph": 0.32559523786462485
            },
            "integer_answers": {
                "spirit duplicator": 2,
                "hydraulophone": 0,
                "hectograph": 4
            },
            "data": {
                "result_count_important_words": [
                    2700.0,
                    144000.0,
                    37600.0
                ],
                "wikipedia_search": [
                    0.6138613861386139,
                    0.998493327593629,
                    1.387645286267757
                ],
                "result_count_noun_chunks": [
                    27500.0,
                    351000.0,
                    61700.0
                ],
                "word_relation_to_question": [
                    0.32646829521829523,
                    0.9737525987525988,
                    1.6997791060291059
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    2360.0,
                    111000.0,
                    28200.0
                ],
                "word_count_appended": [
                    199.0,
                    284.0,
                    130.0
                ]
            },
            "z-best_answer_by_ml": [
                "hydraulophone",
                "spirit duplicator"
            ]
        },
        "lines": [
            [
                1,
                0.49166431195252897,
                0.49267498643516006,
                0.46876419809177644,
                0.3976897689768977,
                0.3376835236541599,
                0,
                0,
                0.44558861746361744
            ],
            [
                0,
                0.10794009607233684,
                0.10933260987520349,
                0.101317582916856,
                0.3335844454010618,
                0.26835236541598695,
                0,
                0,
                0.3377079002079002
            ],
            [
                0,
                0.4003955919751342,
                0.39799240368963645,
                0.42991821899136756,
                0.2687257856220405,
                0.3939641109298532,
                0,
                0,
                0.21670348232848236
            ]
        ]
    },
    "One of Tupac Shakur\u2019s biggest posthumous hits samples what singer?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.02157848063848655,
                    0.0585325638911789,
                    0.020891799189273464,
                    0.35713597406130965,
                    0.16455696202531644,
                    0.0,
                    0.0,
                    0.40596064814814814
                ],
                [
                    0.8010641442506651,
                    0.5185490519373455,
                    0.8169628936700967,
                    0.462362635286293,
                    0.1518987341772152,
                    0.0,
                    0.0,
                    0.08697089947089948
                ],
                [
                    0.17735737511084837,
                    0.4229183841714757,
                    0.16214530714062989,
                    0.18050139065239737,
                    0.6835443037974683,
                    1.0,
                    1.0,
                    0.5070684523809524
                ]
            ],
            "fraction_answers": {
                "john mellencamp": 0.35472604484906434,
                "bruce hornsby": 0.5166919016567215,
                "christopher cross": 0.12858205349421414
            },
            "question": "One of Tupac Shakur\u2019s biggest posthumous hits samples what singer?",
            "rate_limited": false,
            "answers": [
                "christopher cross",
                "john mellencamp",
                "bruce hornsby"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "john mellencamp": 0.26516270432121314,
                "bruce hornsby": 0.4420214376981768,
                "christopher cross": 0.31157143216045696
            },
            "integer_answers": {
                "john mellencamp": 4,
                "bruce hornsby": 4,
                "christopher cross": 0
            },
            "data": {
                "result_count_important_words": [
                    71.0,
                    629.0,
                    513.0
                ],
                "wikipedia_search": [
                    1.4285438962452386,
                    1.849450541145172,
                    0.7220055626095895
                ],
                "result_count_noun_chunks": [
                    67.0,
                    2620.0,
                    520.0
                ],
                "word_relation_to_question": [
                    3.247685185185185,
                    0.6957671957671958,
                    4.0565476190476195
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    7.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    4.0
                ],
                "word_count_appended": [
                    13.0,
                    12.0,
                    54.0
                ],
                "result_count": [
                    73.0,
                    2710.0,
                    600.0
                ]
            },
            "z-best_answer_by_ml": [
                "bruce hornsby"
            ]
        },
        "lines": [
            [
                0,
                0.02157848063848655,
                0.0585325638911789,
                0.020891799189273464,
                0.35713597406130965,
                0.16455696202531644,
                0.0,
                0.0,
                0.40596064814814814
            ],
            [
                0,
                0.8010641442506651,
                0.5185490519373455,
                0.8169628936700967,
                0.462362635286293,
                0.1518987341772152,
                0.0,
                0.0,
                0.08697089947089948
            ],
            [
                1,
                0.17735737511084837,
                0.4229183841714757,
                0.16214530714062989,
                0.18050139065239737,
                0.6835443037974683,
                1.0,
                1.0,
                0.5070684523809524
            ]
        ]
    },
    "Which of these countries is closest to the International Date Line?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.44314689612784264,
                    0.4644766997708174,
                    0.06492411467116357,
                    0.32236730863275115,
                    0.32228915662650603,
                    0.3333333333333333,
                    0.25,
                    0.31761904761904763
                ],
                [
                    0.28457283343577133,
                    0.34835752482811305,
                    0.8263069139966274,
                    0.3777416673453997,
                    0.3453815261044177,
                    0.0,
                    0.25,
                    0.31253968253968256
                ],
                [
                    0.272280270436386,
                    0.18716577540106952,
                    0.1087689713322091,
                    0.29989102402184914,
                    0.33232931726907633,
                    0.6666666666666666,
                    0.5,
                    0.36984126984126986
                ]
            ],
            "fraction_answers": {
                "brazil": 0.34311251853125146,
                "japan": 0.31476956959768276,
                "spain": 0.3421179118710658
            },
            "question": "Which of these countries is closest to the International Date Line?",
            "rate_limited": false,
            "answers": [
                "japan",
                "brazil",
                "spain"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "brazil": 0.27595726218627614,
                "japan": 0.29448244372114396,
                "spain": 0.3729554388261791
            },
            "z-best_answer_by_ml": [
                "spain"
            ],
            "data": {
                "result_count_important_words": [
                    6080000.0,
                    4560000.0,
                    2450000.0
                ],
                "wikipedia_search": [
                    1.2894692345310046,
                    1.5109666693815988,
                    1.1995640960873966
                ],
                "result_count_noun_chunks": [
                    3850000.0,
                    49000000.0,
                    6450000.0
                ],
                "word_relation_to_question": [
                    1.2704761904761905,
                    1.2501587301587302,
                    1.4793650793650794
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    2.0
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    2.0
                ],
                "result_count": [
                    7210000.0,
                    4630000.0,
                    4430000.0
                ],
                "word_count_appended": [
                    321.0,
                    344.0,
                    331.0
                ]
            },
            "integer_answers": {
                "brazil": 3,
                "japan": 2,
                "spain": 3
            }
        },
        "lines": [
            [
                1,
                0.44314689612784264,
                0.4644766997708174,
                0.06492411467116357,
                0.32236730863275115,
                0.32228915662650603,
                0.3333333333333333,
                0.25,
                0.31761904761904763
            ],
            [
                0,
                0.28457283343577133,
                0.34835752482811305,
                0.8263069139966274,
                0.3777416673453997,
                0.3453815261044177,
                0.0,
                0.25,
                0.31253968253968256
            ],
            [
                0,
                0.272280270436386,
                0.18716577540106952,
                0.1087689713322091,
                0.29989102402184914,
                0.33232931726907633,
                0.6666666666666666,
                0.5,
                0.36984126984126986
            ]
        ]
    },
    "By definition, an Anglophile would be most interested in which of these things?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.40955946932926546,
                    0.09613557252071477,
                    0.4094778007821486,
                    0.26666666666666666,
                    0.7175925925925926,
                    0,
                    0.5,
                    0.39999999999999997
                ],
                [
                    0.589839596912125,
                    0.9034963528567175,
                    0.5898320680929376,
                    0.7333333333333334,
                    0.24537037037037038,
                    0,
                    0.5,
                    0.6
                ],
                [
                    0.0006009337586095317,
                    0.00036807462256773665,
                    0.0006901311249137336,
                    0.0,
                    0.037037037037037035,
                    0,
                    0.0,
                    0.0
                ]
            ],
            "fraction_answers": {
                "geometry": 0.39991887169876966,
                "downton abbey": 0.5945531030807834,
                "trout fishing": 0.005528025220446861
            },
            "question": "By definition, an Anglophile would be most interested in which of these things?",
            "rate_limited": false,
            "answers": [
                "geometry",
                "downton abbey",
                "trout fishing"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "geometry": 0.3635484425931417,
                "downton abbey": 0.27595726218627614,
                "trout fishing": 0.29035060350393144
            },
            "z-best_answer_by_ml": [
                "geometry"
            ],
            "data": {
                "result_count_important_words": [
                    216000.0,
                    2030000.0,
                    827.0
                ],
                "wikipedia_search": [
                    0.8,
                    2.2,
                    0.0
                ],
                "result_count_noun_chunks": [
                    44500.0,
                    64100.0,
                    75.0
                ],
                "word_relation_to_question": [
                    1.2,
                    1.8,
                    0.0
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    0.0
                ],
                "word_count_appended": [
                    155.0,
                    53.0,
                    8.0
                ],
                "result_count": [
                    44300.0,
                    63800.0,
                    65.0
                ]
            },
            "integer_answers": {
                "geometry": 2,
                "downton abbey": 5,
                "trout fishing": 0
            }
        },
        "lines": [
            [
                0,
                0.40955946932926546,
                0.09613557252071477,
                0.4094778007821486,
                0.26666666666666666,
                0.7175925925925926,
                0,
                0.5,
                0.39999999999999997
            ],
            [
                1,
                0.589839596912125,
                0.9034963528567175,
                0.5898320680929376,
                0.7333333333333334,
                0.24537037037037038,
                0,
                0.5,
                0.6
            ],
            [
                0,
                0.0006009337586095317,
                0.00036807462256773665,
                0.0006901311249137336,
                0.0,
                0.037037037037037035,
                0,
                0.0,
                0.0
            ]
        ]
    },
    "Who is NOT considered an official member of the Eagles?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.49969319146158364,
                    0.3840820854132002,
                    0.3868131868131868,
                    0.3856070087609512,
                    0.4353932584269663,
                    0.5,
                    0.5,
                    0.4015066338440978
                ],
                [
                    0.49968666362034075,
                    0.28369384359400995,
                    0.2824175824175824,
                    0.2327385422254008,
                    0.2556179775280899,
                    0.25,
                    0.2,
                    0.2942461141340788
                ],
                [
                    0.0006201449180756047,
                    0.33222407099278983,
                    0.33076923076923076,
                    0.38165444901364803,
                    0.3089887640449438,
                    0.25,
                    0.3,
                    0.3042472520218234
                ]
            ],
            "fraction_answers": {
                "j.d. souther": 0.12672615882000351,
                "randy meisner": 0.42539981912012426,
                "bernie leadon": 0.4478740220598722
            },
            "question": "Who is NOT considered an official member of the Eagles?",
            "rate_limited": false,
            "answers": [
                "j.d. souther",
                "randy meisner",
                "bernie leadon"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "j.d. souther": 0.4911473732310635,
                "randy meisner": 0.27767891177194265,
                "bernie leadon": 0.1897073222779066
            },
            "integer_answers": {
                "j.d. souther": 0,
                "randy meisner": 7,
                "bernie leadon": 1
            },
            "data": {
                "result_count_important_words": [
                    83600.0,
                    156000.0,
                    121000.0
                ],
                "wikipedia_search": [
                    1.143929912390488,
                    2.6726145777459918,
                    1.1834555098635198
                ],
                "result_count_noun_chunks": [
                    103000.0,
                    198000.0,
                    154000.0
                ],
                "word_relation_to_question": [
                    0.9849336615590223,
                    2.057538858659212,
                    1.9575274797817659
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    3.0,
                    2.0
                ],
                "word_count_appended": [
                    46.0,
                    174.0,
                    136.0
                ],
                "result_count": [
                    94.0,
                    96.0,
                    153000.0
                ]
            },
            "z-best_answer_by_ml": [
                "j.d. souther"
            ]
        },
        "lines": [
            [
                1,
                0.49969319146158364,
                0.3840820854132002,
                0.3868131868131868,
                0.3856070087609512,
                0.4353932584269663,
                0.5,
                0.5,
                0.4015066338440978
            ],
            [
                0,
                0.49968666362034075,
                0.28369384359400995,
                0.2824175824175824,
                0.2327385422254008,
                0.2556179775280899,
                0.25,
                0.2,
                0.2942461141340788
            ],
            [
                0,
                0.0006201449180756047,
                0.33222407099278983,
                0.33076923076923076,
                0.38165444901364803,
                0.3089887640449438,
                0.25,
                0.3,
                0.3042472520218234
            ]
        ]
    },
    "What SportsCenter anchor shares their last name with Linus & Lucy from \u201cPeanuts\u201d?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.3333333333333333,
                    0.4523809523809524,
                    0.003116938950988822,
                    0.5222222222222223,
                    0.16666666666666666,
                    0.5,
                    0.16666666666666666,
                    0.30535624285624285
                ],
                [
                    0.3333333333333333,
                    0.34523809523809523,
                    0.24989251934651763,
                    0.13888888888888887,
                    0.6666666666666666,
                    0.0,
                    0.6666666666666666,
                    0.4055269055269055
                ],
                [
                    0.3333333333333333,
                    0.20238095238095238,
                    0.7469905417024936,
                    0.33888888888888885,
                    0.16666666666666666,
                    0.5,
                    0.16666666666666666,
                    0.28911685161685163
                ]
            ],
            "fraction_answers": {
                "jemele hill": 0.3062178778846341,
                "kenny mayne": 0.34300548765698163,
                "scott van pelt": 0.3507766344583842
            },
            "question": "What SportsCenter anchor shares their last name with Linus & Lucy from \u201cPeanuts\u201d?",
            "rate_limited": false,
            "answers": [
                "jemele hill",
                "scott van pelt",
                "kenny mayne"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "jemele hill": 0.3729554388261791,
                "kenny mayne": 0.35443025729131133,
                "scott van pelt": 0.3635484425931417
            },
            "integer_answers": {
                "jemele hill": 4,
                "kenny mayne": 1,
                "scott van pelt": 3
            },
            "data": {
                "result_count_important_words": [
                    38.0,
                    29.0,
                    17.0
                ],
                "wikipedia_search": [
                    1.5666666666666667,
                    0.41666666666666663,
                    1.0166666666666666
                ],
                "result_count_noun_chunks": [
                    58.0,
                    4650.0,
                    13900.0
                ],
                "word_relation_to_question": [
                    1.2214249714249714,
                    1.622107622107622,
                    1.1564674064674065
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    2.0
                ],
                "word_count_raw": [
                    1.0,
                    4.0,
                    1.0
                ],
                "result_count": [
                    163000.0,
                    163000.0,
                    163000.0
                ],
                "word_count_appended": [
                    1.0,
                    4.0,
                    1.0
                ]
            },
            "z-best_answer_by_ml": [
                "jemele hill"
            ]
        },
        "lines": [
            [
                0,
                0.3333333333333333,
                0.4523809523809524,
                0.003116938950988822,
                0.5222222222222223,
                0.16666666666666666,
                0.5,
                0.16666666666666666,
                0.30535624285624285
            ],
            [
                1,
                0.3333333333333333,
                0.34523809523809523,
                0.24989251934651763,
                0.13888888888888887,
                0.6666666666666666,
                0.0,
                0.6666666666666666,
                0.4055269055269055
            ],
            [
                0,
                0.3333333333333333,
                0.20238095238095238,
                0.7469905417024936,
                0.33888888888888885,
                0.16666666666666666,
                0.5,
                0.16666666666666666,
                0.28911685161685163
            ]
        ]
    },
    "How do you let someone on Tinder know you're interested?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.9982365537221313,
                    0.9981521090692159,
                    0.999458443222395,
                    0.47840668750202936,
                    0.8317757009345794,
                    1.0,
                    1.0,
                    0.45289056356081764
                ],
                [
                    0.0,
                    0.0008439186927990455,
                    0.00012768411829713414,
                    0.029962411777542358,
                    0.028037383177570093,
                    0.0,
                    0.0,
                    0.2595067217368151
                ],
                [
                    0.0017634462778687492,
                    0.0010039722379850714,
                    0.00041387265930795206,
                    0.4916309007204283,
                    0.14018691588785046,
                    0.0,
                    0.0,
                    0.28760271470236737
                ]
            ],
            "fraction_answers": {
                "draw circle around face": 0.03980976493787797,
                "shake phone": 0.11532522781072599,
                "swipe right": 0.844865007251396
            },
            "question": "How do you let someone on Tinder know you're interested?",
            "rate_limited": false,
            "answers": [
                "swipe right",
                "draw circle around face",
                "shake phone"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "draw circle around face": 0.22380845234698638,
                "shake phone": 0.19990142425800322,
                "swipe right": 0.7224139102363077
            },
            "z-best_answer_by_ml": [
                "swipe right"
            ],
            "data": {
                "result_count_important_words": [
                    68600.0,
                    58.0,
                    69.0
                ],
                "wikipedia_search": [
                    2.870440125012176,
                    0.17977447066525415,
                    2.94978540432257
                ],
                "result_count_noun_chunks": [
                    227000.0,
                    29.0,
                    94.0
                ],
                "word_relation_to_question": [
                    1.8115622542432703,
                    1.0380268869472602,
                    1.1504108588094693
                ],
                "word_count_noun_chunks": [
                    10.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    8.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    31700.0,
                    0,
                    56.0
                ],
                "word_count_appended": [
                    89.0,
                    3.0,
                    15.0
                ]
            },
            "integer_answers": {
                "draw circle around face": 0,
                "shake phone": 1,
                "swipe right": 7
            }
        },
        "lines": [
            [
                1,
                0.9982365537221313,
                0.9981521090692159,
                0.999458443222395,
                0.47840668750202936,
                0.8317757009345794,
                1.0,
                1.0,
                0.45289056356081764
            ],
            [
                0,
                0.0,
                0.0008439186927990455,
                0.00012768411829713414,
                0.029962411777542358,
                0.028037383177570093,
                0.0,
                0.0,
                0.2595067217368151
            ],
            [
                0,
                0.0017634462778687492,
                0.0010039722379850714,
                0.00041387265930795206,
                0.4916309007204283,
                0.14018691588785046,
                0.0,
                0.0,
                0.28760271470236737
            ]
        ]
    },
    "Which of these sharks is NOT a Lamniforme?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.47027042478989195,
                    0.4777684563758389,
                    0.4513154199045033,
                    0.4302948619210034,
                    0.2869897959183674,
                    0,
                    0,
                    0.3169626593806921
                ],
                [
                    0.05405637184837919,
                    0.04488255033557048,
                    0.08852167400056177,
                    0.16195951149741444,
                    0.3010204081632653,
                    0,
                    0,
                    0.3027094717668488
                ],
                [
                    0.4756732033617289,
                    0.4773489932885906,
                    0.46016290609493493,
                    0.40774562658158214,
                    0.4119897959183674,
                    0,
                    0,
                    0.380327868852459
                ]
            ],
            "fraction_answers": {
                "goblin shark": 0.18879946056990096,
                "great white shark": 0.6822833374626533,
                "hammerhead shark": 0.1289172019674457
            },
            "question": "Which of these sharks is NOT a Lamniforme?",
            "rate_limited": false,
            "answers": [
                "goblin shark",
                "great white shark",
                "hammerhead shark"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "goblin shark": 0.28638353297773866,
                "great white shark": 0.31157143216045696,
                "hammerhead shark": 0.3739747133846042
            },
            "z-best_answer_by_ml": [
                "hammerhead shark"
            ],
            "data": {
                "result_count_important_words": [
                    106000.0,
                    2170000.0,
                    108000.0
                ],
                "wikipedia_search": [
                    0.27882055231598635,
                    1.3521619540103422,
                    0.3690174936736715
                ],
                "result_count_noun_chunks": [
                    104000.0,
                    879000.0,
                    85100.0
                ],
                "word_relation_to_question": [
                    1.098224043715847,
                    1.1837431693989071,
                    0.7180327868852459
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    104000.0,
                    1560000.0,
                    85100.0
                ],
                "word_count_appended": [
                    167.0,
                    156.0,
                    69.0
                ]
            },
            "integer_answers": {
                "goblin shark": 1,
                "great white shark": 5,
                "hammerhead shark": 0
            }
        },
        "lines": [
            [
                0,
                0.47027042478989195,
                0.4777684563758389,
                0.4513154199045033,
                0.4302948619210034,
                0.2869897959183674,
                0,
                0,
                0.3169626593806921
            ],
            [
                0,
                0.05405637184837919,
                0.04488255033557048,
                0.08852167400056177,
                0.16195951149741444,
                0.3010204081632653,
                0,
                0,
                0.3027094717668488
            ],
            [
                1,
                0.4756732033617289,
                0.4773489932885906,
                0.46016290609493493,
                0.40774562658158214,
                0.4119897959183674,
                0,
                0,
                0.380327868852459
            ]
        ]
    },
    "Which of these restaurant brands has its original location in Europe?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.06016745908192031,
                    0.29791721592407067,
                    0.0672623723487824,
                    0.6653508771929825,
                    0.9,
                    0.5,
                    0.5,
                    0.3668186982892442
                ],
                [
                    0.9046156435393613,
                    0.5378328499868178,
                    0.8935585231736056,
                    0.2655701754385965,
                    0.0069767441860465115,
                    0.0,
                    0.0,
                    0.39854049642325606
                ],
                [
                    0.035216897378718394,
                    0.1642499340891115,
                    0.03917910447761194,
                    0.06907894736842105,
                    0.09302325581395349,
                    0.5,
                    0.5,
                    0.2346408052874998
                ]
            ],
            "fraction_answers": {
                "mr. chow": 0.2044236180519145,
                "benihana": 0.41968957785462496,
                "p.f. chang's": 0.3758868040934604
            },
            "question": "Which of these restaurant brands has its original location in Europe?",
            "rate_limited": false,
            "answers": [
                "benihana",
                "p.f. chang's",
                "mr. chow"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "mr. chow": 0.3796181564740296,
                "benihana": 0.4420214376981768,
                "p.f. chang's": 0.28638353297773866
            },
            "integer_answers": {
                "mr. chow": 0,
                "benihana": 4,
                "p.f. chang's": 4
            },
            "data": {
                "result_count_important_words": [
                    113000.0,
                    204000.0,
                    62300.0
                ],
                "wikipedia_search": [
                    2.66140350877193,
                    1.062280701754386,
                    0.2763157894736842
                ],
                "result_count_noun_chunks": [
                    137000.0,
                    1820000.0,
                    79800.0
                ],
                "word_relation_to_question": [
                    1.8340934914462208,
                    1.9927024821162802,
                    1.173204026437499
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    1.0
                ],
                "word_count_appended": [
                    387.0,
                    3.0,
                    40.0
                ],
                "result_count": [
                    143000.0,
                    2150000.0,
                    83700.0
                ]
            },
            "z-best_answer_by_ml": [
                "benihana"
            ]
        },
        "lines": [
            [
                0,
                0.06016745908192031,
                0.29791721592407067,
                0.0672623723487824,
                0.6653508771929825,
                0.9,
                0.5,
                0.5,
                0.3668186982892442
            ],
            [
                0,
                0.9046156435393613,
                0.5378328499868178,
                0.8935585231736056,
                0.2655701754385965,
                0.0069767441860465115,
                0.0,
                0.0,
                0.39854049642325606
            ],
            [
                1,
                0.035216897378718394,
                0.1642499340891115,
                0.03917910447761194,
                0.06907894736842105,
                0.09302325581395349,
                0.5,
                0.5,
                0.2346408052874998
            ]
        ]
    },
    "In which of these movies is the title NOT spoken by any character?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.4999938803461849,
                    0.31842818428184283,
                    0.41583166332665333,
                    0.1344883691284378,
                    0.30863039399624764,
                    0.5,
                    0.5,
                    0.2204986613119143
                ],
                [
                    0.4999948568866873,
                    0.3685636856368564,
                    0.2685370741482966,
                    0.39951875848944185,
                    0.3292682926829268,
                    0.5,
                    0.5,
                    0.40100401606425706
                ],
                [
                    1.1262767127762086e-05,
                    0.3130081300813008,
                    0.31563126252505014,
                    0.4659928723821203,
                    0.3621013133208255,
                    0.0,
                    0.0,
                    0.3784973226238286
                ]
            ],
            "fraction_answers": {
                "inception": 0.2755322119021798,
                "speed": 0.5411894590749367,
                "gravity": 0.18327832902288352
            },
            "question": "In which of these movies is the title NOT spoken by any character?",
            "rate_limited": false,
            "answers": [
                "inception",
                "gravity",
                "speed"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "inception": 0.35443025729131133,
                "speed": 0.3739747133846042,
                "gravity": 0.3729554388261791
            },
            "integer_answers": {
                "inception": 3,
                "speed": 4,
                "gravity": 1
            },
            "data": {
                "result_count_important_words": [
                    1340000.0,
                    970000.0,
                    1380000.0
                ],
                "wikipedia_search": [
                    2.924093046972497,
                    0.8038499320844649,
                    0.27205702094303785
                ],
                "result_count_noun_chunks": [
                    2520000.0,
                    6930000.0,
                    5520000.0
                ],
                "word_relation_to_question": [
                    2.2360107095046855,
                    0.7919678714859437,
                    0.9720214190093709
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_appended": [
                    408.0,
                    364.0,
                    294.0
                ],
                "result_count": [
                    94.0,
                    79.0,
                    7680000.0
                ]
            },
            "z-best_answer_by_ml": [
                "speed"
            ]
        },
        "lines": [
            [
                0,
                0.4999938803461849,
                0.31842818428184283,
                0.41583166332665333,
                0.1344883691284378,
                0.30863039399624764,
                0.5,
                0.5,
                0.2204986613119143
            ],
            [
                1,
                0.4999948568866873,
                0.3685636856368564,
                0.2685370741482966,
                0.39951875848944185,
                0.3292682926829268,
                0.5,
                0.5,
                0.40100401606425706
            ],
            [
                0,
                1.1262767127762086e-05,
                0.3130081300813008,
                0.31563126252505014,
                0.4659928723821203,
                0.3621013133208255,
                0.0,
                0.0,
                0.3784973226238286
            ]
        ]
    },
    "Tom from MySpace shares his name with a key character in what film franchise?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.9087325054208555,
                    0.8240680183126227,
                    0.7928870292887029,
                    0.23463153875525009,
                    0.3514644351464435,
                    0,
                    0.1111111111111111,
                    0.4141629044646286
                ],
                [
                    0.023851764242065837,
                    0.1026814911706998,
                    0.11338912133891213,
                    0.2946366297569047,
                    0.16736401673640167,
                    0,
                    0.0,
                    0.34057459542373336
                ],
                [
                    0.06741573033707865,
                    0.07325049051667756,
                    0.09372384937238494,
                    0.4707318314878452,
                    0.4811715481171548,
                    0,
                    0.8888888888888888,
                    0.24526250011163805
                ]
            ],
            "fraction_answers": {
                "the godfather": 0.14892823123838822,
                "the matrix": 0.3314921198330954,
                "harry potter": 0.5195796489285163
            },
            "question": "Tom from MySpace shares his name with a key character in what film franchise?",
            "rate_limited": false,
            "answers": [
                "harry potter",
                "the godfather",
                "the matrix"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "the godfather": 0.10911205857899192,
                "the matrix": 0.5194682033924446,
                "harry potter": 0.3363331833913913
            },
            "z-best_answer_by_ml": [
                "the matrix"
            ],
            "data": {
                "result_count_important_words": [
                    378000.0,
                    47100.0,
                    33600.0
                ],
                "wikipedia_search": [
                    1.1731576937762505,
                    1.4731831487845235,
                    2.353659157439226
                ],
                "result_count_noun_chunks": [
                    379000.0,
                    54200.0,
                    44800.0
                ],
                "word_relation_to_question": [
                    2.4849774267877716,
                    2.0434475725424,
                    1.4715750006698283
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    8.0
                ],
                "result_count": [
                    4610000.0,
                    121000.0,
                    342000.0
                ],
                "word_count_appended": [
                    84.0,
                    40.0,
                    115.0
                ]
            },
            "integer_answers": {
                "the godfather": 0,
                "the matrix": 3,
                "harry potter": 4
            }
        },
        "lines": [
            [
                0,
                0.9087325054208555,
                0.8240680183126227,
                0.7928870292887029,
                0.23463153875525009,
                0.3514644351464435,
                0,
                0.1111111111111111,
                0.4141629044646286
            ],
            [
                0,
                0.023851764242065837,
                0.1026814911706998,
                0.11338912133891213,
                0.2946366297569047,
                0.16736401673640167,
                0,
                0.0,
                0.34057459542373336
            ],
            [
                1,
                0.06741573033707865,
                0.07325049051667756,
                0.09372384937238494,
                0.4707318314878452,
                0.4811715481171548,
                0,
                0.8888888888888888,
                0.24526250011163805
            ]
        ]
    },
    "Which player won Rookie of the Year in their sport most recently?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.264,
                    0.2972972972972973,
                    0.00010173682225549507,
                    0.39216705261113155,
                    0.20212765957446807,
                    0.5,
                    1.0,
                    0.4239793532357275
                ],
                [
                    0.32,
                    0.28378378378378377,
                    0.20347364451099015,
                    0.16188290419375945,
                    0.40425531914893614,
                    0.0,
                    0.0,
                    0.185642437066045
                ],
                [
                    0.416,
                    0.4189189189189189,
                    0.7964246186667544,
                    0.445950043195109,
                    0.39361702127659576,
                    0.5,
                    0.0,
                    0.39037820969822756
                ]
            ],
            "fraction_answers": {
                "mike trout": 0.38495913744261,
                "von miller": 0.19487976108793933,
                "blake griffin": 0.42016110146945074
            },
            "question": "Which player won Rookie of the Year in their sport most recently?",
            "rate_limited": false,
            "answers": [
                "mike trout",
                "von miller",
                "blake griffin"
            ],
            "ml_answers": {
                "mike trout": 0.6863446736251442,
                "von miller": 0.2713581334649178,
                "blake griffin": 0.40264468233565515
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "mike trout": 3,
                "von miller": 1,
                "blake griffin": 4
            },
            "data": {
                "result_count_important_words": [
                    22.0,
                    21.0,
                    31.0
                ],
                "wikipedia_search": [
                    1.9608352630556578,
                    0.8094145209687973,
                    2.229750215975545
                ],
                "result_count_noun_chunks": [
                    99.0,
                    198000.0,
                    775000.0
                ],
                "word_relation_to_question": [
                    2.543876119414365,
                    1.11385462239627,
                    2.3422692581893654
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    2.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    38.0,
                    76.0,
                    74.0
                ],
                "result_count": [
                    33.0,
                    40.0,
                    52.0
                ]
            },
            "z-best_answer_by_ml": [
                "mike trout"
            ]
        },
        "lines": [
            [
                1,
                0.264,
                0.2972972972972973,
                0.00010173682225549507,
                0.39216705261113155,
                0.20212765957446807,
                0.5,
                1.0,
                0.4239793532357275
            ],
            [
                0,
                0.32,
                0.28378378378378377,
                0.20347364451099015,
                0.16188290419375945,
                0.40425531914893614,
                0.0,
                0.0,
                0.185642437066045
            ],
            [
                0,
                0.416,
                0.4189189189189189,
                0.7964246186667544,
                0.445950043195109,
                0.39361702127659576,
                0.5,
                0.0,
                0.39037820969822756
            ]
        ]
    },
    "In computing, what unit is half a byte?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.47420333839150225,
                    0.4742813918305598,
                    0.4742813918305598,
                    1.0,
                    0.6628041714947857,
                    0.8538011695906432,
                    0.8588235294117647,
                    0.3321169278216312
                ],
                [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.01853997682502897,
                    0.0,
                    0.0,
                    0.2561539234401688
                ],
                [
                    0.5257966616084977,
                    0.5257186081694403,
                    0.5257186081694403,
                    0.0,
                    0.3186558516801854,
                    0.14619883040935672,
                    0.1411764705882353,
                    0.4117291487382
                ]
            ],
            "fraction_answers": {
                "demibyte": 0.03433673753314972,
                "nibble": 0.6412889900464308,
                "octet": 0.3243742724204195
            },
            "question": "In computing, what unit is half a byte?",
            "rate_limited": false,
            "answers": [
                "nibble",
                "demibyte",
                "octet"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "demibyte": 0.32559523786462485,
                "nibble": 0.34393939425547915,
                "octet": 0.32559523786462485
            },
            "integer_answers": {
                "demibyte": 0,
                "nibble": 4,
                "octet": 4
            },
            "data": {
                "result_count_important_words": [
                    627000.0,
                    0,
                    695000.0
                ],
                "wikipedia_search": [
                    4.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    627000.0,
                    0,
                    695000.0
                ],
                "word_relation_to_question": [
                    1.3284677112865249,
                    1.0246156937606752,
                    1.6469165949528
                ],
                "word_count_noun_chunks": [
                    146.0,
                    0.0,
                    25.0
                ],
                "word_count_raw": [
                    146.0,
                    0.0,
                    24.0
                ],
                "word_count_appended": [
                    572.0,
                    16.0,
                    275.0
                ],
                "result_count": [
                    625000.0,
                    0,
                    693000.0
                ]
            },
            "z-best_answer_by_ml": [
                "nibble"
            ]
        },
        "lines": [
            [
                1,
                0.47420333839150225,
                0.4742813918305598,
                0.4742813918305598,
                1.0,
                0.6628041714947857,
                0.8538011695906432,
                0.8588235294117647,
                0.3321169278216312
            ],
            [
                0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.01853997682502897,
                0.0,
                0.0,
                0.2561539234401688
            ],
            [
                0,
                0.5257966616084977,
                0.5257186081694403,
                0.5257186081694403,
                0.0,
                0.3186558516801854,
                0.14619883040935672,
                0.1411764705882353,
                0.4117291487382
            ]
        ]
    },
    "Which of these has NEVER been named Pantone\u2019s Color of the Year?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.3615586977967774,
                    0.45938485804416407,
                    0.2154963680387409,
                    0.26118521293867863,
                    0.17808219178082194,
                    0,
                    0.33333333333333337,
                    0.4456653225806452
                ],
                [
                    0.14156527458073004,
                    0.21608832807570977,
                    0.486682808716707,
                    0.4236090295991008,
                    0.4109589041095891,
                    0,
                    0.33333333333333337,
                    0.36078629032258064
                ],
                [
                    0.4968760276224926,
                    0.32452681388012616,
                    0.29782082324455206,
                    0.31520575746222057,
                    0.4109589041095891,
                    0,
                    0.33333333333333337,
                    0.1935483870967742
                ]
            ],
            "fraction_answers": {
                "chili pepper": 0.3222085580716892,
                "cucumber": 0.35579829013909675,
                "sand dollar": 0.32199315178921406
            },
            "question": "Which of these has NEVER been named Pantone\u2019s Color of the Year?",
            "rate_limited": false,
            "answers": [
                "cucumber",
                "sand dollar",
                "chili pepper"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "chili pepper": 0.34232761393661615,
                "cucumber": 0.29448244372114396,
                "sand dollar": 0.3635484425931417
            },
            "z-best_answer_by_ml": [
                "sand dollar"
            ],
            "data": {
                "result_count_important_words": [
                    618000.0,
                    4320000.0,
                    2670000.0
                ],
                "wikipedia_search": [
                    2.3881478706132135,
                    0.763909704008992,
                    1.8479424253777945
                ],
                "result_count_noun_chunks": [
                    23500000.0,
                    1100000.0,
                    16700000.0
                ],
                "word_relation_to_question": [
                    0.4346774193548387,
                    1.113709677419355,
                    2.4516129032258065
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    1.0
                ],
                "result_count": [
                    4210000.0,
                    10900000.0,
                    95000.0
                ],
                "word_count_appended": [
                    188.0,
                    52.0,
                    52.0
                ]
            },
            "integer_answers": {
                "chili pepper": 1,
                "cucumber": 4,
                "sand dollar": 2
            }
        },
        "lines": [
            [
                1,
                0.3615586977967774,
                0.45938485804416407,
                0.2154963680387409,
                0.26118521293867863,
                0.17808219178082194,
                0,
                0.33333333333333337,
                0.4456653225806452
            ],
            [
                0,
                0.14156527458073004,
                0.21608832807570977,
                0.486682808716707,
                0.4236090295991008,
                0.4109589041095891,
                0,
                0.33333333333333337,
                0.36078629032258064
            ],
            [
                0,
                0.4968760276224926,
                0.32452681388012616,
                0.29782082324455206,
                0.31520575746222057,
                0.4109589041095891,
                0,
                0.33333333333333337,
                0.1935483870967742
            ]
        ]
    },
    "What book that heavily influenced \u201cThe Matrix\u201d makes a cameo in the movie?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.7633043230132687,
                    0.12508164598301763,
                    0.11539477158700819,
                    0.8084415584415584,
                    0.9905362776025236,
                    0,
                    1.0,
                    0.6100188489700874
                ],
                [
                    0.1804822371236981,
                    0.8491182233834096,
                    0.8027462371270134,
                    0.08311688311688312,
                    0.0,
                    0,
                    0.0,
                    0.17469230438464187
                ],
                [
                    0.056213439863033246,
                    0.025800130633572827,
                    0.08185899128597834,
                    0.10844155844155844,
                    0.00946372239747634,
                    0,
                    0.0,
                    0.2152888466452708
                ]
            ],
            "fraction_answers": {
                "simulacra & simulation": 0.2985936978765209,
                "neuromancer": 0.630396775085352,
                "gravity's rainbow": 0.07100952703812714
            },
            "question": "What book that heavily influenced \u201cThe Matrix\u201d makes a cameo in the movie?",
            "rate_limited": false,
            "answers": [
                "neuromancer",
                "simulacra & simulation",
                "gravity's rainbow"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "simulacra & simulation": 0.5687960251100523,
                "neuromancer": 0.3280542701777351,
                "gravity's rainbow": 0.14603843893289
            },
            "integer_answers": {
                "simulacra & simulation": 2,
                "neuromancer": 5,
                "gravity's rainbow": 0
            },
            "data": {
                "result_count_important_words": [
                    383.0,
                    2600.0,
                    79.0
                ],
                "wikipedia_search": [
                    5.659090909090909,
                    0.5818181818181818,
                    0.759090909090909
                ],
                "result_count_noun_chunks": [
                    4370.0,
                    30400.0,
                    3100.0
                ],
                "word_relation_to_question": [
                    3.660113093820524,
                    1.0481538263078511,
                    1.2917330798716247
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    18.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    314.0,
                    0.0,
                    3.0
                ],
                "result_count": [
                    10700.0,
                    2530.0,
                    788.0
                ]
            },
            "z-best_answer_by_ml": [
                "simulacra & simulation"
            ]
        },
        "lines": [
            [
                0,
                0.7633043230132687,
                0.12508164598301763,
                0.11539477158700819,
                0.8084415584415584,
                0.9905362776025236,
                0,
                1.0,
                0.6100188489700874
            ],
            [
                1,
                0.1804822371236981,
                0.8491182233834096,
                0.8027462371270134,
                0.08311688311688312,
                0.0,
                0,
                0.0,
                0.17469230438464187
            ],
            [
                0,
                0.056213439863033246,
                0.025800130633572827,
                0.08185899128597834,
                0.10844155844155844,
                0.00946372239747634,
                0,
                0.0,
                0.2152888466452708
            ]
        ]
    },
    "Which of these things was created by a person who chose to remain anonymous?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.8141592920353983,
                    0.9098727210182318,
                    0.8159698884258637,
                    0.8973684210526316,
                    0.7584493041749503,
                    0,
                    1.0,
                    0.36538461538461536
                ],
                [
                    0.10644753476611883,
                    0.0500515995872033,
                    0.03212797419007931,
                    0.08947368421052632,
                    0.16998011928429424,
                    0,
                    0.0,
                    0.25384615384615383
                ],
                [
                    0.07939317319848294,
                    0.04007567939456484,
                    0.151902137384057,
                    0.013157894736842105,
                    0.07157057654075547,
                    0,
                    0.0,
                    0.38076923076923075
                ]
            ],
            "fraction_answers": {
                "hoverboards": 0.10027529512633941,
                "fidget spinners": 0.10526695600341902,
                "bitcoin": 0.7944577488702416
            },
            "question": "Which of these things was created by a person who chose to remain anonymous?",
            "rate_limited": false,
            "answers": [
                "bitcoin",
                "hoverboards",
                "fidget spinners"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "hoverboards": 0.31157143216045696,
                "fidget spinners": 0.31157143216045696,
                "bitcoin": 0.3635484425931417
            },
            "z-best_answer_by_ml": [
                "bitcoin"
            ],
            "data": {
                "result_count_important_words": [
                    529000.0,
                    29100.0,
                    23300.0
                ],
                "wikipedia_search": [
                    3.5894736842105264,
                    0.35789473684210527,
                    0.05263157894736842
                ],
                "result_count_noun_chunks": [
                    6070000.0,
                    239000.0,
                    1130000.0
                ],
                "word_relation_to_question": [
                    1.8269230769230769,
                    1.2692307692307692,
                    1.9038461538461537
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    27.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    763.0,
                    171.0,
                    72.0
                ],
                "result_count": [
                    322000.0,
                    42100.0,
                    31400.0
                ]
            },
            "integer_answers": {
                "hoverboards": 0,
                "fidget spinners": 1,
                "bitcoin": 6
            }
        },
        "lines": [
            [
                1,
                0.8141592920353983,
                0.9098727210182318,
                0.8159698884258637,
                0.8973684210526316,
                0.7584493041749503,
                0,
                1.0,
                0.36538461538461536
            ],
            [
                0,
                0.10644753476611883,
                0.0500515995872033,
                0.03212797419007931,
                0.08947368421052632,
                0.16998011928429424,
                0,
                0.0,
                0.25384615384615383
            ],
            [
                0,
                0.07939317319848294,
                0.04007567939456484,
                0.151902137384057,
                0.013157894736842105,
                0.07157057654075547,
                0,
                0.0,
                0.38076923076923075
            ]
        ]
    },
    "In which town were a President, Governor, Senator, NFL owner and late night host all born?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.9660297239915074,
                    0.24287118977384464,
                    0.6060105680317041,
                    0.38285929032272115,
                    0.3333333333333333,
                    0,
                    0,
                    0.2588050314465409
                ],
                [
                    0.01634819532908705,
                    0.022943297279580464,
                    0.012549537648612946,
                    0.17637664562531583,
                    0.16666666666666666,
                    0,
                    0,
                    0.2938482704402516
                ],
                [
                    0.01762208067940552,
                    0.7341855129465749,
                    0.38143989431968295,
                    0.44076406405196294,
                    0.5,
                    0,
                    0,
                    0.44734669811320754
                ]
            ],
            "fraction_answers": {
                "muncie, in": 0.4202263750184723,
                "hope, ar": 0.11478876883158577,
                "brookline, ma": 0.4649848561499419
            },
            "question": "In which town were a President, Governor, Senator, NFL owner and late night host all born?",
            "rate_limited": false,
            "answers": [
                "brookline, ma",
                "hope, ar",
                "muncie, in"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "muncie, in": 0.34393939425547915,
                "hope, ar": 0.32559523786462485,
                "brookline, ma": 0.34393939425547915
            },
            "integer_answers": {
                "muncie, in": 4,
                "hope, ar": 0,
                "brookline, ma": 2
            },
            "data": {
                "result_count_important_words": [
                    741.0,
                    70.0,
                    2240.0
                ],
                "wikipedia_search": [
                    3.062874322581769,
                    1.4110131650025266,
                    3.5261125124157036
                ],
                "result_count_noun_chunks": [
                    3670.0,
                    76.0,
                    2310.0
                ],
                "word_relation_to_question": [
                    2.070440251572327,
                    2.3507861635220126,
                    3.5787735849056603
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    6.0,
                    3.0,
                    9.0
                ],
                "result_count": [
                    4550.0,
                    77.0,
                    83.0
                ]
            },
            "z-best_answer_by_ml": [
                "brookline, ma",
                "muncie, in"
            ]
        },
        "lines": [
            [
                1,
                0.9660297239915074,
                0.24287118977384464,
                0.6060105680317041,
                0.38285929032272115,
                0.3333333333333333,
                0,
                0,
                0.2588050314465409
            ],
            [
                0,
                0.01634819532908705,
                0.022943297279580464,
                0.012549537648612946,
                0.17637664562531583,
                0.16666666666666666,
                0,
                0,
                0.2938482704402516
            ],
            [
                0,
                0.01762208067940552,
                0.7341855129465749,
                0.38143989431968295,
                0.44076406405196294,
                0.5,
                0,
                0,
                0.44734669811320754
            ]
        ]
    },
    "Though it now conveys something different, which of these words originally meant \u201cparrot\u201d?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.3415516851391088,
                    0.3510054844606947,
                    0.2838434631128723,
                    0.0,
                    0.3522727272727273,
                    0,
                    0.0,
                    0.14411764705882354
                ],
                [
                    0.21737163537661164,
                    0.2376599634369287,
                    0.7161018088991419,
                    0.041666666666666664,
                    0.3181818181818182,
                    0,
                    0.0,
                    0.28970588235294115
                ],
                [
                    0.4410766794842796,
                    0.4113345521023766,
                    5.472798798581525e-05,
                    0.9583333333333334,
                    0.32954545454545453,
                    0,
                    1.0,
                    0.5661764705882353
                ]
            ],
            "fraction_answers": {
                "thespian": 0.26009825355915833,
                "popinjay": 0.5295030311488094,
                "warble": 0.21039871529203236
            },
            "question": "Though it now conveys something different, which of these words originally meant \u201cparrot\u201d?",
            "rate_limited": false,
            "answers": [
                "warble",
                "thespian",
                "popinjay"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "thespian": 0.29304625062558914,
                "popinjay": 0.29448244372114396,
                "warble": 0.26516270432121314
            },
            "integer_answers": {
                "thespian": 1,
                "popinjay": 5,
                "warble": 1
            },
            "data": {
                "result_count_important_words": [
                    19200.0,
                    13000.0,
                    22500.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.125,
                    2.875
                ],
                "result_count_noun_chunks": [
                    306000.0,
                    772000.0,
                    59.0
                ],
                "word_relation_to_question": [
                    0.5764705882352942,
                    1.1588235294117646,
                    2.264705882352941
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_appended": [
                    124.0,
                    112.0,
                    116.0
                ],
                "result_count": [
                    15100.0,
                    9610.0,
                    19500.0
                ]
            },
            "z-best_answer_by_ml": [
                "popinjay"
            ]
        },
        "lines": [
            [
                0,
                0.3415516851391088,
                0.3510054844606947,
                0.2838434631128723,
                0.0,
                0.3522727272727273,
                0,
                0.0,
                0.14411764705882354
            ],
            [
                0,
                0.21737163537661164,
                0.2376599634369287,
                0.7161018088991419,
                0.041666666666666664,
                0.3181818181818182,
                0,
                0.0,
                0.28970588235294115
            ],
            [
                1,
                0.4410766794842796,
                0.4113345521023766,
                5.472798798581525e-05,
                0.9583333333333334,
                0.32954545454545453,
                0,
                1.0,
                0.5661764705882353
            ]
        ]
    },
    "What gargantuan fruit is the subject of a Roald Dahl children's book?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.001394959546173161,
                    0.0012794735880095047,
                    0.001394959546173161,
                    0.3448665442734077,
                    0.20689655172413793,
                    0.0,
                    0.0,
                    0.27351656571719596
                ],
                [
                    0.003533897516972008,
                    0.0025589471760190095,
                    0.003533897516972008,
                    0.26655839570682105,
                    0.15862068965517243,
                    0.0625,
                    0.09090909090909091,
                    0.20631195660082216
                ],
                [
                    0.9950711429368548,
                    0.9961615792359715,
                    0.9950711429368548,
                    0.3885750600197712,
                    0.6344827586206897,
                    0.9375,
                    0.9090909090909091,
                    0.5201714776819819
                ]
            ],
            "fraction_answers": {
                "loquat": 0.0993158593852337,
                "dragonfruit": 0.10366863179938718,
                "peach": 0.7970155088153791
            },
            "question": "What gargantuan fruit is the subject of a Roald Dahl children's book?",
            "rate_limited": false,
            "answers": [
                "dragonfruit",
                "loquat",
                "peach"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "loquat": 0.151284743352713,
                "dragonfruit": 0.15562150519059464,
                "peach": 0.6429398941929336
            },
            "z-best_answer_by_ml": [
                "peach"
            ],
            "data": {
                "result_count_important_words": [
                    14.0,
                    28.0,
                    10900.0
                ],
                "wikipedia_search": [
                    1.7243327213670385,
                    1.3327919785341054,
                    1.9428753000988561
                ],
                "result_count_noun_chunks": [
                    15.0,
                    38.0,
                    10700.0
                ],
                "word_relation_to_question": [
                    1.36758282858598,
                    1.0315597830041108,
                    2.6008573884099095
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    15.0
                ],
                "word_count_raw": [
                    0.0,
                    1.0,
                    10.0
                ],
                "result_count": [
                    15.0,
                    38.0,
                    10700.0
                ],
                "word_count_appended": [
                    60.0,
                    46.0,
                    184.0
                ]
            },
            "integer_answers": {
                "loquat": 0,
                "dragonfruit": 0,
                "peach": 8
            }
        },
        "lines": [
            [
                0,
                0.001394959546173161,
                0.0012794735880095047,
                0.001394959546173161,
                0.3448665442734077,
                0.20689655172413793,
                0.0,
                0.0,
                0.27351656571719596
            ],
            [
                0,
                0.003533897516972008,
                0.0025589471760190095,
                0.003533897516972008,
                0.26655839570682105,
                0.15862068965517243,
                0.0625,
                0.09090909090909091,
                0.20631195660082216
            ],
            [
                1,
                0.9950711429368548,
                0.9961615792359715,
                0.9950711429368548,
                0.3885750600197712,
                0.6344827586206897,
                0.9375,
                0.9090909090909091,
                0.5201714776819819
            ]
        ]
    },
    "The only person who owns more U.S. land than Ted Turner made his fortune in what business?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.2851063829787234,
                    0.3784188834769577,
                    0.3340961098398169,
                    0.33905134292231065,
                    0.8466257668711656,
                    0,
                    0,
                    0.2089308343186585
                ],
                [
                    0.2506769825918762,
                    0.26376920194829523,
                    0.2242562929061785,
                    0.3952115626309175,
                    0.0736196319018405,
                    0,
                    0,
                    0.48650493554012014
                ],
                [
                    0.46421663442940037,
                    0.3578119145747471,
                    0.4416475972540046,
                    0.26573709444677185,
                    0.07975460122699386,
                    0,
                    0,
                    0.3045642301412213
                ]
            ],
            "fraction_answers": {
                "pharmaceuticals": 0.39870488673460547,
                "cable tv": 0.28233976791987137,
                "fast food": 0.31895534534552317
            },
            "question": "The only person who owns more U.S. land than Ted Turner made his fortune in what business?",
            "rate_limited": false,
            "answers": [
                "pharmaceuticals",
                "cable tv",
                "fast food"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "pharmaceuticals": 0.18343915419143672,
                "cable tv": 0.13664301870661316,
                "fast food": 0.32842919687119937
            },
            "z-best_answer_by_ml": [
                "fast food"
            ],
            "data": {
                "result_count_important_words": [
                    101000.0,
                    70400.0,
                    95500.0
                ],
                "wikipedia_search": [
                    1.6952567146115534,
                    1.9760578131545874,
                    1.3286854722338592
                ],
                "result_count_noun_chunks": [
                    146000.0,
                    98000.0,
                    193000.0
                ],
                "word_relation_to_question": [
                    1.4625158402306095,
                    3.405534548780841,
                    2.131949610988549
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    138.0,
                    12.0,
                    13.0
                ],
                "result_count": [
                    73700.0,
                    64800.0,
                    120000.0
                ]
            },
            "integer_answers": {
                "pharmaceuticals": 2,
                "cable tv": 2,
                "fast food": 2
            }
        },
        "lines": [
            [
                0,
                0.2851063829787234,
                0.3784188834769577,
                0.3340961098398169,
                0.33905134292231065,
                0.8466257668711656,
                0,
                0,
                0.2089308343186585
            ],
            [
                1,
                0.2506769825918762,
                0.26376920194829523,
                0.2242562929061785,
                0.3952115626309175,
                0.0736196319018405,
                0,
                0,
                0.48650493554012014
            ],
            [
                0,
                0.46421663442940037,
                0.3578119145747471,
                0.4416475972540046,
                0.26573709444677185,
                0.07975460122699386,
                0,
                0,
                0.3045642301412213
            ]
        ]
    },
    "Which game is an example of combinatorics?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.12735849056603774,
                    0.14334213504337986,
                    0.1270643747893495,
                    0.14516129032258066,
                    0.31079717457114026,
                    0,
                    0,
                    0.6072961373390557
                ],
                [
                    0.41778975741239893,
                    0.34741606940777064,
                    0.41793056959892144,
                    0.3544142614601019,
                    0.06357214934409687,
                    0,
                    0,
                    0.14163090128755365
                ],
                [
                    0.45485175202156336,
                    0.5092417955488495,
                    0.455005055611729,
                    0.5004244482173175,
                    0.6256306760847629,
                    0,
                    0,
                    0.2510729613733906
                ]
            ],
            "fraction_answers": {
                "sudoku": 0.4660377814762688,
                "risk": 0.2435032671052573,
                "crossword puzzles": 0.2904589514184739
            },
            "question": "Which game is an example of combinatorics?",
            "rate_limited": false,
            "answers": [
                "risk",
                "crossword puzzles",
                "sudoku"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "sudoku": 0.34393939425547915,
                "risk": 0.32559523786462485,
                "crossword puzzles": 0.32559523786462485
            },
            "integer_answers": {
                "sudoku": 5,
                "risk": 1,
                "crossword puzzles": 0
            },
            "data": {
                "result_count_important_words": [
                    380000.0,
                    921000.0,
                    1350000.0
                ],
                "wikipedia_search": [
                    0.2903225806451613,
                    0.7088285229202038,
                    1.000848896434635
                ],
                "result_count_noun_chunks": [
                    377000.0,
                    1240000.0,
                    1350000.0
                ],
                "word_relation_to_question": [
                    1.2145922746781115,
                    0.2832618025751073,
                    0.5021459227467812
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    378000.0,
                    1240000.0,
                    1350000.0
                ],
                "word_count_appended": [
                    308.0,
                    63.0,
                    620.0
                ]
            },
            "z-best_answer_by_ml": [
                "sudoku"
            ]
        },
        "lines": [
            [
                0,
                0.12735849056603774,
                0.14334213504337986,
                0.1270643747893495,
                0.14516129032258066,
                0.31079717457114026,
                0,
                0,
                0.6072961373390557
            ],
            [
                0,
                0.41778975741239893,
                0.34741606940777064,
                0.41793056959892144,
                0.3544142614601019,
                0.06357214934409687,
                0,
                0,
                0.14163090128755365
            ],
            [
                1,
                0.45485175202156336,
                0.5092417955488495,
                0.455005055611729,
                0.5004244482173175,
                0.6256306760847629,
                0,
                0,
                0.2510729613733906
            ]
        ]
    },
    "Queen Victoria is credited with starting what fashion trend?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "white wedding dress"
            ],
            "question": "Queen Victoria is credited with starting what fashion trend?",
            "answers": [
                "mini dress",
                "little black dress",
                "white wedding dress"
            ],
            "integer_answers": {
                "white wedding dress": 4,
                "mini dress": 2,
                "little black dress": 1
            },
            "data": {
                "result_count_important_words": [
                    40.0,
                    61.0,
                    46.0
                ],
                "wikipedia_search": [
                    2.3746031746031746,
                    0.4761904761904762,
                    1.1492063492063491
                ],
                "result_count_noun_chunks": [
                    18400000.0,
                    6910000.0,
                    3370000.0
                ],
                "word_relation_to_question": [
                    0.9711394302848576,
                    1.6359320339830086,
                    3.392928535732134
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    5.0
                ],
                "result_count": [
                    25.0,
                    22.0,
                    32.0
                ],
                "word_count_appended": [
                    19.0,
                    33.0,
                    38.0
                ]
            },
            "negative_question": false,
            "fraction_answers": {
                "white wedding dress": 0.4443576923957307,
                "mini dress": 0.3138207257674573,
                "little black dress": 0.24182158183681204
            },
            "lines": [
                [
                    0.31645569620253167,
                    0.272108843537415,
                    0.6415620641562064,
                    0.5936507936507937,
                    0.2111111111111111,
                    0,
                    0.0,
                    0.16185657171414294
                ],
                [
                    0.27848101265822783,
                    0.41496598639455784,
                    0.2409344490934449,
                    0.11904761904761905,
                    0.36666666666666664,
                    0,
                    0.0,
                    0.2726553389971681
                ],
                [
                    0.4050632911392405,
                    0.3129251700680272,
                    0.11750348675034868,
                    0.2873015873015873,
                    0.4222222222222222,
                    0,
                    1.0,
                    0.565488089288689
                ]
            ],
            "rate_limited": false,
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "white wedding dress": 0.6118744182503886,
                "mini dress": 0.18769143408010108,
                "little black dress": 0.41485694086294755
            }
        },
        "lines": [
            [
                0,
                0.31645569620253167,
                0.272108843537415,
                0.6415620641562064,
                0.5936507936507937,
                0.2111111111111111,
                0,
                0.0,
                0.16185657171414294
            ],
            [
                0,
                0.27848101265822783,
                0.41496598639455784,
                0.2409344490934449,
                0.11904761904761905,
                0.36666666666666664,
                0,
                0.0,
                0.2726553389971681
            ],
            [
                1,
                0.4050632911392405,
                0.3129251700680272,
                0.11750348675034868,
                0.2873015873015873,
                0.4222222222222222,
                0,
                1.0,
                0.565488089288689
            ]
        ]
    },
    "According to the old saying, what kind of animal can NOT change its spots?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.4768267223382046,
                    0.312039312039312,
                    0.46114864864864863,
                    0.4819903327066832,
                    0.3601141924959217,
                    0.4703703703703704,
                    0.5,
                    0.38824698873962993
                ],
                [
                    0.4782881002087683,
                    0.3593366093366093,
                    0.46452702702702703,
                    0.34403857088142176,
                    0.29975530179445353,
                    0.0851851851851852,
                    0.01315789473684209,
                    0.245483808048359
                ],
                [
                    0.044885177453027114,
                    0.3286240786240786,
                    0.07432432432432434,
                    0.17397109641189512,
                    0.3401305057096248,
                    0.4444444444444444,
                    0.4868421052631579,
                    0.36626920321201106
                ]
            ],
            "fraction_answers": {
                "tiger": 0.43512726613935915,
                "leopard": 0.4275568756953335,
                "zebra": 0.1373158581653074
            },
            "question": "According to the old saying, what kind of animal can NOT change its spots?",
            "rate_limited": false,
            "answers": [
                "zebra",
                "leopard",
                "tiger"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "tiger": 0.24450185633794466,
                "leopard": 0.322284294401472,
                "zebra": 0.3702415217355716
            },
            "integer_answers": {
                "tiger": 3,
                "leopard": 4,
                "zebra": 1
            },
            "data": {
                "result_count_important_words": [
                    3060000.0,
                    2290000.0,
                    2790000.0
                ],
                "wikipedia_search": [
                    0.25213534210643584,
                    2.183460007660096,
                    4.5644046502334685
                ],
                "result_count_noun_chunks": [
                    1150000.0,
                    1050000.0,
                    12600000.0
                ],
                "word_relation_to_question": [
                    1.7880481801659212,
                    4.072259071226256,
                    2.139692748607823
                ],
                "word_count_noun_chunks": [
                    8.0,
                    112.0,
                    15.0
                ],
                "word_count_raw": [
                    0.0,
                    74.0,
                    2.0
                ],
                "word_count_appended": [
                    343.0,
                    491.0,
                    392.0
                ],
                "result_count": [
                    1110000.0,
                    1040000.0,
                    21800000.0
                ]
            },
            "z-best_answer_by_ml": [
                "zebra"
            ]
        },
        "lines": [
            [
                0,
                0.4768267223382046,
                0.312039312039312,
                0.46114864864864863,
                0.4819903327066832,
                0.3601141924959217,
                0.4703703703703704,
                0.5,
                0.38824698873962993
            ],
            [
                1,
                0.4782881002087683,
                0.3593366093366093,
                0.46452702702702703,
                0.34403857088142176,
                0.29975530179445353,
                0.0851851851851852,
                0.01315789473684209,
                0.245483808048359
            ],
            [
                0,
                0.044885177453027114,
                0.3286240786240786,
                0.07432432432432434,
                0.17397109641189512,
                0.3401305057096248,
                0.4444444444444444,
                0.4868421052631579,
                0.36626920321201106
            ]
        ]
    },
    "Which of these astronomical objects orbits the Earth?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.4070450097847358,
                    0.37014061207609594,
                    0.34384384384384387,
                    0.40514200711569137,
                    0.4827981651376147,
                    0.41964285714285715,
                    0.3508771929824561,
                    0.45
                ],
                [
                    0.29354207436399216,
                    0.2882547559966915,
                    0.2972972972972973,
                    0.3276821862348178,
                    0.09977064220183486,
                    0.0625,
                    0.03508771929824561,
                    0.3576923076923077
                ],
                [
                    0.299412915851272,
                    0.3416046319272126,
                    0.3588588588588589,
                    0.26717580664949087,
                    0.41743119266055045,
                    0.5178571428571429,
                    0.6140350877192983,
                    0.1923076923076923
                ]
            ],
            "fraction_answers": {
                "sun": 0.37608541610393975,
                "milky way": 0.2202283728856484,
                "moon": 0.40368621101041186
            },
            "question": "Which of these astronomical objects orbits the Earth?",
            "rate_limited": false,
            "answers": [
                "moon",
                "milky way",
                "sun"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "sun": 0.36587544692390656,
                "milky way": 0.22672583234455007,
                "moon": 0.41569035721153436
            },
            "integer_answers": {
                "sun": 3,
                "milky way": 0,
                "moon": 5
            },
            "data": {
                "result_count_important_words": [
                    895000.0,
                    697000.0,
                    826000.0
                ],
                "wikipedia_search": [
                    1.6205680284627655,
                    1.3107287449392713,
                    1.0687032265979635
                ],
                "result_count_noun_chunks": [
                    2290000.0,
                    1980000.0,
                    2390000.0
                ],
                "word_relation_to_question": [
                    1.8,
                    1.4307692307692308,
                    0.7692307692307692
                ],
                "word_count_noun_chunks": [
                    47.0,
                    7.0,
                    58.0
                ],
                "word_count_raw": [
                    60.0,
                    6.0,
                    105.0
                ],
                "result_count": [
                    2080000.0,
                    1500000.0,
                    1530000.0
                ],
                "word_count_appended": [
                    421.0,
                    87.0,
                    364.0
                ]
            },
            "z-best_answer_by_ml": [
                "moon"
            ]
        },
        "lines": [
            [
                1,
                0.4070450097847358,
                0.37014061207609594,
                0.34384384384384387,
                0.40514200711569137,
                0.4827981651376147,
                0.41964285714285715,
                0.3508771929824561,
                0.45
            ],
            [
                0,
                0.29354207436399216,
                0.2882547559966915,
                0.2972972972972973,
                0.3276821862348178,
                0.09977064220183486,
                0.0625,
                0.03508771929824561,
                0.3576923076923077
            ],
            [
                0,
                0.299412915851272,
                0.3416046319272126,
                0.3588588588588589,
                0.26717580664949087,
                0.41743119266055045,
                0.5178571428571429,
                0.6140350877192983,
                0.1923076923076923
            ]
        ]
    },
    "What generation of the iPod was the first to offer video?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.5033259423503326,
                    0.44798890429958393,
                    0.4311663479923518,
                    0.375,
                    0.25668449197860965,
                    0.0,
                    0.0,
                    0.23704150019983983
                ],
                [
                    0.24390243902439024,
                    0.06657420249653259,
                    0.3135755258126195,
                    0.625,
                    0.5240641711229946,
                    1.0,
                    1.0,
                    0.4629906744546931
                ],
                [
                    0.25277161862527714,
                    0.4854368932038835,
                    0.2552581261950287,
                    0.0,
                    0.2192513368983957,
                    0.0,
                    0.0,
                    0.29996782534546707
                ]
            ],
            "fraction_answers": {
                "third generation": 0.28140089835258975,
                "u2 special edition": 0.5295133766139037,
                "fifth generation": 0.1890857250335065
            },
            "question": "What generation of the iPod was the first to offer video?",
            "rate_limited": false,
            "answers": [
                "third generation",
                "u2 special edition",
                "fifth generation"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "third generation": 0.26516270432121314,
                "u2 special edition": 0.4420214376981768,
                "fifth generation": 0.31157143216045696
            },
            "z-best_answer_by_ml": [
                "u2 special edition"
            ],
            "data": {
                "result_count_important_words": [
                    969000.0,
                    144000.0,
                    1050000.0
                ],
                "wikipedia_search": [
                    1.5,
                    2.5,
                    0.0
                ],
                "result_count_noun_chunks": [
                    451000.0,
                    328000.0,
                    267000.0
                ],
                "word_relation_to_question": [
                    1.1852075009991991,
                    2.3149533722734654,
                    1.4998391267273352
                ],
                "word_count_noun_chunks": [
                    0.0,
                    2.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    2.0,
                    0.0
                ],
                "result_count": [
                    227000.0,
                    110000.0,
                    114000.0
                ],
                "word_count_appended": [
                    48.0,
                    98.0,
                    41.0
                ]
            },
            "integer_answers": {
                "third generation": 2,
                "u2 special edition": 5,
                "fifth generation": 1
            }
        },
        "lines": [
            [
                0,
                0.5033259423503326,
                0.44798890429958393,
                0.4311663479923518,
                0.375,
                0.25668449197860965,
                0.0,
                0.0,
                0.23704150019983983
            ],
            [
                0,
                0.24390243902439024,
                0.06657420249653259,
                0.3135755258126195,
                0.625,
                0.5240641711229946,
                1.0,
                1.0,
                0.4629906744546931
            ],
            [
                1,
                0.25277161862527714,
                0.4854368932038835,
                0.2552581261950287,
                0.0,
                0.2192513368983957,
                0.0,
                0.0,
                0.29996782534546707
            ]
        ]
    },
    "Which of these is NOT a geometric shape?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.320318832747906,
                    0.4220830070477682,
                    0.48086195386098957,
                    0.32568493150684935,
                    0.3372093023255814,
                    0.33333333333333337,
                    0.5,
                    0.35111111111111115
                ],
                [
                    0.47824912185895707,
                    0.23492560689115116,
                    0.4938802759439211,
                    0.4009132420091325,
                    0.3911627906976744,
                    0.5,
                    0.5,
                    0.4533333333333333
                ],
                [
                    0.201432045393137,
                    0.34299138606108065,
                    0.025257770195089402,
                    0.2734018264840183,
                    0.2716279069767442,
                    0.16666666666666669,
                    0.0,
                    0.19555555555555554
                ]
            ],
            "fraction_answers": {
                "hexagon": 0.6307667106669271,
                "octagon": 0.23234938201661526,
                "tarragon": 0.13688390731645766
            },
            "question": "Which of these is NOT a geometric shape?",
            "rate_limited": false,
            "answers": [
                "octagon",
                "tarragon",
                "hexagon"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "hexagon": 0.32559523786462485,
                "octagon": 0.34393939425547915,
                "tarragon": 0.34393939425547915
            },
            "integer_answers": {
                "hexagon": 7,
                "octagon": 0,
                "tarragon": 1
            },
            "data": {
                "result_count_important_words": [
                    1990000.0,
                    6770000.0,
                    4010000.0
                ],
                "wikipedia_search": [
                    0.6972602739726027,
                    0.3963470319634703,
                    0.9063926940639269
                ],
                "result_count_noun_chunks": [
                    516000.0,
                    165000.0,
                    12800000.0
                ],
                "word_relation_to_question": [
                    0.8933333333333333,
                    0.28,
                    1.8266666666666667
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    2.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    4.0
                ],
                "word_count_appended": [
                    350.0,
                    234.0,
                    491.0
                ],
                "result_count": [
                    1330000.0,
                    161000.0,
                    2210000.0
                ]
            },
            "z-best_answer_by_ml": [
                "octagon",
                "tarragon"
            ]
        },
        "lines": [
            [
                0,
                0.320318832747906,
                0.4220830070477682,
                0.48086195386098957,
                0.32568493150684935,
                0.3372093023255814,
                0.33333333333333337,
                0.5,
                0.35111111111111115
            ],
            [
                1,
                0.47824912185895707,
                0.23492560689115116,
                0.4938802759439211,
                0.4009132420091325,
                0.3911627906976744,
                0.5,
                0.5,
                0.4533333333333333
            ],
            [
                0,
                0.201432045393137,
                0.34299138606108065,
                0.025257770195089402,
                0.2734018264840183,
                0.2716279069767442,
                0.16666666666666669,
                0.0,
                0.19555555555555554
            ]
        ]
    },
    "What tech mogul became a billionaire the youngest?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.15482423335826478,
                    0.4512893982808023,
                    0.9528748298437804,
                    0.3331207482993197,
                    0.3941908713692946,
                    0.0,
                    0.375,
                    0.49118326118326117
                ],
                [
                    0.4988780852655198,
                    0.4670487106017192,
                    0.036948207687820055,
                    0.41326530612244894,
                    0.3817427385892116,
                    1.0,
                    0.5416666666666666,
                    0.32423623995052564
                ],
                [
                    0.3462976813762154,
                    0.08166189111747851,
                    0.01017696246839956,
                    0.2536139455782313,
                    0.22406639004149378,
                    0.0,
                    0.08333333333333333,
                    0.18458049886621314
                ]
            ],
            "fraction_answers": {
                "mark zuckerberg": 0.45797324436048903,
                "evan spiegel": 0.3940604177918403,
                "larry page": 0.14796633784767063
            },
            "question": "What tech mogul became a billionaire the youngest?",
            "rate_limited": false,
            "answers": [
                "evan spiegel",
                "mark zuckerberg",
                "larry page"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "mark zuckerberg": 0.4420214376981768,
                "evan spiegel": 0.3635484425931417,
                "larry page": 0.27326161506461843
            },
            "z-best_answer_by_ml": [
                "mark zuckerberg"
            ],
            "data": {
                "result_count_important_words": [
                    31500.0,
                    32600.0,
                    5700.0
                ],
                "wikipedia_search": [
                    1.3324829931972788,
                    1.6530612244897958,
                    1.0144557823129252
                ],
                "result_count_noun_chunks": [
                    1470000.0,
                    57000.0,
                    15700.0
                ],
                "word_relation_to_question": [
                    2.455916305916306,
                    1.6211811997526282,
                    0.9229024943310657
                ],
                "word_count_noun_chunks": [
                    0.0,
                    8.0,
                    0.0
                ],
                "word_count_raw": [
                    9.0,
                    13.0,
                    2.0
                ],
                "word_count_appended": [
                    95.0,
                    92.0,
                    54.0
                ],
                "result_count": [
                    20700.0,
                    66700.0,
                    46300.0
                ]
            },
            "integer_answers": {
                "mark zuckerberg": 5,
                "evan spiegel": 3,
                "larry page": 0
            }
        },
        "lines": [
            [
                0,
                0.15482423335826478,
                0.4512893982808023,
                0.9528748298437804,
                0.3331207482993197,
                0.3941908713692946,
                0.0,
                0.375,
                0.49118326118326117
            ],
            [
                1,
                0.4988780852655198,
                0.4670487106017192,
                0.036948207687820055,
                0.41326530612244894,
                0.3817427385892116,
                1.0,
                0.5416666666666666,
                0.32423623995052564
            ],
            [
                0,
                0.3462976813762154,
                0.08166189111747851,
                0.01017696246839956,
                0.2536139455782313,
                0.22406639004149378,
                0.0,
                0.08333333333333333,
                0.18458049886621314
            ]
        ]
    },
    "Which of these is NOT a marsupial?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.49281920125909895,
                    0.4914927555496478,
                    0.455030734390165,
                    0.3670212765957447,
                    0.4706368899917287,
                    0.5,
                    0.5,
                    0.4772727272727273
                ],
                [
                    0.4675388550068857,
                    0.4704240329655722,
                    0.4558395341313491,
                    0.4925531914893617,
                    0.2969396195202647,
                    0.5,
                    0.45,
                    0.21748251748251746
                ],
                [
                    0.03964194373401536,
                    0.03808321148478,
                    0.08912973147848591,
                    0.1404255319148936,
                    0.2324234904880066,
                    0.0,
                    0.04999999999999999,
                    0.30524475524475525
                ]
            ],
            "fraction_answers": {
                "cuscus": 0.16230556235101232,
                "quintana roo": 0.06143160373522191,
                "wombat": 0.7762628339137658
            },
            "question": "Which of these is NOT a marsupial?",
            "rate_limited": false,
            "answers": [
                "quintana roo",
                "cuscus",
                "wombat"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "cuscus": 0.32285225250080163,
                "quintana roo": 0.5193098958422995,
                "wombat": 0.1688004428152544
            },
            "integer_answers": {
                "cuscus": 1,
                "quintana roo": 0,
                "wombat": 7
            },
            "data": {
                "result_count_important_words": [
                    51200.0,
                    178000.0,
                    2780000.0
                ],
                "wikipedia_search": [
                    0.5319148936170213,
                    0.029787234042553193,
                    1.4382978723404256
                ],
                "result_count_noun_chunks": [
                    55600.0,
                    54600.0,
                    508000.0
                ],
                "word_relation_to_question": [
                    0.09090909090909091,
                    1.1300699300699302,
                    0.779020979020979
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    15.0
                ],
                "word_count_raw": [
                    0.0,
                    2.0,
                    18.0
                ],
                "word_count_appended": [
                    71.0,
                    491.0,
                    647.0
                ],
                "result_count": [
                    36500.0,
                    165000.0,
                    2340000.0
                ]
            },
            "z-best_answer_by_ml": [
                "quintana roo"
            ]
        },
        "lines": [
            [
                1,
                0.49281920125909895,
                0.4914927555496478,
                0.455030734390165,
                0.3670212765957447,
                0.4706368899917287,
                0.5,
                0.5,
                0.4772727272727273
            ],
            [
                0,
                0.4675388550068857,
                0.4704240329655722,
                0.4558395341313491,
                0.4925531914893617,
                0.2969396195202647,
                0.5,
                0.45,
                0.21748251748251746
            ],
            [
                0,
                0.03964194373401536,
                0.03808321148478,
                0.08912973147848591,
                0.1404255319148936,
                0.2324234904880066,
                0.0,
                0.04999999999999999,
                0.30524475524475525
            ]
        ]
    },
    "Catching a catfish with your bare hands is called what?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.3454057679409792,
                    0.6698535456566087,
                    0.6381263714119385,
                    0.06666666666666667,
                    0.19796954314720813,
                    0.0,
                    0.0,
                    0.039047619047619046
                ],
                [
                    0.0006706908115358819,
                    0.0013092592028742806,
                    0.0013882047378084277,
                    0.06666666666666667,
                    0.05752961082910321,
                    0.0,
                    0.0,
                    0.11056910569105691
                ],
                [
                    0.6539235412474849,
                    0.328837195140517,
                    0.360485423850253,
                    0.8666666666666666,
                    0.7445008460236887,
                    1.0,
                    1.0,
                    0.8503832752613241
                ]
            ],
            "fraction_answers": {
                "noodling": 0.7255996185237418,
                "whiskering": 0.02976669224238067,
                "strumming": 0.24463368923387752
            },
            "question": "Catching a catfish with your bare hands is called what?",
            "rate_limited": false,
            "answers": [
                "strumming",
                "whiskering",
                "noodling"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "noodling": 0.4420214376981768,
                "whiskering": 0.29035060350393144,
                "strumming": 0.26516270432121314
            },
            "integer_answers": {
                "noodling": 6,
                "whiskering": 0,
                "strumming": 2
            },
            "data": {
                "result_count_important_words": [
                    22000.0,
                    43.0,
                    10800.0
                ],
                "wikipedia_search": [
                    0.3333333333333333,
                    0.3333333333333333,
                    4.333333333333333
                ],
                "result_count_noun_chunks": [
                    28500.0,
                    62.0,
                    16100.0
                ],
                "word_relation_to_question": [
                    0.19523809523809524,
                    0.5528455284552846,
                    4.25191637630662
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    119.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    217.0
                ],
                "word_count_appended": [
                    117.0,
                    34.0,
                    440.0
                ],
                "result_count": [
                    20600.0,
                    40.0,
                    39000.0
                ]
            },
            "z-best_answer_by_ml": [
                "noodling"
            ]
        },
        "lines": [
            [
                0,
                0.3454057679409792,
                0.6698535456566087,
                0.6381263714119385,
                0.06666666666666667,
                0.19796954314720813,
                0.0,
                0.0,
                0.039047619047619046
            ],
            [
                0,
                0.0006706908115358819,
                0.0013092592028742806,
                0.0013882047378084277,
                0.06666666666666667,
                0.05752961082910321,
                0.0,
                0.0,
                0.11056910569105691
            ],
            [
                1,
                0.6539235412474849,
                0.328837195140517,
                0.360485423850253,
                0.8666666666666666,
                0.7445008460236887,
                1.0,
                1.0,
                0.8503832752613241
            ]
        ]
    },
    "What is the total cost of all the vowels on \u201cWheel of Fortune\u201d?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.31756756756756754,
                    0.2593703148425787,
                    0.2593703148425787,
                    0.0,
                    0.375,
                    0,
                    0,
                    0.13338709677419355
                ],
                [
                    0.4189189189189189,
                    0.6334332833583208,
                    0.6334332833583208,
                    0.5,
                    0.35,
                    0,
                    0,
                    0.495
                ],
                [
                    0.2635135135135135,
                    0.10719640179910045,
                    0.10719640179910045,
                    0.5,
                    0.275,
                    0,
                    0,
                    0.3716129032258064
                ]
            ],
            "fraction_answers": {
                "$2,500": 0.5051309142725934,
                "$1,250": 0.22411588233781973,
                "$2,900": 0.2707532033895868
            },
            "question": "What is the total cost of all the vowels on \u201cWheel of Fortune\u201d?",
            "rate_limited": false,
            "answers": [
                "$1,250",
                "$2,500",
                "$2,900"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "$2,500": 0.3732936123345509,
                "$1,250": 0.2875163736948735,
                "$2,900": 0.20406866938992557
            },
            "integer_answers": {
                "$2,500": 5,
                "$1,250": 1,
                "$2,900": 0
            },
            "data": {
                "result_count_important_words": [
                    3460.0,
                    8450.0,
                    1430.0
                ],
                "wikipedia_search": [
                    0.0,
                    1.0,
                    1.0
                ],
                "result_count_noun_chunks": [
                    3460.0,
                    8450.0,
                    1430.0
                ],
                "word_relation_to_question": [
                    0.6669354838709678,
                    2.475,
                    1.8580645161290321
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    15.0,
                    14.0,
                    11.0
                ],
                "result_count": [
                    47.0,
                    62.0,
                    39.0
                ]
            },
            "z-best_answer_by_ml": [
                "$2,500"
            ]
        },
        "lines": [
            [
                1,
                0.31756756756756754,
                0.2593703148425787,
                0.2593703148425787,
                0.0,
                0.375,
                0,
                0,
                0.13338709677419355
            ],
            [
                0,
                0.4189189189189189,
                0.6334332833583208,
                0.6334332833583208,
                0.5,
                0.35,
                0,
                0,
                0.495
            ],
            [
                0,
                0.2635135135135135,
                0.10719640179910045,
                0.10719640179910045,
                0.5,
                0.275,
                0,
                0,
                0.3716129032258064
            ]
        ]
    },
    "Which brand mascot was NOT a real person?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.48443756998348864,
                    0.4901601945398481,
                    0.48443756998348864,
                    0.3121871871871872,
                    0.2782608695652174,
                    0.5,
                    0.5,
                    0.36960237676701096
                ],
                [
                    0.4909756884477425,
                    0.4949359472536288,
                    0.4909756884477425,
                    0.4007632632632633,
                    0.3579710144927536,
                    0.5,
                    0.5,
                    0.3308109939207501
                ],
                [
                    0.024586741568768833,
                    0.014903858206523124,
                    0.024586741568768833,
                    0.28704954954954953,
                    0.36376811594202896,
                    0.0,
                    0.0,
                    0.29958662931223906
                ]
            ],
            "fraction_answers": {
                "betty crocker": 0.7463795909630304,
                "sara lee": 0.10839185104352983,
                "little debbie": 0.14522855799343978
            },
            "question": "Which brand mascot was NOT a real person?",
            "rate_limited": false,
            "answers": [
                "little debbie",
                "sara lee",
                "betty crocker"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "betty crocker": 0.34393939425547915,
                "sara lee": 0.34393939425547915,
                "little debbie": 0.32559523786462485
            },
            "integer_answers": {
                "betty crocker": 7,
                "sara lee": 0,
                "little debbie": 1
            },
            "data": {
                "result_count_important_words": [
                    157000.0,
                    80800.0,
                    7740000.0
                ],
                "wikipedia_search": [
                    1.878128128128128,
                    0.9923673673673674,
                    2.1295045045045047
                ],
                "result_count_noun_chunks": [
                    164000.0,
                    95100.0,
                    5010000.0
                ],
                "word_relation_to_question": [
                    1.0431809858639127,
                    1.3535120486339998,
                    1.6033069655020877
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    2.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    2.0
                ],
                "result_count": [
                    164000.0,
                    95100.0,
                    5010000.0
                ],
                "word_count_appended": [
                    153.0,
                    98.0,
                    94.0
                ]
            },
            "z-best_answer_by_ml": [
                "sara lee",
                "betty crocker"
            ]
        },
        "lines": [
            [
                0,
                0.48443756998348864,
                0.4901601945398481,
                0.48443756998348864,
                0.3121871871871872,
                0.2782608695652174,
                0.5,
                0.5,
                0.36960237676701096
            ],
            [
                0,
                0.4909756884477425,
                0.4949359472536288,
                0.4909756884477425,
                0.4007632632632633,
                0.3579710144927536,
                0.5,
                0.5,
                0.3308109939207501
            ],
            [
                1,
                0.024586741568768833,
                0.014903858206523124,
                0.024586741568768833,
                0.28704954954954953,
                0.36376811594202896,
                0.0,
                0.0,
                0.29958662931223906
            ]
        ]
    },
    "What iconic painting once hung in Napoleon's bedroom?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "mona lisa"
            ],
            "question": "What iconic painting once hung in Napoleon's bedroom?",
            "answers": [
                "the starry night",
                "mona lisa",
                "the birth of venus"
            ],
            "integer_answers": {
                "the birth of venus": 1,
                "mona lisa": 6,
                "the starry night": 1
            },
            "data": {
                "result_count_important_words": [
                    76.0,
                    131000.0,
                    43800.0
                ],
                "wikipedia_search": [
                    2.7616459627329193,
                    0.7606107660455487,
                    0.47774327122153204
                ],
                "result_count_noun_chunks": [
                    45000.0,
                    274000.0,
                    51900.0
                ],
                "word_relation_to_question": [
                    2.2077639751552796,
                    0.9003105590062112,
                    2.891925465838509
                ],
                "word_count_noun_chunks": [
                    0.0,
                    24.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    50.0,
                    1.0
                ],
                "result_count": [
                    40500.0,
                    144000.0,
                    48600.0
                ],
                "word_count_appended": [
                    46.0,
                    222.0,
                    72.0
                ]
            },
            "negative_question": false,
            "fraction_answers": {
                "the birth of venus": 0.18396040481804515,
                "mona lisa": 0.6298930275123498,
                "the starry night": 0.18614656766960516
            },
            "lines": [
                [
                    0.17374517374517376,
                    0.000434593654932638,
                    0.1213265031005662,
                    0.6904114906832298,
                    0.13529411764705881,
                    0.0,
                    0.0,
                    0.36796066252587994
                ],
                [
                    0.6177606177606177,
                    0.7491022210023102,
                    0.7387435966567808,
                    0.19015269151138717,
                    0.6529411764705882,
                    0.96,
                    0.9803921568627451,
                    0.15005175983436853
                ],
                [
                    0.2084942084942085,
                    0.25046318534275713,
                    0.139929900242653,
                    0.11943581780538301,
                    0.21176470588235294,
                    0.04,
                    0.0196078431372549,
                    0.48198757763975153
                ]
            ],
            "rate_limited": false,
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "the birth of venus": 0.35830457346269085,
                "mona lisa": 0.7224139102363077,
                "the starry night": 0.1614799710562439
            }
        },
        "lines": [
            [
                0,
                0.17374517374517376,
                0.000434593654932638,
                0.1213265031005662,
                0.6904114906832298,
                0.13529411764705881,
                0.0,
                0.0,
                0.36796066252587994
            ],
            [
                1,
                0.6177606177606177,
                0.7491022210023102,
                0.7387435966567808,
                0.19015269151138717,
                0.6529411764705882,
                0.96,
                0.9803921568627451,
                0.15005175983436853
            ],
            [
                0,
                0.2084942084942085,
                0.25046318534275713,
                0.139929900242653,
                0.11943581780538301,
                0.21176470588235294,
                0.04,
                0.0196078431372549,
                0.48198757763975153
            ]
        ]
    },
    "Which of these is NOT among the four \u201cC\u2019s\u201d of diamond buying?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.2994542974079127,
                    0.26534296028880866,
                    0.2915468660149511,
                    0.14864525977212018,
                    0.2703619909502263,
                    0.20661157024793386,
                    0.2090909090909091,
                    0.3207052974065235
                ],
                [
                    0.2694406548431105,
                    0.27668901495616294,
                    0.3553766532489937,
                    0.4619854985553072,
                    0.31447963800904977,
                    0.29338842975206614,
                    0.2909090909090909,
                    0.29343355365633517
                ],
                [
                    0.4311050477489768,
                    0.45796802475502835,
                    0.35307648073605524,
                    0.38936924167257264,
                    0.41515837104072395,
                    0.5,
                    0.5,
                    0.3858611489371414
                ]
            ],
            "fraction_answers": {
                "color": 0.49706021220515373,
                "core": 0.1418654212773754,
                "cut": 0.36107436651747093
            },
            "question": "Which of these is NOT among the four \u201cC\u2019s\u201d of diamond buying?",
            "rate_limited": false,
            "answers": [
                "color",
                "cut",
                "core"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "color": 0.20288458033377557,
                "core": 0.30959422163460687,
                "cut": 0.20494311049075703
            },
            "z-best_answer_by_ml": [
                "core"
            ],
            "data": {
                "result_count_important_words": [
                    9100000.0,
                    8660000.0,
                    1630000.0
                ],
                "wikipedia_search": [
                    1.4054189609115193,
                    0.1520580057787712,
                    0.44252303330970943
                ],
                "result_count_noun_chunks": [
                    7250000.0,
                    5030000.0,
                    5110000.0
                ],
                "word_relation_to_question": [
                    1.4343576207478121,
                    1.6525315707493187,
                    0.913110808502869
                ],
                "word_count_noun_chunks": [
                    142.0,
                    100.0,
                    0.0
                ],
                "word_count_raw": [
                    160.0,
                    115.0,
                    0.0
                ],
                "result_count": [
                    588000.0,
                    676000.0,
                    202000.0
                ],
                "word_count_appended": [
                    406.0,
                    328.0,
                    150.0
                ]
            },
            "integer_answers": {
                "color": 6,
                "core": 0,
                "cut": 2
            }
        },
        "lines": [
            [
                0,
                0.2994542974079127,
                0.26534296028880866,
                0.2915468660149511,
                0.14864525977212018,
                0.2703619909502263,
                0.20661157024793386,
                0.2090909090909091,
                0.3207052974065235
            ],
            [
                0,
                0.2694406548431105,
                0.27668901495616294,
                0.3553766532489937,
                0.4619854985553072,
                0.31447963800904977,
                0.29338842975206614,
                0.2909090909090909,
                0.29343355365633517
            ],
            [
                1,
                0.4311050477489768,
                0.45796802475502835,
                0.35307648073605524,
                0.38936924167257264,
                0.41515837104072395,
                0.5,
                0.5,
                0.3858611489371414
            ]
        ]
    },
    "What is the correct pronunciation of the performer who sings \u201cSmooth Operator\u201d?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.8043478260869565,
                    0.5106382978723404,
                    0.4346108257605689,
                    0.0,
                    0.7526881720430108,
                    1.0,
                    1.0,
                    0.5365853658536586
                ],
                [
                    0.14130434782608695,
                    0.2978723404255319,
                    0.002370604504148558,
                    0.0,
                    0.12903225806451613,
                    0.0,
                    0.0,
                    0.06504065040650407
                ],
                [
                    0.05434782608695652,
                    0.19148936170212766,
                    0.5630185697352825,
                    1.0,
                    0.11827956989247312,
                    0.0,
                    0.0,
                    0.3983739837398374
                ]
            ],
            "fraction_answers": {
                "say-dee": 0.29068866389458464,
                "sayd": 0.07945252515334846,
                "shah-day": 0.629858810952067
            },
            "question": "What is the correct pronunciation of the performer who sings \u201cSmooth Operator\u201d?",
            "rate_limited": false,
            "answers": [
                "shah-day",
                "sayd",
                "say-dee"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "say-dee": 0.29304625062558914,
                "sayd": 0.29035060350393144,
                "shah-day": 0.4420214376981768
            },
            "z-best_answer_by_ml": [
                "shah-day"
            ],
            "data": {
                "result_count_important_words": [
                    24.0,
                    14.0,
                    9.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    2.0
                ],
                "result_count_noun_chunks": [
                    2200.0,
                    12.0,
                    2850.0
                ],
                "word_relation_to_question": [
                    1.6097560975609757,
                    0.1951219512195122,
                    1.1951219512195121
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    140.0,
                    24.0,
                    22.0
                ],
                "result_count": [
                    74.0,
                    13.0,
                    5.0
                ]
            },
            "integer_answers": {
                "say-dee": 2,
                "sayd": 0,
                "shah-day": 6
            }
        },
        "lines": [
            [
                1,
                0.8043478260869565,
                0.5106382978723404,
                0.4346108257605689,
                0.0,
                0.7526881720430108,
                1.0,
                1.0,
                0.5365853658536586
            ],
            [
                0,
                0.14130434782608695,
                0.2978723404255319,
                0.002370604504148558,
                0.0,
                0.12903225806451613,
                0.0,
                0.0,
                0.06504065040650407
            ],
            [
                0,
                0.05434782608695652,
                0.19148936170212766,
                0.5630185697352825,
                1.0,
                0.11827956989247312,
                0.0,
                0.0,
                0.3983739837398374
            ]
        ]
    },
    "In which version of \u201cDragnet\u201d is the line \u201cJust the facts, ma\u2019am\u201d first said?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.3191489361702128,
                    0.23333333333333334,
                    0.35714285714285715,
                    0.009057971014492754,
                    0.3333333333333333,
                    0,
                    0,
                    0.37625401713161527
                ],
                [
                    0.2127659574468085,
                    0.21666666666666667,
                    0.37142857142857144,
                    0.8049516908212561,
                    0.3333333333333333,
                    0,
                    0,
                    0.17502779916174835
                ],
                [
                    0.46808510638297873,
                    0.55,
                    0.2714285714285714,
                    0.1859903381642512,
                    0.3333333333333333,
                    0,
                    0,
                    0.44871818370663635
                ]
            ],
            "fraction_answers": {
                "'80s movie": 0.3762592555026285,
                "'50s movie": 0.3523623364763974,
                "'50s tv show": 0.2713784080209741
            },
            "question": "In which version of \u201cDragnet\u201d is the line \u201cJust the facts, ma\u2019am\u201d first said?",
            "rate_limited": false,
            "answers": [
                "'50s tv show",
                "'50s movie",
                "'80s movie"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "'80s movie": 0.3049087145126065,
                "'50s movie": 0.2718254219690636,
                "'50s tv show": 0.28638353297773866
            },
            "integer_answers": {
                "'80s movie": 3,
                "'50s movie": 2,
                "'50s tv show": 1
            },
            "data": {
                "result_count_important_words": [
                    14.0,
                    13.0,
                    33.0
                ],
                "wikipedia_search": [
                    0.036231884057971016,
                    3.2198067632850242,
                    0.7439613526570048
                ],
                "result_count_noun_chunks": [
                    25.0,
                    26.0,
                    19.0
                ],
                "word_relation_to_question": [
                    2.2575241027896915,
                    1.05016679497049,
                    2.6923091022398182
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    15.0,
                    10.0,
                    22.0
                ],
                "word_count_appended": [
                    2.0,
                    2.0,
                    2.0
                ]
            },
            "z-best_answer_by_ml": [
                "'80s movie"
            ]
        },
        "lines": [
            [
                0,
                0.3191489361702128,
                0.23333333333333334,
                0.35714285714285715,
                0.009057971014492754,
                0.3333333333333333,
                0,
                0,
                0.37625401713161527
            ],
            [
                0,
                0.2127659574468085,
                0.21666666666666667,
                0.37142857142857144,
                0.8049516908212561,
                0.3333333333333333,
                0,
                0,
                0.17502779916174835
            ],
            [
                1,
                0.46808510638297873,
                0.55,
                0.2714285714285714,
                0.1859903381642512,
                0.3333333333333333,
                0,
                0,
                0.44871818370663635
            ]
        ]
    },
    "Which of these companies went public first?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.9209302325581395,
                    0.9833327005581077,
                    0.9892376904531106,
                    0.5554804804804805,
                    0.2840269966254218,
                    1.0,
                    1.0,
                    0.4833333333333333
                ],
                [
                    0.030299003322259137,
                    0.007897034815292912,
                    0.005070464544031019,
                    0.12832475332475332,
                    0.3458942632170979,
                    0.0,
                    0.0,
                    0.06111111111111111
                ],
                [
                    0.04877076411960133,
                    0.00877026462659934,
                    0.00569184500285835,
                    0.31619476619476616,
                    0.3700787401574803,
                    0.0,
                    0.0,
                    0.45555555555555555
                ]
            ],
            "fraction_answers": {
                "facebook": 0.7770426792510742,
                "ferrari": 0.07232457879181817,
                "alibaba": 0.15063274195710763
            },
            "question": "Which of these companies went public first?",
            "rate_limited": false,
            "answers": [
                "facebook",
                "ferrari",
                "alibaba"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "facebook": 0.7115347094820504,
                "ferrari": 0.1616680319665847,
                "alibaba": 0.32728216592972825
            },
            "integer_answers": {
                "facebook": 7,
                "ferrari": 0,
                "alibaba": 1
            },
            "data": {
                "result_count_important_words": [
                    259000000.0,
                    2080000.0,
                    2310000.0
                ],
                "wikipedia_search": [
                    1.6664414414414415,
                    0.38497425997425994,
                    0.9485842985842985
                ],
                "result_count_noun_chunks": [
                    398000000.0,
                    2040000.0,
                    2290000.0
                ],
                "word_relation_to_question": [
                    0.9666666666666666,
                    0.12222222222222222,
                    0.9111111111111111
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    13.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    505.0,
                    615.0,
                    658.0
                ],
                "result_count": [
                    69300000.0,
                    2280000.0,
                    3670000.0
                ]
            },
            "z-best_answer_by_ml": [
                "facebook"
            ]
        },
        "lines": [
            [
                1,
                0.9209302325581395,
                0.9833327005581077,
                0.9892376904531106,
                0.5554804804804805,
                0.2840269966254218,
                1.0,
                1.0,
                0.4833333333333333
            ],
            [
                0,
                0.030299003322259137,
                0.007897034815292912,
                0.005070464544031019,
                0.12832475332475332,
                0.3458942632170979,
                0.0,
                0.0,
                0.06111111111111111
            ],
            [
                0,
                0.04877076411960133,
                0.00877026462659934,
                0.00569184500285835,
                0.31619476619476616,
                0.3700787401574803,
                0.0,
                0.0,
                0.45555555555555555
            ]
        ]
    },
    "What topic would a herpetologist study?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.8637316561844863,
                    0.8449152023510684,
                    0.4844517184942717,
                    0.6317460317460317,
                    0.45121951219512196,
                    0,
                    0,
                    0.2646414887794198
                ],
                [
                    0.09685534591194969,
                    0.12245147860160412,
                    0.3878887070376432,
                    0.09523809523809523,
                    0.2804878048780488,
                    0,
                    0,
                    0.12452107279693486
                ],
                [
                    0.039412997903563944,
                    0.0326333190473275,
                    0.1276595744680851,
                    0.273015873015873,
                    0.2682926829268293,
                    0,
                    0,
                    0.6108374384236454
                ]
            ],
            "fraction_answers": {
                "venereal disease": 0.5901176016250667,
                "mushroom farming": 0.184573750744046,
                "crocodile teeth": 0.22530864763088734
            },
            "question": "What topic would a herpetologist study?",
            "rate_limited": false,
            "answers": [
                "venereal disease",
                "mushroom farming",
                "crocodile teeth"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "venereal disease": 0.3739747133846042,
                "mushroom farming": 0.2718254219690636,
                "crocodile teeth": 0.31157143216045696
            },
            "z-best_answer_by_ml": [
                "venereal disease"
            ],
            "data": {
                "result_count_important_words": [
                    13800.0,
                    2000.0,
                    533.0
                ],
                "wikipedia_search": [
                    1.8952380952380952,
                    0.2857142857142857,
                    0.819047619047619
                ],
                "result_count_noun_chunks": [
                    296000.0,
                    237000.0,
                    78000.0
                ],
                "word_relation_to_question": [
                    0.7939244663382594,
                    0.3735632183908046,
                    1.832512315270936
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    2060.0,
                    231.0,
                    94.0
                ],
                "word_count_appended": [
                    37.0,
                    23.0,
                    22.0
                ]
            },
            "integer_answers": {
                "venereal disease": 5,
                "mushroom farming": 0,
                "crocodile teeth": 1
            }
        },
        "lines": [
            [
                0,
                0.8637316561844863,
                0.8449152023510684,
                0.4844517184942717,
                0.6317460317460317,
                0.45121951219512196,
                0,
                0,
                0.2646414887794198
            ],
            [
                0,
                0.09685534591194969,
                0.12245147860160412,
                0.3878887070376432,
                0.09523809523809523,
                0.2804878048780488,
                0,
                0,
                0.12452107279693486
            ],
            [
                1,
                0.039412997903563944,
                0.0326333190473275,
                0.1276595744680851,
                0.273015873015873,
                0.2682926829268293,
                0,
                0,
                0.6108374384236454
            ]
        ]
    },
    "In the U.K., who appoints the Prime Minister?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.2708456534594914,
                    0.4645917907081642,
                    0.4914933837429111,
                    0.0,
                    0.24152542372881355,
                    1.0,
                    0.05555555555555555,
                    0.380952380952381
                ],
                [
                    0.5233589591957422,
                    0.20974289580514208,
                    0.27504725897920607,
                    0.393939393939394,
                    0.24152542372881355,
                    0.0,
                    0.05555555555555555,
                    0.4761904761904763
                ],
                [
                    0.2057953873447664,
                    0.32566531348669375,
                    0.2334593572778828,
                    0.6060606060606061,
                    0.5169491525423728,
                    0.0,
                    0.8888888888888888,
                    0.14285714285714288
                ]
            ],
            "fraction_answers": {
                "the people": 0.3631205235184146,
                "the parliament": 0.27191999542429124,
                "the queen": 0.3649594810572942
            },
            "question": "In the U.K., who appoints the Prime Minister?",
            "rate_limited": false,
            "answers": [
                "the people",
                "the parliament",
                "the queen"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "the people": 0.35443025729131133,
                "the parliament": 0.29448244372114396,
                "the queen": 0.34232761393661615
            },
            "integer_answers": {
                "the people": 3,
                "the parliament": 2,
                "the queen": 3
            },
            "data": {
                "result_count_important_words": [
                    1030000.0,
                    465000.0,
                    722000.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.7878787878787878,
                    1.212121212121212
                ],
                "result_count_noun_chunks": [
                    1040000.0,
                    582000.0,
                    494000.0
                ],
                "word_relation_to_question": [
                    1.1428571428571428,
                    1.4285714285714286,
                    0.42857142857142855
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    16.0
                ],
                "word_count_appended": [
                    57.0,
                    57.0,
                    122.0
                ],
                "result_count": [
                    458000.0,
                    885000.0,
                    348000.0
                ]
            },
            "z-best_answer_by_ml": [
                "the people"
            ]
        },
        "lines": [
            [
                0,
                0.2708456534594914,
                0.4645917907081642,
                0.4914933837429111,
                0.0,
                0.24152542372881355,
                1.0,
                0.05555555555555555,
                0.380952380952381
            ],
            [
                0,
                0.5233589591957422,
                0.20974289580514208,
                0.27504725897920607,
                0.393939393939394,
                0.24152542372881355,
                0.0,
                0.05555555555555555,
                0.4761904761904763
            ],
            [
                1,
                0.2057953873447664,
                0.32566531348669375,
                0.2334593572778828,
                0.6060606060606061,
                0.5169491525423728,
                0.0,
                0.8888888888888888,
                0.14285714285714288
            ]
        ]
    },
    "Which writer has stated that his/her trademark series of books would never be adapted for film?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.05211435378201312,
                    0.3121316306483301,
                    0.09999999999999998,
                    0.19386621622785488,
                    0.3007246376811594,
                    0.25,
                    0,
                    0.36476446425006515
                ],
                [
                    0.47421083978558665,
                    0.34184675834970535,
                    0.46015037593984964,
                    0.3061337837721452,
                    0.32608695652173914,
                    0.5,
                    0,
                    0.293810996026059
                ],
                [
                    0.47367480643240023,
                    0.34602161100196466,
                    0.4398496240601504,
                    0.5,
                    0.37318840579710144,
                    0.25,
                    0,
                    0.3414245397238759
                ]
            ],
            "fraction_answers": {
                "jeff kinney": 0.22166886085271642,
                "james patterson": 0.5503996278315936,
                "sue grafton": 0.22793151131569003
            },
            "question": "Which writer has stated that his/her trademark series of books would never be adapted for film?",
            "rate_limited": false,
            "answers": [
                "james patterson",
                "sue grafton",
                "jeff kinney"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "jeff kinney": 0.28047812244909137,
                "james patterson": 0.16956773439689754,
                "sue grafton": 0.3296844956842122
            },
            "integer_answers": {
                "jeff kinney": 0,
                "james patterson": 6,
                "sue grafton": 1
            },
            "data": {
                "result_count_important_words": [
                    76500.0,
                    64400.0,
                    62700.0
                ],
                "wikipedia_search": [
                    3.6736054052657416,
                    2.3263945947342584,
                    0.0
                ],
                "result_count_noun_chunks": [
                    26600.0,
                    2650.0,
                    4000.0
                ],
                "word_relation_to_question": [
                    1.6228264289992183,
                    2.474268047687292,
                    1.90290552331349
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    55.0,
                    48.0,
                    35.0
                ],
                "result_count": [
                    7520.0,
                    433.0,
                    442.0
                ]
            },
            "z-best_answer_by_ml": [
                "sue grafton"
            ]
        },
        "lines": [
            [
                0,
                0.05211435378201312,
                0.3121316306483301,
                0.09999999999999998,
                0.19386621622785488,
                0.3007246376811594,
                0.25,
                0,
                0.36476446425006515
            ],
            [
                1,
                0.47421083978558665,
                0.34184675834970535,
                0.46015037593984964,
                0.3061337837721452,
                0.32608695652173914,
                0.5,
                0,
                0.293810996026059
            ],
            [
                0,
                0.47367480643240023,
                0.34602161100196466,
                0.4398496240601504,
                0.5,
                0.37318840579710144,
                0.25,
                0,
                0.3414245397238759
            ]
        ]
    },
    "In Harry Potter's Quidditch, what ALWAYS happens when one team catches the snitch?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.03024193548387097,
                    0.0051700680272108845,
                    0.0042525773195876285,
                    0.0625,
                    0.11864406779661017,
                    0.0,
                    0.0,
                    0.18830645161290321
                ],
                [
                    0.11290322580645161,
                    0.0016326530612244899,
                    0.0009020618556701031,
                    0.9375,
                    0.1016949152542373,
                    0.0,
                    0.0,
                    0.15512562034739455
                ],
                [
                    0.8568548387096774,
                    0.9931972789115646,
                    0.9948453608247423,
                    0.0,
                    0.7796610169491526,
                    1.0,
                    1.0,
                    0.6565679280397022
                ]
            ],
            "fraction_answers": {
                "the game ends": 0.785140802929355,
                "that team loses": 0.16371980954062226,
                "that team wins": 0.05113938753002285
            },
            "question": "In Harry Potter's Quidditch, what ALWAYS happens when one team catches the snitch?",
            "rate_limited": false,
            "answers": [
                "that team wins",
                "that team loses",
                "the game ends"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "the game ends": 0.4420214376981768,
                "that team loses": 0.29035060350393144,
                "that team wins": 0.29035060350393144
            },
            "z-best_answer_by_ml": [
                "the game ends"
            ],
            "data": {
                "result_count_important_words": [
                    19.0,
                    6.0,
                    3650.0
                ],
                "wikipedia_search": [
                    0.0625,
                    0.9375,
                    0.0
                ],
                "result_count_noun_chunks": [
                    33.0,
                    7.0,
                    7720.0
                ],
                "word_relation_to_question": [
                    1.5064516129032257,
                    1.2410049627791564,
                    5.252543424317618
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    6.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    6.0
                ],
                "word_count_appended": [
                    7.0,
                    6.0,
                    46.0
                ],
                "result_count": [
                    15.0,
                    56.0,
                    425.0
                ]
            },
            "integer_answers": {
                "the game ends": 7,
                "that team loses": 1,
                "that team wins": 0
            }
        },
        "lines": [
            [
                0,
                0.03024193548387097,
                0.0051700680272108845,
                0.0042525773195876285,
                0.0625,
                0.11864406779661017,
                0.0,
                0.0,
                0.18830645161290321
            ],
            [
                0,
                0.11290322580645161,
                0.0016326530612244899,
                0.0009020618556701031,
                0.9375,
                0.1016949152542373,
                0.0,
                0.0,
                0.15512562034739455
            ],
            [
                1,
                0.8568548387096774,
                0.9931972789115646,
                0.9948453608247423,
                0.0,
                0.7796610169491526,
                1.0,
                1.0,
                0.6565679280397022
            ]
        ]
    },
    "Laurie Metcalf, Amy Morton and Tracy Letts are members of a theatre company from what city?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.4014167650531287,
                    0.4076086956521739,
                    0.38638454461821525,
                    0.2384799271652288,
                    0.15743440233236153,
                    0.11016949152542373,
                    0.09734513274336283,
                    0.4552981552981553
                ],
                [
                    0.21192443919716647,
                    0.20176630434782608,
                    0.22109782275375653,
                    0.2840352524392133,
                    0.07871720116618076,
                    0.00847457627118644,
                    0.008849557522123894,
                    0.315958815958816
                ],
                [
                    0.38665879574970485,
                    0.390625,
                    0.3925176326280282,
                    0.47748482039555795,
                    0.7638483965014577,
                    0.8813559322033898,
                    0.8938053097345132,
                    0.22874302874302876
                ]
            ],
            "fraction_answers": {
                "new york": 0.2817671392985063,
                "los angeles": 0.16635299620703367,
                "chicago": 0.5518798644944601
            },
            "question": "Laurie Metcalf, Amy Morton and Tracy Letts are members of a theatre company from what city?",
            "rate_limited": false,
            "answers": [
                "new york",
                "los angeles",
                "chicago"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "new york": 0.35540641618896895,
                "los angeles": 0.11481299604292367,
                "chicago": 0.3956576576410099
            },
            "z-best_answer_by_ml": [
                "chicago"
            ],
            "data": {
                "result_count_important_words": [
                    12000.0,
                    5940.0,
                    11500.0
                ],
                "wikipedia_search": [
                    0.9539197086609152,
                    1.1361410097568532,
                    1.9099392815822318
                ],
                "result_count_noun_chunks": [
                    12600.0,
                    7210.0,
                    12800.0
                ],
                "word_relation_to_question": [
                    2.2764907764907765,
                    1.57979407979408,
                    1.1437151437151438
                ],
                "word_count_noun_chunks": [
                    13.0,
                    1.0,
                    104.0
                ],
                "word_count_raw": [
                    11.0,
                    1.0,
                    101.0
                ],
                "result_count": [
                    13600.0,
                    7180.0,
                    13100.0
                ],
                "word_count_appended": [
                    54.0,
                    27.0,
                    262.0
                ]
            },
            "integer_answers": {
                "new york": 3,
                "los angeles": 0,
                "chicago": 5
            }
        },
        "lines": [
            [
                0,
                0.4014167650531287,
                0.4076086956521739,
                0.38638454461821525,
                0.2384799271652288,
                0.15743440233236153,
                0.11016949152542373,
                0.09734513274336283,
                0.4552981552981553
            ],
            [
                0,
                0.21192443919716647,
                0.20176630434782608,
                0.22109782275375653,
                0.2840352524392133,
                0.07871720116618076,
                0.00847457627118644,
                0.008849557522123894,
                0.315958815958816
            ],
            [
                1,
                0.38665879574970485,
                0.390625,
                0.3925176326280282,
                0.47748482039555795,
                0.7638483965014577,
                0.8813559322033898,
                0.8938053097345132,
                0.22874302874302876
            ]
        ]
    },
    "Which of these two U.S. cities are in the same time zone?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.9999320434533575,
                    0.0,
                    0.9999323717236516,
                    0.014492753623188406,
                    0.5,
                    0,
                    0,
                    0.2454948646125117
                ],
                [
                    6.79565466424612e-05,
                    6.55129491516073e-05,
                    6.762827634845952e-05,
                    0.7671497584541062,
                    0.2857142857142857,
                    0,
                    0,
                    0.48940242763772174
                ],
                [
                    0.0,
                    0.9999344870508484,
                    0.0,
                    0.2183574879227053,
                    0.21428571428571427,
                    0,
                    0,
                    0.2651027077497666
                ]
            ],
            "fraction_answers": {
                "el paso / pierre": 0.4599753389021182,
                "pensacola / sioux falls": 0.28294673283483907,
                "bismarck / cheyenne": 0.25707792826304265
            },
            "question": "Which of these two U.S. cities are in the same time zone?",
            "rate_limited": false,
            "answers": [
                "el paso / pierre",
                "bismarck / cheyenne",
                "pensacola / sioux falls"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "el paso / pierre": 0.34393939425547915,
                "pensacola / sioux falls": 0.32559523786462485,
                "bismarck / cheyenne": 0.32559523786462485
            },
            "integer_answers": {
                "el paso / pierre": 3,
                "pensacola / sioux falls": 1,
                "bismarck / cheyenne": 2
            },
            "data": {
                "result_count_important_words": [
                    0,
                    19.0,
                    290000.0
                ],
                "wikipedia_search": [
                    0.043478260869565216,
                    2.3014492753623186,
                    0.6550724637681159
                ],
                "result_count_noun_chunks": [
                    207000.0,
                    14.0,
                    0
                ],
                "word_relation_to_question": [
                    0.9819794584500467,
                    1.957609710550887,
                    1.0604108309990663
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    7.0,
                    4.0,
                    3.0
                ],
                "result_count": [
                    206000.0,
                    14.0,
                    0
                ]
            },
            "z-best_answer_by_ml": [
                "el paso / pierre"
            ]
        },
        "lines": [
            [
                0,
                0.9999320434533575,
                0.0,
                0.9999323717236516,
                0.014492753623188406,
                0.5,
                0,
                0,
                0.2454948646125117
            ],
            [
                0,
                6.79565466424612e-05,
                6.55129491516073e-05,
                6.762827634845952e-05,
                0.7671497584541062,
                0.2857142857142857,
                0,
                0,
                0.48940242763772174
            ],
            [
                1,
                0.0,
                0.9999344870508484,
                0.0,
                0.2183574879227053,
                0.21428571428571427,
                0,
                0,
                0.2651027077497666
            ]
        ]
    },
    "Until it was banned, lithium was a key ingredient in which of these brands?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.21347172435931733,
                    0.21846138232010556,
                    0.20127638684339716,
                    0.13513513513513514,
                    0.03684210526315789,
                    0.0,
                    0.0,
                    0.0793343653250774
                ],
                [
                    0.7864747739553796,
                    0.7814878717141988,
                    0.7985599738177058,
                    0.8648648648648649,
                    0.9421052631578948,
                    1.0,
                    1.0,
                    0.7502321981424149
                ],
                [
                    5.3501685303087046e-05,
                    5.074596569572719e-05,
                    0.00016363933889707084,
                    0.0,
                    0.021052631578947368,
                    0.0,
                    0.0,
                    0.17043343653250775
                ]
            ],
            "fraction_answers": {
                "cracker jack": 0.11056513740577381,
                "7up": 0.8654656182065573,
                "good and plenty": 0.023969244387668877
            },
            "question": "Until it was banned, lithium was a key ingredient in which of these brands?",
            "rate_limited": false,
            "answers": [
                "cracker jack",
                "7up",
                "good and plenty"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "cracker jack": 0.12445214226563382,
                "7up": 0.6397709768978518,
                "good and plenty": 0.13115916706799896
            },
            "z-best_answer_by_ml": [
                "7up"
            ],
            "data": {
                "result_count_important_words": [
                    8610.0,
                    30800.0,
                    2.0
                ],
                "wikipedia_search": [
                    0.40540540540540543,
                    2.5945945945945947,
                    0.0
                ],
                "result_count_noun_chunks": [
                    12300.0,
                    48800.0,
                    10.0
                ],
                "word_relation_to_question": [
                    0.3173374613003096,
                    3.0009287925696597,
                    0.681733746130031
                ],
                "word_count_noun_chunks": [
                    0.0,
                    11.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    6.0,
                    0.0
                ],
                "result_count": [
                    7980.0,
                    29400.0,
                    2.0
                ],
                "word_count_appended": [
                    7.0,
                    179.0,
                    4.0
                ]
            },
            "integer_answers": {
                "cracker jack": 0,
                "7up": 8,
                "good and plenty": 0
            }
        },
        "lines": [
            [
                0,
                0.21347172435931733,
                0.21846138232010556,
                0.20127638684339716,
                0.13513513513513514,
                0.03684210526315789,
                0.0,
                0.0,
                0.0793343653250774
            ],
            [
                1,
                0.7864747739553796,
                0.7814878717141988,
                0.7985599738177058,
                0.8648648648648649,
                0.9421052631578948,
                1.0,
                1.0,
                0.7502321981424149
            ],
            [
                0,
                5.3501685303087046e-05,
                5.074596569572719e-05,
                0.00016363933889707084,
                0.0,
                0.021052631578947368,
                0.0,
                0.0,
                0.17043343653250775
            ]
        ]
    },
    "Which of these phrases appears in a Shakespeare play?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    3.697342283142472e-05,
                    0.00010404322793439061,
                    5.785379560182589e-05,
                    0.813953488372093,
                    0.28125,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.8318112633181126
                ],
                [
                    0.9999630265771686,
                    0.9998959567720657,
                    0.9999421462043981,
                    0.13953488372093023,
                    0.6770833333333334,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.08637747336377473
                ],
                [
                    0.0,
                    0.0,
                    0.0,
                    0.046511627906976744,
                    0.041666666666666664,
                    0.3333333333333333,
                    0.3333333333333333,
                    0.08181126331811263
                ]
            ],
            "fraction_answers": {
                "at a loss": 0.5711829358297922,
                "in such a pickle": 0.32423503610040494,
                "up a dark creek": 0.10458202806980284
            },
            "question": "Which of these phrases appears in a Shakespeare play?",
            "rate_limited": false,
            "answers": [
                "in such a pickle",
                "at a loss",
                "up a dark creek"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "at a loss": 0.34232761393661615,
                "in such a pickle": 0.30114516136899444,
                "up a dark creek": 0.2799243327124689
            },
            "z-best_answer_by_ml": [
                "at a loss"
            ],
            "data": {
                "result_count_important_words": [
                    77.0,
                    740000.0,
                    0
                ],
                "wikipedia_search": [
                    0.813953488372093,
                    0.13953488372093023,
                    0.046511627906976744
                ],
                "result_count_noun_chunks": [
                    81.0,
                    1400000.0,
                    0
                ],
                "word_relation_to_question": [
                    3.3272450532724505,
                    0.3455098934550989,
                    0.3272450532724505
                ],
                "word_count_noun_chunks": [
                    1.0,
                    1.0,
                    1.0
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    1.0
                ],
                "word_count_appended": [
                    27.0,
                    65.0,
                    4.0
                ],
                "result_count": [
                    44.0,
                    1190000.0,
                    0
                ]
            },
            "integer_answers": {
                "at a loss": 4,
                "in such a pickle": 4,
                "up a dark creek": 0
            }
        },
        "lines": [
            [
                1,
                3.697342283142472e-05,
                0.00010404322793439061,
                5.785379560182589e-05,
                0.813953488372093,
                0.28125,
                0.3333333333333333,
                0.3333333333333333,
                0.8318112633181126
            ],
            [
                0,
                0.9999630265771686,
                0.9998959567720657,
                0.9999421462043981,
                0.13953488372093023,
                0.6770833333333334,
                0.3333333333333333,
                0.3333333333333333,
                0.08637747336377473
            ],
            [
                0,
                0.0,
                0.0,
                0.0,
                0.046511627906976744,
                0.041666666666666664,
                0.3333333333333333,
                0.3333333333333333,
                0.08181126331811263
            ]
        ]
    },
    "Which of these Kentucky Derby winners was named for its trainer?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.47520486609782886,
                    0.5369003690036901,
                    0.5331929792443763,
                    0.7333333333333334,
                    0.6194690265486725,
                    0,
                    1.0,
                    0.8540414915160627
                ],
                [
                    0.5237813635211624,
                    0.4022140221402214,
                    0.4645415226893064,
                    0.0,
                    0.34513274336283184,
                    0,
                    0.0,
                    0.027921818907060227
                ],
                [
                    0.0010137703810087016,
                    0.06088560885608856,
                    0.002265498066317307,
                    0.26666666666666666,
                    0.035398230088495575,
                    0,
                    0.0,
                    0.11803668957687703
                ]
            ],
            "fraction_answers": {
                "lieut. gibson": 0.06918092337649341,
                "paul jones": 0.25194163866008323,
                "clyde van dusen": 0.6788774379634234
            },
            "question": "Which of these Kentucky Derby winners was named for its trainer?",
            "rate_limited": false,
            "answers": [
                "clyde van dusen",
                "paul jones",
                "lieut. gibson"
            ],
            "ml_answers": {
                "lieut. gibson": 0.12380268039835694,
                "paul jones": 0.2077785325965988,
                "clyde van dusen": 0.5669251621355246
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "lieut. gibson": 0,
                "paul jones": 1,
                "clyde van dusen": 6
            },
            "data": {
                "result_count_important_words": [
                    29100.0,
                    21800.0,
                    3300.0
                ],
                "wikipedia_search": [
                    2.2,
                    0.0,
                    0.8
                ],
                "result_count_noun_chunks": [
                    23300.0,
                    20300.0,
                    99.0
                ],
                "word_relation_to_question": [
                    4.270207457580314,
                    0.13960909453530115,
                    0.5901834478843853
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    2.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    70.0,
                    39.0,
                    4.0
                ],
                "result_count": [
                    22500.0,
                    24800.0,
                    48.0
                ]
            },
            "z-best_answer_by_ml": [
                "clyde van dusen"
            ]
        },
        "lines": [
            [
                1,
                0.47520486609782886,
                0.5369003690036901,
                0.5331929792443763,
                0.7333333333333334,
                0.6194690265486725,
                0,
                1.0,
                0.8540414915160627
            ],
            [
                0,
                0.5237813635211624,
                0.4022140221402214,
                0.4645415226893064,
                0.0,
                0.34513274336283184,
                0,
                0.0,
                0.027921818907060227
            ],
            [
                0,
                0.0010137703810087016,
                0.06088560885608856,
                0.002265498066317307,
                0.26666666666666666,
                0.035398230088495575,
                0,
                0.0,
                0.11803668957687703
            ]
        ]
    },
    "Which of these Hebrew texts does NOT form a significant part of the Christian Old Testament?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.12247191011235953,
                    0.20167064439140814,
                    0.09548813691170749,
                    0.34888075894904846,
                    0.29171528588098017,
                    0.02941176470588236,
                    0.019230769230769218,
                    0.3482146311196409
                ],
                [
                    0.4669662921348315,
                    0.47732696897374705,
                    0.4723842862699339,
                    0.44733373213524646,
                    0.3687281213535589,
                    0.5,
                    0.5,
                    0.30882049095289177
                ],
                [
                    0.410561797752809,
                    0.32100238663484487,
                    0.4321275768183586,
                    0.20378550891570507,
                    0.3395565927654609,
                    0.47058823529411764,
                    0.4807692307692308,
                    0.3429648779274674
                ]
            ],
            "fraction_answers": {
                "talmud": 0.24966094828050145,
                "ketuvim": 0.11461002704494762,
                "torah": 0.635729024674551
            },
            "question": "Which of these Hebrew texts does NOT form a significant part of the Christian Old Testament?",
            "rate_limited": false,
            "answers": [
                "torah",
                "ketuvim",
                "talmud"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "talmud": 0.34393939425547915,
                "ketuvim": 0.34393939425547915,
                "torah": 0.32559523786462485
            },
            "integer_answers": {
                "talmud": 1,
                "ketuvim": 1,
                "torah": 6
            },
            "data": {
                "result_count_important_words": [
                    2250000.0,
                    171000.0,
                    1350000.0
                ],
                "wikipedia_search": [
                    2.4179078568152246,
                    0.8426602858360566,
                    4.739431857348719
                ],
                "result_count_noun_chunks": [
                    2080000.0,
                    142000.0,
                    349000.0
                ],
                "word_relation_to_question": [
                    2.428565902085746,
                    3.058872144753732,
                    2.512561953160522
                ],
                "word_count_noun_chunks": [
                    16.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    25.0,
                    0.0,
                    1.0
                ],
                "word_count_appended": [
                    357.0,
                    225.0,
                    275.0
                ],
                "result_count": [
                    1680000.0,
                    147000.0,
                    398000.0
                ]
            },
            "z-best_answer_by_ml": [
                "ketuvim",
                "talmud"
            ]
        },
        "lines": [
            [
                0,
                0.12247191011235953,
                0.20167064439140814,
                0.09548813691170749,
                0.34888075894904846,
                0.29171528588098017,
                0.02941176470588236,
                0.019230769230769218,
                0.3482146311196409
            ],
            [
                0,
                0.4669662921348315,
                0.47732696897374705,
                0.4723842862699339,
                0.44733373213524646,
                0.3687281213535589,
                0.5,
                0.5,
                0.30882049095289177
            ],
            [
                1,
                0.410561797752809,
                0.32100238663484487,
                0.4321275768183586,
                0.20378550891570507,
                0.3395565927654609,
                0.47058823529411764,
                0.4807692307692308,
                0.3429648779274674
            ]
        ]
    },
    "How many of the three Baltic countries border Russia?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.024865748307261265,
                    0.8507175617694925,
                    0.4815533980582524,
                    0.23643892339544514,
                    0.4739583333333333,
                    0.6743002544529262,
                    0.6657608695652174,
                    0.3808616863905326
                ],
                [
                    0.029535372402521597,
                    0.11762094984465157,
                    0.3504854368932039,
                    0.17163561076604553,
                    0.22708333333333333,
                    0.022900763358778626,
                    0.016304347826086956,
                    0.4142566568047337
                ],
                [
                    0.9455988792902171,
                    0.0316614883858559,
                    0.1679611650485437,
                    0.5919254658385094,
                    0.2989583333333333,
                    0.30279898218829515,
                    0.3179347826086957,
                    0.20488165680473375
                ]
            ],
            "fraction_answers": {
                "none": 0.35771509418727304,
                "three": 0.4735570969090576,
                "two": 0.16872780890366942
            },
            "question": "How many of the three Baltic countries border Russia?",
            "rate_limited": false,
            "answers": [
                "three",
                "two",
                "none"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "none": 0.23169766494206856,
                "three": 0.6208548915605777,
                "two": 0.12005857948983416
            },
            "integer_answers": {
                "none": 2,
                "three": 5,
                "two": 1
            },
            "data": {
                "result_count_important_words": [
                    11500000.0,
                    1590000.0,
                    428000.0
                ],
                "wikipedia_search": [
                    0.7093167701863354,
                    0.5149068322981366,
                    1.775776397515528
                ],
                "result_count_noun_chunks": [
                    4960000.0,
                    3610000.0,
                    1730000.0
                ],
                "word_relation_to_question": [
                    1.5234467455621303,
                    1.657026627218935,
                    0.819526627218935
                ],
                "word_count_noun_chunks": [
                    265.0,
                    9.0,
                    119.0
                ],
                "word_count_raw": [
                    245.0,
                    6.0,
                    117.0
                ],
                "result_count": [
                    2130000.0,
                    2530000.0,
                    81000000.0
                ],
                "word_count_appended": [
                    455.0,
                    218.0,
                    287.0
                ]
            },
            "z-best_answer_by_ml": [
                "three"
            ]
        },
        "lines": [
            [
                1,
                0.024865748307261265,
                0.8507175617694925,
                0.4815533980582524,
                0.23643892339544514,
                0.4739583333333333,
                0.6743002544529262,
                0.6657608695652174,
                0.3808616863905326
            ],
            [
                0,
                0.029535372402521597,
                0.11762094984465157,
                0.3504854368932039,
                0.17163561076604553,
                0.22708333333333333,
                0.022900763358778626,
                0.016304347826086956,
                0.4142566568047337
            ],
            [
                0,
                0.9455988792902171,
                0.0316614883858559,
                0.1679611650485437,
                0.5919254658385094,
                0.2989583333333333,
                0.30279898218829515,
                0.3179347826086957,
                0.20488165680473375
            ]
        ]
    },
    "Which of these actors was a high school cheerleader?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.25115562403698,
                    0.2925531914893617,
                    0.2507082152974504,
                    0.36481481481481487,
                    0.4825174825174825,
                    0.0,
                    0,
                    0.2738095238095238
                ],
                [
                    0.576271186440678,
                    0.5833333333333334,
                    0.5821529745042493,
                    0.10555555555555556,
                    0.23076923076923078,
                    1.0,
                    0,
                    0.44047619047619047
                ],
                [
                    0.17257318952234207,
                    0.12411347517730496,
                    0.1671388101983003,
                    0.5296296296296297,
                    0.2867132867132867,
                    0.0,
                    0,
                    0.28571428571428575
                ]
            ],
            "fraction_answers": {
                "john travolta": 0.22369752527930706,
                "george clooney": 0.27365126456651623,
                "michael douglas": 0.5026512101541768
            },
            "question": "Which of these actors was a high school cheerleader?",
            "rate_limited": false,
            "answers": [
                "george clooney",
                "michael douglas",
                "john travolta"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "john travolta": 0.17157019866324608,
                "george clooney": 0.42919856437288756,
                "michael douglas": 0.4519026346005104
            },
            "integer_answers": {
                "john travolta": 1,
                "george clooney": 1,
                "michael douglas": 5
            },
            "data": {
                "result_count_important_words": [
                    330000.0,
                    658000.0,
                    140000.0
                ],
                "wikipedia_search": [
                    1.0944444444444446,
                    0.31666666666666665,
                    1.588888888888889
                ],
                "result_count_noun_chunks": [
                    177000.0,
                    411000.0,
                    118000.0
                ],
                "word_relation_to_question": [
                    0.8214285714285714,
                    1.3214285714285714,
                    0.8571428571428572
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    69.0,
                    33.0,
                    41.0
                ],
                "result_count": [
                    163000.0,
                    374000.0,
                    112000.0
                ]
            },
            "z-best_answer_by_ml": [
                "michael douglas"
            ]
        },
        "lines": [
            [
                0,
                0.25115562403698,
                0.2925531914893617,
                0.2507082152974504,
                0.36481481481481487,
                0.4825174825174825,
                0.0,
                0,
                0.2738095238095238
            ],
            [
                1,
                0.576271186440678,
                0.5833333333333334,
                0.5821529745042493,
                0.10555555555555556,
                0.23076923076923078,
                1.0,
                0,
                0.44047619047619047
            ],
            [
                0,
                0.17257318952234207,
                0.12411347517730496,
                0.1671388101983003,
                0.5296296296296297,
                0.2867132867132867,
                0.0,
                0,
                0.28571428571428575
            ]
        ]
    },
    "J.K. Rowling\u2019s first book published in England was titled \u201cHarry Potter and the\u201d what?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.29314546453282336,
                    0.1423065976415566,
                    0.805974769423457,
                    0.0700354609929078,
                    0.4,
                    0,
                    0,
                    0.2543926106587581
                ],
                [
                    0.7068367275651255,
                    0.8576857101099222,
                    0.19400454626830116,
                    0.874113475177305,
                    0.3,
                    0,
                    0,
                    0.6378660118765775
                ],
                [
                    1.7807902051059364e-05,
                    7.692248521165222e-06,
                    2.0684308241840934e-05,
                    0.05585106382978724,
                    0.3,
                    0,
                    0,
                    0.10774137746466436
                ]
            ],
            "fraction_answers": {
                "magician's stone": 0.07727310429221093,
                "philosopher's stone": 0.3276424838749172,
                "sorcerer's stone": 0.5950844118328719
            },
            "question": "J.K. Rowling\u2019s first book published in England was titled \u201cHarry Potter and the\u201d what?",
            "rate_limited": false,
            "answers": [
                "philosopher's stone",
                "sorcerer's stone",
                "magician's stone"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "magician's stone": 0.15760893429315498,
                "philosopher's stone": 0.2789755185857349,
                "sorcerer's stone": 0.3836535389980767
            },
            "z-best_answer_by_ml": [
                "sorcerer's stone"
            ],
            "data": {
                "result_count_important_words": [
                    370000.0,
                    2230000.0,
                    20.0
                ],
                "wikipedia_search": [
                    0.42021276595744683,
                    5.24468085106383,
                    0.3351063829787234
                ],
                "result_count_noun_chunks": [
                    1130000.0,
                    272000.0,
                    29.0
                ],
                "word_relation_to_question": [
                    2.035140885270065,
                    5.10292809501262,
                    0.8619310197173149
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    214000.0,
                    516000.0,
                    13.0
                ],
                "word_count_appended": [
                    4.0,
                    3.0,
                    3.0
                ]
            },
            "integer_answers": {
                "magician's stone": 0,
                "philosopher's stone": 2,
                "sorcerer's stone": 4
            }
        },
        "lines": [
            [
                1,
                0.29314546453282336,
                0.1423065976415566,
                0.805974769423457,
                0.0700354609929078,
                0.4,
                0,
                0,
                0.2543926106587581
            ],
            [
                0,
                0.7068367275651255,
                0.8576857101099222,
                0.19400454626830116,
                0.874113475177305,
                0.3,
                0,
                0,
                0.6378660118765775
            ],
            [
                0,
                1.7807902051059364e-05,
                7.692248521165222e-06,
                2.0684308241840934e-05,
                0.05585106382978724,
                0.3,
                0,
                0,
                0.10774137746466436
            ]
        ]
    },
    "Which former NFL star does NOT have a football video game named after him?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.4232594936708861,
                    0.4531872509960159,
                    0.4391040072039622,
                    0.41392722148689676,
                    0.35542168674698793,
                    0,
                    0,
                    0.29804653521056235
                ],
                [
                    0.2895569620253165,
                    0.18874501992031872,
                    0.1645655110310671,
                    0.2819775742620657,
                    0.3253012048192771,
                    0,
                    0,
                    0.33986889088101413
                ],
                [
                    0.2871835443037975,
                    0.35806772908366535,
                    0.39633048176497077,
                    0.30409520425103753,
                    0.3192771084337349,
                    0,
                    0,
                    0.36208457390842347
                ]
            ],
            "fraction_answers": {
                "kurt warner": 0.3243204527514569,
                "brett favre": 0.46999494568698025,
                "emmitt smith": 0.2056846015615629
            },
            "question": "Which former NFL star does NOT have a football video game named after him?",
            "rate_limited": false,
            "answers": [
                "emmitt smith",
                "brett favre",
                "kurt warner"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "kurt warner": 0.28638353297773866,
                "brett favre": 0.3049087145126065,
                "emmitt smith": 0.28638353297773866
            },
            "z-best_answer_by_ml": [
                "brett favre"
            ],
            "data": {
                "result_count_important_words": [
                    37600.0,
                    250000.0,
                    114000.0
                ],
                "wikipedia_search": [
                    1.2050188991834456,
                    3.05231396033108,
                    2.7426671404854748
                ],
                "result_count_noun_chunks": [
                    54100.0,
                    298000.0,
                    92100.0
                ],
                "word_relation_to_question": [
                    2.8273485070521267,
                    2.2418355276658017,
                    1.9308159652820716
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    120.0,
                    145.0,
                    150.0
                ],
                "result_count": [
                    29100.0,
                    79800.0,
                    80700.0
                ]
            },
            "integer_answers": {
                "kurt warner": 2,
                "brett favre": 3,
                "emmitt smith": 1
            }
        },
        "lines": [
            [
                0,
                0.4232594936708861,
                0.4531872509960159,
                0.4391040072039622,
                0.41392722148689676,
                0.35542168674698793,
                0,
                0,
                0.29804653521056235
            ],
            [
                1,
                0.2895569620253165,
                0.18874501992031872,
                0.1645655110310671,
                0.2819775742620657,
                0.3253012048192771,
                0,
                0,
                0.33986889088101413
            ],
            [
                0,
                0.2871835443037975,
                0.35806772908366535,
                0.39633048176497077,
                0.30409520425103753,
                0.3192771084337349,
                0,
                0,
                0.36208457390842347
            ]
        ]
    },
    "How do you spell the last name of Duke University\u2019s men's basketball coach?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.00014632005072428425,
                    0.0,
                    5.0417626002050315e-05,
                    0.0,
                    0.0572289156626506,
                    0.0,
                    0.0,
                    0.34490262777371916
                ],
                [
                    0.9998536799492758,
                    1.0,
                    0.999949582373998,
                    1.0,
                    0.8945783132530121,
                    1.0,
                    1.0,
                    0.3137801787941273
                ],
                [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.04819277108433735,
                    0.0,
                    0.0,
                    0.34131719343215355
                ]
            ],
            "fraction_answers": {
                "khzyrweski": 0.04868874556456136,
                "crzyzewski": 0.05029103513913701,
                "krzyzewski": 0.9010202192963016
            },
            "question": "How do you spell the last name of Duke University\u2019s men's basketball coach?",
            "rate_limited": false,
            "answers": [
                "crzyzewski",
                "krzyzewski",
                "khzyrweski"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "khzyrweski": 0.31157143216045696,
                "crzyzewski": 0.31157143216045696,
                "krzyzewski": 0.4420214376981768
            },
            "z-best_answer_by_ml": [
                "krzyzewski"
            ],
            "data": {
                "result_count_important_words": [
                    0,
                    399000.0,
                    0
                ],
                "wikipedia_search": [
                    0.0,
                    6.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    3.0,
                    59500.0,
                    0
                ],
                "word_relation_to_question": [
                    2.4143183944160342,
                    2.196461251558891,
                    2.389220354025075
                ],
                "word_count_noun_chunks": [
                    0.0,
                    118.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    33.0,
                    0.0
                ],
                "result_count": [
                    3.0,
                    20500.0,
                    0
                ],
                "word_count_appended": [
                    19.0,
                    297.0,
                    16.0
                ]
            },
            "integer_answers": {
                "khzyrweski": 0,
                "crzyzewski": 1,
                "krzyzewski": 7
            }
        },
        "lines": [
            [
                0,
                0.00014632005072428425,
                0.0,
                5.0417626002050315e-05,
                0.0,
                0.0572289156626506,
                0.0,
                0.0,
                0.34490262777371916
            ],
            [
                1,
                0.9998536799492758,
                1.0,
                0.999949582373998,
                1.0,
                0.8945783132530121,
                1.0,
                1.0,
                0.3137801787941273
            ],
            [
                0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.04819277108433735,
                0.0,
                0.0,
                0.34131719343215355
            ]
        ]
    },
    "Which of these consists of frozen water?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.969797013231539,
                    0.255860150973381,
                    0.9062424680645939,
                    0.6818181818181819,
                    0.5117096018735363,
                    0,
                    0,
                    0.5833333333333334
                ],
                [
                    0.02656624796884269,
                    0.520460866110449,
                    0.06724511930585683,
                    0.0,
                    0.04566744730679157,
                    0,
                    0,
                    0.08333333333333333
                ],
                [
                    0.0036367387996182714,
                    0.22367898291617005,
                    0.02651241262954929,
                    0.3181818181818182,
                    0.4426229508196721,
                    0,
                    0,
                    0.3333333333333333
                ]
            ],
            "fraction_answers": {
                "garden rake": 0.12387883567087891,
                "snowflake": 0.6514601248824275,
                "drake": 0.2246610394466935
            },
            "question": "Which of these consists of frozen water?",
            "rate_limited": false,
            "answers": [
                "snowflake",
                "garden rake",
                "drake"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "garden rake": 0.29035060350393144,
                "snowflake": 0.3739747133846042,
                "drake": 0.3739747133846042
            },
            "z-best_answer_by_ml": [
                "snowflake",
                "drake"
            ],
            "data": {
                "result_count_important_words": [
                    644000.0,
                    1310000.0,
                    563000.0
                ],
                "wikipedia_search": [
                    1.3636363636363638,
                    0.0,
                    0.6363636363636364
                ],
                "result_count_noun_chunks": [
                    37600000.0,
                    2790000.0,
                    1100000.0
                ],
                "word_relation_to_question": [
                    1.1666666666666667,
                    0.16666666666666666,
                    0.6666666666666666
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    376000000.0,
                    10300000.0,
                    1410000.0
                ],
                "word_count_appended": [
                    437.0,
                    39.0,
                    378.0
                ]
            },
            "integer_answers": {
                "garden rake": 1,
                "snowflake": 5,
                "drake": 0
            }
        },
        "lines": [
            [
                1,
                0.969797013231539,
                0.255860150973381,
                0.9062424680645939,
                0.6818181818181819,
                0.5117096018735363,
                0,
                0,
                0.5833333333333334
            ],
            [
                0,
                0.02656624796884269,
                0.520460866110449,
                0.06724511930585683,
                0.0,
                0.04566744730679157,
                0,
                0,
                0.08333333333333333
            ],
            [
                0,
                0.0036367387996182714,
                0.22367898291617005,
                0.02651241262954929,
                0.3181818181818182,
                0.4426229508196721,
                0,
                0,
                0.3333333333333333
            ]
        ]
    },
    "What word describes joining a cause just to feel good about it?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.002414077885775999,
                    0.003265784625690839,
                    0.0019958622368260923,
                    0.07491289198606273,
                    0.08634538152610442,
                    0,
                    0,
                    0.30848595848595844
                ],
                [
                    0.0001905850962454736,
                    0.0002512142019762184,
                    7.301935012778386e-05,
                    0.2667426405735593,
                    0.04417670682730924,
                    0,
                    0,
                    0.10348901098901098
                ],
                [
                    0.9973953370179786,
                    0.9964830011723329,
                    0.9979311184130462,
                    0.6583444674403779,
                    0.8694779116465864,
                    0,
                    0,
                    0.5880250305250305
                ]
            ],
            "fraction_answers": {
                "slacktivism": 0.8512761443692254,
                "gung-faux": 0.06915386283970484,
                "joinerism": 0.07956999279106977
            },
            "question": "What word describes joining a cause just to feel good about it?",
            "rate_limited": false,
            "answers": [
                "joinerism",
                "gung-faux",
                "slacktivism"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "slacktivism": 0.36557513195159863,
                "gung-faux": 0.13568204103564258,
                "joinerism": 0.17355949582468344
            },
            "z-best_answer_by_ml": [
                "slacktivism"
            ],
            "data": {
                "result_count_important_words": [
                    39.0,
                    3.0,
                    11900.0
                ],
                "wikipedia_search": [
                    0.524390243902439,
                    1.8671984840149152,
                    4.608411272082646
                ],
                "result_count_noun_chunks": [
                    82.0,
                    3.0,
                    41000.0
                ],
                "word_relation_to_question": [
                    2.1594017094017093,
                    0.7244230769230768,
                    4.116175213675214
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    38.0,
                    3.0,
                    15700.0
                ],
                "word_count_appended": [
                    43.0,
                    22.0,
                    433.0
                ]
            },
            "integer_answers": {
                "slacktivism": 6,
                "gung-faux": 0,
                "joinerism": 0
            }
        },
        "lines": [
            [
                0,
                0.002414077885775999,
                0.003265784625690839,
                0.0019958622368260923,
                0.07491289198606273,
                0.08634538152610442,
                0,
                0,
                0.30848595848595844
            ],
            [
                0,
                0.0001905850962454736,
                0.0002512142019762184,
                7.301935012778386e-05,
                0.2667426405735593,
                0.04417670682730924,
                0,
                0,
                0.10348901098901098
            ],
            [
                1,
                0.9973953370179786,
                0.9964830011723329,
                0.9979311184130462,
                0.6583444674403779,
                0.8694779116465864,
                0,
                0,
                0.5880250305250305
            ]
        ]
    },
    "One of Apple\u2019s biggest flops was a product named after a man who did what?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.3007518796992481,
                    0.31496062992125984,
                    0.4029126213592233,
                    0.39820885250651983,
                    0.38095238095238093,
                    0,
                    0,
                    0.33379552174732896
                ],
                [
                    0.45864661654135336,
                    0.4566929133858268,
                    0.34951456310679613,
                    0.2520847419427541,
                    0.42857142857142855,
                    0,
                    0,
                    0.47473209521402293
                ],
                [
                    0.24060150375939848,
                    0.2283464566929134,
                    0.24757281553398058,
                    0.349706405550726,
                    0.19047619047619047,
                    0,
                    0,
                    0.19147238303864808
                ]
            ],
            "fraction_answers": {
                "invented the transistor": 0.4033737264603636,
                "developed calculus": 0.24136262584197618,
                "discovered saturn": 0.3552636476976601
            },
            "question": "One of Apple\u2019s biggest flops was a product named after a man who did what?",
            "rate_limited": false,
            "answers": [
                "discovered saturn",
                "invented the transistor",
                "developed calculus"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "invented the transistor": 0.4444108098531048,
                "developed calculus": 0.22277106702271024,
                "discovered saturn": 0.2866161390246948
            },
            "z-best_answer_by_ml": [
                "invented the transistor"
            ],
            "data": {
                "result_count_important_words": [
                    40.0,
                    58.0,
                    29.0
                ],
                "wikipedia_search": [
                    1.5928354100260793,
                    1.0083389677710164,
                    1.398825622202904
                ],
                "result_count_noun_chunks": [
                    83.0,
                    72.0,
                    51.0
                ],
                "word_relation_to_question": [
                    1.6689776087366448,
                    2.3736604760701145,
                    0.9573619151932404
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    40.0,
                    61.0,
                    32.0
                ],
                "word_count_appended": [
                    8.0,
                    9.0,
                    4.0
                ]
            },
            "integer_answers": {
                "invented the transistor": 4,
                "developed calculus": 0,
                "discovered saturn": 2
            }
        },
        "lines": [
            [
                0,
                0.3007518796992481,
                0.31496062992125984,
                0.4029126213592233,
                0.39820885250651983,
                0.38095238095238093,
                0,
                0,
                0.33379552174732896
            ],
            [
                0,
                0.45864661654135336,
                0.4566929133858268,
                0.34951456310679613,
                0.2520847419427541,
                0.42857142857142855,
                0,
                0,
                0.47473209521402293
            ],
            [
                1,
                0.24060150375939848,
                0.2283464566929134,
                0.24757281553398058,
                0.349706405550726,
                0.19047619047619047,
                0,
                0,
                0.19147238303864808
            ]
        ]
    },
    "What dish is made with ham, poached eggs and Hollandaise sauce?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.005584287721182546,
                    0.00784387719345535,
                    0.06006779661016949,
                    0.0,
                    0.16333333333333333,
                    0.0,
                    0.0,
                    0.10522342270241432
                ],
                [
                    0.9942756186495753,
                    0.9919404835709857,
                    0.8094915254237288,
                    0.7856215213358071,
                    0.7066666666666667,
                    1.0,
                    1.0,
                    0.8020792872868342
                ],
                [
                    0.00014009362924221022,
                    0.00021563923555890994,
                    0.1304406779661017,
                    0.21437847866419293,
                    0.13,
                    0.0,
                    0.0,
                    0.09269729001075147
                ]
            ],
            "fraction_answers": {
                "eggs benedict": 0.8862593878666998,
                "benedict cumberbatch": 0.0709840224382309,
                "pope benedict": 0.04275658969506938
            },
            "question": "What dish is made with ham, poached eggs and Hollandaise sauce?",
            "rate_limited": false,
            "answers": [
                "pope benedict",
                "eggs benedict",
                "benedict cumberbatch"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "eggs benedict": 0.4420214376981768,
                "benedict cumberbatch": 0.29035060350393144,
                "pope benedict": 0.29035060350393144
            },
            "z-best_answer_by_ml": [
                "eggs benedict"
            ],
            "data": {
                "result_count_important_words": [
                    2910.0,
                    368000.0,
                    80.0
                ],
                "wikipedia_search": [
                    0.0,
                    5.499350649350649,
                    1.5006493506493506
                ],
                "result_count_noun_chunks": [
                    44300.0,
                    597000.0,
                    96200.0
                ],
                "word_relation_to_question": [
                    0.7365639589169002,
                    5.614555011007839,
                    0.6488810300752603
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    155.0,
                    0.0
                ],
                "result_count": [
                    2870.0,
                    511000.0,
                    72.0
                ],
                "word_count_appended": [
                    49.0,
                    212.0,
                    39.0
                ]
            },
            "integer_answers": {
                "eggs benedict": 8,
                "benedict cumberbatch": 0,
                "pope benedict": 0
            }
        },
        "lines": [
            [
                0,
                0.005584287721182546,
                0.00784387719345535,
                0.06006779661016949,
                0.0,
                0.16333333333333333,
                0.0,
                0.0,
                0.10522342270241432
            ],
            [
                1,
                0.9942756186495753,
                0.9919404835709857,
                0.8094915254237288,
                0.7856215213358071,
                0.7066666666666667,
                1.0,
                1.0,
                0.8020792872868342
            ],
            [
                0,
                0.00014009362924221022,
                0.00021563923555890994,
                0.1304406779661017,
                0.21437847866419293,
                0.13,
                0.0,
                0.0,
                0.09269729001075147
            ]
        ]
    },
    "Which Oscar-winning actress has NOT won the award for playing a real person?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.4743912018853103,
                    0.4058205335489087,
                    0.4706616729088639,
                    0.1600394205661591,
                    0.28075709779179814,
                    0.5,
                    0,
                    0.36796230021509735
                ],
                [
                    0.04438334642576591,
                    0.17502021018593372,
                    0.050561797752809,
                    0.3952511703368479,
                    0.35962145110410093,
                    0.5,
                    0,
                    0.34697846067169436
                ],
                [
                    0.4812254516889238,
                    0.41915925626515765,
                    0.4787765293383271,
                    0.44470940909699297,
                    0.35962145110410093,
                    0.0,
                    0,
                    0.2850592391132083
                ]
            ],
            "fraction_answers": {
                "hilary swank": 0.2946996181123684,
                "emma thompson": 0.24010507802396072,
                "susan sarandon": 0.46519530386367086
            },
            "question": "Which Oscar-winning actress has NOT won the award for playing a real person?",
            "rate_limited": false,
            "answers": [
                "emma thompson",
                "susan sarandon",
                "hilary swank"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "hilary swank": 0.28638353297773866,
                "emma thompson": 0.35443025729131133,
                "susan sarandon": 0.3796181564740296
            },
            "z-best_answer_by_ml": [
                "susan sarandon"
            ],
            "data": {
                "result_count_important_words": [
                    2330000.0,
                    8040000.0,
                    2000000.0
                ],
                "wikipedia_search": [
                    6.119290429809136,
                    1.8854789339367375,
                    0.9952306362541261
                ],
                "result_count_noun_chunks": [
                    329000.0,
                    5040000.0,
                    238000.0
                ],
                "word_relation_to_question": [
                    2.3766785961282473,
                    2.754387707909502,
                    3.8689336959622507
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    326000.0,
                    5800000.0,
                    239000.0
                ],
                "word_count_appended": [
                    139.0,
                    89.0,
                    89.0
                ]
            },
            "integer_answers": {
                "hilary swank": 2,
                "emma thompson": 2,
                "susan sarandon": 3
            }
        },
        "lines": [
            [
                1,
                0.4743912018853103,
                0.4058205335489087,
                0.4706616729088639,
                0.1600394205661591,
                0.28075709779179814,
                0.5,
                0,
                0.36796230021509735
            ],
            [
                0,
                0.04438334642576591,
                0.17502021018593372,
                0.050561797752809,
                0.3952511703368479,
                0.35962145110410093,
                0.5,
                0,
                0.34697846067169436
            ],
            [
                0,
                0.4812254516889238,
                0.41915925626515765,
                0.4787765293383271,
                0.44470940909699297,
                0.35962145110410093,
                0.0,
                0,
                0.2850592391132083
            ]
        ]
    },
    "The best-selling book \u201cThe Chocolate War\u201d is about what?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.0,
                    0.0009366036410466546,
                    0.000571390158644797,
                    0.061224489795918366,
                    0.13636363636363635,
                    0,
                    0,
                    0.2323134543359262
                ],
                [
                    1.0,
                    0.9980682549903412,
                    0.9982522183382629,
                    0.2571428571428571,
                    0.7727272727272727,
                    0,
                    0,
                    0.12333599442270855
                ],
                [
                    0.0,
                    0.0009951413686120705,
                    0.001176391503092229,
                    0.6816326530612244,
                    0.09090909090909091,
                    0,
                    0,
                    0.6443505512413653
                ]
            ],
            "fraction_answers": {
                "high school conformity": 0.07190159571586206,
                "the rise of hershey's": 0.2365106380138975,
                "sugar addiction": 0.6915877662702403
            },
            "question": "The best-selling book \u201cThe Chocolate War\u201d is about what?",
            "rate_limited": false,
            "answers": [
                "high school conformity",
                "sugar addiction",
                "the rise of hershey's"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "high school conformity": 0.29035060350393144,
                "the rise of hershey's": 0.31157143216045696,
                "sugar addiction": 0.35275388472807867
            },
            "integer_answers": {
                "high school conformity": 0,
                "the rise of hershey's": 2,
                "sugar addiction": 4
            },
            "data": {
                "result_count_important_words": [
                    32.0,
                    34100.0,
                    34.0
                ],
                "wikipedia_search": [
                    0.30612244897959184,
                    1.2857142857142856,
                    3.4081632653061225
                ],
                "result_count_noun_chunks": [
                    17.0,
                    29700.0,
                    35.0
                ],
                "word_relation_to_question": [
                    1.161567271679631,
                    0.6166799721135428,
                    3.2217527562068264
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    3.0,
                    17.0,
                    2.0
                ],
                "result_count": [
                    0,
                    80.0,
                    0
                ]
            },
            "z-best_answer_by_ml": [
                "sugar addiction"
            ]
        },
        "lines": [
            [
                1,
                0.0,
                0.0009366036410466546,
                0.000571390158644797,
                0.061224489795918366,
                0.13636363636363635,
                0,
                0,
                0.2323134543359262
            ],
            [
                0,
                1.0,
                0.9980682549903412,
                0.9982522183382629,
                0.2571428571428571,
                0.7727272727272727,
                0,
                0,
                0.12333599442270855
            ],
            [
                0,
                0.0,
                0.0009951413686120705,
                0.001176391503092229,
                0.6816326530612244,
                0.09090909090909091,
                0,
                0,
                0.6443505512413653
            ]
        ]
    },
    "What form of transportation counts Jay-Z as a prominent investor?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.5224132119986519,
                    0.5646747863697118,
                    0.09217264081931237,
                    0.18208815028901731,
                    0.44591029023746703,
                    0.5,
                    0.3333333333333333,
                    0.3779274481426236
                ],
                [
                    0.00910010111223458,
                    0.008815534563803745,
                    0.015362106803218726,
                    0.5325722543352601,
                    0.14775725593667546,
                    0.25,
                    0.3333333333333333,
                    0.261251699193967
                ],
                [
                    0.4684866868891136,
                    0.42650967906648446,
                    0.8924652523774689,
                    0.2853395953757225,
                    0.40633245382585753,
                    0.25,
                    0.3333333333333333,
                    0.3608208526634094
                ]
            ],
            "fraction_answers": {
                "boats": 0.4279109816914238,
                "aviation": 0.3773149826487647,
                "e-bikes": 0.1947740356598116
            },
            "question": "What form of transportation counts Jay-Z as a prominent investor?",
            "rate_limited": false,
            "answers": [
                "aviation",
                "e-bikes",
                "boats"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "boats": 0.42671822599500986,
                "aviation": 0.4237575328734394,
                "e-bikes": 0.31067248119132707
            },
            "integer_answers": {
                "boats": 1,
                "aviation": 6,
                "e-bikes": 1
            },
            "data": {
                "result_count_important_words": [
                    3760000.0,
                    58700.0,
                    2840000.0
                ],
                "wikipedia_search": [
                    0.9104407514450866,
                    2.6628612716763005,
                    1.4266979768786126
                ],
                "result_count_noun_chunks": [
                    189000.0,
                    31500.0,
                    1830000.0
                ],
                "word_relation_to_question": [
                    2.2675646888557415,
                    1.5675101951638022,
                    2.1649251159804566
                ],
                "word_count_noun_chunks": [
                    2.0,
                    1.0,
                    1.0
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    1.0
                ],
                "word_count_appended": [
                    169.0,
                    56.0,
                    154.0
                ],
                "result_count": [
                    1550000.0,
                    27000.0,
                    1390000.0
                ]
            },
            "z-best_answer_by_ml": [
                "boats"
            ]
        },
        "lines": [
            [
                1,
                0.5224132119986519,
                0.5646747863697118,
                0.09217264081931237,
                0.18208815028901731,
                0.44591029023746703,
                0.5,
                0.3333333333333333,
                0.3779274481426236
            ],
            [
                0,
                0.00910010111223458,
                0.008815534563803745,
                0.015362106803218726,
                0.5325722543352601,
                0.14775725593667546,
                0.25,
                0.3333333333333333,
                0.261251699193967
            ],
            [
                0,
                0.4684866868891136,
                0.42650967906648446,
                0.8924652523774689,
                0.2853395953757225,
                0.40633245382585753,
                0.25,
                0.3333333333333333,
                0.3608208526634094
            ]
        ]
    },
    "Which three-letter-titled movie grossed the most worldwide?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.2764227642276423,
                    0.529758657224735,
                    0.26565242832065533,
                    0.0,
                    0.3337531486146096,
                    0.2,
                    0.14285714285714285,
                    0.5966386554621849
                ],
                [
                    0.20121951219512196,
                    0.4701724462187894,
                    0.1989467524868344,
                    0.0,
                    0.3967254408060453,
                    0.4,
                    0.7142857142857143,
                    0.3949579831932773
                ],
                [
                    0.5223577235772358,
                    6.889655647562459e-05,
                    0.5354008191925103,
                    1.0,
                    0.2695214105793451,
                    0.4,
                    0.14285714285714285,
                    0.008403361344537815
                ]
            ],
            "fraction_answers": {
                "big": 0.35982616926340594,
                "saw": 0.2931353495883713,
                "ray": 0.34703848114822283
            },
            "question": "Which three-letter-titled movie grossed the most worldwide?",
            "rate_limited": false,
            "answers": [
                "saw",
                "ray",
                "big"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "big": 0.2547364335297506,
                "saw": 0.29448244372114396,
                "ray": 0.3635484425931417
            },
            "integer_answers": {
                "big": 3,
                "saw": 2,
                "ray": 3
            },
            "data": {
                "result_count_important_words": [
                    569000.0,
                    505000.0,
                    74.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count_noun_chunks": [
                    908000.0,
                    680000.0,
                    1830000.0
                ],
                "word_relation_to_question": [
                    1.7899159663865545,
                    1.184873949579832,
                    0.025210084033613446
                ],
                "word_count_noun_chunks": [
                    2.0,
                    4.0,
                    4.0
                ],
                "word_count_raw": [
                    1.0,
                    5.0,
                    1.0
                ],
                "word_count_appended": [
                    265.0,
                    315.0,
                    214.0
                ],
                "result_count": [
                    13600000.0,
                    9900000.0,
                    25700000.0
                ]
            },
            "z-best_answer_by_ml": [
                "ray"
            ]
        },
        "lines": [
            [
                0,
                0.2764227642276423,
                0.529758657224735,
                0.26565242832065533,
                0.0,
                0.3337531486146096,
                0.2,
                0.14285714285714285,
                0.5966386554621849
            ],
            [
                0,
                0.20121951219512196,
                0.4701724462187894,
                0.1989467524868344,
                0.0,
                0.3967254408060453,
                0.4,
                0.7142857142857143,
                0.3949579831932773
            ],
            [
                1,
                0.5223577235772358,
                6.889655647562459e-05,
                0.5354008191925103,
                1.0,
                0.2695214105793451,
                0.4,
                0.14285714285714285,
                0.008403361344537815
            ]
        ]
    },
    "Talking is discouraged on what Amtrak car?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.03183023872679045,
                    0.07068062827225131,
                    0.0,
                    0.47410358565737054,
                    0.05405405405405406,
                    0,
                    0.0,
                    0.12419354838709677
                ],
                [
                    0.129973474801061,
                    0.034031413612565446,
                    0.9986518200429421,
                    0.017928286852589643,
                    0.0,
                    0,
                    0.0,
                    0.09309564233163553
                ],
                [
                    0.8381962864721485,
                    0.8952879581151832,
                    0.0013481799570579716,
                    0.5079681274900398,
                    0.9459459459459459,
                    0,
                    1.0,
                    0.7827108092812677
                ]
            ],
            "fraction_answers": {
                "sports argument car": 0.10783743644250901,
                "quiet car": 0.7102081867516634,
                "meet & greet car": 0.18195437680582766
            },
            "question": "Talking is discouraged on what Amtrak car?",
            "rate_limited": false,
            "answers": [
                "sports argument car",
                "meet & greet car",
                "quiet car"
            ],
            "ml_answers": {
                "sports argument car": 0.1141907960774729,
                "quiet car": 0.5803613943320475,
                "meet & greet car": 0.1644224272741195
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "sports argument car": 0,
                "quiet car": 6,
                "meet & greet car": 1
            },
            "data": {
                "result_count_important_words": [
                    27.0,
                    13.0,
                    342.0
                ],
                "wikipedia_search": [
                    0.9482071713147411,
                    0.035856573705179286,
                    1.0159362549800797
                ],
                "result_count_noun_chunks": [
                    0,
                    120000000.0,
                    162000.0
                ],
                "word_relation_to_question": [
                    0.3725806451612903,
                    0.2792869269949066,
                    2.348132427843803
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    3.0
                ],
                "word_count_appended": [
                    6.0,
                    0.0,
                    105.0
                ],
                "result_count": [
                    12.0,
                    49.0,
                    316.0
                ]
            },
            "z-best_answer_by_ml": [
                "quiet car"
            ]
        },
        "lines": [
            [
                0,
                0.03183023872679045,
                0.07068062827225131,
                0.0,
                0.47410358565737054,
                0.05405405405405406,
                0,
                0.0,
                0.12419354838709677
            ],
            [
                0,
                0.129973474801061,
                0.034031413612565446,
                0.9986518200429421,
                0.017928286852589643,
                0.0,
                0,
                0.0,
                0.09309564233163553
            ],
            [
                1,
                0.8381962864721485,
                0.8952879581151832,
                0.0013481799570579716,
                0.5079681274900398,
                0.9459459459459459,
                0,
                1.0,
                0.7827108092812677
            ]
        ]
    },
    "The creator of Wonder Woman also created an early version of what device?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.32575757575757575,
                    0.9956475976445822,
                    0.9945712983299491,
                    0.036281179138321996,
                    0.136,
                    0.0,
                    0.0,
                    0.3747474747474747
                ],
                [
                    0.17424242424242425,
                    0.0019628481210707477,
                    0.0024864282458248728,
                    0.34076886568580916,
                    0.328,
                    0.0,
                    0.0,
                    0.3376262626262627
                ],
                [
                    0.5,
                    0.0023895542343469974,
                    0.002942273424226099,
                    0.6229499551758689,
                    0.536,
                    1.0,
                    1.0,
                    0.2876262626262627
                ]
            ],
            "fraction_answers": {
                "lie detector": 0.49398850568258806,
                "hearing aid": 0.14813585361517398,
                "magic marker": 0.35787564070223804
            },
            "question": "The creator of Wonder Woman also created an early version of what device?",
            "rate_limited": false,
            "answers": [
                "magic marker",
                "hearing aid",
                "lie detector"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "lie detector": 0.7224139102363077,
                "hearing aid": 0.20048683113058333,
                "magic marker": 0.24585191443741794
            },
            "z-best_answer_by_ml": [
                "lie detector"
            ],
            "data": {
                "result_count_important_words": [
                    35000.0,
                    69.0,
                    84.0
                ],
                "wikipedia_search": [
                    0.25396825396825395,
                    2.3853820598006643,
                    4.360649686231082
                ],
                "result_count_noun_chunks": [
                    24000.0,
                    60.0,
                    71.0
                ],
                "word_relation_to_question": [
                    2.248484848484848,
                    2.025757575757576,
                    1.7257575757575758
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    13.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    26.0
                ],
                "result_count": [
                    43.0,
                    23.0,
                    66.0
                ],
                "word_count_appended": [
                    17.0,
                    41.0,
                    67.0
                ]
            },
            "integer_answers": {
                "lie detector": 5,
                "hearing aid": 0,
                "magic marker": 3
            }
        },
        "lines": [
            [
                0,
                0.32575757575757575,
                0.9956475976445822,
                0.9945712983299491,
                0.036281179138321996,
                0.136,
                0.0,
                0.0,
                0.3747474747474747
            ],
            [
                0,
                0.17424242424242425,
                0.0019628481210707477,
                0.0024864282458248728,
                0.34076886568580916,
                0.328,
                0.0,
                0.0,
                0.3376262626262627
            ],
            [
                1,
                0.5,
                0.0023895542343469974,
                0.002942273424226099,
                0.6229499551758689,
                0.536,
                1.0,
                1.0,
                0.2876262626262627
            ]
        ]
    },
    "Which of these verbs has two meanings that are opposites of each other?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.6174068910575583,
                    0.8243021346469622,
                    0.9603943566207717,
                    0,
                    0.332871012482663,
                    0.0,
                    0.0,
                    0.0
                ],
                [
                    0.15952997410874328,
                    0.06124794745484401,
                    0.01640319564847867,
                    0,
                    0.2621359223300971,
                    0.0,
                    0.0,
                    0.3
                ],
                [
                    0.22306313483369847,
                    0.11444991789819375,
                    0.023202447730749618,
                    0,
                    0.40499306518723993,
                    1.0,
                    1.0,
                    0.7
                ]
            ],
            "fraction_answers": {
                "cleave": 0.4951012236642689,
                "branch": 0.39071062782970784,
                "jut": 0.11418814850602328
            },
            "question": "Which of these verbs has two meanings that are opposites of each other?",
            "rate_limited": false,
            "answers": [
                "branch",
                "jut",
                "cleave"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "cleave": 0.4420214376981768,
                "branch": 0.26516270432121314,
                "jut": 0.31157143216045696
            },
            "integer_answers": {
                "cleave": 4,
                "branch": 3,
                "jut": 0
            },
            "data": {
                "result_count_important_words": [
                    502000.0,
                    37300.0,
                    69700.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    2260000.0,
                    38600.0,
                    54600.0
                ],
                "word_relation_to_question": [
                    0.0,
                    0.6,
                    1.4
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count": [
                    310000.0,
                    80100.0,
                    112000.0
                ],
                "word_count_appended": [
                    240.0,
                    189.0,
                    292.0
                ]
            },
            "z-best_answer_by_ml": [
                "cleave"
            ]
        },
        "lines": [
            [
                0,
                0.6174068910575583,
                0.8243021346469622,
                0.9603943566207717,
                0,
                0.332871012482663,
                0.0,
                0.0,
                0.0
            ],
            [
                0,
                0.15952997410874328,
                0.06124794745484401,
                0.01640319564847867,
                0,
                0.2621359223300971,
                0.0,
                0.0,
                0.3
            ],
            [
                1,
                0.22306313483369847,
                0.11444991789819375,
                0.023202447730749618,
                0,
                0.40499306518723993,
                1.0,
                1.0,
                0.7
            ]
        ]
    },
    "Which of these is NOT a real animal?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.28745519713261647,
                    0.2787371829465731,
                    0.3533755274261603,
                    0.17317644495287904,
                    0.3202284946236559,
                    0.5,
                    0.5,
                    0.38130306823179194
                ],
                [
                    0.2734767025089606,
                    0.29897463572585,
                    0.3488045007032349,
                    0.38127144958326525,
                    0.2879704301075269,
                    0.0,
                    0.0,
                    0.22854323827565598
                ],
                [
                    0.4390681003584229,
                    0.4222881813275769,
                    0.29781997187060477,
                    0.44555210546385576,
                    0.3918010752688172,
                    0.5,
                    0.5,
                    0.39015369349255213
                ]
            ],
            "fraction_answers": {
                "liger": 0.5452397607738766,
                "wholphin": 0.15332921805454258,
                "jackalope": 0.3014310211715808
            },
            "question": "Which of these is NOT a real animal?",
            "rate_limited": false,
            "answers": [
                "jackalope",
                "liger",
                "wholphin"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "liger": 0.26516270432121314,
                "wholphin": 0.4420214376981768,
                "jackalope": 0.35443025729131133
            },
            "integer_answers": {
                "liger": 5,
                "wholphin": 1,
                "jackalope": 2
            },
            "data": {
                "result_count_important_words": [
                    820000.0,
                    745000.0,
                    288000.0
                ],
                "wikipedia_search": [
                    1.9609413302827259,
                    0.7123713025004086,
                    0.32668736721686553
                ],
                "result_count_noun_chunks": [
                    834000.0,
                    860000.0,
                    1150000.0
                ],
                "word_relation_to_question": [
                    0.7121815906092486,
                    1.6287405703460642,
                    0.6590778390446873
                ],
                "word_count_noun_chunks": [
                    0.0,
                    15.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    8.0,
                    0.0
                ],
                "word_count_appended": [
                    535.0,
                    631.0,
                    322.0
                ],
                "result_count": [
                    593000.0,
                    632000.0,
                    170000.0
                ]
            },
            "z-best_answer_by_ml": [
                "wholphin"
            ]
        },
        "lines": [
            [
                1,
                0.28745519713261647,
                0.2787371829465731,
                0.3533755274261603,
                0.17317644495287904,
                0.3202284946236559,
                0.5,
                0.5,
                0.38130306823179194
            ],
            [
                0,
                0.2734767025089606,
                0.29897463572585,
                0.3488045007032349,
                0.38127144958326525,
                0.2879704301075269,
                0.0,
                0.0,
                0.22854323827565598
            ],
            [
                0,
                0.4390681003584229,
                0.4222881813275769,
                0.29781997187060477,
                0.44555210546385576,
                0.3918010752688172,
                0.5,
                0.5,
                0.39015369349255213
            ]
        ]
    },
    "Pixies, Bon Iver, Iron & Wine and Bauhaus were all once signed to which record label?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.3333333333333333,
                    0.0024441024712591655,
                    0.01801200800533689,
                    0.3267619047619048,
                    0.021739130434782608,
                    0.0,
                    0.0,
                    0.2983249661914583
                ],
                [
                    0.3333333333333333,
                    0.8979813524033674,
                    0.9272848565710473,
                    0.6703809523809524,
                    0.9565217391304348,
                    0.9939393939393939,
                    1.0,
                    0.5209389456771656
                ],
                [
                    0.3333333333333333,
                    0.0995745451253734,
                    0.05470313542361575,
                    0.002857142857142858,
                    0.021739130434782608,
                    0.006060606060606061,
                    0.0,
                    0.18073608813137604
                ]
            ],
            "fraction_answers": {
                "4ad": 0.7875475716794619,
                "geffen": 0.08737549767077878,
                "subpop": 0.1250769306497594
            },
            "question": "Pixies, Bon Iver, Iron & Wine and Bauhaus were all once signed to which record label?",
            "rate_limited": false,
            "answers": [
                "subpop",
                "4ad",
                "geffen"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "4ad": 0.4420214376981768,
                "geffen": 0.28368788585608096,
                "subpop": 0.3049087145126065
            },
            "z-best_answer_by_ml": [
                "4ad"
            ],
            "data": {
                "result_count_important_words": [
                    27.0,
                    9920.0,
                    1100.0
                ],
                "wikipedia_search": [
                    2.287333333333333,
                    4.692666666666666,
                    0.02
                ],
                "result_count_noun_chunks": [
                    27.0,
                    1390.0,
                    82.0
                ],
                "word_relation_to_question": [
                    2.6849246957231254,
                    4.688450511094491,
                    1.6266247931823847
                ],
                "word_count_noun_chunks": [
                    0.0,
                    164.0,
                    1.0
                ],
                "word_count_raw": [
                    0.0,
                    43.0,
                    0.0
                ],
                "result_count": [
                    223000.0,
                    223000.0,
                    223000.0
                ],
                "word_count_appended": [
                    1.0,
                    44.0,
                    1.0
                ]
            },
            "integer_answers": {
                "4ad": 7,
                "geffen": 0,
                "subpop": 1
            }
        },
        "lines": [
            [
                0,
                0.3333333333333333,
                0.0024441024712591655,
                0.01801200800533689,
                0.3267619047619048,
                0.021739130434782608,
                0.0,
                0.0,
                0.2983249661914583
            ],
            [
                1,
                0.3333333333333333,
                0.8979813524033674,
                0.9272848565710473,
                0.6703809523809524,
                0.9565217391304348,
                0.9939393939393939,
                1.0,
                0.5209389456771656
            ],
            [
                0,
                0.3333333333333333,
                0.0995745451253734,
                0.05470313542361575,
                0.002857142857142858,
                0.021739130434782608,
                0.006060606060606061,
                0.0,
                0.18073608813137604
            ]
        ]
    },
    "Which of these would an oologist study?": {
        "raw_data": {
            "z-best_answer_by_ml": [
                "human liver"
            ],
            "question": "Which of these would an oologist study?",
            "answers": [
                "ice cave",
                "human liver",
                "ostrich egg"
            ],
            "integer_answers": {
                "ice cave": 1,
                "human liver": 1,
                "ostrich egg": 4
            },
            "data": {
                "result_count_important_words": [
                    13100.0,
                    55.0,
                    7940.0
                ],
                "wikipedia_search": [
                    0.0,
                    0.08333333333333333,
                    0.9166666666666666
                ],
                "result_count_noun_chunks": [
                    34.0,
                    56.0,
                    450.0
                ],
                "word_relation_to_question": [
                    0.0,
                    0.8571428571428571,
                    0.14285714285714285
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_appended": [
                    17.0,
                    20.0,
                    40.0
                ],
                "result_count": [
                    271.0,
                    51.0,
                    410.0
                ]
            },
            "negative_question": false,
            "fraction_answers": {
                "ice cave": 0.21249350000002462,
                "human liver": 0.22936658966187104,
                "ostrich egg": 0.5581399103381043
            },
            "lines": [
                [
                    0.3702185792349727,
                    0.6210002370229912,
                    0.06296296296296296,
                    0.0,
                    0.22077922077922077,
                    0,
                    0,
                    0.0
                ],
                [
                    0.06967213114754098,
                    0.0026072529035316427,
                    0.1037037037037037,
                    0.08333333333333333,
                    0.2597402597402597,
                    0,
                    0,
                    0.8571428571428571
                ],
                [
                    0.5601092896174863,
                    0.3763925100734771,
                    0.8333333333333334,
                    0.9166666666666666,
                    0.5194805194805194,
                    0,
                    0,
                    0.14285714285714285
                ]
            ],
            "rate_limited": false,
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "ice cave": 0.29540948404493517,
                "human liver": 0.36400701949839315,
                "ostrich egg": 0.25157563790719745
            }
        },
        "lines": [
            [
                0,
                0.3702185792349727,
                0.6210002370229912,
                0.06296296296296296,
                0.0,
                0.22077922077922077,
                0,
                0,
                0.0
            ],
            [
                0,
                0.06967213114754098,
                0.0026072529035316427,
                0.1037037037037037,
                0.08333333333333333,
                0.2597402597402597,
                0,
                0,
                0.8571428571428571
            ],
            [
                1,
                0.5601092896174863,
                0.3763925100734771,
                0.8333333333333334,
                0.9166666666666666,
                0.5194805194805194,
                0,
                0,
                0.14285714285714285
            ]
        ]
    },
    "Who holds the record as the youngest solo artist with a Billboard #1 hit?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.3314917127071823,
                    0.9185827580304673,
                    0.3351749539594843,
                    0.31717337270625123,
                    0.5263157894736842,
                    0.48,
                    0.5263157894736842,
                    0.360119744985347
                ],
                [
                    0.3333333333333333,
                    0.027614537570605353,
                    0.3314917127071823,
                    0.5286803307743662,
                    0.05263157894736842,
                    0.24,
                    0.05263157894736842,
                    0.3289039141091672
                ],
                [
                    0.3351749539594843,
                    0.05380270439892737,
                    0.3333333333333333,
                    0.1541462965193826,
                    0.42105263157894735,
                    0.28,
                    0.42105263157894735,
                    0.3109763409054859
                ]
            ],
            "fraction_answers": {
                "justin bieber": 0.47439676516701257,
                "stevie wonder": 0.28869236153431355,
                "michael jackson": 0.23691087329867389
            },
            "question": "Who holds the record as the youngest solo artist with a Billboard #1 hit?",
            "rate_limited": false,
            "answers": [
                "justin bieber",
                "michael jackson",
                "stevie wonder"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "justin bieber": 0.4420214376981768,
                "stevie wonder": 0.3635484425931417,
                "michael jackson": 0.27595726218627614
            },
            "integer_answers": {
                "justin bieber": 6,
                "stevie wonder": 1,
                "michael jackson": 1
            },
            "data": {
                "result_count_important_words": [
                    1610000.0,
                    48400.0,
                    94300.0
                ],
                "wikipedia_search": [
                    2.854560354356261,
                    4.758122976969296,
                    1.3873166686744436
                ],
                "result_count_noun_chunks": [
                    1820000.0,
                    1800000.0,
                    1810000.0
                ],
                "word_relation_to_question": [
                    2.8809579598827755,
                    2.631231312873337,
                    2.487810727243887
                ],
                "word_count_noun_chunks": [
                    12.0,
                    6.0,
                    7.0
                ],
                "word_count_raw": [
                    10.0,
                    1.0,
                    8.0
                ],
                "word_count_appended": [
                    10.0,
                    1.0,
                    8.0
                ],
                "result_count": [
                    1800000.0,
                    1810000.0,
                    1820000.0
                ]
            },
            "z-best_answer_by_ml": [
                "justin bieber"
            ]
        },
        "lines": [
            [
                0,
                0.3314917127071823,
                0.9185827580304673,
                0.3351749539594843,
                0.31717337270625123,
                0.5263157894736842,
                0.48,
                0.5263157894736842,
                0.360119744985347
            ],
            [
                0,
                0.3333333333333333,
                0.027614537570605353,
                0.3314917127071823,
                0.5286803307743662,
                0.05263157894736842,
                0.24,
                0.05263157894736842,
                0.3289039141091672
            ],
            [
                1,
                0.3351749539594843,
                0.05380270439892737,
                0.3333333333333333,
                0.1541462965193826,
                0.42105263157894735,
                0.28,
                0.42105263157894735,
                0.3109763409054859
            ]
        ]
    },
    "Which of these states does NOT touch the Mason-Dixon Line?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.07187922487607029,
                    0.08812098681987157,
                    0.3521739130434782,
                    0.321084960584732,
                    0.4423728813559322,
                    0.3793103448275862,
                    0.35714285714285715,
                    0.3382889632889633
                ],
                [
                    0.47731710980922337,
                    0.46045961473470765,
                    0.3739130434782609,
                    0.3378531653578983,
                    0.2406779661016949,
                    0.15517241379310343,
                    0.2142857142857143,
                    0.3417213417213417
                ],
                [
                    0.45080366531470634,
                    0.4514193984454207,
                    0.27391304347826084,
                    0.34106187405736976,
                    0.3169491525423729,
                    0.46551724137931033,
                    0.4285714285714286,
                    0.319989694989695
                ]
            ],
            "fraction_answers": {
                "tennessee": 0.23794362530535887,
                "delaware": 0.3496499076795138,
                "west virginia": 0.4124064670151273
            },
            "question": "Which of these states does NOT touch the Mason-Dixon Line?",
            "rate_limited": false,
            "answers": [
                "west virginia",
                "delaware",
                "tennessee"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "tennessee": 0.29448244372114396,
                "delaware": 0.27595726218627614,
                "west virginia": 0.3635484425931417
            },
            "z-best_answer_by_ml": [
                "west virginia"
            ],
            "data": {
                "result_count_important_words": [
                    9750000.0,
                    936000.0,
                    1150000.0
                ],
                "wikipedia_search": [
                    1.0734902364916081,
                    0.9728810078526103,
                    0.9536287556557816
                ],
                "result_count_noun_chunks": [
                    136000.0,
                    116000.0,
                    208000.0
                ],
                "word_relation_to_question": [
                    0.9702662202662203,
                    0.9496719496719497,
                    1.08006183006183
                ],
                "word_count_noun_chunks": [
                    7.0,
                    20.0,
                    2.0
                ],
                "word_count_raw": [
                    4.0,
                    8.0,
                    2.0
                ],
                "result_count": [
                    1140000.0,
                    60400.0,
                    131000.0
                ],
                "word_count_appended": [
                    68.0,
                    306.0,
                    216.0
                ]
            },
            "integer_answers": {
                "tennessee": 2,
                "delaware": 3,
                "west virginia": 3
            }
        },
        "lines": [
            [
                0,
                0.07187922487607029,
                0.08812098681987157,
                0.3521739130434782,
                0.321084960584732,
                0.4423728813559322,
                0.3793103448275862,
                0.35714285714285715,
                0.3382889632889633
            ],
            [
                0,
                0.47731710980922337,
                0.46045961473470765,
                0.3739130434782609,
                0.3378531653578983,
                0.2406779661016949,
                0.15517241379310343,
                0.2142857142857143,
                0.3417213417213417
            ],
            [
                1,
                0.45080366531470634,
                0.4514193984454207,
                0.27391304347826084,
                0.34106187405736976,
                0.3169491525423729,
                0.46551724137931033,
                0.4285714285714286,
                0.319989694989695
            ]
        ]
    },
    "To help first create Maps, Google acquired what company?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.13279036827195467,
                    0.3587180241105557,
                    0.5810022288447572,
                    0.10200665237429944,
                    0.33967391304347827,
                    0.0,
                    0.10526315789473684,
                    0.24744684922206286
                ],
                [
                    0.007391997167138811,
                    0.02087621287856513,
                    0.013297938944679573,
                    0.7842139097653803,
                    0.059782608695652176,
                    0.08333333333333333,
                    0.07017543859649122,
                    0.5060561007126169
                ],
                [
                    0.8598176345609065,
                    0.6204057630108791,
                    0.4056998322105632,
                    0.1137794378603202,
                    0.6005434782608695,
                    0.9166666666666666,
                    0.8245614035087719,
                    0.24649705006532016
                ]
            ],
            "fraction_answers": {
                "mapquest": 0.23336264922023062,
                "waze": 0.5734964082680373,
                "where 2 technologies": 0.1931409425117322
            },
            "question": "To help first create Maps, Google acquired what company?",
            "rate_limited": false,
            "answers": [
                "mapquest",
                "where 2 technologies",
                "waze"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "mapquest": 0.292489911503956,
                "waze": 0.5584313012345334,
                "where 2 technologies": 0.3137138399805534
            },
            "integer_answers": {
                "mapquest": 1,
                "waze": 5,
                "where 2 technologies": 2
            },
            "data": {
                "result_count_important_words": [
                    122000.0,
                    7100.0,
                    211000.0
                ],
                "wikipedia_search": [
                    0.6120399142457966,
                    4.705283458592282,
                    0.6826766271619212
                ],
                "result_count_noun_chunks": [
                    2320000.0,
                    53100.0,
                    1620000.0
                ],
                "word_relation_to_question": [
                    1.4846810953323772,
                    3.036336604275701,
                    1.478982300391921
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    11.0
                ],
                "word_count_raw": [
                    6.0,
                    4.0,
                    47.0
                ],
                "word_count_appended": [
                    375.0,
                    66.0,
                    663.0
                ],
                "result_count": [
                    120000.0,
                    6680.0,
                    777000.0
                ]
            },
            "z-best_answer_by_ml": [
                "waze"
            ]
        },
        "lines": [
            [
                0,
                0.13279036827195467,
                0.3587180241105557,
                0.5810022288447572,
                0.10200665237429944,
                0.33967391304347827,
                0.0,
                0.10526315789473684,
                0.24744684922206286
            ],
            [
                1,
                0.007391997167138811,
                0.02087621287856513,
                0.013297938944679573,
                0.7842139097653803,
                0.059782608695652176,
                0.08333333333333333,
                0.07017543859649122,
                0.5060561007126169
            ],
            [
                0,
                0.8598176345609065,
                0.6204057630108791,
                0.4056998322105632,
                0.1137794378603202,
                0.6005434782608695,
                0.9166666666666666,
                0.8245614035087719,
                0.24649705006532016
            ]
        ]
    },
    "The lyrics to \u201cThe Star-Spangled Banner\u201d were written during what conflict?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.4352647629609093,
                    0.45454545454545453,
                    0.5482758620689655,
                    0.19507073386383733,
                    0.28415300546448086,
                    0.10526315789473684,
                    0.038461538461538464,
                    0.16613756613756614
                ],
                [
                    0.318824507901303,
                    0.3285123966942149,
                    0.31724137931034485,
                    0.09250663129973476,
                    0.22404371584699453,
                    0.0,
                    0.07692307692307693,
                    0.10148148148148148
                ],
                [
                    0.24591072913778764,
                    0.21694214876033058,
                    0.13448275862068965,
                    0.712422634836428,
                    0.4918032786885246,
                    0.8947368421052632,
                    0.8846153846153846,
                    0.7323809523809524
                ]
            ],
            "fraction_answers": {
                "american revolution": 0.1824416486821438,
                "the war of 1812": 0.5391618411431701,
                "the civil war": 0.27839651017468614
            },
            "question": "The lyrics to \u201cThe Star-Spangled Banner\u201d were written during what conflict?",
            "rate_limited": false,
            "answers": [
                "the civil war",
                "american revolution",
                "the war of 1812"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "american revolution": 0.1284775995538017,
                "the war of 1812": 0.2616203718282265,
                "the civil war": 0.31745615942127153
            },
            "integer_answers": {
                "american revolution": 0,
                "the war of 1812": 5,
                "the civil war": 3
            },
            "data": {
                "result_count_important_words": [
                    220000.0,
                    159000.0,
                    105000.0
                ],
                "wikipedia_search": [
                    1.170424403183024,
                    0.5550397877984086,
                    4.274535809018568
                ],
                "result_count_noun_chunks": [
                    795000.0,
                    460000.0,
                    195000.0
                ],
                "word_relation_to_question": [
                    0.8306878306878307,
                    0.5074074074074074,
                    3.6619047619047618
                ],
                "word_count_noun_chunks": [
                    2.0,
                    0.0,
                    17.0
                ],
                "word_count_raw": [
                    1.0,
                    2.0,
                    23.0
                ],
                "word_count_appended": [
                    52.0,
                    41.0,
                    90.0
                ],
                "result_count": [
                    157000.0,
                    115000.0,
                    88700.0
                ]
            },
            "z-best_answer_by_ml": [
                "the civil war"
            ]
        },
        "lines": [
            [
                0,
                0.4352647629609093,
                0.45454545454545453,
                0.5482758620689655,
                0.19507073386383733,
                0.28415300546448086,
                0.10526315789473684,
                0.038461538461538464,
                0.16613756613756614
            ],
            [
                0,
                0.318824507901303,
                0.3285123966942149,
                0.31724137931034485,
                0.09250663129973476,
                0.22404371584699453,
                0.0,
                0.07692307692307693,
                0.10148148148148148
            ],
            [
                1,
                0.24591072913778764,
                0.21694214876033058,
                0.13448275862068965,
                0.712422634836428,
                0.4918032786885246,
                0.8947368421052632,
                0.8846153846153846,
                0.7323809523809524
            ]
        ]
    },
    "Where do marsupials keep their undeveloped young?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.004272363150867824,
                    0.004895211870888787,
                    0.054328797180271234,
                    0.75,
                    0.13861386138613863,
                    0,
                    0.0,
                    0.8541666666666666
                ],
                [
                    0.001068090787716956,
                    0.0007648768548263729,
                    0.00031511869470834014,
                    0.0,
                    0.024752475247524754,
                    0,
                    0.0,
                    0.041666666666666664
                ],
                [
                    0.9946595460614153,
                    0.9943399112742849,
                    0.9453560841250205,
                    0.25,
                    0.8366336633663366,
                    0,
                    1.0,
                    0.10416666666666667
                ]
            ],
            "fraction_answers": {
                "in their pouches": 0.25803955717926186,
                "underwater": 0.7321651244991034,
                "in a paper bag": 0.009795318321634727
            },
            "question": "Where do marsupials keep their undeveloped young?",
            "rate_limited": false,
            "answers": [
                "in their pouches",
                "in a paper bag",
                "underwater"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "in their pouches": 0.3403096074155279,
                "underwater": 0.5500858554004421,
                "in a paper bag": 0.1396178755065776
            },
            "z-best_answer_by_ml": [
                "underwater"
            ],
            "data": {
                "result_count_important_words": [
                    64.0,
                    10.0,
                    13000.0
                ],
                "wikipedia_search": [
                    0.75,
                    0.0,
                    0.25
                ],
                "result_count_noun_chunks": [
                    9310.0,
                    54.0,
                    162000.0
                ],
                "word_relation_to_question": [
                    2.5625,
                    0.125,
                    0.3125
                ],
                "word_count_noun_chunks": [
                    0.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    0.0,
                    1.0
                ],
                "result_count": [
                    64.0,
                    16.0,
                    14900.0
                ],
                "word_count_appended": [
                    28.0,
                    5.0,
                    169.0
                ]
            },
            "integer_answers": {
                "in their pouches": 2,
                "underwater": 5,
                "in a paper bag": 0
            }
        },
        "lines": [
            [
                1,
                0.004272363150867824,
                0.004895211870888787,
                0.054328797180271234,
                0.75,
                0.13861386138613863,
                0,
                0.0,
                0.8541666666666666
            ],
            [
                0,
                0.001068090787716956,
                0.0007648768548263729,
                0.00031511869470834014,
                0.0,
                0.024752475247524754,
                0,
                0.0,
                0.041666666666666664
            ],
            [
                0,
                0.9946595460614153,
                0.9943399112742849,
                0.9453560841250205,
                0.25,
                0.8366336633663366,
                0,
                1.0,
                0.10416666666666667
            ]
        ]
    },
    "Mount Rushmore was named after a person with what profession?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.03908316191595651,
                    0.043348281016442454,
                    0.05953735305271141,
                    0.265625,
                    0.26766304347826086,
                    0.0,
                    0.1111111111111111,
                    0.2601023391812866
                ],
                [
                    0.49074346165148397,
                    0.4334828101644245,
                    0.4247250663632916,
                    0.6437190594059405,
                    0.34375,
                    1.0,
                    0.8888888888888888,
                    0.3760964912280702
                ],
                [
                    0.4701733764325595,
                    0.523168908819133,
                    0.515737580583997,
                    0.0906559405940594,
                    0.38858695652173914,
                    0.0,
                    0.0,
                    0.3638011695906433
                ]
            ],
            "fraction_answers": {
                "architect": 0.2940154915677664,
                "prospector": 0.13080878621947112,
                "lawyer": 0.5751757222127625
            },
            "question": "Mount Rushmore was named after a person with what profession?",
            "rate_limited": false,
            "answers": [
                "prospector",
                "lawyer",
                "architect"
            ],
            "ml_answers": {
                "architect": 0.35174029356299585,
                "prospector": 0.18394746795613665,
                "lawyer": 0.6628903074549203
            },
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "integer_answers": {
                "architect": 3,
                "prospector": 0,
                "lawyer": 5
            },
            "data": {
                "result_count_important_words": [
                    11600.0,
                    116000.0,
                    140000.0
                ],
                "wikipedia_search": [
                    1.0625,
                    2.574876237623762,
                    0.3626237623762376
                ],
                "result_count_noun_chunks": [
                    15700.0,
                    112000.0,
                    136000.0
                ],
                "word_relation_to_question": [
                    1.0404093567251462,
                    1.5043859649122806,
                    1.455204678362573
                ],
                "word_count_noun_chunks": [
                    0.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    8.0,
                    0.0
                ],
                "result_count": [
                    13300.0,
                    167000.0,
                    160000.0
                ],
                "word_count_appended": [
                    197.0,
                    253.0,
                    286.0
                ]
            },
            "z-best_answer_by_ml": [
                "lawyer"
            ]
        },
        "lines": [
            [
                0,
                0.03908316191595651,
                0.043348281016442454,
                0.05953735305271141,
                0.265625,
                0.26766304347826086,
                0.0,
                0.1111111111111111,
                0.2601023391812866
            ],
            [
                1,
                0.49074346165148397,
                0.4334828101644245,
                0.4247250663632916,
                0.6437190594059405,
                0.34375,
                1.0,
                0.8888888888888888,
                0.3760964912280702
            ],
            [
                0,
                0.4701733764325595,
                0.523168908819133,
                0.515737580583997,
                0.0906559405940594,
                0.38858695652173914,
                0.0,
                0.0,
                0.3638011695906433
            ]
        ]
    },
    "Which verb describes the sound minerals make when they are heated?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    1.0,
                    1.0,
                    1.0,
                    1.0,
                    0.7217391304347827,
                    1.0,
                    1.0,
                    0.8998316498316499
                ],
                [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.1391304347826087,
                    0.0,
                    0.0,
                    0.004629629629629629
                ],
                [
                    0.0,
                    0.0,
                    0.0,
                    0.0,
                    0.1391304347826087,
                    0.0,
                    0.0,
                    0.09553872053872053
                ]
            ],
            "fraction_answers": {
                "frangelle": 0.01797000805152979,
                "decrepitate": 0.952696347533304,
                "recleft": 0.029333644415166154
            },
            "question": "Which verb describes the sound minerals make when they are heated?",
            "rate_limited": false,
            "answers": [
                "decrepitate",
                "frangelle",
                "recleft"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "frangelle": 0.29035060350393144,
                "decrepitate": 0.4420214376981768,
                "recleft": 0.29035060350393144
            },
            "z-best_answer_by_ml": [
                "decrepitate"
            ],
            "data": {
                "result_count_important_words": [
                    4430.0,
                    0,
                    0
                ],
                "wikipedia_search": [
                    2.0,
                    0.0,
                    0.0
                ],
                "result_count_noun_chunks": [
                    4640.0,
                    0,
                    0
                ],
                "word_relation_to_question": [
                    3.5993265993265995,
                    0.018518518518518517,
                    0.38215488215488214
                ],
                "word_count_noun_chunks": [
                    10.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    6.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    4420.0,
                    0,
                    0
                ],
                "word_count_appended": [
                    83.0,
                    16.0,
                    16.0
                ]
            },
            "integer_answers": {
                "frangelle": 0,
                "decrepitate": 8,
                "recleft": 0
            }
        },
        "lines": [
            [
                1,
                1.0,
                1.0,
                1.0,
                1.0,
                0.7217391304347827,
                1.0,
                1.0,
                0.8998316498316499
            ],
            [
                0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.1391304347826087,
                0.0,
                0.0,
                0.004629629629629629
            ],
            [
                0,
                0.0,
                0.0,
                0.0,
                0.0,
                0.1391304347826087,
                0.0,
                0.0,
                0.09553872053872053
            ]
        ]
    },
    "Which of these celebrities has NOT been a ProActiv spokesperson?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.21863775442330718,
                    0.11437046802900463,
                    0.21843116470120577,
                    0.41156311908430554,
                    0.28222222222222226,
                    0.05555555555555558,
                    0.08333333333333331,
                    0.39651639344262296
                ],
                [
                    0.34568311826526543,
                    0.4208965062623599,
                    0.34563402676560223,
                    0.26316207627118643,
                    0.30666666666666664,
                    0.46296296296296297,
                    0.4722222222222222,
                    0.41956967213114754
                ],
                [
                    0.43567912731142744,
                    0.46473302570863545,
                    0.435934808533192,
                    0.32527480464450803,
                    0.4111111111111111,
                    0.4814814814814815,
                    0.4444444444444444,
                    0.1839139344262295
                ]
            ],
            "fraction_answers": {
                "selena gomez": 0.20435681558474267,
                "katy perry": 0.5548424973021107,
                "lindsay lohan": 0.24080068711314667
            },
            "question": "Which of these celebrities has NOT been a ProActiv spokesperson?",
            "rate_limited": false,
            "answers": [
                "katy perry",
                "lindsay lohan",
                "selena gomez"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "selena gomez": 0.44386284145438887,
                "katy perry": 0.13142124602629307,
                "lindsay lohan": 0.2640667766396724
            },
            "integer_answers": {
                "selena gomez": 1,
                "katy perry": 6,
                "lindsay lohan": 1
            },
            "data": {
                "result_count_important_words": [
                    1170000.0,
                    240000.0,
                    107000.0
                ],
                "wikipedia_search": [
                    0.7074950473255558,
                    1.8947033898305086,
                    1.3978015628439358
                ],
                "result_count_noun_chunks": [
                    425000.0,
                    233000.0,
                    96700.0
                ],
                "word_relation_to_question": [
                    0.41393442622950816,
                    0.32172131147540983,
                    1.264344262295082
                ],
                "word_count_noun_chunks": [
                    24.0,
                    2.0,
                    1.0
                ],
                "word_count_raw": [
                    15.0,
                    1.0,
                    2.0
                ],
                "result_count": [
                    423000.0,
                    232000.0,
                    96700.0
                ],
                "word_count_appended": [
                    98.0,
                    87.0,
                    40.0
                ]
            },
            "z-best_answer_by_ml": [
                "selena gomez"
            ]
        },
        "lines": [
            [
                0,
                0.21863775442330718,
                0.11437046802900463,
                0.21843116470120577,
                0.41156311908430554,
                0.28222222222222226,
                0.05555555555555558,
                0.08333333333333331,
                0.39651639344262296
            ],
            [
                0,
                0.34568311826526543,
                0.4208965062623599,
                0.34563402676560223,
                0.26316207627118643,
                0.30666666666666664,
                0.46296296296296297,
                0.4722222222222222,
                0.41956967213114754
            ],
            [
                1,
                0.43567912731142744,
                0.46473302570863545,
                0.435934808533192,
                0.32527480464450803,
                0.4111111111111111,
                0.4814814814814815,
                0.4444444444444444,
                0.1839139344262295
            ]
        ]
    },
    "Which of these do NOT have flippers?": {
        "raw_data": {
            "negative_question": true,
            "lines": [
                [
                    0.4543381598449436,
                    0.4670433145009416,
                    0.46570898980537534,
                    0.4520420792079208,
                    0.44248366013071894,
                    0.5,
                    0.5,
                    0.41203703703703703
                ],
                [
                    0.10494812450119712,
                    0.1704331450094162,
                    0.1609514983008959,
                    0.30426980198019804,
                    0.11307189542483659,
                    0.375,
                    0.0,
                    0.42592592592592593
                ],
                [
                    0.4407137156538593,
                    0.3625235404896422,
                    0.37333951189372877,
                    0.24368811881188118,
                    0.4444444444444444,
                    0.125,
                    0.5,
                    0.16203703703703703
                ]
            ],
            "fraction_answers": {
                "new yorkers": 0.07658668986826568,
                "pinball machines": 0.3370634079173518,
                "dolphins": 0.5863499022143825
            },
            "question": "Which of these do NOT have flippers?",
            "rate_limited": false,
            "answers": [
                "new yorkers",
                "dolphins",
                "pinball machines"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "new yorkers": 0.48881279908372116,
                "pinball machines": 0.28300096717253154,
                "dolphins": 0.17082612776807218
            },
            "z-best_answer_by_ml": [
                "new yorkers"
            ],
            "data": {
                "result_count_important_words": [
                    35000.0,
                    350000.0,
                    146000.0
                ],
                "wikipedia_search": [
                    0.19183168316831684,
                    0.7829207920792078,
                    1.0252475247524753
                ],
                "result_count_noun_chunks": [
                    44400.0,
                    439000.0,
                    164000.0
                ],
                "word_relation_to_question": [
                    0.35185185185185186,
                    0.2962962962962963,
                    1.3518518518518519
                ],
                "word_count_noun_chunks": [
                    0.0,
                    2.0,
                    6.0
                ],
                "word_count_raw": [
                    0.0,
                    17.0,
                    0.0
                ],
                "result_count": [
                    80100.0,
                    693000.0,
                    104000.0
                ],
                "word_count_appended": [
                    88.0,
                    592.0,
                    85.0
                ]
            },
            "integer_answers": {
                "new yorkers": 0,
                "pinball machines": 3,
                "dolphins": 5
            }
        },
        "lines": [
            [
                1,
                0.4543381598449436,
                0.4670433145009416,
                0.46570898980537534,
                0.4520420792079208,
                0.44248366013071894,
                0.5,
                0.5,
                0.41203703703703703
            ],
            [
                0,
                0.10494812450119712,
                0.1704331450094162,
                0.1609514983008959,
                0.30426980198019804,
                0.11307189542483659,
                0.375,
                0.0,
                0.42592592592592593
            ],
            [
                0,
                0.4407137156538593,
                0.3625235404896422,
                0.37333951189372877,
                0.24368811881188118,
                0.4444444444444444,
                0.125,
                0.5,
                0.16203703703703703
            ]
        ]
    },
    "In \u201cPeanuts,\u201d what breed of dog is Snoopy?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.09311062351731453,
                    0.14737991266375547,
                    0.25276548672566373,
                    0.3698524365133837,
                    0.09237875288683603,
                    0.007633587786259542,
                    0.009345794392523364,
                    0.29249705937391396
                ],
                [
                    7.286918362224615e-05,
                    0.2718340611353712,
                    0.18307522123893805,
                    0.31762611530542206,
                    0.20554272517321015,
                    0.0,
                    0.0,
                    0.10912563156628437
                ],
                [
                    0.9068165072990633,
                    0.5807860262008734,
                    0.5641592920353983,
                    0.3125214481811942,
                    0.7020785219399538,
                    0.9923664122137404,
                    0.9906542056074766,
                    0.5983773090598016
                ]
            ],
            "fraction_answers": {
                "beagle": 0.7059699653171878,
                "pitbull": 0.13590957795035602,
                "border collie": 0.15812045673245628
            },
            "question": "In \u201cPeanuts,\u201d what breed of dog is Snoopy?",
            "rate_limited": false,
            "answers": [
                "border collie",
                "pitbull",
                "beagle"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "beagle": 0.4420214376981768,
                "pitbull": 0.29035060350393144,
                "border collie": 0.31157143216045696
            },
            "z-best_answer_by_ml": [
                "beagle"
            ],
            "data": {
                "result_count_important_words": [
                    135000.0,
                    249000.0,
                    532000.0
                ],
                "wikipedia_search": [
                    0.7397048730267674,
                    0.6352522306108441,
                    0.6250428963623884
                ],
                "result_count_noun_chunks": [
                    457000.0,
                    331000.0,
                    1020000.0
                ],
                "word_relation_to_question": [
                    0.8774911781217419,
                    0.3273768946988531,
                    1.7951319271794048
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    130.0
                ],
                "word_count_raw": [
                    1.0,
                    0.0,
                    106.0
                ],
                "word_count_appended": [
                    80.0,
                    178.0,
                    608.0
                ],
                "result_count": [
                    115000.0,
                    90.0,
                    1120000.0
                ]
            },
            "integer_answers": {
                "beagle": 7,
                "pitbull": 0,
                "border collie": 1
            }
        },
        "lines": [
            [
                0,
                0.09311062351731453,
                0.14737991266375547,
                0.25276548672566373,
                0.3698524365133837,
                0.09237875288683603,
                0.007633587786259542,
                0.009345794392523364,
                0.29249705937391396
            ],
            [
                0,
                7.286918362224615e-05,
                0.2718340611353712,
                0.18307522123893805,
                0.31762611530542206,
                0.20554272517321015,
                0.0,
                0.0,
                0.10912563156628437
            ],
            [
                1,
                0.9068165072990633,
                0.5807860262008734,
                0.5641592920353983,
                0.3125214481811942,
                0.7020785219399538,
                0.9923664122137404,
                0.9906542056074766,
                0.5983773090598016
            ]
        ]
    },
    "The '70s sitcom \u201cThree's Company\u201d was about three people who were what?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.9998750156230471,
                    0.8394904458598726,
                    0.9990700146140561,
                    0.33653846153846156,
                    0.9537037037037037,
                    1.0,
                    1.0,
                    0.2132936507936508
                ],
                [
                    0.0,
                    0.0,
                    0.0,
                    0.2916666666666667,
                    0.018518518518518517,
                    0.0,
                    0.0,
                    0.6453373015873016
                ],
                [
                    0.00012498437695288088,
                    0.16050955414012738,
                    0.0009299853859439352,
                    0.3717948717948718,
                    0.027777777777777776,
                    0.0,
                    0.0,
                    0.14136904761904764
                ]
            ],
            "fraction_answers": {
                "trivia show hosts": 0.11944031084656084,
                "roommates": 0.792746411516599,
                "panda bears": 0.08781327763684019
            },
            "question": "The '70s sitcom \u201cThree's Company\u201d was about three people who were what?",
            "rate_limited": false,
            "answers": [
                "roommates",
                "trivia show hosts",
                "panda bears"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "trivia show hosts": 0.31157143216045696,
                "roommates": 0.4420214376981768,
                "panda bears": 0.29035060350393144
            },
            "z-best_answer_by_ml": [
                "roommates"
            ],
            "data": {
                "result_count_important_words": [
                    65900.0,
                    0,
                    12600.0
                ],
                "wikipedia_search": [
                    1.3461538461538463,
                    1.1666666666666667,
                    1.4871794871794872
                ],
                "result_count_noun_chunks": [
                    75200.0,
                    0,
                    70.0
                ],
                "word_relation_to_question": [
                    0.6398809523809523,
                    1.9360119047619047,
                    0.42410714285714285
                ],
                "word_count_noun_chunks": [
                    1.0,
                    0.0,
                    0.0
                ],
                "word_count_raw": [
                    5.0,
                    0.0,
                    0.0
                ],
                "result_count": [
                    184000.0,
                    0,
                    23.0
                ],
                "word_count_appended": [
                    206.0,
                    4.0,
                    6.0
                ]
            },
            "integer_answers": {
                "trivia show hosts": 1,
                "roommates": 6,
                "panda bears": 1
            }
        },
        "lines": [
            [
                1,
                0.9998750156230471,
                0.8394904458598726,
                0.9990700146140561,
                0.33653846153846156,
                0.9537037037037037,
                1.0,
                1.0,
                0.2132936507936508
            ],
            [
                0,
                0.0,
                0.0,
                0.0,
                0.2916666666666667,
                0.018518518518518517,
                0.0,
                0.0,
                0.6453373015873016
            ],
            [
                0,
                0.00012498437695288088,
                0.16050955414012738,
                0.0009299853859439352,
                0.3717948717948718,
                0.027777777777777776,
                0.0,
                0.0,
                0.14136904761904764
            ]
        ]
    },
    "What was the first popular home video game?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.25247593806667595,
                    0.012669962917181705,
                    0.017952228814848623,
                    0.0763193161706173,
                    0.1397712833545108,
                    0.0,
                    0.0,
                    0.30589099625376287
                ],
                [
                    0.7267401311201004,
                    0.9682735887927483,
                    0.9546630153658907,
                    0.5660520195836181,
                    0.7331639135959339,
                    1.0,
                    1.0,
                    0.3604392849858268
                ],
                [
                    0.020783930813223602,
                    0.019056448290070045,
                    0.02738475581926061,
                    0.35762866424576456,
                    0.12706480304955528,
                    0.0,
                    0.0,
                    0.3336697187604104
                ]
            ],
            "fraction_answers": {
                "tekken 2": 0.10063496569719965,
                "pong": 0.7886664941805148,
                "half-life 3": 0.11069854012228555
            },
            "question": "What was the first popular home video game?",
            "rate_limited": false,
            "answers": [
                "tekken 2",
                "pong",
                "half-life 3"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "tekken 2": 0.32559523786462485,
                "pong": 0.34393939425547915,
                "half-life 3": 0.32559523786462485
            },
            "integer_answers": {
                "tekken 2": 0,
                "pong": 8,
                "half-life 3": 0
            },
            "data": {
                "result_count_important_words": [
                    123000.0,
                    9400000.0,
                    185000.0
                ],
                "wikipedia_search": [
                    0.38159658085308645,
                    2.83026009791809,
                    1.788143321228823
                ],
                "result_count_noun_chunks": [
                    94400.0,
                    5020000.0,
                    144000.0
                ],
                "word_relation_to_question": [
                    1.5294549812688143,
                    1.8021964249291338,
                    1.6683485938020521
                ],
                "word_count_noun_chunks": [
                    0.0,
                    26.0,
                    0.0
                ],
                "word_count_raw": [
                    0.0,
                    26.0,
                    0.0
                ],
                "result_count": [
                    1810000.0,
                    5210000.0,
                    149000.0
                ],
                "word_count_appended": [
                    110.0,
                    577.0,
                    100.0
                ]
            },
            "z-best_answer_by_ml": [
                "pong"
            ]
        },
        "lines": [
            [
                0,
                0.25247593806667595,
                0.012669962917181705,
                0.017952228814848623,
                0.0763193161706173,
                0.1397712833545108,
                0.0,
                0.0,
                0.30589099625376287
            ],
            [
                1,
                0.7267401311201004,
                0.9682735887927483,
                0.9546630153658907,
                0.5660520195836181,
                0.7331639135959339,
                1.0,
                1.0,
                0.3604392849858268
            ],
            [
                0,
                0.020783930813223602,
                0.019056448290070045,
                0.02738475581926061,
                0.35762866424576456,
                0.12706480304955528,
                0.0,
                0.0,
                0.3336697187604104
            ]
        ]
    },
    "Featuring 20 scoops of ice cream, the Vermonster is found on what chain's menu?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.03450754852624011,
                    0.01187168476888103,
                    0.006537282941777324,
                    0.15921843066037847,
                    0.7291666666666666,
                    0.6666666666666666,
                    0.5,
                    0.6421371661366827
                ],
                [
                    0.030913012221423435,
                    0.010608739580702197,
                    0.010827374872318692,
                    0.36529698691253404,
                    0.2708333333333333,
                    0.3333333333333333,
                    0.5,
                    0.21963833116483
                ],
                [
                    0.9345794392523364,
                    0.9775195756504168,
                    0.982635342185904,
                    0.47548458242708747,
                    0.0,
                    0.0,
                    0.0,
                    0.13822450269848738
                ]
            ],
            "fraction_answers": {
                "ben & jerry's": 0.438555430276779,
                "dairy queen": 0.21768138892730937,
                "baskin-robbins": 0.3437631807959116
            },
            "question": "Featuring 20 scoops of ice cream, the Vermonster is found on what chain's menu?",
            "rate_limited": false,
            "answers": [
                "baskin-robbins",
                "dairy queen",
                "ben & jerry's"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "ben & jerry's": 0.26516270432121314,
                "dairy queen": 0.2799243327124689,
                "baskin-robbins": 0.4420214376981768
            },
            "z-best_answer_by_ml": [
                "baskin-robbins"
            ],
            "data": {
                "result_count_important_words": [
                    47.0,
                    42.0,
                    3870.0
                ],
                "wikipedia_search": [
                    1.2737474452830277,
                    2.9223758953002723,
                    3.8038766594166997
                ],
                "result_count_noun_chunks": [
                    32.0,
                    53.0,
                    4810.0
                ],
                "word_relation_to_question": [
                    5.137097329093462,
                    1.75710664931864,
                    1.105796021587899
                ],
                "word_count_noun_chunks": [
                    2.0,
                    1.0,
                    0.0
                ],
                "word_count_raw": [
                    1.0,
                    1.0,
                    0.0
                ],
                "result_count": [
                    48.0,
                    43.0,
                    1300.0
                ],
                "word_count_appended": [
                    70.0,
                    26.0,
                    0.0
                ]
            },
            "integer_answers": {
                "ben & jerry's": 4,
                "dairy queen": 0,
                "baskin-robbins": 4
            }
        },
        "lines": [
            [
                0,
                0.03450754852624011,
                0.01187168476888103,
                0.006537282941777324,
                0.15921843066037847,
                0.7291666666666666,
                0.6666666666666666,
                0.5,
                0.6421371661366827
            ],
            [
                0,
                0.030913012221423435,
                0.010608739580702197,
                0.010827374872318692,
                0.36529698691253404,
                0.2708333333333333,
                0.3333333333333333,
                0.5,
                0.21963833116483
            ],
            [
                1,
                0.9345794392523364,
                0.9775195756504168,
                0.982635342185904,
                0.47548458242708747,
                0.0,
                0.0,
                0.0,
                0.13822450269848738
            ]
        ]
    },
    "The U.S. has never had a Miss America from what state?": {
        "raw_data": {
            "negative_question": false,
            "lines": [
                [
                    0.4099526066350711,
                    0.39109697933227344,
                    0.33710407239819007,
                    0.36239015231934196,
                    0.13424124513618677,
                    0.0,
                    0.10344827586206896,
                    0.2776521071261604
                ],
                [
                    0.20853080568720378,
                    0.23529411764705882,
                    0.22171945701357465,
                    0.41010432147332226,
                    0.2607003891050584,
                    1.0,
                    0.5517241379310345,
                    0.3755344235007629
                ],
                [
                    0.3815165876777251,
                    0.37360890302066774,
                    0.4411764705882353,
                    0.22750552620733577,
                    0.6050583657587548,
                    0.0,
                    0.3448275862068966,
                    0.34681346937307667
                ]
            ],
            "fraction_answers": {
                "nebraska": 0.34006336360408645,
                "new mexico": 0.25198567985116155,
                "north dakota": 0.40795095654475194
            },
            "question": "The U.S. has never had a Miss America from what state?",
            "rate_limited": false,
            "answers": [
                "new mexico",
                "north dakota",
                "nebraska"
            ],
            "columns_in_order": [
                "result_count",
                "result_count_important_words",
                "result_count_noun_chunks",
                "wikipedia_search",
                "word_count_appended",
                "word_count_noun_chunks",
                "word_count_raw",
                "word_relation_to_question"
            ],
            "ml_answers": {
                "nebraska": 0.3635484425931417,
                "new mexico": 0.27595726218627614,
                "north dakota": 0.3796181564740296
            },
            "z-best_answer_by_ml": [
                "north dakota"
            ],
            "data": {
                "result_count_important_words": [
                    2460000.0,
                    1480000.0,
                    2350000.0
                ],
                "wikipedia_search": [
                    1.0871704569580258,
                    1.2303129644199668,
                    0.6825165786220073
                ],
                "result_count_noun_chunks": [
                    4470000.0,
                    2940000.0,
                    5850000.0
                ],
                "word_relation_to_question": [
                    1.1106084285046416,
                    1.5021376940030515,
                    1.3872538774923067
                ],
                "word_count_noun_chunks": [
                    0.0,
                    45.0,
                    0.0
                ],
                "word_count_raw": [
                    3.0,
                    16.0,
                    10.0
                ],
                "result_count": [
                    1730000.0,
                    880000.0,
                    1610000.0
                ],
                "word_count_appended": [
                    69.0,
                    134.0,
                    311.0
                ]
            },
            "integer_answers": {
                "nebraska": 2,
                "new mexico": 2,
                "north dakota": 4
            }
        },
        "lines": [
            [
                1,
                0.4099526066350711,
                0.39109697933227344,
                0.33710407239819007,
                0.36239015231934196,
                0.13424124513618677,
                0.0,
                0.10344827586206896,
                0.2776521071261604
            ],
            [
                0,
                0.20853080568720378,
                0.23529411764705882,
                0.22171945701357465,
                0.41010432147332226,
                0.2607003891050584,
                1.0,
                0.5517241379310345,
                0.3755344235007629
            ],
            [
                0,
                0.3815165876777251,
                0.37360890302066774,
                0.4411764705882353,
                0.22750552620733577,
                0.6050583657587548,
                0.0,
                0.3448275862068966,
                0.34681346937307667
            ]
        ]
    }
}